# app.py (The new, unified, single-page application)
# ==============================================================================
# LIBRARIES & IMPORTS (All imports are here)
# ==============================================================================
import streamlit as st
import numpy as np
import pandas as pd
import io
import contextlib
import os
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
import math
from plotly.subplots import make_subplots
from scipy import stats
from scipy.stats import beta, norm, t, f, f_oneway
from scipy.optimize import curve_fit
import statsmodels.api as sm
from numba import jit
from statsmodels.formula.api import ols
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from statsmodels.tsa.arima.model import ARIMA
# --- NEW IMPORTS FOR FORECASTING SUITE ---
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.api import ETSModel
from tbats import TBATS
# --- END NEW IMPORTS ---
from prophet import Prophet
from sklearn.metrics import silhouette_score 
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.cross_decomposition import PLSRegression
from sklearn.cluster import KMeans
from sklearn.metrics import roc_curve, auc, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.inspection import PartialDependenceDisplay
from lifelines.statistics import logrank_test
from lifelines import KaplanMeierFitter
from PIL import Image
import shap
import xgboost as xgb
from scipy.special import logsumexp
from scipy.stats import mannwhitneyu

import shutil
from whoosh.index import create_in, open_dir
from whoosh.fields import Schema, TEXT, ID
from whoosh.qparser import MultifieldParser, OrGroup, WildcardPlugin, FuzzyTermPlugin
from whoosh.highlight import HtmlFormatter

from fpdf import FPDF
import base64
# ==============================================================================
# APP CONFIGURATION
# ==============================================================================
st.set_page_config(
    layout="wide",
    page_title="Biotech V&V Analytics Toolkit",
    page_icon="üî¨"
)

# --- ADD THESE COLOR CONSTANTS BENEATH YOUR CONFIG ---
PRIMARY_COLOR = "#0068C9"
DARK_GREY = "#333333"
SUCCESS_GREEN = "#2ca02c"

# ==============================================================================
# CSS STYLES
# ==============================================================================
st.markdown("""
<style>
    body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
        color: #333;
    }
    .main .block-container {
        padding: 2rem 5rem;
        max-width: 1600px;
    }
    .stTabs [data-baseweb="tab-list"] { gap: 2px; }
    .stTabs [data-baseweb="tab"] {
        height: 50px; background-color: #F0F2F6; border-radius: 4px 4px 0px 0px;
        padding: 0px 24px; border-bottom: 2px solid transparent; transition: all 0.3s;
    }
    .stTabs [aria-selected="true"] {
        background-color: #FFFFFF; font-weight: 600; border-bottom: 2px solid #0068C9;
    }
    [data-testid="stMetric"] {
        background-color: #FFFFFF; border: 1px solid #E0E0E0;
        box-shadow: 0 1px 3px rgba(0,0,0,0.04); padding: 15px 20px; border-radius: 8px;
    }
</style>
""", unsafe_allow_html=True)

# ==============================================================================
# ALL HELPER & PLOTTING FUNCTIONS
# ==============================================================================
# SNIPPET 2: Add these two new functions to the "ALL HELPER & PLOTTING FUNCTIONS" section.

def build_search_corpus():
    """
    Creates a structured list of all text content in the app to be indexed for search.
    This is a simplified representation of the app's content.
    """
    corpus = [
        # Introduction
        {'title': 'Introduction', 'path': 'Project Framework > Introduction', 'content': 'Welcome to V&V Sentinel, your guide through the complex landscape of Verification and Validation. This application is an interactive, educational journey for scientists, engineers, and quality professionals. It covers planning, characterization, qualification, and lifecycle management.'},
        {'title': 'V-Model', 'path': 'Project Framework > V-Model', 'content': 'The Verification & Validation (V&V) Model provides a structured framework for ensuring a system meets its intended purpose, from initial user requirements (URS) to final validation. Verification asks Are we building the system right? Validation asks Are we building the right system?'},
        # Act 0
        {'title': 'TPP & CQA Cascade', 'path': 'TPP & CQA Cascade > Business Case', 'content': 'The TPP/CQA/CPP Cascade is a formal contract between all stakeholders (R&D, Manufacturing, Quality). It translates the high-level business need (Target Product Profile) into specific, measurable, technical targets (Critical Quality Attributes) and the process controls required to hit them (Critical Process Parameters). This prevents late-stage failures and accelerates time-to-market.'},
        {'title': 'Analytical Target Profile (ATP)', 'path': 'Analytical Target Profile (ATP) Builder > Business Case', 'content': 'The ATP is a formal, negotiated Service Level Agreement (SLA) between the method developer (AD) and the end user (QC). It bridges the chasm between R&D and QC by quantitatively defining the contract for a new methods performance, preventing transfer failures.'},
        {'title': 'Quality Risk Management (QRM)', 'path': 'Quality Risk Management (QRM) Suite > Business Case', 'content': 'QRM tools like FMEA and FTA are not paperwork exercises; they are a systematic, cross-functional hunt for failure. They transform validation from a generic checklist into a targeted, intelligent, and cost-effective strategy focused on the highest-risk areas.'},
        # ... Add similar concise summaries for every tool's business case and key insights ...
        # NOTE: For a real implementation, you would programmatically extract this text.
        # For this app, we will use representative summaries.
        {'title': 'Causal ML / Double ML', 'path': 'Causal ML / Double ML > Key Insights', 'content': 'Predictive power is not the same as causal understanding. A standard ML model is a correlation engine. A Causal ML model is a sophisticated causal inference engine. For Root Cause Analysis and process optimization, you must use the right tool for the job. Double Machine Learning DML is the right tool.'},
        {'title': 'Lean Manufacturing', 'path': 'Lean Manufacturing & VSM > Core Concepts', 'content': 'The 3 Ms of Lean are Muda (waste), Muri (overburden), and Mura (unevenness). Waste is the symptom of deeper problems. Key tools include VSM, 5S for workplace organization, Kaizen for continuous improvement, and Kanban for creating a pull system.'},
        {'title': 'Digital Twin', 'path': 'Digital Twin & Real-Time Simulation > Business Case', 'content': 'A Digital Twin is a high-fidelity, real-time "flight simulator" for your process. It accelerates process optimization, reduces the risk of change, and provides a powerful platform for operator training. It is the foundational technology for Pharma 4.0.'}
    ]
    # In a real-world app, you would have an entry for each tab of each tool.
    return corpus

@st.cache_resource
def create_search_index():
    """
    Creates a Whoosh search index from the app's text corpus.
    Uses @st.cache_resource to build the index only once per session.
    """
    schema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT(stored=True))
    index_dir = "whoosh_index"
    
    # Clean up old index directory if it exists
    if os.path.exists(index_dir):
        shutil.rmtree(index_dir)
    os.mkdir(index_dir)
    
    ix = create_in(index_dir, schema)
    writer = ix.writer()
    
    corpus = build_search_corpus()
    for doc in corpus:
        writer.add_document(title=doc['title'], path=doc['path'], content=doc['content'])
    writer.commit()
    return ix

@contextlib.contextmanager
def suppress_stdout():
    """A context manager to temporarily redirect stdout."""
    with open(os.devnull, 'w') as fnull:
        saved_stdout = sys.stdout
        sys.stdout = fnull
        try:
            yield
        finally:
            sys.stdout = saved_stdout


@st.cache_data
def plot_v_model():
    """
    Generates a high-quality, static V-Model diagram with generic biotech examples.
    """
    fig = go.Figure()
    
    # --- CORRECTED: Use the consistent BASE_STAGES dictionary for a clean, static plot ---
    # This ensures the main diagram is simple and generic, while the table holds the details.
    v_model_stages = {
        'URS': {'name': 'User Requirements', 'icon': 'üéØ', 'x': 0, 'y': 5, 'question': 'What does the user/business need?', 'tools': 'Business Case, Target Product Profile'},
        'FS':  {'name': 'Functional Specs', 'icon': '‚öôÔ∏è', 'x': 1, 'y': 4, 'question': 'What must the system *do*?', 'tools': 'Assay Specs, Throughput Goals, User Roles'},
        'DS':  {'name': 'Design Specs', 'icon': '‚úçÔ∏è', 'x': 2, 'y': 3, 'question': 'How will it be built/configured?', 'tools': 'DOE, Component Selection, Architecture'},
        'BUILD': {'name': 'Implementation', 'icon': 'üõ†Ô∏è', 'x': 3, 'y': 2, 'question': 'Build, Code, Write SOPs, Train', 'tools': 'Physical Transfer, Coding, Training'},
        'IQOQ':{'name': 'IQ / OQ', 'icon': 'üîå', 'x': 4, 'y': 3, 'question': 'Is it installed and operating correctly?', 'tools': 'Calibration, Unit & Integration Tests'},
        'PQ':  {'name': 'Performance Qualification', 'icon': 'üìà', 'x': 5, 'y': 4, 'question': 'Does it perform reliably in its environment?', 'tools': 'Gage R&R, Method Comp, PPQ Runs'},
        'VAL': {'name': 'Final Validation', 'icon': '‚úÖ', 'x': 6, 'y': 5, 'question': 'Does it meet the original user need?', 'tools': 'Validation Report, UAT, Final Release'}
    }
    # --- END OF CORRECTION ---
    
    verification_color = '#008080'
    validation_color = '#0068C9'
    path_keys = ['URS', 'FS', 'DS', 'BUILD', 'IQOQ', 'PQ', 'VAL']
    path_x = [v_model_stages[p]['x'] for p in path_keys]
    path_y = [v_model_stages[p]['y'] for p in path_keys]
    
    fig.add_trace(go.Scatter(x=path_x, y=path_y, mode='lines', line=dict(color='lightgrey', width=5), hoverinfo='none'))

    for i in range(3):
        start_key, end_key = path_keys[i], path_keys[-(i+1)]
        fig.add_shape(type="line", x0=v_model_stages[start_key]['x'], y0=v_model_stages[start_key]['y'],
                      x1=v_model_stages[end_key]['x'], y1=v_model_stages[end_key]['y'],
                      line=dict(color="darkgrey", width=2, dash="dot"))

    for i, (key, stage) in enumerate(v_model_stages.items()):
        color = verification_color if i < 3 else validation_color if i > 3 else '#636EFA'
        fig.add_shape(type="rect", x0=stage['x']-0.5, y0=stage['y']-0.4,
                      x1=stage['x']+0.5, y1=stage['y']+0.4,
                      line=dict(color="black", width=2), fillcolor=color, layer='above')
        fig.add_annotation(x=stage['x'], y=stage['y']+0.15, text=f"{stage['icon']} <b>{stage['name']}</b>",
                           showarrow=False, font=dict(color='white', size=12), align='center')
        hover_text = (f"<b>{stage['icon']} {stage['name']}</b><br><br>"
                      f"<i>{stage['question']}</i><br><br>"
                      f"<b>Generic Examples:</b><br>{stage.get('tools', 'N/A')}")
        fig.add_trace(go.Scatter(
            x=[stage['x']], y=[stage['y']], mode='markers',
            marker=dict(color='rgba(0,0,0,0)', size=100),
            hoverinfo='text', text=hover_text,
            hoverlabel=dict(bgcolor="white", font_size=14, font_family="Arial", bordercolor="black")
        ))
        
    fig.add_annotation(x=1, y=5.2, text="<span style='color:#008080; font-size: 20px;'><b>VERIFICATION</b></span><br><span style='font-size: 12px;'>(Are we building the system right?)</span>",
                       showarrow=False, align='center')
    fig.add_annotation(x=5, y=5.2, text="<span style='color:#0068C9; font-size: 20px;'><b>VALIDATION</b></span><br><span style='font-size: 12px;'>(Are we building the right system?)</span>",
                       showarrow=False, align='center')

    fig.update_layout(
        title_text="<b>The V-Model for Technology Transfer (Hover for Details)</b>",
        title_x=0.5, showlegend=False,
        xaxis=dict(visible=False, range=[-0.7, 6.7]), yaxis=dict(visible=False, range=[1.5, 6.0]),
        height=650, margin=dict(l=20, r=20, t=80, b=20),
        plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)'
    )
    
    return fig
    
@st.cache_data
def create_v_model_summary_table():
    """
    Creates a pandas DataFrame summarizing the V-Model contexts for display as a table.
    """
    # UPDATED: Added IVD and Pharma, and made other column names more specific.
    all_contexts = {
        'R&D Assay Method': {
            'URS': {'tools': 'Target Product Profile (TPP), Required sensitivity/specificity'},
            'FS':  {'tools': 'Assay type (e.g., ELISA, HPLC), Linearity, LOD/LOQ goals'},
            'DS':  {'tools': 'Reagent selection, SOP drafting, Robustness DOE plan'},
            'BUILD': {'tools': 'Method development experiments, SOP finalization, Analyst training'},
            'IQOQ':{'tools': 'Reagent qualification, Instrument calibration for this assay'},
            'PQ':  {'tools': 'Intermediate Precision, Gage R&R, Method Comparison'},
            'VAL': {'tools': 'Final Validation Report, Control charting plan'}
        },
        'Instrument': {
            'URS': {'tools': 'Required throughput, Sample types, User skill level'},
            'FS':  {'tools': 'Automation level, Data output format (LIMS), Footprint'},
            'DS':  {'tools': 'Vendor selection, Site prep requirements, Service contract'},
            'BUILD': {'tools': 'Purchase, Delivery, Physical installation'},
            'IQOQ':{'tools': 'Utility connections check (IQ), Factory tests run (OQ)'},
            'PQ':  {'tools': 'Performance on representative samples, Throughput testing'},
            'VAL': {'tools': 'System meets all URS criteria, Final release for GMP use'}
        },
        'Software System': {
            'URS': {'tools': 'Business process map, 21 CFR Part 11 requirements'},
            'FS':  {'tools': 'User roles, Required calculations, Audit trail specifications'},
            'DS':  {'tools': 'System architecture, Database schema, UI mockups'},
            'BUILD': {'tools': 'Coding, Configuration of COTS system, Writing user manuals'},
            'IQOQ':{'tools': 'Server setup validation (IQ), Unit & Integration testing (OQ)'},
            'PQ':  {'tools': 'User Acceptance Testing (UAT) with real-world scenarios'},
            'VAL': {'tools': 'System Validation Report, Release notes, Go-live approval'}
        },
        'IVD Development': {
            'URS': {'tools': 'Diagnostic claim, Clinical sensitivity/specificity reqs, Sample matrix'},
            'FS':  {'tools': 'Assay principle (e.g., PCR), Analytical performance goals (LOD, precision)'},
            'DS':  {'tools': 'Antibody/reagent selection, Protocol optimization, Verification/Validation plan'},
            'BUILD': {'tools': 'Prototype kit assembly, Reagent formulation, SOP drafting'},
            'IQOQ':{'tools': 'Manufacturing equipment qualification, QC test method validation'},
            'PQ':  {'tools': 'Clinical performance studies, Stability (shelf-life), Shipping validation'},
            'VAL': {'tools': '510(k)/PMA submission to FDA, Design History File (DHF) finalization'}
        },
        'Pharma Process': {
            'URS': {'tools': 'Target Product Profile (TPP), Critical Quality Attributes (CQAs), Cost of goods'},
            'FS':  {'tools': 'Unit operations (e.g., cell culture), In-Process Controls (IPCs)'},
            'DS':  {'tools': 'Critical Process Parameters (CPPs), Bill of Materials, Scale-down model design'},
            'BUILD': {'tools': 'Engineering runs, Master Batch Record (MBR) authoring'},
            'IQOQ':{'tools': 'Facility/utility qualification, Equipment commissioning'},
            'PQ':  {'tools': 'Process Performance Qualification (PPQ) runs, Cpk analysis'},
            'VAL': {'tools': 'Final PPQ report, Submission to regulatory agency'}
        }
    }
    
    df_data = {}
    for context, stages in all_contexts.items():
        df_data[context] = {stage: data['tools'] for stage, data in stages.items()}

    # Ensure a consistent column order
    column_order = ['R&D Assay Method', 'Instrument', 'Software System', 'IVD Development', 'Pharma Process']
    df = pd.DataFrame(df_data)[column_order]
    
    stage_order = ['URS', 'FS', 'DS', 'BUILD', 'IQOQ', 'PQ', 'VAL']
    stage_names = {
        'URS': 'User Requirements', 'FS': 'Functional Specs', 'DS': 'Design Specs',
        'BUILD': 'Implementation', 'IQOQ': 'IQ / OQ', 'PQ': 'Performance Qualification',
        'VAL': 'Final Validation'
    }
    
    df = df.reindex(stage_order)
    df.index = df.index.map(lambda key: f"{stage_names[key]} ({key})")
    df.index.name = "V-Model Stage"
    
    return df

@st.cache_data
def create_styled_v_model_table(df):
    """
    Transforms the V-Model summary DataFrame into a styled Plotly Table.
    """
    # UPDATED: Added colors for IVD and Pharma, and renamed other keys to match the new DataFrame.
    app_colors = {
        'R&D Assay Method': {'header': '#0068C9', 'cell': 'rgba(0, 104, 201, 0.1)'},
        'Instrument': {'header': '#008080', 'cell': 'rgba(0, 128, 128, 0.1)'},
        'Software System': {'header': '#636EFA', 'cell': 'rgba(99, 110, 250, 0.1)'},
        'IVD Development': {'header': '#2ca02c', 'cell': 'rgba(44, 160, 44, 0.1)'}, # New color for IVD
        'Pharma Process': {'header': '#FF7F0E', 'cell': 'rgba(255, 127, 14, 0.1)'}
    }
    
    # Prepare header values and colors
    header_values = [f"<b>{df.index.name}</b>"] + [f"<b>{col}</b>" for col in df.columns]
    header_fill_colors = ['#F0F2F6'] + [app_colors[col]['header'] for col in df.columns]
    header_font_colors = ['black'] + ['white'] * len(df.columns)
    
    # Prepare cell values and colors, transposing the dataframe
    cell_values = [df.index.tolist()] + [df[col].tolist() for col in df.columns]
    cell_fill_colors = [['#F8F9FA'] * len(df)] + [[app_colors[col]['cell']] * len(df) for col in df.columns]

    fig = go.Figure(data=[go.Table(
        # Increase column widths to better accommodate the new content
        columnwidth = [120] + [200]*len(df.columns),
        header=dict(
            values=header_values,
            fill_color=header_fill_colors,
            font=dict(color=header_font_colors, size=14),
            align=['left', 'center'],
            height=40,
            line_color='darkslategray'
        ),
        cells=dict(
            values=cell_values,
            fill_color=cell_fill_colors,
            align=['left', 'left'],
            font_size=12,
            height=30,
            line_color='darkslategray'
        ))
    ])

    fig.update_layout(
        margin=dict(l=10, r=10, t=10, b=10),
        # Increase height slightly for better viewing
        height=500
    )
    
    return fig
    
@st.cache_data
def plot_act_grouped_timeline():
    """Generates the project-based timeline with all tools, including all added modules."""
    all_tools_data = [
        # --- ACT 0: PLANNING & STRATEGY ---
        {'name': 'TPP & CQA Cascade', 'act': 0, 'year': 2009, 'inventor': 'ICH Q8', 'desc': 'Defines the "golden thread" of Quality by Design.'},
        {'name': 'Analytical Target Profile', 'act': 0, 'year': 2012, 'inventor': 'FDA/AAPS', 'desc': 'Creates the "contract" for a new analytical method or system.'},
        {'name': 'Quality Risk Management (QRM) Suite', 'act': 0, 'year': 1949, 'inventor': 'US Military', 'desc': 'Proactively identifies and mitigates process risks (FMEA, FTA, etc.).'},
        {'name': 'V&V Strategy & Justification', 'act': 0, 'year': 1990, 'inventor': 'FDA/Industry', 'desc': 'The foundational "why" behind all validation activities.'},
        {'name': 'Design Controls & DHF', 'act': 0, 'year': 1990, 'inventor': 'FDA (SMDA)', 'desc': 'The formal, auditable framework for medical device and complex system development.'},
        {'name': 'FAT & SAT', 'act': 0, 'year': 1970, 'inventor': 'Good Engineering Practice', 'desc': 'The "Trust, but Verify" framework for accepting new equipment and systems.'},
        {'name': 'Design for Excellence (DfX)', 'act': 0, 'year': 1980, 'inventor': 'Concurrent Engineering', 'desc': 'Optimizing design for the entire product lifecycle.'},
        {'name': 'Validation Master Plan', 'act': 0, 'year': 1990, 'inventor': 'GAMP Forum', 'desc': 'The master project plan for any validation effort.'},
        {'name': 'Requirements Traceability Matrix', 'act': 0, 'year': 1980, 'inventor': 'Systems Engineering', 'desc': 'Ensures all requirements are built and tested.'},
        {'name': 'Gap Analysis & Change Control', 'act': 0, 'year': 1980, 'inventor': 'ITSM/QMS', 'desc': 'The formal framework for managing the evolution of a validated system.'},
        {'name': 'Root Cause Analysis (RCA)', 'act': 0, 'year': 1930, 'inventor': 'Toyota/Ishikawa', 'desc': 'The forensic toolkit for moving from symptom to the true underlying cause.'},

        # --- ACT I: FOUNDATION & CHARACTERIZATION ---
        {'name': 'Exploratory Data Analysis (EDA)', 'act': 1, 'year': 1977, 'inventor': 'John Tukey', 'desc': 'The critical first step of understanding any dataset.'},
        {'name': 'Confidence Interval Concept', 'act': 1, 'year': 1937, 'inventor': 'Jerzy Neyman', 'desc': 'Neyman formalizes the frequentist confidence interval.'},
        {'name': 'Confidence Intervals for Proportions', 'act': 1, 'year': 1927, 'inventor': 'Edwin B. Wilson', 'desc': 'Wilson develops a superior confidence interval for pass/fail data.'},
        {'name': 'Core Validation Parameters', 'act': 1, 'year': 1980, 'inventor': 'ICH / FDA', 'desc': 'Accuracy, Precision, Specificity codified.'},
        {'name': 'LOD & LOQ', 'act': 1, 'year': 1968, 'inventor': 'Lloyd Currie (NIST)', 'desc': 'Currie at NIST formalizes the statistical basis.'},
        {'name': 'Linearity & Range', 'act': 1, 'year': 1805, 'inventor': 'Legendre/Gauss', 'desc': 'Using linear regression to verify proportionality.'},
        {'name': 'Non-Linear Regression (4PL/5PL)', 'act': 1, 'year': 1975, 'inventor': 'Bioassay Field', 'desc': 'Models for sigmoidal curves common in immunoassays.'},
        {'name': 'Gage R&R / VCA', 'act': 1, 'year': 1982, 'inventor': 'AIAG', 'desc': 'AIAG codifies Measurement Systems Analysis (MSA).'},
        {'name': 'Attribute Agreement Analysis', 'act': 1, 'year': 1960, 'inventor': 'Cohen/Fleiss', 'desc': 'Validating human inspectors for pass/fail decisions.'},
        {'name': 'Comprehensive Diagnostic Validation', 'act': 1, 'year': 1950, 'inventor': 'Multi-Disciplinary', 'desc': 'A full suite of metrics for validating IVDs.'},
        {'name': 'Component Reliability Testing', 'act': 1, 'year': 1951, 'inventor': 'Waloddi Weibull', 'desc': 'Using life data to model failure rates and predict component lifetime.'},
        {'name': 'ROC Curve Analysis', 'act': 1, 'year': 1945, 'inventor': 'Signal Processing Labs', 'desc': 'Developed for radar, now the standard for diagnostic tests.'},
        {'name': 'Assay Robustness (DOE)', 'act': 1, 'year': 1926, 'inventor': 'R.A. Fisher', 'desc': 'Fisher publishes his work on Design of Experiments.'},
        {'name': 'Mixture Design (Formulations)', 'act': 1, 'year': 1958, 'inventor': 'Henry Scheff√©', 'desc': 'Specialized DOE for optimizing formulations and blends.'},
        {'name': 'Process Optimization: From DOE to AI', 'act': 1, 'year': 2017, 'inventor': 'Modern Synthesis', 'desc': 'Combining classic DOE with modern ML for deep optimization.'},
        {'name': 'Bayesian Optimization', 'act': 1, 'year': 1998, 'inventor': 'Jones et al.', 'desc': 'Intelligent, sequential search for optimizing expensive "black box" processes.'},
        {'name': 'Split-Plot Designs', 'act': 1, 'year': 1930, 'inventor': 'R.A. Fisher & F. Yates', 'desc': 'Specialized DOE for factors that are "hard-to-change".'},
        {'name': 'Causal Inference', 'act': 1, 'year': 1990, 'inventor': 'Judea Pearl et al.', 'desc': 'Moving beyond correlation to identify root causes.'},
        {'name': 'Causal ML / Double ML', 'act': 1, 'year': 2018, 'inventor': 'Chernozhukov et al.', 'desc': 'Using AI to find true causal effects in complex observational data.'},

        # --- ACT II: TRANSFER & STABILITY ---
        {'name': 'Sample Size for Qualification', 'act': 2, 'year': 1940, 'inventor': 'Dodge/Romig', 'desc': 'Statistically justifying the number of samples for validation.'},
        {'name': 'Advanced Stability Design', 'act': 2, 'year': 2003, 'inventor': 'ICH Q1D', 'desc': 'Using Bracketing & Matrixing to create efficient stability studies.'},
        {'name': 'Method Comparison', 'act': 2, 'year': 1986, 'inventor': 'Bland & Altman', 'desc': 'Bland & Altman revolutionize method agreement studies.'},
        {'name': 'Equivalence Testing (TOST)', 'act': 2, 'year': 1987, 'inventor': 'Donald Schuirmann', 'desc': 'Schuirmann proposes TOST for bioequivalence.'},
        {'name': 'Wasserstein Distance', 'act': 2, 'year': 1990, 'inventor': 'CS Community', 'desc': "A robust metric for comparing the entire 'fingerprint' of two process distributions."},
        {'name': 'Statistical Equivalence for Process Transfer', 'act': 2, 'year': 1990, 'inventor': 'Modern Synthesis', 'desc': 'Proving two processes perform equivalently after a transfer.'},
        {'name': 'Process Stability (SPC)', 'act': 2, 'year': 1924, 'inventor': 'Walter Shewhart', 'desc': 'Shewhart invents the control chart at Bell Labs.'},
        {'name': 'Process Capability (Cpk)', 'act': 2, 'year': 1986, 'inventor': 'Bill Smith (Motorola)', 'desc': 'Motorola popularizes Cpk with the Six Sigma initiative.'},
        {'name': 'Production Line Sync (ODE)', 'act': 2, 'year': 1984, 'inventor': 'TOC/Goldratt', 'desc': 'Modeling process flow to identify bottlenecks and optimize throughput.'},
        {'name': 'Lean Manufacturing & VSM', 'act': 2, 'year': 1990, 'inventor': 'TPS/Womack', 'desc': 'A toolkit for maximizing value by systematically eliminating waste from a process.'},
        {'name': 'Monte Carlo Simulation for Risk Analysis', 'act': 2, 'year': 1940, 'inventor': 'Ulam/Von Neumann', 'desc': 'A "virtual factory" to simulate thousands of future runs and predict failure rates.'},
        {'name': 'First Time Yield & Cost of Quality', 'act': 2, 'year': 1980, 'inventor': 'Six Sigma/TQM', 'desc': 'Quantifying the business impact of process performance.'},
        {'name': 'Tolerance Intervals', 'act': 2, 'year': 1942, 'inventor': 'Abraham Wald', 'desc': 'Wald develops intervals to cover a proportion of a population.'},
        {'name': 'Bayesian Inference', 'act': 2, 'year': 1990, 'inventor': 'Metropolis et al.', 'desc': 'Computational methods (MCMC) make Bayes practical.'},

        # --- ACT III: LIFECYCLE & PREDICTIVE MGMT ---
        {'name': 'Overall Equipment Effectiveness (OEE)', 'act': 3, 'year': 1960, 'inventor': 'Seiichi Nakajima', 'desc': 'The single best metric for measuring manufacturing productivity.'},
        {'name': 'Process Control Plan Builder', 'act': 3, 'year': 1980, 'inventor': 'Automotive Industry', 'desc': 'Creating the operational playbook for process monitoring.'},
        {'name': 'Run Validation (Westgard)', 'act': 3, 'year': 1981, 'inventor': 'James Westgard', 'desc': 'Westgard publishes his multi-rule QC system.'},
        {'name': 'Small Shift Detection', 'act': 3, 'year': 1954, 'inventor': 'Page/Roberts', 'desc': 'Charts for faster detection of small process drifts.'},
        {'name': 'Multivariate SPC', 'act': 3, 'year': 1931, 'inventor': 'Harold Hotelling', 'desc': 'Hotelling develops the multivariate analog to the t-test.'},
        {'name': 'Stability Analysis (Shelf-Life)', 'act': 3, 'year': 1993, 'inventor': 'ICH', 'desc': 'ICH guidelines formalize statistical shelf-life estimation.'},
        {'name': 'Reliability / Survival Analysis', 'act': 3, 'year': 1958, 'inventor': 'Kaplan & Meier', 'desc': 'Kaplan-Meier estimator for time-to-event data.'},
        {'name': 'Time Series Forecasting Suite', 'act': 3, 'year': 1970, 'inventor': 'Box/Jenkins', 'desc': 'A suite of models (Holt-Winters, SARIMA, Prophet) for forecasting.'},
        {'name': 'Prophet Forecasting', 'act': 3, 'year': 2017, 'inventor': 'Meta', 'desc': 'An automated forecasting engine for complex business time series.'},
        {'name': 'Multivariate Analysis (MVA)', 'act': 3, 'year': 1975, 'inventor': 'Herman Wold', 'desc': 'Partial Least Squares for modeling complex process data.'},
        {'name': 'Predictive Modeling Suite', 'act': 3, 'year': 1958, 'inventor': 'David Cox et al.', 'desc': 'A suite of models (LR, RF, MLP) for predicting pass/fail outcomes.'},
        {'name': 'Explainable AI (XAI)', 'act': 3, 'year': 2017, 'inventor': 'Lundberg et al.', 'desc': 'Methods like SHAP to open the AI "black box".'},
        {'name': 'Clustering (Unsupervised)', 'act': 3, 'year': 1957, 'inventor': 'Stuart Lloyd', 'desc': 'Algorithm for finding hidden groups in data.'},
        {'name': 'Anomaly Detection', 'act': 3, 'year': 2008, 'inventor': 'Liu et al.', 'desc': 'Using Isolation Forests to find novel failures.'},
        {'name': 'Advanced AI Concepts', 'act': 3, 'year': 2017, 'inventor': 'Vaswani et al.', 'desc': 'Transformers and other advanced architectures emerge.'},
        {'name': 'MEWMA + XGBoost Diagnostics', 'act': 3, 'year': 1992, 'inventor': 'Lowry et al.', 'desc': 'Multivariate EWMA for sensitive drift detection, enhanced with modern AI for diagnosis.'},
        {'name': 'BOCPD + ML Features', 'act': 3, 'year': 2007, 'inventor': 'Adams & MacKay', 'desc': 'Probabilistic real-time detection of process changes (changepoints).'},
        {'name': 'Kalman Filter + Residual Chart', 'act': 3, 'year': 1960, 'inventor': 'Rudolf E. K√°lm√°n', 'desc': 'Optimal state estimation for dynamic systems, used for intelligent fault detection.'},
        {'name': 'RL for Chart Tuning', 'act': 3, 'year': 2005, 'inventor': 'RL Community', 'desc': 'Using AI to economically optimize control chart parameters, balancing risk and cost.'},
        {'name': 'TCN + CUSUM', 'act': 3, 'year': 2018, 'inventor': 'Bai, Kolter & Koltun', 'desc': 'Hybrid model using AI to de-seasonalize data for ultra-sensitive drift detection.'},
        {'name': 'LSTM Autoencoder + Hybrid Monitoring', 'act': 3, 'year': 1997, 'inventor': 'Hochreiter/Schmidhuber', 'desc': 'Unsupervised anomaly detection by learning a process\'s normal dynamic fingerprint.'},
        {'name': 'PSO + Autoencoder', 'act': 3, 'year': 2010, 'inventor': 'Modern Synthesis', 'desc': 'Using AI to intelligently search for hidden process failure modes.'},
        {'name': 'Digital Twin & Real-Time Simulation', 'act': 3, 'year': 1970, 'inventor': 'NASA/Grieves', 'desc': 'A live, virtual replica of a physical process for advanced monitoring and what-if analysis.'},
    ]
    all_tools_data.sort(key=lambda x: (x['act'], x['year']))
    
    act_ranges = {
        0: (-5, 30),
        1: (35, 85),
        2: (90, 115),
        3: (120, 180)
    }
    
    tools_by_act = {0: [], 1: [], 2: [], 3: []}
    for tool in all_tools_data: tools_by_act[tool['act']].append(tool)
    for act_num, tools_in_act in tools_by_act.items():
        start, end = act_ranges[act_num]
        x_coords = np.linspace(start, end, len(tools_in_act))
        for i, tool in enumerate(tools_in_act):
            tool['x'] = x_coords[i]
            
    y_offsets = [3.0, -3.0, 3.8, -3.8, 2.5, -2.5, 4.5, -4.5, 2.0, -2.0, 5.0, -5.0, 1.5, -1.5, 3.3, -3.3, 4.2, -4.2]
    for i, tool in enumerate(all_tools_data):
        tool['y'] = y_offsets[i % len(y_offsets)]
    
    fig = go.Figure()
    
    acts = {
        0: {'name': 'Act 0: Planning & Strategy', 'color': 'rgba(128, 128, 128, 0.9)', 'boundary': (-10, 33)},
        1: {'name': 'Act I: Characterization', 'color': 'rgba(0, 128, 128, 0.9)', 'boundary': (33, 88)},
        2: {'name': 'Act II: Qualification & Transfer', 'color': 'rgba(0, 104, 201, 0.9)', 'boundary': (88, 118)},
        3: {'name': 'Act III: Lifecycle Management', 'color': 'rgba(100, 0, 100, 0.9)', 'boundary': (118, 185)}
    }
    
    for act_info in acts.values():
        x0, x1 = act_info['boundary']
        fig.add_shape(type="rect", x0=x0, y0=-6.0, x1=x1, y1=6.0, line=dict(width=0), fillcolor='rgba(230, 230, 230, 0.7)', layer='below')
        fig.add_annotation(x=(x0 + x1) / 2, y=7.0, text=f"<b>{act_info['name']}</b>", showarrow=False, font=dict(size=20, color="#555"))

    fig.add_shape(type="line", x0=-5, y0=0, x1=180, y1=0, line=dict(color="black", width=3), layer='below')

    for act_num, act_info in acts.items():
        act_tools = [tool for tool in all_tools_data if tool['act'] == act_num]
        fig.add_trace(go.Scatter(x=[tool['x'] for tool in act_tools], y=[tool['y'] for tool in act_tools], mode='markers',
                                 marker=dict(size=12, color=act_info['color'], symbol='circle', line=dict(width=2, color='black')),
                                 hoverinfo='text', text=[f"<b>{tool['name']} ({tool['year']})</b><br><i>{tool['desc']}</i>" for tool in act_tools], name=act_info['name']))

    for tool in all_tools_data:
        fig.add_shape(type="line", x0=tool['x'], y0=0, x1=tool['x'], y1=tool['y'], line=dict(color='grey', width=1))
        fig.add_annotation(x=tool['x'], y=tool['y'], text=f"<b>{tool['name'].replace(': ', ':<br>')}</b><br><i>({tool['year']})</i>",
                           showarrow=False, yshift=40 if tool['y'] > 0 else -40, font=dict(size=11, color=acts[tool['act']]['color']), align="center")

    fig.update_layout(title_text='<b>The V&V Analytics Toolkit: A Project-Based View</b>', title_font_size=28, title_x=0.5,
                      xaxis=dict(visible=False), yaxis=dict(visible=False, range=[-8, 8]), plot_bgcolor='white', paper_bgcolor='white',
                      height=1000, margin=dict(l=20, r=20, t=140, b=20), showlegend=True,
                      legend=dict(title_text="<b>Project Phase</b>", title_font_size=16, font_size=14, orientation="h",
                                  yanchor="bottom", y=1.02, xanchor="center", x=0.5))
    return fig
    
# FIX: Replace the entire plot_chronological_timeline function with this new, complete version.
@st.cache_data
def plot_chronological_timeline():
    """Generates the chronological timeline, now including all tools."""
    all_tools_data = [
        # --- 'Linearity & Range' (year 1805) has been removed from this list as it's a concept, not an invention.
        {'name': 'Process Stability (SPC)', 'year': 1924, 'inventor': 'Walter Shewhart', 'reason': 'The dawn of mass manufacturing (telephones) required new methods for controlling process variation.'},
        {'name': 'Assay Robustness (DOE)', 'year': 1926, 'inventor': 'R.A. Fisher', 'reason': 'To revolutionize agricultural science by efficiently testing multiple factors (fertilizers, varieties) at once.'},
        {'name': 'Confidence Intervals for Proportions', 'year': 1927, 'inventor': 'Edwin B. Wilson', 'reason': 'To solve the poor performance of the standard binomial confidence interval, especially for small samples.'},
        {'name': 'Split-Plot Designs', 'year': 1930, 'inventor': 'R.A. Fisher & F. Yates', 'reason': 'To solve agricultural experiments with factors that were difficult or expensive to change on a small scale.'},
        {'name': 'Multivariate SPC', 'year': 1931, 'inventor': 'Harold Hotelling', 'reason': 'To generalize the t-test and control charts to monitor multiple correlated variables simultaneously.'},
        {'name': 'Confidence Interval Concept', 'year': 1937, 'inventor': 'Jerzy Neyman', 'reason': 'A need for rigorous, objective methods in the growing field of mathematical statistics.'},
        {'name': 'Sample Size for Qualification', 'year': 1940, 'inventor': 'Dodge & Romig', 'reason': 'WWII demanded a statistical basis for accepting or rejecting massive lots of military supplies.'},
        {'name': 'Tolerance Intervals', 'year': 1942, 'inventor': 'Abraham Wald', 'reason': 'WWII demanded mass production of interchangeable military parts that would fit together reliably.'},
        {'name': 'ROC Curve Analysis', 'year': 1945, 'inventor': 'Signal Processing Labs', 'reason': 'Developed during WWII to distinguish enemy radar signals from noise, a classic signal detection problem.'},
        {'name': 'Quality Risk Management (FMEA)', 'year': 1949, 'inventor': 'US Military', 'reason': 'To proactively assess and mitigate reliability risks in complex military systems.'},
        {'name': 'Comprehensive Diagnostic Validation', 'year': 1950, 'inventor': 'Multi-Disciplinary', 'reason': 'The post-war boom in epidemiology required a full suite of metrics to validate new disease screening tests.'},
        {'name': 'Component Reliability Testing', 'year': 1951, 'inventor': 'Waloddi Weibull', 'reason': 'The need to model the lifetime of variable components like ball bearings, moving beyond simple averages.'},
        {'name': 'Small Shift Detection', 'year': 1954, 'inventor': 'Page (CUSUM) & Roberts (EWMA)', 'reason': 'Maturing industries required charts more sensitive to small, slow process drifts than Shewhart\'s original design.'},
        {'name': 'Clustering (Unsupervised)', 'year': 1957, 'inventor': 'Stuart Lloyd', 'reason': 'The advent of early digital computing at Bell Labs made iterative, data-driven grouping algorithms feasible.'},
        {'name': 'Predictive QC (Classification)', 'year': 1958, 'inventor': 'David Cox', 'reason': 'A need to model binary outcomes (pass/fail, live/die) in a regression framework.'},
        {'name': 'Reliability / Survival Analysis', 'year': 1958, 'inventor': 'Kaplan & Meier', 'reason': 'The rise of clinical trials necessitated a formal way to handle \'censored\' data where the event has not yet occurred.'},
        {'name': 'Mixture Design (Formulations)', 'year': 1958, 'inventor': 'Henry Scheff√©', 'reason': 'To provide a systematic way for chemists and food scientists to optimize recipes and formulations.'},
        {'name': 'Attribute Agreement Analysis', 'year': 1960, 'inventor': 'Cohen/Fleiss', 'reason': 'Psychologists needed to measure the reliability of judgments between raters, corrected for chance agreement.'},
        {'name': 'Kalman Filter + Residual Chart', 'year': 1960, 'inventor': 'Rudolf E. K√°lm√°n', 'reason': 'The Apollo program needed a way to navigate to the moon using noisy sensor data, requiring optimal state estimation.'},
        {'name': 'Root Cause Analysis (RCA)', 'year': 1960, 'inventor': 'Kaoru Ishikawa', 'reason': 'To provide a structured, visual tool for brainstorming and categorizing the potential causes of a problem.'},
        {'name': 'LOD & LOQ', 'year': 1968, 'inventor': 'Lloyd Currie (NIST)', 'reason': 'To create a harmonized, statistically rigorous framework for defining the sensitivity of analytical methods.'},
        {'name': 'FAT & SAT', 'year': 1970, 'inventor': 'Good Engineering Practice', 'reason': 'To de-risk large capital projects by finding issues at the factory, not the final site.'},
        {'name': 'Time Series Forecasting Suite', 'year': 1970, 'inventor': 'Box & Jenkins', 'reason': 'To provide a comprehensive statistical methodology for forecasting and control in industrial and economic processes.'},
        {'name': 'Digital Twin & Real-Time Simulation', 'year': 1970, 'inventor': 'NASA (Apollo 13)', 'reason': 'To solve the Apollo 13 crisis, NASA used mirrored simulators on the ground, creating the first true digital twin.'},
        {'name': 'Multivariate Analysis (MVA)', 'year': 1975, 'inventor': 'Herman Wold', 'reason': 'To model "data-rich but theory-poor" systems in social science, later adapted for chemometrics.'},
        {'name': 'Core Validation Parameters', 'year': 1980, 'inventor': 'ICH / FDA', 'reason': 'Globalization of the pharmaceutical industry required harmonized standards for drug approval.'},
        {'name': 'Requirements Traceability Matrix', 'year': 1980, 'inventor': 'Systems Engineering', 'reason': 'To manage complexity in large-scale projects, ensuring all requirements are built and tested.'},
        {'name': 'Design for Excellence (DfX)', 'year': 1980, 'inventor': 'Concurrent Engineering', 'reason': 'The auto industry, facing a quality crisis, needed to formally integrate manufacturing and lifecycle concerns into the design phase.'},
        {'name': 'Run Validation (Westgard)', 'year': 1981, 'inventor': 'James Westgard', 'reason': 'The automation of clinical labs demanded a more sensitive, diagnostic system for daily quality control.'},
        {'name': 'Gage R&R / VCA', 'year': 1982, 'inventor': 'AIAG', 'reason': 'The US auto industry, facing a quality crisis, needed to formalize the analysis of their measurement systems.'},
        {'name': 'Method Comparison', 'year': 1986, 'inventor': 'Bland & Altman', 'reason': 'A direct response to the widespread misuse of correlation for comparing clinical measurement methods.'},
        {'name': 'Process Capability (Cpk)', 'year': 1986, 'inventor': 'Bill Smith (Motorola)', 'reason': 'The Six Sigma quality revolution at Motorola popularized a simple metric to quantify process capability.'},
        {'name': 'Equivalence Testing (TOST)', 'year': 1987, 'inventor': 'Donald Schuirmann', 'reason': 'The rise of the generic drug industry created a regulatory need to statistically *prove* equivalence.'},
        {'name': 'Design Controls & DHF', 'year': 1990, 'inventor': 'FDA (SMDA)', 'reason': 'A response to medical device failures caused by poor design, mandating a formal, auditable process.'},
        {'name': 'V&V Strategy & Justification', 'year': 1990, 'inventor': 'FDA/Industry', 'reason': 'To formalize the principles of V&V as a response to product failures and establish a clear regulatory framework.'},
        {'name': 'Validation Master Plan', 'year': 1990, 'inventor': 'GAMP Forum', 'reason': 'Increasingly complex computerized systems required a high-level strategic plan for validation.'},
        {'name': 'Bayesian Inference', 'year': 1990, 'inventor': 'Metropolis et al.', 'reason': 'The explosion in computing power made simulation-based methods (MCMC) practical, unlocking Bayesian inference.'},
        {'name': 'Lean Manufacturing & VSM', 'year': 1990, 'inventor': 'Womack et al. (TPS)', 'reason': 'To codify the principles of the Toyota Production System, focusing on waste elimination and value stream optimization.'},
        {'name': 'MEWMA + XGBoost Diagnostics', 'year': 1992, 'inventor': 'Lowry et al.', 'reason': 'A need to generalize the sensitive EWMA chart to monitor multiple correlated variables at once.'},
        {'name': 'Stability Analysis (Shelf-Life)', 'year': 1993, 'inventor': 'ICH', 'reason': 'To harmonize global pharmaceutical regulations for determining a product\'s shelf-life.'},
        {'name': 'LSTM Autoencoder + Hybrid Monitoring', 'year': 1997, 'inventor': 'Hochreiter/Schmidhuber', 'reason': 'A need to model long-range temporal dependencies in data, later adapted for unsupervised anomaly detection.'},
        {'name': 'Advanced Stability Design', 'year': 2003, 'inventor': 'ICH Q1D', 'reason': 'To provide a risk-based statistical framework for reducing the cost of complex stability studies.'},
        {'name': 'RL for Chart Tuning', 'year': 2005, 'inventor': 'RL Community', 'reason': 'A desire to move beyond purely statistical chart design to an economically optimal framework balancing risk and cost.'},
        {'name': 'BOCPD + ML Features', 'year': 2007, 'inventor': 'Adams & MacKay', 'reason': 'A need for a more robust, probabilistic method for detecting changepoints in real-time streaming data.'},
        {'name': 'Anomaly Detection', 'year': 2008, 'inventor': 'Liu et al.', 'reason': 'A need for a fast, efficient algorithm (Isolation Forest) to find outliers in high-dimensional data.'},
        {'name': 'TPP & CQA Cascade', 'year': 2009, 'inventor': 'ICH Q8', 'reason': 'The Quality by Design movement required a formal framework to link patient needs to process controls.'},
        {'name': 'PSO + Autoencoder', 'year': 2010, 'inventor': 'Modern Synthesis', 'reason': 'Using AI (PSO) to intelligently search for the failure modes of another AI model (Autoencoder).'},
        {'name': 'Analytical Target Profile', 'year': 2012, 'inventor': 'FDA/AAPS', 'reason': 'To extend QbD principles to the lifecycle of analytical methods, defining a "contract" for method performance.'},
        {'name': 'Explainable AI (XAI)', 'year': 2017, 'inventor': 'Lundberg et al.', 'reason': 'The rise of powerful but opaque "black box" models necessitated methods to explain their reasoning (XAI).'},
        {'name': 'Prophet Forecasting', 'year': 2017, 'inventor': 'Meta (Facebook)', 'reason': 'A need for a scalable, automated forecasting tool that could be used by non-experts to handle complex business time series.'},
        {'name': 'Advanced AI Concepts', 'year': 2017, 'inventor': 'Vaswani et al.', 'reason': 'The Deep Learning revolution produced powerful new architectures like Transformers for sequence modeling.'},
        {'name': 'TCN + CUSUM', 'year': 2018, 'inventor': 'Bai, Kolter & Koltun', 'reason': 'A need for a faster, more effective deep learning architecture for sequence modeling to rival LSTMs.'},
        {'name': 'Causal ML / Double ML', 'year': 2018, 'inventor': 'Chernozhukov et al.', 'reason': 'A fusion of machine learning and econometrics to find true causal effects in complex, messy observational data.'},
        {'name': 'Causal Inference', 'year': 2018, 'inventor': 'Judea Pearl et al.', 'reason': 'The limitations of purely predictive models spurred a "causal revolution" to answer "why" questions.'},
    ]
    all_tools_data.sort(key=lambda x: x['year'])
    
    y_offsets = [3.0, -3.0, 3.5, -3.5, 2.5, -2.5, 4.0, -4.0, 2.0, -2.0, 4.5, -4.5, 1.5, -1.5, 3.2, -3.2, 3.8, -3.8]
    for i, tool in enumerate(all_tools_data):
        tool['y'] = y_offsets[i % len(y_offsets)]
    
    fig = go.Figure()
    eras = {
        'The Foundations (1800-1949)': {'color': 'rgba(0, 128, 128, 0.7)', 'boundary': (1800, 1949)},
        'Post-War & Industrial Boom (1950-1979)': {'color': 'rgba(0, 104, 201, 0.7)', 'boundary': (1950, 1979)},
        'The Quality Revolution (1980-1999)': {'color': 'rgba(100, 0, 100, 0.7)', 'boundary': (1980, 1999)},
        'The AI & Data Era (2000-Present)': {'color': 'rgba(214, 39, 40, 0.7)', 'boundary': (2000, 2025)}
    }
    
    for era_name, era_info in eras.items():
        x0, x1 = era_info['boundary']
        fig.add_shape(type="rect", x0=x0, y0=-5.5, x1=x1, y1=5.5, line=dict(width=0), fillcolor=era_info['color'], opacity=0.15, layer='below')
        fig.add_annotation(x=(x0 + x1) / 2, y=6.5, text=f"<b>{era_name}</b>", showarrow=False, font=dict(size=18, color=era_info['color']))

    fig.add_shape(type="line", x0=1920, y0=0, x1=2025, y1=0, line=dict(color="black", width=3), layer='below')

    for tool in all_tools_data:
        x_coord, y_coord = tool['year'], tool['y']
        tool_color = next((era['color'] for era in eras.values() if era['boundary'][0] <= x_coord <= era['boundary'][1]), 'grey')
        fig.add_trace(go.Scatter(x=[x_coord], y=[y_coord], mode='markers', marker=dict(size=12, color=tool_color, line=dict(width=2, color='black')),
                                 hoverinfo='text', text=f"<b>{tool['name']} ({tool['year']})</b><br><i>Inventor(s): {tool['inventor']}</i><br><br><b>Reason:</b> {tool['reason']}"))
        fig.add_shape(type="line", x0=x_coord, y0=0, x1=x_coord, y1=y_coord, line=dict(color='grey', width=1))
        fig.add_annotation(x=x_coord, y=y_coord, text=f"<b>{tool['name']}</b>", showarrow=False, yshift=25 if y_coord > 0 else -25, font=dict(size=11, color=tool_color), align="center")

    fig.update_layout(title_text='<b>A Chronological Timeline of V&V Analytics</b>', title_font_size=28, title_x=0.5,
                      xaxis=dict(range=[1920, 2025], showgrid=True), yaxis=dict(visible=False, range=[-8, 8]),
                      plot_bgcolor='white', paper_bgcolor='white', height=800, margin=dict(l=20, r=20, t=100, b=20), showlegend=False)
    return fig
    
@st.cache_data
def create_toolkit_conceptual_map():
    """Generates the conceptual map, now including all tools."""
    structure = {
        'Validation Planning & Strategy': ['Risk Management', 'Requirements Definition', 'Design Principles', 'Project Management'],
        'Method & Process Characterization': ['Foundational Statistics', 'Measurement Systems Analysis', 'Experimental Design'],
        'Process & Lifecycle Management': ['Statistical Process Control', 'Validation & Qualification', 'Operational Excellence'],
        'Advanced Analytics (ML/AI)': ['Predictive Modeling', 'Unsupervised Learning', 'Time Series & Sequential']
    }
    sub_structure = {
        'Risk Management': ['Quality Risk Management (QRM) Suite', 'Root Cause Analysis (RCA)'],
        'Requirements Definition': ['TPP & CQA Cascade', 'Analytical Target Profile (ATP) Builder', 'V&V Strategy & Justification', 'Design Controls & DHF'],
        'Design Principles': ['Design for Excellence (DfX)', 'FAT & SAT'],
        'Project Management': ['Validation Master Plan (VMP) Builder', 'Requirements Traceability Matrix (RTM)', 'Gap Analysis & Change Control'],
        'Foundational Statistics': ['Exploratory Data Analysis (EDA)', 'Confidence Interval Concept', 'Confidence Intervals for Proportions', 'Bayesian Inference', 'Comprehensive Diagnostic Validation'],
        'Measurement Systems Analysis': ['Gage R&R / VCA', 'Attribute Agreement Analysis', 'Method Comparison', 'LOD & LOQ', 'ROC Curve Analysis'],
        'Experimental Design': ['Assay Robustness (DOE)', 'Mixture Design (Formulations)', 'Split-Plot Designs', 'Causal Inference', 'Bayesian Optimization'],
        'Statistical Process Control': ['Process Stability (SPC)', 'Small Shift Detection', 'Multivariate SPC', 'MEWMA + XGBoost Diagnostics', 'Run Validation (Westgard)'],
        'Validation & Qualification': ['Process Capability (Cpk)', 'Tolerance Intervals', 'Component Reliability Testing', 'Reliability / Survival Analysis', 'Stability Analysis (Shelf-Life)', 'Sample Size for Qualification', 'Statistical Equivalence for Process Transfer', 'Advanced Stability Design', 'Equivalence Testing (TOST)', 'Wasserstein Distance'],
        'Operational Excellence': ['First Time Yield & Cost of Quality', 'Overall Equipment Effectiveness (OEE)', 'Lean Manufacturing & VSM', 'Production Line Sync (ODE)'],
        'Predictive Modeling': ['Linearity & Range', 'Non-Linear Regression (4PL/5PL)', 'Multivariate Analysis (MVA)', 'Predictive Modeling Suite', 'Explainable AI (XAI)', 'Causal ML / Double ML'],
        'Unsupervised Learning': ['Clustering (Unsupervised)', 'Anomaly Detection', 'LSTM Autoencoder + Hybrid Monitoring', 'PSO + Autoencoder'],
        'Time Series & Sequential': ['Time Series Forecasting Suite', 'Prophet Forecasting', 'BOCPD + ML Features', 'Kalman Filter + Residual Chart', 'TCN + CUSUM', 'RL for Chart Tuning', 'Advanced AI Concepts', 'Digital Twin & Real-Time Simulation']
    }
    tool_origins = {
        # Act 0
        'TPP & CQA Cascade': 'Biostatistics', 'Analytical Target Profile (ATP) Builder': 'Biostatistics',
        'Quality Risk Management (QRM) Suite': 'Industrial Quality Control', 'Root Cause Analysis (RCA)': 'Industrial Quality Control',
        'V&V Strategy & Justification': 'Industrial Quality Control', 'Design Controls & DHF': 'Industrial Quality Control',
        'Design for Excellence (DfX)': 'Industrial Quality Control', 'FAT & SAT': 'Industrial Quality Control',
        'Validation Master Plan (VMP) Builder': 'Industrial Quality Control', 'Requirements Traceability Matrix (RTM)': 'Industrial Quality Control',
        'Gap Analysis & Change Control': 'Industrial Quality Control',
        # Act I
        'Exploratory Data Analysis (EDA)': 'Statistics', 'Confidence Interval Concept': 'Statistics',
        'Confidence Intervals for Proportions': 'Statistics', 'Bayesian Inference': 'Statistics',
        'Comprehensive Diagnostic Validation': 'Biostatistics', 'Gage R&R / VCA': 'Industrial Quality Control',
        'Attribute Agreement Analysis': 'Statistics', 'Method Comparison': 'Biostatistics',
        'LOD & LOQ': 'Statistics', 'ROC Curve Analysis': 'Statistics', 'Assay Robustness (DOE)': 'Statistics',
        'Mixture Design (Formulations)': 'Statistics', 'Split-Plot Designs': 'Statistics',
        'Causal Inference': 'Data Science / ML', 'Bayesian Optimization': 'Data Science / ML', 'Causal ML / Double ML': 'Data Science / ML',
        'Component Reliability Testing': 'Industrial Quality Control', 'Process Optimization: From DOE to AI': 'Data Science / ML',
        # Act II
        'Process Stability (SPC)': 'Industrial Quality Control', 'Small Shift Detection': 'Industrial Quality Control',
        'Multivariate SPC': 'Industrial Quality Control', 'MEWMA + XGBoost Diagnostics': 'Data Science / ML',
        'Run Validation (Westgard)': 'Biostatistics', 'Process Capability (Cpk)': 'Industrial Quality Control',
        'Tolerance Intervals': 'Statistics', 'Reliability / Survival Analysis': 'Biostatistics',
        'Stability Analysis (Shelf-Life)': 'Biostatistics', 'Sample Size for Qualification': 'Industrial Quality Control',
        'Statistical Equivalence for Process Transfer': 'Biostatistics', 'Advanced Stability Design': 'Biostatistics',
        'Equivalence Testing (TOST)': 'Biostatistics', 'Wasserstein Distance': 'Data Science / ML',
        'First Time Yield & Cost of Quality': 'Industrial Quality Control', 'Overall Equipment Effectiveness (OEE)': 'Industrial Quality Control',
        'Lean Manufacturing & VSM': 'Industrial Quality Control', 'Production Line Sync (ODE)': 'Industrial Quality Control',
        'Monte Carlo Simulation for Risk Analysis': 'Statistics',
        # Act III
        'Linearity & Range': 'Statistics', 'Non-Linear Regression (4PL/5PL)': 'Biostatistics',
        'Multivariate Analysis (MVA)': 'Data Science / ML', 'Predictive Modeling Suite': 'Data Science / ML',
        'Explainable AI (XAI)': 'Data Science / ML', 'Clustering (Unsupervised)': 'Data Science / ML',
        'Anomaly Detection': 'Data Science / ML', 'LSTM Autoencoder + Hybrid Monitoring': 'Data Science / ML',
        'PSO + Autoencoder': 'Data Science / ML', 'Time Series Forecasting Suite': 'Statistics', 'Prophet Forecasting': 'Data Science / ML',
        'BOCPD + ML Features': 'Data Science / ML', 'Kalman Filter + Residual Chart': 'Statistics',
        'TCN + CUSUM': 'Data Science / ML', 'RL for Chart Tuning': 'Data Science / ML',
        'Advanced AI Concepts': 'Data Science / ML', 'Digital Twin & Real-Time Simulation': 'Data Science / ML',
        'Process Control Plan Builder': 'Industrial Quality Control'
    }
    origin_colors = {'Statistics': '#1f77b4', 'Biostatistics': '#2ca02c', 'Industrial Quality Control': '#ff7f0e', 'Data Science / ML': '#d62728', 'Structure': '#6A5ACD'}

    nodes = {}
    vertical_spacing = 1.8
    all_tools_flat = [tool for sublist in sub_structure.values() for tool in sublist]
    y_coords = np.linspace(len(all_tools_flat) * vertical_spacing, -len(all_tools_flat) * vertical_spacing, len(all_tools_flat))
    x_positions = [4, 5]
    for i, tool_key in enumerate(all_tools_flat):
        short_name = tool_key.replace(' +', '<br>+').replace(' (', '<br>(').replace('Comprehensive ', 'Comprehensive<br>').replace(': From', ':<br>From').replace('V&V Strategy', 'V&V<br>Strategy').replace('Statistical Equivalence', 'Stat.<br>Equivalence')
        nodes[tool_key] = {'x': x_positions[i % 2], 'y': y_coords[i], 'name': tool_key, 'short': short_name, 'origin': tool_origins.get(tool_key, 'Statistics')}

    for l2_key, l3_keys in sub_structure.items():
        child_ys = [nodes[child_key]['y'] for child_key in l3_keys if child_key in nodes]
        if child_ys:
            nodes[l2_key] = {'x': 2.5, 'y': np.mean(child_ys), 'name': l2_key, 'short': l2_key.replace(' ', '<br>'), 'origin': 'Structure'}

    for l1_key, l2_keys in structure.items():
        child_ys = [nodes[child_key]['y'] for child_key in l2_keys if child_key in nodes]
        if child_ys:
            nodes[l1_key] = {'x': 1, 'y': np.mean(child_ys), 'name': l1_key, 'short': l1_key.replace(' ', '<br>').replace('Validation Planning', 'Validation<br>Planning'), 'origin': 'Structure'}

    nodes['CENTER'] = {'x': -0.5, 'y': 0, 'name': 'V&V Analytics Toolkit', 'short': 'V&V Analytics<br>Toolkit', 'origin': 'Structure'}
    
    fig = go.Figure()
    all_edges = [('CENTER', l1) for l1 in structure.keys()] + \
                [(l1, l2) for l1, l2s in structure.items() for l2 in l2s] + \
                [(l2, l3) for l2, l3s in sub_structure.items() for l3 in l3s]
    for start_key, end_key in all_edges:
        if start_key in nodes and end_key in nodes:
            fig.add_shape(type="line", x0=nodes[start_key]['x'], y0=nodes[start_key]['y'],
                          x1=nodes[end_key]['x'], y1=nodes[end_key]['y'], line=dict(color="lightgrey", width=1.5))
    
    data_by_origin = {name: {'x': [], 'y': [], 'short': [], 'full': [], 'size': [], 'font_size': []} for name in origin_colors.keys()}
    size_map = {'CENTER': 150, 'Level1': 130, 'Level2': 110, 'Tool': 90}
    font_map = {'CENTER': 16, 'Level1': 14, 'Level2': 12, 'Tool': 11}

    for key, data in nodes.items():
        if key == 'CENTER': level = 'CENTER'
        elif key in structure: level = 'Level1'
        elif key in sub_structure: level = 'Level2'
        else: level = 'Tool'
        if data['origin']:
            data_by_origin[data['origin']]['x'].append(data['x'])
            data_by_origin[data['origin']]['y'].append(data['y'])
            data_by_origin[data['origin']]['short'].append(data['short'])
            data_by_origin[data['origin']]['full'].append(data['name'])
            data_by_origin[data['origin']]['size'].append(size_map[level])
            data_by_origin[data['origin']]['font_size'].append(font_map[level])
        
    for origin_name, data in data_by_origin.items():
        if not data['x']: continue
        is_structure = origin_name == 'Structure'
        fig.add_trace(go.Scatter(
            x=data['x'], y=data['y'], text=data['short'], mode='markers+text', textposition="middle center",
            marker=dict(size=data['size'], color=origin_colors[origin_name], symbol='circle',
                        line=dict(width=2, color='black' if not is_structure else origin_colors[origin_name])),
            textfont=dict(size=data['font_size'], color='white', family="Arial, sans-serif"),
            hovertext=[f"<b>{name}</b><br>Origin: {origin_name}" for name in data['full']], hoverinfo='text',
            name=origin_name))

    fig.update_layout(
        title_text='<b>Conceptual Map of the V&V Analytics Toolkit</b>', showlegend=True,
        legend=dict(title="<b>Tool Origin</b>", x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.7)', bordercolor="Black", borderwidth=1),
        xaxis=dict(visible=False, range=[-1, 6]),
        yaxis=dict(visible=False, range=[-len(all_tools_flat)*1.2, len(all_tools_flat)*1.2]),
        height=len(all_tools_flat) * 45, # Adjusted height
        margin=dict(l=20, r=20, t=60, b=20),
        plot_bgcolor='#FFFFFF', paper_bgcolor='#f0f2f6'
    )
    return fig

@st.cache_data
def create_business_case_map():
    """
    Generates a conceptual map organizing the toolkit by core business case themes.
    """
    business_themes = {
        'De-Risking Launches & Transfers': {'icon': 'üöÄ', 'color': '#0068C9'},
        'Maximizing Profitability & Value': {'icon': 'üí∞', 'color': '#2ca02c'},
        'Accelerating Innovation & Troubleshooting': {'icon': 'üí°', 'color': '#ff7f0e'},
        'Ensuring Compliance & Data Integrity': {'icon': 'üèõÔ∏è', 'color': '#d62728'},
        'Proactive Lifecycle Management': {'icon': 'üõ°Ô∏è', 'color': '#9467bd'}
    }

    tool_to_theme = {
        'De-Risking Launches & Transfers': [
            'Analytical Target Profile (ATP) Builder', 'Quality Risk Management (QRM) Suite',
            'FAT & SAT', 'Sample Size for Qualification', 'Method Comparison',
            'Equivalence Testing (TOST)', 'Wasserstein Distance', 'Statistical Equivalence for Process Transfer'
        ],
        'Maximizing Profitability & Value': [
            'Design for Excellence (DfX)', 'LOD & LOQ', 'Comprehensive Diagnostic Validation',
            'Process Capability (Cpk)', 'First Time Yield & Cost of Quality', 'Overall Equipment Effectiveness (OEE)',
            'Lean Manufacturing & VSM', 'Production Line Sync (ODE)', 'Tolerance Intervals'
        ],
        'Accelerating Innovation & Troubleshooting': [
            'Root Cause Analysis (RCA)', 'Mixture Design (Formulations)',
            'Process Optimization: From DOE to AI', 'Bayesian Optimization', 'Causal Inference',
            'Causal ML / Double ML', 'Clustering (Unsupervised)', 'PSO + Autoencoder'
        ],
        'Ensuring Compliance & Data Integrity': [
            'TPP & CQA Cascade', 'V&V Strategy & Justification', 'Design Controls & DHF',
            'Validation Master Plan (VMP) Builder', 'Requirements Traceability Matrix (RTM)', 'Gap Analysis & Change Control',
            'Exploratory Data Analysis (EDA)', 'Confidence Interval Concept', 'Confidence Intervals for Proportions',
            'Core Validation Parameters', 'Linearity & Range', 'Non-Linear Regression (4PL/5PL)',
            'Gage R&R / VCA', 'Attribute Agreement Analysis', 'ROC Curve Analysis'
        ],
        'Proactive Lifecycle Management': [
            'Component Reliability Testing', 'Advanced Stability Design', 'Process Stability (SPC)',
            'Process Control Plan Builder', 'Run Validation (Westgard)', 'Small Shift Detection',
            'Multivariate SPC', 'Stability Analysis (Shelf-Life)', 'Reliability / Survival Analysis',
            'Time Series Forecasting Suite', 'Prophet Forecasting', 'Multivariate Analysis (MVA)',
            'Predictive Modeling Suite', 'Explainable AI (XAI)', 'Anomaly Detection',
            'Advanced AI Concepts', 'MEWMA + XGBoost Diagnostics', 'BOCPD + ML Features',
            'Kalman Filter + Residual Chart', 'RL for Chart Tuning', 'TCN + CUSUM',
            'LSTM Autoencoder + Hybrid Monitoring', 'Digital Twin & Real-Time Simulation', 'Bayesian Inference'
        ]
    }

    fig = go.Figure()
    nodes = {}

    # Center Node
    nodes['CENTER'] = {'x': 0, 'y': 0, 'name': 'Business Value', 'short': 'Business<br>Value', 'color': DARK_GREY, 'size': 150, 'font_size': 16}

    # Theme Nodes (L1)
    num_themes = len(business_themes)
    angle_step_l1 = 2 * np.pi / num_themes
    radius_l1 = 1.5
    for i, (theme, props) in enumerate(business_themes.items()):
        angle = i * angle_step_l1
        x, y = radius_l1 * np.cos(angle), radius_l1 * np.sin(angle)
        nodes[theme] = {'x': x, 'y': y, 'name': theme, 'short': f"{props['icon']}<br><b>{theme.replace(' & ', '<br>& ')}</b>", 'color': props['color'], 'size': 120, 'font_size': 12}
        fig.add_shape(type="line", x0=nodes['CENTER']['x'], y0=nodes['CENTER']['y'], x1=x, y1=y, line=dict(color="lightgrey", width=2))

    # Tool Nodes (L2)
    radius_l2 = 3.5
    total_tools = sum(len(tools) for tools in tool_to_theme.values())
    
    current_angle = 0
    for i, (theme, tools) in enumerate(tool_to_theme.items()):
        arc_angle = (len(tools) / total_tools) * 2 * np.pi
        angle_step_l2 = arc_angle / (len(tools) + 1)
        
        for j, tool in enumerate(tools):
            angle = current_angle + (j + 1) * angle_step_l2
            x, y = radius_l2 * np.cos(angle), radius_l2 * np.sin(angle)
            nodes[tool] = {'x': x, 'y': y, 'name': tool, 'short': tool.replace(' (', '<br>(').replace(' & ', '<br>& ').replace(' for ', '<br>for '), 'color': business_themes[theme]['color'], 'size': 70, 'font_size': 9}
            fig.add_shape(type="line", x0=nodes[theme]['x'], y0=nodes[theme]['y'], x1=x, y1=y, line=dict(color="lightgrey", width=1))
        current_angle += arc_angle

    # Plot Nodes
    for key, props in nodes.items():
        fig.add_trace(go.Scatter(
            x=[props['x']], y=[props['y']], text=[props['short']],
            mode='markers+text', textposition="middle center",
            marker=dict(size=props['size'], color=props['color'], symbol='circle', line=dict(width=2, color='black')),
            textfont=dict(size=props['font_size'], color='white', family="Arial, sans-serif"),
            hovertext=f"<b>{props['name']}</b>", hoverinfo='text',
            showlegend=False
        ))

    fig.update_layout(
        title_text='<b>Toolkit Map by Business Case Theme</b>',
        xaxis=dict(visible=False, showgrid=False, range=[-4.5, 4.5]),
        yaxis=dict(visible=False, showgrid=False, range=[-4.5, 4.5], scaleanchor="x", scaleratio=1),
        height=1000,
        margin=dict(l=20, r=20, t=60, b=20),
        plot_bgcolor='#FFFFFF', paper_bgcolor='#f0f2f6'
    )
    return fig
#==============================================================================================================================================================================================================
#=========================================================================================== ACT 0 BEGGINNG ===================================================================================================
#==============================================================================================================================================================================================================

@st.cache_data
def plot_tpp_cqa_cascade(project_type, target1_val, target2_val, target1_tag, target2_tag):
    """
    Generates a professional, interactive Sankey diagram for TPP -> CQA -> CPP cascade for multiple project types.
    """
    cascade_data = {
        "Monoclonal Antibody": {
            "TPP": "A safe, effective, and stable MAb therapeutic.", "CQAs": {"Purity > 99%": {"link": "Efficacy"}, "Aggregate < 1%": {"link": "Safety"}, "Potency 80-120%": {"link": "Efficacy"}, "Charge Variants": {"link": "Efficacy"}, "Stability": {"link": "Shelf-Life"}},
            "CPPs": {"Bioreactor pH": ["Potency 80-120%", "Charge Variants"], "Column Load": ["Purity > 99%", "Aggregate < 1%"], "Formulation Buffer": ["Stability"]}
        },
        "IVD Kit": {
            "TPP": "A reliable and accurate diagnostic kit.", "CQAs": {"Sensitivity > 98%": {"link": "Efficacy"}, "Specificity > 99%": {"link": "Efficacy"}, "Precision < 15% CV": {"link": "Reliability"}, "Shelf-Life": {"link": "Shelf-Life"}},
            "CPPs": {"Antibody Conc.": ["Sensitivity > 98%"], "Blocking Buffer": ["Specificity > 99%"], "Lyophilization Cycle": ["Shelf-Life"]}
        },
        "Pharma Process (Small Molecule)": {
            "TPP": "A robust and efficient process for Drug Substance X.", "CQAs": {"Impurity Profile < 0.1%": {"link": "Purity"}, "Process Yield > 85%": {"link": "Yield"}, "Correct Polymorph (Form II)": {"link": "Efficacy"}, "Particle Size (D90 < 20Œºm)": {"link": "Efficacy"}},
            "CPPs": {"Reaction Temperature": ["Impurity Profile < 0.1%"], "Reagent Stoichiometry": ["Process Yield > 85%"], "Crystallization Solvent": ["Correct Polymorph (Form II)"], "Milling Speed": ["Particle Size (D90 < 20Œºm)"]}
        },
        "Instrument Qualification": {
            "TPP": "A high-throughput, reliable liquid handler for automated sample prep.", "CQAs": {"Dispense Accuracy & Precision": {"link": "Reliability"}, "Throughput (Plates/hr)": {"link": "Throughput"}, "Cross-Contamination < 0.01%": {"link": "Reliability"}, "Uptime > 99%": {"link": "Reliability"}},
            "CPPs": {"Pump Calibration Algorithm": ["Dispense Accuracy & Precision"], "Robotic Arm Speed": ["Throughput (Plates/hr)"], "Tip Wash Protocol": ["Cross-Contamination < 0.01%"], "Preventive Maintenance": ["Uptime > 99%"]}
        },
        "Computer System Validation": {
            "TPP": "A 21 CFR Part 11 compliant LIMS for managing lab data.", "CQAs": {"Audit Trail Integrity": {"link": "Compliance"}, "User Access Control": {"link": "Compliance"}, "Report Generation Time < 5s": {"link": "Performance"}, "Data Backup & Recovery": {"link": "Compliance"}},
            "CPPs": {"Database Schema": ["Audit Trail Integrity"], "Role-Based Permission Matrix": ["User Access Control"], "Query Optimization": ["Report Generation Time < 5s"], "Replication Strategy": ["Data Backup & Recovery"]}
        }
    }
    
    data = cascade_data[project_type]
    
    labels = [f"<b>TPP:</b><br>{data['TPP']}"]
    labels.extend([f"<b>CQA:</b> {cqa}" for cqa in data['CQAs']])
    labels.extend([f"<b>CPP:</b> {cpp}" for cpp in data['CPPs']])
    node_colors = [PRIMARY_COLOR] + [SUCCESS_GREEN]*len(data['CQAs']) + ['#636EFA']*len(data['CPPs'])

    for i, (key, props) in enumerate(data['CQAs'].items()):
        is_active = (target1_tag in props['link'] and target1_val) or \
                    (target2_tag in props['link'] and target2_val)
        if is_active:
            node_colors[i + 1] = '#FFBF00'
            
    sources, targets, values = [], [], []
    tpp_idx = 0
    for i, cqa in enumerate(data['CQAs']):
        cqa_idx = labels.index(f"<b>CQA:</b> {cqa}")
        sources.append(tpp_idx)
        targets.append(cqa_idx)
        values.append(1)
    for cpp, cqa_links in data['CPPs'].items():
        cpp_idx = labels.index(f"<b>CPP:</b> {cpp}")
        for link in cqa_links:
            cqa_idx = labels.index(f"<b>CQA:</b> {link}")
            sources.append(cqa_idx)
            targets.append(cpp_idx)
            values.append(1)

    fig = go.Figure(data=[go.Sankey(
        node=dict(pad=25, thickness=20, line=dict(color="black", width=0.5), label=labels, color=node_colors),
        link=dict(source=sources, target=targets, value=values, color='rgba(200,200,200,0.5)')
    )])
    
    fig.update_layout(title_text="<b>The 'Golden Thread': TPP ‚Üí CQA ‚Üí CPP Cascade</b>", font_size=12, height=600)
    return fig

@st.cache_data
def plot_atp_radar_chart(project_type, atp_values, achieved_values=None):
    """
    Generates a professional-grade radar chart for an Analytical Target Profile,
    now supporting multiple project types with unique performance criteria.
    """
    profiles = {
        "Pharma Assay (HPLC)": {
            'categories': ['Accuracy<br>(%Rec)', 'Precision<br>(%CV)', 'Linearity<br>(R¬≤)', 'Range<br>(Turn-down)', 'Sensitivity<br>(LOD)'],
            'ranges': [[95, 102], [5, 1], [0.99, 0.9999], [10, 100], [1, 50]],
            'direction': [1, -1, 1, 1, 1]
        },
        "IVD Kit (ELISA)": {
            'categories': ['Clinical<br>Sensitivity', 'Clinical<br>Specificity', 'Precision<br>(%CV)', 'Robustness', 'Shelf-Life<br>(Months)'],
            'ranges': [[90, 100], [90, 100], [20, 10], [1, 10], [6, 24]],
            'direction': [1, 1, -1, 1, 1]
        },
        "Instrument Qualification": {
            'categories': ['Accuracy<br>(Bias %)', 'Precision<br>(%CV)', 'Throughput<br>(Samples/hr)', 'Uptime<br>(%)', 'Footprint<br>(m¬≤)'],
            'ranges': [[5, 0.1], [5, 0.5], [10, 200], [95, 99.9], [5, 1]],
            'direction': [-1, -1, 1, 1, -1]
        },
        "Software System (LIMS)": {
            'categories': ['Reliability<br>(Uptime %)', 'Performance<br>(Query Time)', 'Security<br>(Compliance)', 'Usability<br>(User Score)', 'Scalability<br>(Users)'],
            'ranges': [[99, 99.999], [10, 0.5], [1, 10], [1, 10], [50, 5000]],
            'direction': [1, -1, 1, 1, 1]
        },
        "Pharma Process (MAb)": {
            'categories': ['Yield<br>(g/L)', 'Purity<br>(%)', 'Process<br>Consistency', 'Robustness<br>(PAR Size)', 'Cycle Time<br>(Days)'],
            'ranges': [[1, 10], [98, 99.9], [1, 10], [1, 10], [20, 10]],
            'direction': [1, 1, 1, 1, -1]
        }
    }
    
    profile = profiles[project_type]
    categories = profile['categories']
    
    def normalize_value(val, val_range, direction):
        low, high = val_range
        if direction == -1: low, high = high, low
        scaled = ((val - low) / (high - low)) * 100 if (high - low) != 0 else 50
        return np.clip(scaled, 0, 100)

    scaled_atp = [normalize_value(atp_values[i], profile['ranges'][i], profile['direction'][i]) for i in range(len(categories))]
    
    fig = go.Figure()
    fig.add_trace(go.Scatterpolar(
        r=scaled_atp, theta=categories, fill='toself', name='ATP (Target)',
        line=dict(color=PRIMARY_COLOR), fillcolor='rgba(0, 104, 201, 0.4)'
    ))

    if achieved_values:
        scaled_achieved = [normalize_value(achieved_values[i], profile['ranges'][i], profile['direction'][i]) for i in range(len(categories))]
        fig.add_trace(go.Scatterpolar(
            r=scaled_achieved, theta=categories, fill='toself', name='Achieved Performance',
            line=dict(color=SUCCESS_GREEN), fillcolor='rgba(44, 160, 44, 0.4)'
        ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(visible=True, range=[0, 100], showticklabels=False, ticks='', gridcolor='lightgrey'),
            angularaxis=dict(tickfont=dict(size=12), linecolor='grey', gridcolor='lightgrey')
        ),
        showlegend=True,
        legend=dict(yanchor="bottom", y=-0.2, xanchor="center", x=0.5, orientation="h"),
        title=f"<b>Target Profile for {project_type}</b>",
        margin=dict(t=80)
    )
    
    return fig

# SNIPPET 1: Replace the entire plot_ivd_regulatory_pathway function with this enhanced version.

# SNIPPET: Replace the existing plot_ivd_regulatory_pathway function with this new, complete version.

@st.cache_data
def plot_fda_pathway(highlight_path='510k'):
    """Generates a professional flowchart of the US FDA regulatory pathways."""
    fig = go.Figure()
    nodes = {
        'Start': {'label': 'Start:<br>Product Concept', 'pos': (0, 5), 'color': DARK_GREY},
        'IntendedUse': {'label': 'Define Intended Use<br>& Indications for Use', 'pos': (2, 5), 'color': DARK_GREY},
        'RUO': {'label': '<b>RUO</b><br>(Research Use Only)', 'pos': (4, 7.5), 'color': 'purple'},
        'Device?': {'label': 'Is it a Medical Device?', 'pos': (4, 5), 'color': 'orange'},
        'Classify': {'label': 'Classify Device Risk<br>(Class I, II, or III)', 'pos': (6, 5), 'color': 'orange'},
        'NoPredicate': {'label': 'No Valid<br>Predicate?', 'pos': (8, 6.5), 'color': 'orange'},
        'DeNovo': {'label': '<b>De Novo Request</b><br>Novel Low/Mod Risk', 'pos': (10, 6.5), 'color': 'teal'},
        'ClassI': {'label': '<b>Class I</b><br>(Low Risk)', 'pos': (8, 5), 'color': SUCCESS_GREEN},
        'ClassII': {'label': '<b>Class II</b><br>(Moderate Risk)', 'pos': (8, 3.5), 'color': PRIMARY_COLOR},
        'ClassIII': {'label': '<b>Class III</b><br>(High Risk)', 'pos': (8, 1.5), 'color': 'red'},
        '510k': {'label': '<b>510(k) Submission</b><br>Substantial Equivalence', 'pos': (10, 3.5), 'color': PRIMARY_COLOR},
        'PMA': {'label': '<b>PMA Submission</b><br>Safety & Efficacy', 'pos': (10, 1.5), 'color': 'red'},
        'Market': {'label': '<b>Market Product</b><br>Post-Market Controls', 'pos': (12, 4.5), 'color': '#00BFFF'},
        'Emergency': {'label': 'Public Health<br>Emergency?', 'pos': (2, 1.5), 'color': '#FFBF00'},
        'EUA': {'label': '<b>EUA Request</b><br>May Be Effective', 'pos': (4, 1.5), 'color': '#FFBF00'}
    }
    # (Pathway and edge definitions remain the same as the previous version)
    pathways = {
        'ruo': ['Start', 'IntendedUse', 'Device?', 'RUO'], 'class_i': ['Start', 'IntendedUse', 'Device?', 'Classify', 'ClassI', 'Market'],
        '510k': ['Start', 'IntendedUse', 'Device?', 'Classify', 'ClassII', 'NoPredicate', '510k', 'Market'],
        'pma': ['Start', 'IntendedUse', 'Device?', 'Classify', 'ClassIII', 'PMA', 'Market'],
        'denovo': ['Start', 'IntendedUse', 'Device?', 'Classify', 'ClassII', 'NoPredicate', 'DeNovo', 'Market'],
        'eua': ['Start', 'Emergency', 'EUA', 'Market']
    }
    edges = [('Start', 'IntendedUse'), ('IntendedUse', 'Device?'), ('Device?', 'RUO'), ('Device?', 'Classify'), ('Classify', 'ClassI'), ('Classify', 'ClassII'), ('Classify', 'ClassIII'), ('ClassII', 'NoPredicate'), ('NoPredicate', 'DeNovo'), ('NoPredicate', '510k'), ('ClassI', 'Market'), ('DeNovo', 'Market'), ('510k', 'Market'), ('PMA', 'Market'), ('Start', 'Emergency'), ('Emergency', 'EUA'), ('EUA', 'Market')]
    
    for start, end in edges:
        is_highlighted = start in pathways.get(highlight_path, []) and end in pathways.get(highlight_path, [])
        fig.add_annotation(ax=nodes[start]['pos'][0], ay=nodes[start]['pos'][1], x=nodes[end]['pos'][0], y=nodes[end]['pos'][1], arrowhead=2, arrowwidth=3 if is_highlighted else 1.5, arrowcolor=PRIMARY_COLOR if is_highlighted else 'lightgrey', showarrow=True)

    for name, props in nodes.items():
        is_highlighted = name in pathways.get(highlight_path, [])
        fig.add_shape(type="rect", x0=props['pos'][0]-1.2, y0=props['pos'][1]-0.5, x1=props['pos'][0]+1.2, y1=props['pos'][1]+0.5, fillcolor=props['color'], line=dict(color='black', width=3 if is_highlighted else 1))
        fig.add_annotation(x=props['pos'][0], y=props['pos'][1], text=f"{props['label']}", showarrow=False, font_color="white", font_size=11)

    fig.update_layout(title_text="<b>Pathway: USA (FDA)</b>", xaxis=dict(visible=False, range=[-2, 14]), yaxis=dict(visible=False, range=[0, 9]), height=500, plot_bgcolor='#F0F2F6', margin=dict(l=20, r=20, t=50, b=20))
    return fig

@st.cache_data
def plot_eu_pathway(highlight_path='class_iia'):
    """Generates a professional flowchart of the European Union (MDR/IVDR) pathways."""
    fig = go.Figure()
    nodes = {
        'Start': {'label': 'Start:<br>Product Concept', 'pos': (0, 5), 'color': DARK_GREY},
        'IntendedUse': {'label': 'Define Intended Use<br>& Classification Rules', 'pos': (2, 5), 'color': DARK_GREY},
        'QMS': {'label': 'Implement QMS<br>(ISO 13485)', 'pos': (4, 5), 'color': PRIMARY_COLOR},
        'TechFile': {'label': 'Compile<br>Technical File', 'pos': (6, 5), 'color': PRIMARY_COLOR},
        'Classify': {'label': 'Classify Device Risk<br>(I, IIa, IIb, III)', 'pos': (8, 5), 'color': 'orange'},
        'ClassI': {'label': '<b>Class I</b><br>(non-sterile, no measure)', 'pos': (10, 7), 'color': SUCCESS_GREEN},
        'ClassIsm': {'label': '<b>Class I*</b><br>(sterile/measure/reusable)', 'pos': (10, 5.7), 'color': '#636EFA'},
        'ClassIIa': {'label': '<b>Class IIa</b><br>(Moderate Risk)', 'pos': (10, 4.3), 'color': '#FFBF00'},
        'ClassIIb': {'label': '<b>Class IIb</b><br>(High-Moderate Risk)', 'pos': (10, 2.9), 'color': '#EF553B'},
        'ClassIII': {'label': '<b>Class III</b><br>(Highest Risk)', 'pos': (10, 1.5), 'color': 'red'},
        'SelfCert': {'label': '<b>Self-Declaration</b><br>of Conformity', 'pos': (12, 7), 'color': SUCCESS_GREEN},
        'NB_Audit': {'label': '<b>Notified Body</b><br>Audit & Review', 'pos': (12, 4), 'color': 'purple'},
        'CE_Mark': {'label': '<b>Affix CE Mark</b><br>Market Product', 'pos': (14, 5.5), 'color': '#00BFFF'}
    }
    pathways = {
        'class_i': ['Start', 'IntendedUse', 'QMS', 'TechFile', 'Classify', 'ClassI', 'SelfCert', 'CE_Mark'],
        'class_ism': ['Start', 'IntendedUse', 'QMS', 'TechFile', 'Classify', 'ClassIsm', 'NB_Audit', 'CE_Mark'],
        'class_iia': ['Start', 'IntendedUse', 'QMS', 'TechFile', 'Classify', 'ClassIIa', 'NB_Audit', 'CE_Mark'],
        'class_iib': ['Start', 'IntendedUse', 'QMS', 'TechFile', 'Classify', 'ClassIIb', 'NB_Audit', 'CE_Mark'],
        'class_iii': ['Start', 'IntendedUse', 'QMS', 'TechFile', 'Classify', 'ClassIII', 'NB_Audit', 'CE_Mark']
    }
    edges = [('Start', 'IntendedUse'), ('IntendedUse', 'QMS'), ('QMS', 'TechFile'), ('TechFile', 'Classify'), ('Classify', 'ClassI'), ('Classify', 'ClassIsm'), ('Classify', 'ClassIIa'), ('Classify', 'ClassIIb'), ('Classify', 'ClassIII'), ('ClassI', 'SelfCert'), ('ClassIsm', 'NB_Audit'), ('ClassIIa', 'NB_Audit'), ('ClassIIb', 'NB_Audit'), ('ClassIII', 'NB_Audit'), ('SelfCert', 'CE_Mark'), ('NB_Audit', 'CE_Mark')]
    for start, end in edges:
        is_highlighted = start in pathways.get(highlight_path, []) and end in pathways.get(highlight_path, [])
        fig.add_annotation(ax=nodes[start]['pos'][0], ay=nodes[start]['pos'][1], x=nodes[end]['pos'][0], y=nodes[end]['pos'][1], arrowhead=2, arrowwidth=3 if is_highlighted else 1.5, arrowcolor=PRIMARY_COLOR if is_highlighted else 'lightgrey', showarrow=True)
    for name, props in nodes.items():
        is_highlighted = name in pathways.get(highlight_path, [])
        fig.add_shape(type="rect", x0=props['pos'][0]-1.2, y0=props['pos'][1]-0.5, x1=props['pos'][0]+1.2, y1=props['pos'][1]+0.5, fillcolor=props['color'], line=dict(color='black', width=3 if is_highlighted else 1))
        fig.add_annotation(x=props['pos'][0], y=props['pos'][1], text=f"{props['label']}", showarrow=False, font_color="white", font_size=11)
    fig.update_layout(title_text="<b>Pathway: European Union (MDR/IVDR)</b>", xaxis=dict(visible=False, range=[-2, 16]), yaxis=dict(visible=False, range=[0, 9]), height=500, plot_bgcolor='#F0F2F6', margin=dict(l=20, r=20, t=50, b=20))
    return fig

@st.cache_data
def plot_jpn_pathway(highlight_path='class_ii'):
    """Generates a professional flowchart of the Japanese (PMDA) pathways."""
    fig = go.Figure()
    nodes = {
        'Start': {'label': 'Start:<br>Product Concept', 'pos': (0, 5), 'color': DARK_GREY},
        'QMS': {'label': 'Establish QMS<br>(MHLW Ord. 169)', 'pos': (2, 5), 'color': PRIMARY_COLOR},
        'Foreign': {'label': 'Appoint MAH<br>(Foreign Mfr.)', 'pos': (4, 5), 'color': PRIMARY_COLOR},
        'Classify': {'label': 'Classify Device Risk<br>(I, II, III, IV)', 'pos': (6, 5), 'color': 'orange'},
        'ClassI': {'label': '<b>Class I</b><br>(General)', 'pos': (8, 7), 'color': SUCCESS_GREEN},
        'ClassII': {'label': '<b>Class II</b><br>(Specified Controlled)', 'pos': (8, 5), 'color': '#FFBF00'},
        'ClassIII': {'label': '<b>Class III</b><br>(Highly Controlled)', 'pos': (8, 3), 'color': '#EF553B'},
        'ClassIV': {'label': '<b>Class IV</b><br>(Highest Risk)', 'pos': (8, 1), 'color': 'red'},
        'PMDA_Submit': {'label': '<b>Submit to PMDA</b><br>(Pre-Market App.)', 'pos': (10, 5), 'color': 'purple'},
        'RCB_Cert': {'label': '<b>RCB Certification</b><br>(Third Party)', 'pos': (10, 3), 'color': 'teal'},
        'Shonin': {'label': '<b>Shonin</b><br>(MHLW Approval)', 'pos': (12, 5), 'color': '#00BFFF'}
    }
    pathways = {
        'class_i': ['Start', 'QMS', 'Foreign', 'Classify', 'ClassI', 'PMDA_Submit', 'Shonin'],
        'class_ii': ['Start', 'QMS', 'Foreign', 'Classify', 'ClassII', 'RCB_Cert', 'Shonin'],
        'class_iii': ['Start', 'QMS', 'Foreign', 'Classify', 'ClassIII', 'PMDA_Submit', 'Shonin'],
        'class_iv': ['Start', 'QMS', 'Foreign', 'Classify', 'ClassIV', 'PMDA_Submit', 'Shonin']
    }
    edges = [('Start', 'QMS'), ('QMS', 'Foreign'), ('Foreign', 'Classify'), ('Classify', 'ClassI'), ('Classify', 'ClassII'), ('Classify', 'ClassIII'), ('Classify', 'ClassIV'), ('ClassI', 'PMDA_Submit'), ('ClassII', 'RCB_Cert'), ('ClassIII', 'PMDA_Submit'), ('ClassIV', 'PMDA_Submit'), ('PMDA_Submit', 'Shonin'), ('RCB_Cert', 'Shonin')]
    for start, end in edges:
        is_highlighted = start in pathways.get(highlight_path, []) and end in pathways.get(highlight_path, [])
        fig.add_annotation(ax=nodes[start]['pos'][0], ay=nodes[start]['pos'][1], x=nodes[end]['pos'][0], y=nodes[end]['pos'][1], arrowhead=2, arrowwidth=3 if is_highlighted else 1.5, arrowcolor=PRIMARY_COLOR if is_highlighted else 'lightgrey', showarrow=True)
    for name, props in nodes.items():
        is_highlighted = name in pathways.get(highlight_path, [])
        fig.add_shape(type="rect", x0=props['pos'][0]-1.2, y0=props['pos'][1]-0.5, x1=props['pos'][0]+1.2, y1=props['pos'][1]+0.5, fillcolor=props['color'], line=dict(color='black', width=3 if is_highlighted else 1))
        fig.add_annotation(x=props['pos'][0], y=props['pos'][1], text=f"{props['label']}", showarrow=False, font_color="white", font_size=11)
    fig.update_layout(title_text="<b>Pathway: Japan (MHLW/PMDA)</b>", xaxis=dict(visible=False, range=[-2, 14]), yaxis=dict(visible=False, range=[0, 9]), height=500, plot_bgcolor='#F0F2F6', margin=dict(l=20, r=20, t=50, b=20))
    return fig

@st.cache_data
def plot_pha_matrix(pha_data, project_type):
    """Generates a professional-grade Preliminary Hazard Analysis (PHA) risk matrix."""
    df = pd.DataFrame(pha_data)
    if df.empty:
        return go.Figure().update_layout(title_text="No PHA data available for this scenario.")
        
    fig = go.Figure()
    # Risk regions defined for a 5x4 matrix
    severity_levels = {'Negligible': 1, 'Minor': 2, 'Serious': 3, 'Critical': 4, 'Catastrophic': 5}
    likelihood_levels = {'Improbable': 1, 'Remote': 2, 'Occasional': 3, 'Frequent': 4}

    # Define risk regions with explicit coordinates for clarity
    # High Risk (Red)
    fig.add_shape(type="rect", x0=3.5, y0=2.5, x1=5.5, y1=4.5, fillcolor='rgba(239, 83, 80, 0.3)', line_width=0, layer='below')
    # Medium Risk (Yellow)
    fig.add_shape(type="rect", x0=0.5, y0=2.5, x1=3.5, y1=4.5, fillcolor='rgba(255, 193, 7, 0.2)', line_width=0, layer='below')
    fig.add_shape(type="rect", x0=3.5, y0=0.5, x1=5.5, y1=2.5, fillcolor='rgba(255, 193, 7, 0.2)', line_width=0, layer='below')
    # Low Risk (Green)
    fig.add_shape(type="rect", x0=0.5, y0=0.5, x1=3.5, y1=2.5, fillcolor='rgba(44, 160, 44, 0.2)', line_width=0, layer='below')

    fig.add_trace(go.Scatter(
        x=df['Severity'], y=df['Likelihood'], mode='markers+text',
        text=df.index + 1, textposition='top center',
        marker=dict(color=PRIMARY_COLOR, size=20, symbol='diamond'),
        hovertext='<b>Hazard:</b> ' + df['Hazard'] + '<br><b>Severity:</b> ' + df['Severity'].astype(str) + '<br><b>Likelihood:</b> ' + df['Likelihood'].astype(str),
        hoverinfo='text'
    ))
    
    fig.update_layout(
        title=f'<b>Preliminary Hazard Analysis (PHA) Risk Matrix for {project_type}</b>',
        xaxis_title='<b>Severity of Harm</b>', yaxis_title='<b>Likelihood of Occurrence</b>',
        xaxis=dict(tickvals=list(severity_levels.values()), ticktext=list(severity_levels.keys()), range=[0.5, 5.5]),
        yaxis=dict(tickvals=list(likelihood_levels.values()), ticktext=list(likelihood_levels.keys()), range=[0.5, 4.5]),
        margin=dict(l=40, r=40, b=40, t=60)
    )
    return fig

@st.cache_data
def plot_fmea_dashboard(fmea_df, project_type):
    """
    Generates a professional-grade FMEA dashboard with a Risk Matrix and Pareto Chart from a dataframe.
    This version is corrected to use a standard 1-10 scale for S, O, and D.
    """
    df = fmea_df.copy()
    if df.empty:
        return go.Figure().update_layout(title_text="No FMEA data."), go.Figure().update_layout(title_text="No FMEA data.")
        
    df['RPN_Initial'] = df['S'] * df['O_Initial'] * df['D_Initial']
    df['RPN_Final'] = df['S'] * df['O_Final'] * df['D_Final']
    
    # --- 1. Risk Matrix (S vs O) on a 10x10 grid ---
    fig_matrix = go.Figure()
    # Define risk regions for a 10x10 grid
    # High Risk (Red)
    fig_matrix.add_shape(type="rect", x0=7.5, y0=7.5, x1=10.5, y1=10.5, fillcolor='rgba(239, 83, 80, 0.3)', line_width=0, layer='below')
    # Medium Risk (Yellow)
    fig_matrix.add_shape(type="rect", x0=0.5, y0=7.5, x1=7.5, y1=10.5, fillcolor='rgba(255, 193, 7, 0.2)', line_width=0, layer='below')
    fig_matrix.add_shape(type="rect", x0=7.5, y0=0.5, x1=10.5, y1=7.5, fillcolor='rgba(255, 193, 7, 0.2)', line_width=0, layer='below')
    # Low Risk (Green)
    fig_matrix.add_shape(type="rect", x0=0.5, y0=0.5, x1=7.5, y1=7.5, fillcolor='rgba(44, 160, 44, 0.2)', line_width=0, layer='below')
    
    fig_matrix.add_trace(go.Scatter(x=df['S'], y=df['O_Initial'], mode='markers+text', text=df.index + 1, textposition='top center',
        marker=dict(color='black', size=15, symbol='circle'), name='Initial Risk', 
        hovertext='<b>Mode:</b> ' + df['Failure Mode'] + '<br><b>S:</b> ' + df['S'].astype(str) + ' | <b>O:</b> ' + df['O_Initial'].astype(str), hoverinfo='text'))
    
    fig_matrix.add_trace(go.Scatter(x=df['S'], y=df['O_Final'], mode='markers+text', text=df.index + 1, textposition='bottom center',
        marker=dict(color=PRIMARY_COLOR, size=15, symbol='diamond-open'), name='Final Risk', 
        hovertext='<b>Mode:</b> ' + df['Failure Mode'] + '<br><b>S:</b> ' + df['S'].astype(str) + ' | <b>O:</b> ' + df['O_Final'].astype(str), hoverinfo='text'))
    
    # Add arrows to show mitigation
    for i in df.index:
        if df.loc[i, 'O_Initial'] != df.loc[i, 'O_Final'] or df.loc[i, 'D_Initial'] != df.loc[i, 'D_Final']:
            fig_matrix.add_annotation(x=df.loc[i, 'S'], y=df.loc[i, 'O_Final'], ax=df.loc[i, 'S'], ay=df.loc[i, 'O_Initial'],
                                      xref='x', yref='y', axref='x', ayref='y', showarrow=True, arrowhead=2, arrowcolor=PRIMARY_COLOR, arrowwidth=2)
    
    fig_matrix.update_layout(title=f'<b>1. Risk Matrix (S vs. O) for {project_type}</b>',
                             xaxis_title='<b>Severity (S)</b>', yaxis_title='<b>Occurrence (O)</b>',
                             xaxis=dict(tickvals=list(range(1,11)), range=[0.5, 10.5]), yaxis=dict(tickvals=list(range(1,11)), range=[0.5, 10.5]),
                             legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))

    # --- 2. RPN Pareto Chart ---
    df_pareto = df[['Failure Mode', 'RPN_Initial', 'RPN_Final']].copy().sort_values('RPN_Initial', ascending=False)
    df_pareto = df_pareto.melt(id_vars='Failure Mode', var_name='Stage', value_name='RPN')
    df_pareto['Stage'] = df_pareto['Stage'].replace({'RPN_Initial': 'Initial', 'RPN_Final': 'Final'})
    
    fig_pareto = px.bar(df_pareto, x='Failure Mode', y='RPN', color='Stage', barmode='group',
                        title='<b>2. Risk Priority Number (RPN) Pareto</b>',
                        labels={'Failure Mode': 'Potential Failure Mode', 'RPN': 'Risk Priority Number (S x O x D)'},
                        color_discrete_map={'Initial': 'grey', 'Final': PRIMARY_COLOR})
    
    # Update action threshold for a 1-10 scale (max RPN = 1000)
    action_threshold = 100
    fig_pareto.add_hline(y=action_threshold, line_dash="dash", line_color="red", annotation_text="Action Threshold", annotation_position="bottom right")

    return fig_matrix, fig_pareto

@st.cache_data
def plot_fta_diagram(fta_data, project_type):
    """Generates a professional-grade Fault Tree Analysis (FTA) diagram from a data structure."""
    fig = go.Figure()
    nodes, links = fta_data['nodes'], fta_data['links']
    
    # Draw links first to be in the background
    for link in links:
        fig.add_trace(go.Scatter(x=[nodes[link[0]]['pos'][0], nodes[link[1]]['pos'][0]], 
                                 y=[nodes[link[0]]['pos'][1], nodes[link[1]]['pos'][1]], 
                                 mode='lines', line=dict(color='grey', width=2)))

    # Prepare node data for plotting
    node_x, node_y, node_text, node_color, node_symbols, node_size = [], [], [], [], [], []
    for name, attrs in nodes.items():
        node_x.append(attrs['pos'][0])
        node_y.append(attrs['pos'][1])
        full_label = f"<b>{attrs['label']}</b>" + (f"<br>P={attrs['prob']:.4f}" if attrs.get('prob') is not None else "")
        node_text.append(full_label)
        node_symbols.append(attrs['shape'])
        
        if attrs.get('type') == 'top':
            node_color.append('salmon')
            node_size.append(70)
        elif attrs.get('type') == 'gate':
            node_color.append('lightgrey')
            node_size.append(50)
        else: # Basic Event
            node_color.append('skyblue')
            node_size.append(70)
            
    fig.add_trace(go.Scatter(
        x=node_x, y=node_y, text=node_text, mode='markers+text', textposition="middle center",
        marker=dict(size=node_size, symbol=node_symbols, color=node_color, line=dict(width=2, color='black')),
        textfont=dict(size=10, color='black'),
        hoverinfo='text'
    ))
    
    fig.update_layout(
        title=f"<b>Fault Tree Analysis (FTA) for '{fta_data['title']}'</b>",
        xaxis=dict(visible=False, range=[-0.1, 1.1]), 
        yaxis=dict(visible=False, range=[-0.1, 1.1]), 
        showlegend=False,
        margin=dict(l=20, r=20, b=20, t=60)
    )
    return fig

@st.cache_data
def plot_eta_diagram(eta_data, project_type):
    """Generates a professional-grade Event Tree Analysis (ETA) diagram."""
    fig = go.Figure()
    nodes, paths = eta_data['nodes'], eta_data['paths']
    
    # Draw paths
    for path in paths:
        fig.add_trace(go.Scatter(x=path['x'], y=path['y'], mode='lines', line=dict(color=path['color'], width=2, dash=path.get('dash', 'solid'))))

    # Draw nodes and barriers
    node_x, node_y, node_text = [], [], []
    for name, attrs in nodes.items():
        node_x.append(attrs['pos'][0])
        node_y.append(attrs['pos'][1])
        node_text.append(f"<b>{attrs['label']}</b>")
        # Add probability annotations for barriers
        if 'prob_success' in attrs:
            fig.add_annotation(x=attrs['pos'][0], y=attrs['pos'][1], text=f"Success<br>P={attrs['prob_success']}", showarrow=False, yshift=15, font=dict(size=9))
            fig.add_annotation(x=attrs['pos'][0], y=attrs['pos'][1], text=f"Failure<br>P={1-attrs['prob_success']}", showarrow=False, yshift=-15, font=dict(size=9))

    fig.add_trace(go.Scatter(x=node_x, y=node_y, text=node_text, mode='markers+text', textposition="top center",
                             marker=dict(color='skyblue', size=15, line=dict(width=2, color='black')),
                             textfont=dict(size=10, color='black'), hoverinfo='text'))
    
    # Add outcomes
    for name, attrs in eta_data['outcomes'].items():
         fig.add_annotation(x=attrs['pos'][0], y=attrs['pos'][1],
                           text=f"<b>{name}</b><br>P={attrs['prob']:.5f}",
                           showarrow=False, bgcolor=attrs['color'], borderpad=4, font=dict(color='white'))

    fig.update_layout(
        title=f"<b>Event Tree Analysis (ETA) for '{eta_data['title']}'</b>",
        xaxis=dict(visible=False), yaxis=dict(visible=False),
        showlegend=False, margin=dict(l=20, r=20, b=20, t=60)
    )
    return fig
    
@st.cache_data
def plot_vmp_flow(project_type):
    """
    Generates a dynamic V-Model or flowchart for a selected validation project type,
    highlighting key tools from the toolkit for each stage.
    """
    plans = {
        "Analytical Method Validation": {
            'title': "V-Model for Analytical Method Validation",
            'stages': {
                'URS': {'name': 'Assay Requirements', 'tools': 'ATP Builder'},
                'FS': {'name': 'Performance Specs', 'tools': 'Core Validation, LOD & LOQ'},
                'DS': {'name': 'Method Design', 'tools': 'Mixture Design, DOE'},
                'BUILD': {'name': 'Method Development', 'tools': 'Linearity, 4PL Regression'},
                'IQ/OQ': {'name': 'Reagent & Inst. Qual', 'tools': 'Gage R&R / VCA'},
                'PQ': {'name': 'Method Performance Qual', 'tools': 'Comprehensive Diagnostic Validation'},
                'VAL': {'name': 'Final Validation Report', 'tools': 'Method Comparison, Equivalence'}
            }
        },
        "Instrument Qualification": {
            'title': "V-Model for Instrument Qualification",
            'stages': {
                'URS': {'name': 'User Needs (URS)', 'tools': 'ATP Builder'},
                'FS': {'name': 'Functional Specs (FS)', 'tools': 'Quality Risk Management (FMEA)'},
                'DS': {'name': 'Vendor Selection/Config', 'tools': ''},
                'BUILD': {'name': 'Purchase & Install', 'tools': ''},
                'IQ/OQ': {'name': 'IQ & OQ Execution', 'tools': 'Attribute Agreement Analysis'},
                'PQ': {'name': 'Performance Qualification', 'tools': 'Gage R&R, Process Stability (SPC)'},
                'VAL': {'name': 'Final IQ/OQ/PQ Report', 'tools': ''}
            }
        },
        "Pharma Process (PPQ)": {
            'title': "Workflow for Process Performance Qualification",
            'stages': {
                'PLAN': {'name': 'PPQ Protocol', 'tools': 'FMEA, Sample Size for Qualification'},
                'EXEC': {'name': 'Execute PPQ Runs', 'tools': 'Process Stability (SPC)'},
                'EVAL': {'name': 'Assess Capability', 'tools': 'Process Capability (Cpk), Tolerance Intervals'},
                'REPORT': {'name': 'Final PPQ Report', 'tools': 'Statistical Equivalence for Transfer'}
            },
            'type': 'flowchart'
        },
        "Software System (CSV)": {
            'title': "V-Model for GxP Software Validation",
            'stages': {
                'URS': {'name': 'User Requirements', 'tools': 'RTM Builder'},
                'FS': {'name': 'Functional Specs', 'tools': 'FMEA'},
                'DS': {'name': 'Design Specs', 'tools': 'Advanced AI Concepts'},
                'BUILD': {'name': 'Coding & Configuration', 'tools': 'Explainable AI (XAI)'},
                'IQ/OQ': {'name': 'Installation & Unit Test', 'tools': 'Anomaly Detection'},
                'PQ': {'name': 'Performance Testing (UAT)', 'tools': 'Predictive QC (Classification)'},
                'VAL': {'name': 'Final Validation Summary', 'tools': 'Clustering'}
            }
        }
    }
    
    plan = plans[project_type]
    fig = go.Figure()

    if plan.get('type') == 'flowchart':
        keys = list(plan['stages'].keys())
        for i, key in enumerate(keys):
            stage = plan['stages'][key]
            fig.add_shape(type="rect", x0=i*2, y0=0.4, x1=i*2+1.5, y1=0.6, fillcolor=PRIMARY_COLOR, line=dict(color='black'))
            fig.add_annotation(x=i*2+0.75, y=0.5, text=f"<b>{stage['name']}</b>", font=dict(color='white'), showarrow=False)
            fig.add_annotation(x=i*2+0.75, y=0.3, text=f"<i>Key Tools:<br>{stage['tools']}</i>", showarrow=False)
            if i < len(keys) - 1:
                fig.add_annotation(x=i*2+1.75, y=0.5, text="‚ñ∫", font=dict(size=30, color='grey'), showarrow=False)
        fig.update_layout(title=f'<b>{plan["title"]}</b>', xaxis=dict(visible=False), yaxis=dict(visible=False, range=[0,1]))
    else: # V-Model
        v_model_stages = plan['stages']
        path_keys = list(v_model_stages.keys())
        path_x = [0, 1, 2, 3, 4, 5, 6]
        path_y = [5, 4, 3, 2, 3, 4, 5]
        fig.add_trace(go.Scatter(x=path_x, y=path_y, mode='lines', line=dict(color='lightgrey', width=5), hoverinfo='none'))
        for i in range(3):
            fig.add_shape(type="line", x0=path_x[i], y0=path_y[i], x1=path_x[-(i+1)], y1=path_y[-(i+1)], line=dict(color="darkgrey", width=2, dash="dot"))
        for i, key in enumerate(path_keys):
            stage = v_model_stages[key]
            fig.add_shape(type="rect", x0=path_x[i]-0.6, y0=path_y[i]-0.4, x1=path_x[i]+0.6, y1=path_y[i]+0.4, fillcolor=PRIMARY_COLOR, line=dict(color="black"))
            fig.add_annotation(x=path_x[i], y=path_y[i], text=f"<b>{stage['name']}</b>", font=dict(color='white'), showarrow=False)
            fig.add_annotation(x=path_x[i], y=path_y[i]-0.6, text=f"<i>Key Tool: {stage['tools']}</i>" if stage['tools'] else "", showarrow=False)
        fig.update_layout(title=f'<b>{plan["title"]}</b>', xaxis=dict(visible=False), yaxis=dict(visible=False, range=[1, 6]))
        
    fig.update_layout(height=500)
    return fig

@st.cache_data
def plot_rtm_sankey(completed_streams):
    """
    Generates a professional-grade, multi-stream Sankey diagram simulating a complex tech transfer project.
    """
    # Master data structure for the entire project
    nodes_data = {
        # ID: [Label, Stream, x-pos]
        'GOAL': ["Project Goal:<br>Successful Tech Transfer", "Project", 0],
        'PROC-CQA': ["CQA: Purity > 99%", "Process", 1],
        'PROC-CPP': ["CPP: Column Load", "Process", 2],
        'PROC-TEST': ["PPQ Run Results", "Process", 3],
        'ASSAY-ATP': ["ATP: Accuracy 98-102%", "Assay", 1],
        'ASSAY-VAL': ["Assay Validation Report", "Assay", 2],
        'INST-URS': ["URS: HPLC Throughput", "Instrument", 1],
        'INST-OQ': ["Instrument OQ", "Instrument", 2],
        'INST-PQ': ["Instrument PQ", "Instrument", 3],
        'SOFT-URS': ["URS: LIMS Data Integrity", "Software", 1],
        'SOFT-FS': ["FS: Audit Trail Spec", "Software", 2],
        'SOFT-TEST': ["CSV Test Scripts", "Software", 3],
    }
    
    links_data = [
        # Source, Target, Stream
        ('GOAL', 'PROC-CQA', 'Process'),
        ('PROC-CQA', 'PROC-CPP', 'Process'),
        ('PROC-CPP', 'PROC-TEST', 'Process'),
        ('PROC-CQA', 'ASSAY-ATP', 'Cross-Stream'), # Dependency: Process CQA requires an assay
        ('ASSAY-ATP', 'ASSAY-VAL', 'Assay'),
        ('ASSAY-VAL', 'PROC-TEST', 'Cross-Stream'), # Dependency: Process test relies on validated assay
        ('ASSAY-VAL', 'INST-URS', 'Cross-Stream'), # Dependency: Assay validation requires a qualified instrument
        ('INST-URS', 'INST-OQ', 'Instrument'),
        ('INST-OQ', 'INST-PQ', 'Instrument'),
        ('PROC-TEST', 'SOFT-URS', 'Cross-Stream'), # Dependency: Process results must be stored in LIMS
        ('SOFT-URS', 'SOFT-FS', 'Software'),
        ('SOFT-FS', 'SOFT-TEST', 'Software')
    ]

    all_nodes_ids = list(nodes_data.keys())
    all_nodes_labels = [v[0] for v in nodes_data.values()]
    
    # --- Interactivity Logic ---
    link_colors = []
    for source, target, stream in links_data:
        if stream in completed_streams or nodes_data[source][1] in completed_streams:
            link_colors.append(SUCCESS_GREEN)
        else:
            link_colors.append('rgba(200,200,200,0.5)')

    node_colors = []
    for node_id, values in nodes_data.items():
        if values[1] in completed_streams:
            node_colors.append(SUCCESS_GREEN)
        else:
            node_colors.append(PRIMARY_COLOR)

    fig = go.Figure(data=[go.Sankey(
        arrangement='snap',
        node=dict(
            pad=25, thickness=20, line=dict(color="black", width=0.5),
            label=[k for k in all_nodes_ids], # Use short IDs
            hovertemplate='%{customdata}<extra></extra>',
            customdata=all_nodes_labels, # Full labels on hover
            color=node_colors,
            x=[v[2] / 3 for v in nodes_data.values()], # Force horizontal position for swimlanes
            y=[['Project', 'Process', 'Assay', 'Instrument', 'Software'].index(v[1]) * 0.22 for v in nodes_data.values()]
        ),
        link=dict(
            source=[all_nodes_ids.index(l[0]) for l in links_data],
            target=[all_nodes_ids.index(l[1]) for l in links_data],
            value=[1]*len(links_data),
            color=link_colors
        )
    )])
    
    fig.update_layout(
        title_text="<b>Integrated RTM for Technology Transfer Project</b>", font_size=12, height=600
    )
    return fig, links_data, nodes_data

# ==============================================================================
# REVISED HELPER & PLOTTING FUNCTION (Design Controls & DHF)
# ==============================================================================
@st.cache_data
def plot_design_controls_flow(completed_stages, triggered_change=None):
    """
    Generates an interactive flowchart of the complete Design Controls process,
    highlighting completed stages and visually showing change control feedback loops.
    """
    fig = go.Figure()

    stages = {
        'UserNeeds':    {'label': 'User Needs', 'pos': (0, 6), 'docs': 'User Requirement Specification (URS)'},
        'Inputs':       {'label': 'Design Inputs', 'pos': (2, 6), 'docs': 'Functional & Performance Specs (FS)'},
        'Process':      {'label': 'Design Process', 'pos': (4, 6), 'docs': 'Development Plans, Phase Reviews'},
        'Outputs':      {'label': 'Design Outputs', 'pos': (6, 6), 'docs': 'Specifications, Drawings, Source Code'},
        'Device':       {'label': 'Product/System', 'pos': (8, 6), 'docs': 'Prototypes, Finished Device/Assay/Process'},
        'Review':       {'label': 'Design Review', 'pos': (4, 7.5), 'docs': 'Meeting Minutes, Action Items'},
        'Verification': {'label': 'Design Verification', 'pos': (6, 4), 'docs': 'V&V Protocols & Reports (Unit, IQ, OQ)'},
        'Validation':   {'label': 'Design Validation', 'pos': (2, 2), 'docs': 'V&V Protocols & Reports (PQ, UAT)'},
        'Transfer':     {'label': 'Design Transfer', 'pos': (8, 4), 'docs': 'Manufacturing SOPs, QC Test Methods'},
        'DHF':          {'label': 'Design History File', 'pos': (4, 0.5), 'docs': 'The complete compilation of all evidence.'}
    }
    edges = [('UserNeeds', 'Inputs'), ('Inputs', 'Process'), ('Process', 'Outputs'), ('Outputs', 'Device'),
             ('Outputs', 'Verification'), ('Verification', 'Inputs'), ('Device', 'Validation'), 
             ('Validation', 'UserNeeds'), ('Review', 'Inputs'), ('Review', 'Process'), 
             ('Review', 'Outputs'), ('Outputs', 'Transfer'), ('Device', 'Transfer')]

    # Draw standard edges first
    for start, end in edges:
        fig.add_trace(go.Scatter(x=[stages[start]['pos'][0], stages[end]['pos'][0]],
                                 y=[stages[start]['pos'][1], stages[end]['pos'][1]],
                                 mode='lines', line=dict(color="grey", width=1.5), hoverinfo='none'))

    # Draw nodes
    for name, props in stages.items():
        is_complete = name in completed_stages
        color = SUCCESS_GREEN if is_complete else DARK_GREY
        font_color = 'white'
        if name == 'DHF': color = PRIMARY_COLOR
        
        fig.add_shape(type="rect", x0=props['pos'][0]-1, y0=props['pos'][1]-0.4,
                      x1=props['pos'][0]+1, y1=props['pos'][1]+0.4,
                      line=dict(color="black", width=2), fillcolor=color)
        fig.add_annotation(x=props['pos'][0], y=props['pos'][1], text=f"<b>{props['label'].replace(' ', '<br>')}</b>",
                           showarrow=False, font=dict(color=font_color, size=11))

    # --- NEW: Visually represent the active change control loop ---
    if triggered_change:
        start_node, end_node, label = triggered_change
        fig.add_annotation(
            x=stages[end_node]['pos'][0], y=stages[end_node]['pos'][1],
            ax=stages[start_node]['pos'][0], ay=stages[start_node]['pos'][1],
            xref='x', yref='y', axref='x', ayref='y',
            showarrow=True, arrowhead=3, arrowwidth=4, arrowcolor='red',
            text=f'<b>{label}</b>', font=dict(color='white'), bgcolor='red', borderpad=4
        )

    fig.update_layout(
        title_text="<b>The Design Controls Waterfall & DHF Assembly</b>",
        xaxis=dict(visible=False, range=[-1.5, 9.5]),
        yaxis=dict(visible=False, range=[0, 8.5]),
        height=600, margin=dict(l=20, r=20, t=50, b=20)
    )
    return fig, stages

# ==============================================================================
# REVISED HELPER & PLOTTING FUNCTION (FAT/SAT & Qualification)
# ==============================================================================
@st.cache_data
def plot_cq_workflow(executed_strategy):
    """
    Generates an interactive table comparing the full C&Q workflow based on the executed strategy.
    """
    # Define the full set of test activities
    plan = {
        'Test Category': [
            'Documentation Review', 'Physical & Installation Checks', 
            'Core Functional Tests', 'Safety Interlock Tests',
            'Performance & Throughput', 'Robustness & Error Handling'
        ],
        'FAT (At Vendor)': [
            'Review & redline all docs (manuals, drawings, certs)', 'Verify P&ID, BOM, physical dimensions match spec',
            'Test 100% of core functions per FS', 'Verify 100% of alarms and emergency stops',
            'Run at max capacity for a defined period', 'Test edge cases and invalid user inputs'
        ]
    }

    # Define the three possible outcomes for the rest of the workflow
    scenarios = {
        "Comprehensive": {
            'sat': [
                'Confirm receipt of all *finalized* documents', 'Verify physical integrity post-shipping only',
                'Spot check 2-3 critical functions to confirm no damage', 'Verify 1-2 critical interlocks (e.g., E-stop)',
                'Confirm performance settings are available', 'Execute User Acceptance Testing (UAT) with real users'
            ],
            'iqoq': [
                'Leverage FAT report; verify final docs are in QMS', 'Leverage FAT report for BOM/P&ID check; verify utilities',
                'Execute reduced OQ protocol; focus on site-specific factors', 'Leverage FAT report; test site-integrated interlocks',
                'Execute full Performance Qualification (PQ) protocol', 'PQ challenge testing with worst-case conditions'
            ]
        },
        "Standard": {
            'sat': [
                'Review final docs & confirm key certs', 'Verify physical integrity & critical dimensions',
                'Spot check ~25% of core functions', 'Spot check ~50% of safety systems',
                'Initial performance check before formal PQ', 'Execute User Acceptance Testing (UAT)'
            ],
            'iqoq': [
                'Execute reduced IQ protocol for documentation', 'Execute full IQ for installation & utilities',
                'Execute risk-based OQ protocol (~50% re-test)', 'Leverage FAT; re-test all critical interlocks',
                'Execute full Performance Qualification (PQ) protocol', 'PQ challenge testing'
            ]
        },
        "Minimal": {
            'sat': [
                'Perform full review of all documents on-site', 'Perform full IQ component and installation checks',
                'Perform full Operational Qualification (OQ) dry run', 'Perform full OQ of all safety systems',
                'Initial performance check before formal PQ', 'Execute User Acceptance Testing (UAT)'
            ],
            'iqoq': [
                'Execute full IQ protocol from scratch', 'Execute full IQ protocol from scratch',
                'Execute full OQ protocol from scratch', 'Execute full OQ protocol from scratch',
                'Execute full Performance Qualification (PQ) protocol', 'PQ challenge testing'
            ]
        }
    }
    
    df = pd.DataFrame(plan)
    df['SAT (At User Site)'] = scenarios[executed_strategy]['sat']
    df['IQ/OQ (Formal Qualification)'] = scenarios[executed_strategy]['iqoq']

    # Create a styled Plotly Table
    def get_cell_style(value):
        if 'Leverage' in value: return 'rgba(44, 160, 44, 0.2)' # Green
        if 'Full' in value or 'scratch' in value: return 'rgba(239, 83, 80, 0.2)' # Red
        if 'Spot check' in value or 'reduced' in value: return 'rgba(255, 193, 7, 0.2)' # Yellow
        return 'white'
    
    sat_colors = [get_cell_style(val) for val in df['SAT (At User Site)']]
    iqoq_colors = [get_cell_style(val) for val in df['IQ/OQ (Formal Qualification)']]

    fig = go.Figure(data=[go.Table(
        columnwidth = [120, 200, 200, 200],
        header=dict(values=[f'<b>{col}</b>' for col in df.columns],
                    fill_color=PRIMARY_COLOR, font=dict(color='white', size=14),
                    align='left', height=40),
        cells=dict(values=[df[col] for col in df.columns],
                   fill_color=[['#F8F9FA']*len(df), ['#F8F9FA']*len(df), sat_colors, iqoq_colors],
                   align='left', font_size=12, height=60, line_color='darkslategray')
    )])
    
    fig.update_layout(title_text="<b>Commissioning & Qualification (C&Q) Workflow</b>", margin=dict(l=10, r=10, t=50, b=10))
    return fig
    
@st.cache_data
def plot_dfx_dashboard(project_type, mfg_effort, quality_effort, sustainability_effort, ux_effort):
    """
    Generates a professional-grade DfX dashboard with a Performance Radar Chart and a Cost Structure Pie Chart comparison.
    """
    profiles = {
        "Pharma Assay (ELISA)": {
            'categories': ['Robustness', 'Run Time (hrs)', 'Reagent Cost (RCU)', 'Precision (%CV)', 'Ease of Use'], 'baseline': [5, 4.0, 25.0, 18.0, 5], 'direction': [1, -1, -1, -1, 1], 'reliability_idx': 0,
            'impact': {'mfg': [0.1, -0.1, -0.2, 0, 0.1], 'quality': [0.5, -0.05, 0, -0.6, 0.2], 'sustainability': [0, 0, -0.3, 0, 0], 'ux': [0.1, -0.2, 0, 0, 0.7]}
        },
        "Instrument (Liquid Handler)": {
            'categories': ['Throughput<br>(plates/hr)', 'Uptime (%)', 'Footprint (m¬≤)', 'Service Cost<br>(RCU/yr)', 'Precision (%CV)'], 'baseline': [20, 95.0, 2.5, 5000, 5.0], 'direction': [1, 1, -1, -1, -1], 'reliability_idx': 1,
            'impact': {'mfg': [0.2, 0.1, -0.2, -0.1, 0], 'quality': [0.1, 0.8, 0, -0.2, -0.6], 'sustainability': [0, 0.1, -0.1, -0.4, 0], 'ux': [0, 0.2, 0, -0.6, 0]}
        },
        "Software (LIMS)": {
            'categories': ['Performance<br>(Query Time s)', 'Scalability<br>(Users)', 'Reliability<br>(Uptime %)', 'Compliance<br>Score', 'Dev Cost (RCU)'], 'baseline': [8.0, 100, 99.5, 6, 500], 'direction': [-1, 1, 1, 1, -1], 'reliability_idx': 2,
            'impact': {'mfg': [-0.1, 0.2, 0.2, 0, -0.4], 'quality': [-0.2, 0.1, 0.7, 0.8, 0.2], 'sustainability': [0, 0.5, 0.1, 0, -0.1], 'ux': [-0.4, 0.2, 0, 0.5, 0.1]}
        },
        "Pharma Process (MAb)": {
            'categories': ['Yield (g/L)', 'Cycle Time<br>(days)', 'COGS (RCU/g)', 'Purity (%)', 'Robustness<br>(PAR Size)'], 'baseline': [3.0, 18, 100, 98.5, 5], 'direction': [1, -1, -1, 1, 1], 'reliability_idx': 4,
            'impact': {'mfg': [0.3, -0.2, -0.4, 0.1, 0.2], 'quality': [0.1, 0, -0.1, 0.6, 0.8], 'sustainability': [0.05, -0.1, -0.2, 0, 0.1], 'ux': [0, 0, 0, 0, 0]}
        }
    }
    profile = profiles[project_type]
    efforts = {'mfg': mfg_effort, 'quality': quality_effort, 'sustainability': sustainability_effort, 'ux': ux_effort}
    
    optimized_kpis = profile['baseline'].copy()
    for i in range(len(profile['categories'])):
        total_impact = sum(efforts[k] * profile['impact'][k][i] for k in efforts)
        optimized_kpis[i] += total_impact * (1 if profile['direction'][i] == 1 else -1)
        if '%' in profile['categories'][i]: optimized_kpis[i] = np.clip(optimized_kpis[i], 0, 100)
        if 'Score' in profile['categories'][i]: optimized_kpis[i] = np.clip(optimized_kpis[i], 0, 10)
        
    kpis = {'baseline': profile['baseline'], 'optimized': optimized_kpis}

    # Simplified but more realistic Cost Model
    def get_cost_breakdown(kpis, p_type):
        if p_type == "Pharma Assay (ELISA)": return [kpis[2], 10, 5, kpis[1]*2]
        elif p_type == "Instrument (Liquid Handler)": return [15000, 2000, kpis[3], 1000 * (5/kpis[4])]
        elif p_type == "Software (LIMS)": return [kpis[4], 150, 100, 50]
        elif p_type == "Pharma Process (MAb)": return [kpis[2]*0.4, kpis[2]*0.3, kpis[2]*0.2, kpis[2]*0.1]
        return [1,1,1,1]

    cost_labels = ['Core (Material/Dev)', 'Manufacturing/Labor', 'Service/Validation', 'QC/Consumables']
    base_costs = get_cost_breakdown(profile['baseline'], project_type)
    optimized_costs = get_cost_breakdown(optimized_kpis, project_type)
    
    # --- Create Plots ---
    # Plot 1: Radar Chart for Performance Profile
    fig_radar = go.Figure()
    fig_radar.add_trace(go.Scatterpolar(r=profile['baseline'], theta=profile['categories'], fill='toself', name='Baseline Design', line=dict(color='grey')))
    fig_radar.add_trace(go.Scatterpolar(r=optimized_kpis, theta=profile['categories'], fill='toself', name='DfX Optimized Design', line=dict(color=SUCCESS_GREEN)))
    fig_radar.update_layout(title="<b>1. Project Performance Profile</b>", legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))

    # Plot 2: Cost Structure Pie Charts
    fig_cost = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]], subplot_titles=['<b>Baseline Cost Structure</b>', '<b>Optimized Cost Structure</b>'])
    fig_cost.add_trace(go.Pie(labels=cost_labels, values=base_costs, name="Base", marker_colors=['#636EFA', '#EF553B', '#00CC96', '#AB63FA']), 1, 1)
    fig_cost.add_trace(go.Pie(labels=cost_labels, values=optimized_costs, name="Optimized", marker_colors=['#636EFA', '#EF553B', '#00CC96', '#AB63FA']), 1, 2)
    fig_cost.update_traces(hole=.4, hoverinfo="label+percent+value", textinfo='percent', textfont_size=14)
    fig_cost.update_layout(title_text="<b>2. Lifecycle Cost Breakdown (Total RCU)</b>", showlegend=False, annotations=[
        dict(text=f'{sum(base_costs):,.0f} RCU', x=0.18, y=0.5, font_size=20, showarrow=False),
        dict(text=f'{sum(optimized_costs):,.0f} RCU', x=0.82, y=0.5, font_size=20, showarrow=False)
    ])
    
    return fig_radar, fig_cost, kpis, profile['categories'], base_costs, optimized_costs

@st.cache_data
def plot_gap_analysis_radar(current_scores):
    """
    Generates an interactive radar chart for Gap Analysis.
    """
    categories = list(current_scores.keys())
    required_scores = [10] * len(categories) # Required state is always the ideal (10)

    fig = go.Figure()

    fig.add_trace(go.Scatterpolar(
        r=list(current_scores.values()),
        theta=categories,
        fill='toself',
        name='Current State',
        line=dict(color=PRIMARY_COLOR),
        fillcolor='rgba(0, 104, 201, 0.4)'
    ))
    
    fig.add_trace(go.Scatterpolar(
        r=required_scores,
        theta=categories,
        fill='none',
        name='Required State',
        line=dict(color=SUCCESS_GREEN, dash='dash')
    ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 10]
            )
        ),
        title="<b>Gap Analysis: Current State vs. Required State</b>",
        legend=dict(yanchor="bottom", y=-0.2, xanchor="center", x=0.5, orientation="h")
    )
    return fig

@st.cache_data
def plot_ishikawa_diagram(problem, causes):
    """
    Generates a professional Ishikawa (Fishbone) Diagram using Plotly.
    """
    fig = go.Figure()
    
    # Main Spine
    fig.add_shape(type='line', x0=0, y0=5, x1=10, y1=5, line=dict(color=PRIMARY_COLOR, width=3))
    
    # Head (Problem Statement)
    fig.add_shape(type='rect', x0=10, y0=4, x1=12, y1=6, fillcolor=PRIMARY_COLOR, line=dict(color=DARK_GREY))
    fig.add_annotation(x=11, y=5, text=f"<b>{problem}</b>", font=dict(color='white', size=14), showarrow=False)

    # Main Bones (Categories)
    bone_positions = [(2, 7, 1), (2, 3, -1), (5, 7, 1), (5, 3, -1), (8, 7, 1), (8, 3, -1)]
    categories = list(causes.keys())
    for i, cat in enumerate(categories):
        x, y, side = bone_positions[i]
        fig.add_shape(type='line', x0=x, y0=5, x1=x+1, y1=y, line=dict(color=DARK_GREY, width=2))
        fig.add_annotation(x=x+1.1, y=y + (side*0.2), text=f"<b>{cat}</b>", font=dict(color=DARK_GREY, size=16), showarrow=False)
        
        # Sub-Bones (Specific Causes)
        for j, cause in enumerate(causes[cat]):
            sub_x = x + 0.5 + (j * 0.4)
            sub_y = 5 + side * (1.5 - (j * 0.4))
            fig.add_shape(type='line', x0=sub_x, y0=sub_y, x1=x+1, y1=y, line=dict(color='grey', width=1))
            fig.add_annotation(x=sub_x - 0.1, y=sub_y + (side*0.1), text=cause, showarrow=False, xanchor='right' if side==1 else 'left')
            
    fig.update_layout(
        title="<b>Ishikawa (Fishbone) Diagram for Root Cause Analysis</b>",
        xaxis=dict(visible=False, range=[-1, 13]),
        yaxis=dict(visible=False, range=[0, 10]),
        showlegend=False,
        height=600
    )
    return fig


#==================================================================ACT 0 END ==============================================================================================================================
#==========================================================================================================================================================================================================
# ======================================= 3 files for EDA, first load datasets follows =================
def apply_gadget_transformations(df, noise_level, missing_pct, outlier_magnitude, categorical_effect):
    """Takes a DataFrame and applies transformations based on user-controlled gadgets."""
    df_mod = df.copy()
    numeric_cols = df_mod.select_dtypes(include=np.number).columns

    # 1. Add Random Noise
    if noise_level > 0:
        for col in numeric_cols:
            noise = np.random.normal(0, df_mod[col].std() * noise_level / 10, len(df_mod))
            df_mod[col] += noise

    # 2. Inject Categorical Effect
    if 'Raw_Material_Lot' in df_mod.columns and 'Yield (%)' in df_mod.columns:
        df_mod.loc[df_mod['Raw_Material_Lot'] == 'Lot B', 'Yield (%)'] += categorical_effect
    if 'Operator' in df_mod.columns and 'Dispense_Volume (¬µL)' in df_mod.columns:
        df_mod.loc[df_mod['Operator'] == 'Alice', 'Dispense_Volume (¬µL)'] += categorical_effect

    # 3. Inject Outlier
    if outlier_magnitude > 0:
        target_col = 'Yield (%)' if 'Yield (%)' in df_mod.columns else 'Dispense_Volume (¬µL)'
        outlier_idx = df_mod.index[10] # Pick a consistent index for the outlier
        df_mod.loc[outlier_idx, target_col] = df_mod[target_col].mean() + outlier_magnitude * df_mod[target_col].std()

    # 4. Inject Missing Values
    if missing_pct > 0:
        target_col = 'Purity (%)' if 'Purity (%)' in df_mod.columns else 'Pressure (psi)'
        n_missing = int(len(df_mod) * missing_pct / 100)
        # Ensure we don't try to sample more than available rows if df is small
        if n_missing > 0:
            missing_indices = np.random.choice(df_mod.index.drop(outlier_idx, errors='ignore'), n_missing, replace=False)
            df_mod.loc[missing_indices, target_col] = np.nan
        
    return df_mod
    
@st.cache_data
def load_datasets():
    """Generates more realistic sample datasets for EDA."""
    np.random.seed(42)
    # --- Pharma Process Data with more structure ---
    n_pharma = 150
    # Create a base yield that depends non-linearly on pH
    base_yield = 85 + 15 * np.sin((np.random.normal(7.1, 0.15, n_pharma) - 6.8) * np.pi)
    # Add a temperature effect
    temp_effect = (np.random.normal(37, 0.8, n_pharma) - 37) * -1.5
    # Add a categorical lot effect
    lot = np.random.choice(['Lot A', 'Lot B', 'Lot C'], n_pharma, p=[0.5, 0.3, 0.2])
    lot_effect = np.array([0 if l == 'Lot A' else (-5 if l == 'Lot B' else 3) for l in lot])
    # Combine effects with noise
    yield_final = base_yield + temp_effect + lot_effect + np.random.normal(0, 2.5, n_pharma)
    # Purity is inversely related to yield with some noise
    purity = 99.5 - (yield_final / 100) + np.random.normal(0, 0.2, n_pharma)
    
    pharma_data = pd.DataFrame({
        'Yield (%)': np.clip(yield_final, 70, 100),
        'Purity (%)': np.clip(purity, 97, 100),
        'pH': base_yield / 15 + 6.8 + np.random.normal(0, 0.05, n_pharma), # Recreate pH with noise
        'Temperature (¬∞C)': (temp_effect / -1.5) + 37 + np.random.normal(0, 0.1, n_pharma),
        'Raw_Material_Lot': lot
    })
    # Add realistic missing data and an outlier
    pharma_data.loc[np.random.choice(pharma_data.index, 5, replace=False), 'Purity (%)'] = np.nan
    pharma_data.loc[20, 'Yield (%)'] = 55.0

    # --- Instrument Data with more structure ---
    n_inst = 200
    # Simulate wear-and-tear drift over time
    time_drift = np.linspace(0, -2.5, n_inst)
    operator = np.random.choice(['Alice', 'Bob', 'Charlie'], n_inst)
    operator_effect = np.array([0 if o == 'Bob' else (-0.5 if o == 'Alice' else 0.3) for o in operator])
    dispense_volume = 50 + time_drift + operator_effect + np.random.normal(0, 0.4, n_inst)
    pressure = 20 + (dispense_volume - 50) * 0.8 + np.random.normal(0, 0.5, n_inst)
    
    instrument_data = pd.DataFrame({
        'Dispense_Volume (¬µL)': dispense_volume,
        'Pressure (psi)': pressure,
        'Run_Time (sec)': np.random.gamma(20, 2, n_inst),
        'Operator': operator
    })
    instrument_data.loc[150, 'Pressure (psi)'] = 45 # Outlier
        
    return {
        "Pharma Manufacturing Process": pharma_data,
        "Instrument Performance Data": instrument_data
    }

@st.cache_data
def plot_eda_dashboard(df, numeric_cols, cat_cols, corr_method='pearson'):
    """
    Generates a professional-grade, multi-part EDA dashboard from a dataframe.
    This version includes richer plots and dedicated categorical analysis.
    """
    # --- THIS IS THE FIX: CONVERT TUPLES BACK TO LISTS FOR PANDAS ---
    numeric_cols = list(numeric_cols)
    cat_cols = list(cat_cols)
    # -----------------------------------------------------------------

    figs = {}

    # --- Plot 1: Correlation Heatmap (with method selection) ---
    corr_matrix = df[numeric_cols].corr(method=corr_method)
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    fig_heatmap = px.imshow(corr_matrix.mask(mask), text_auto=".2f", aspect="auto",
                            color_continuous_scale=px.colors.diverging.RdBu,
                            range_color=[-1, 1],
                            title=f"<b>1. {corr_method.capitalize()} Correlation Heatmap</b>")
    fig_heatmap.update_layout(height=500)
    figs['heatmap'] = fig_heatmap

    # --- Plot 2: Scatter Plot Matrix (with regression lines) ---
# --- Plot 2: Scatter Plot Matrix ---
    fig_pairplot = px.scatter_matrix(df, dimensions=numeric_cols,
                                     color=cat_cols[0] if cat_cols else None,
                                     title="<b>2. Scatter Plot Matrix</b>")
    fig_pairplot.update_traces(diagonal_visible=False)
    fig_pairplot.update_layout(height=700)
    figs['pairplot'] = fig_pairplot

    # --- Plot 3: Rich Univariate Distributions ---
    rows = (len(numeric_cols) + 1) // 2
    fig_dist = make_subplots(rows=rows, cols=2,
                             subplot_titles=[f"Distribution of {col}" for col in numeric_cols])
    for i, col in enumerate(numeric_cols):
        row, c = i // 2 + 1, i % 2 + 1
        # Histogram
        fig_dist.add_trace(go.Histogram(x=df[col], name=col, histnorm='probability density',
                                        marker_color='#636EFA', showlegend=False), row=row, col=c)
        # KDE Curve
        try:
            kde = stats.gaussian_kde(df[col].dropna())
            x_range = np.linspace(df[col].min(), df[col].max(), 100)
            fig_dist.add_trace(go.Scatter(x=x_range, y=kde(x_range), mode='lines',
                                          line=dict(color='darkorange', width=3),
                                          name='KDE', showlegend=False), row=row, col=c)
        except Exception:
            pass # KDE can fail if there's no variance
    fig_dist.update_layout(title_text="<b>3. Univariate Distributions (Histogram + KDE)</b>",
                          height=250 * rows, showlegend=False)
    figs['distributions'] = fig_dist
    
    # --- Plot 4: Dedicated Categorical Analysis (NEW) ---
    if cat_cols:
        cat_col = cat_cols[0]
        rows_cat = (len(numeric_cols) + 1) // 2
        fig_cat = make_subplots(rows=rows_cat, cols=2,
                                subplot_titles=[f"{num_col} by {cat_col}" for num_col in numeric_cols])
        for i, num_col in enumerate(numeric_cols):
            row, c = i // 2 + 1, i % 2 + 1
            box_traces = px.box(df, x=cat_col, y=num_col, color=cat_col).data
            for trace in box_traces:
                fig_cat.add_trace(trace, row=row, col=c)
        fig_cat.update_layout(title_text=f"<b>4. Group Analysis by {cat_col}</b>",
                              height=300 * rows_cat, showlegend=False)
        figs['categorical'] = fig_cat
    else:
        # Create an empty figure to avoid a NameError if there are no cat_cols
        figs['categorical'] = go.Figure().update_layout(title_text="<b>4. Group Analysis (No Categorical Variables Found)</b>")

    return figs
    
@st.cache_data
def plot_ci_concept(n=30):
    """
    Generates enhanced, more realistic, and pedagogically sound plots for the
    confidence interval concept module.
    """
    np.random.seed(42)
    pop_mean, pop_std = 100, 15
    
    # --- Plot 1: Population vs. Sampling Distribution ---
    x_range = np.linspace(pop_mean - 4*pop_std, pop_mean + 4*pop_std, 400)
    pop_dist = norm.pdf(x_range, pop_mean, pop_std)
    
    sampling_dist_std = pop_std / np.sqrt(n)
    sampling_dist = norm.pdf(x_range, pop_mean, sampling_dist_std)
    
    fig1 = go.Figure()
    fig1.add_trace(go.Scatter(x=x_range, y=pop_dist, fill='tozeroy', name='Population Distribution (Unseen "Truth")', line=dict(color='skyblue', width=3)))
    fig1.add_trace(go.Scatter(x=x_range, y=sampling_dist, fill='tozeroy', name=f'Sampling Distribution of Mean (n={n})', line=dict(color='orange', width=3)))
    fig1.add_vline(x=pop_mean, line=dict(color='black', dash='dash', width=2), annotation_text="<b>True Population Mean (Œº)</b>", annotation_position="top left")

    # --- Simulation for Plot 2 and Highlight for Plot 1 ---
    n_sims = 1000
    samples = np.random.normal(pop_mean, pop_std, size=(n_sims, n))
    sample_means = samples.mean(axis=1)
    sample_stds = samples.std(axis=1, ddof=1)
    
    t_crit = t.ppf(0.975, df=n-1)
    margin_of_error = t_crit * sample_stds / np.sqrt(n)
    
    ci_lowers = sample_means - margin_of_error
    ci_uppers = sample_means + margin_of_error
    
    capture_mask = (ci_lowers <= pop_mean) & (ci_uppers >= pop_mean)
    capture_count = np.sum(capture_mask)
    avg_width = np.mean(ci_uppers - ci_lowers)
    
    # --- SME ENHANCEMENT: Visually link the two plots ---
    # Select one random sample from our simulation to visualize its origin
    highlight_idx = 15 # A good example that is slightly off-center
    highlight_sample = samples[highlight_idx, :]
    highlight_mean = sample_means[highlight_idx]
    
    # Add the individual data points from this sample to Plot 1
    fig1.add_trace(go.Scatter(
        x=highlight_sample, y=np.zeros_like(highlight_sample),
        mode='markers', name=f'One Sample (n={n})',
        marker=dict(color='darkred', size=8, symbol='line-ns-open', line_width=2)
    ))
    # Add the sample mean from this sample to Plot 1
    fig1.add_trace(go.Scatter(
        x=[highlight_mean], y=[0], mode='markers', name='Sample Mean (xÃÑ)',
        marker=dict(color='red', size=12, symbol='x', line_width=3)
    ))
    fig1.update_layout(title_text=f"<b>The Theory: Where a Sample Comes From (n={n})</b>", showlegend=True,
                      legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.7)'),
                      xaxis_title="Measured Value", yaxis_title="Probability Density")

    # --- Plot 2: CI Simulation ---
    fig2 = go.Figure()
    
    # Plot first 100 CIs for visualization
    n_to_plot = min(n_sims, 100)
    for i in range(n_to_plot):
        is_captured = capture_mask[i]
        color = '#636EFA' if is_captured else '#EF553B' # Streamlit's default blue/red
        is_highlight = (i == highlight_idx)
        
        # Create a hover text for each interval
        hover_text = (f"<b>Experiment #{i}</b><br>"
                      f"Sample Mean: {sample_means[i]:.2f}<br>"
                      f"CI: [{ci_lowers[i]:.2f}, {ci_uppers[i]:.2f}]<br>"
                      f"<b>Captured True Mean? {'Yes' if is_captured else 'No'}</b>")
                      
        fig2.add_trace(go.Scatter(
            x=[ci_lowers[i], ci_uppers[i]], y=[i, i], mode='lines',
            line=dict(color=color, width=4 if is_highlight else 2),
            name=f'CI #{i}', showlegend=False,
            hoverinfo='text', hovertext=hover_text
        ))
        fig2.add_trace(go.Scatter(
            x=[sample_means[i]], y=[i], mode='markers',
            marker=dict(color=color, size=6, symbol='line-ns-open', line_width=2),
            showlegend=False, hoverinfo='none'
        ))

    # Add specific traces for the legend to make them interactive
    fig2.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Captured Mean (Success)', line=dict(color='#636EFA', width=3)))
    fig2.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Missed Mean (Failure)', line=dict(color='#EF553B', width=3)))

    fig2.add_vline(x=pop_mean, line=dict(color='black', dash='dash', width=2), annotation_text="<b>True Population Mean (Œº)</b>", annotation_position="bottom right")
    
    # Highlight the specific interval corresponding to the sample in Plot 1
    fig2.add_annotation(x=ci_uppers[highlight_idx], y=highlight_idx,
                        text="This CI comes<br>from the sample<br>shown above",
                        showarrow=True, arrowhead=2, ax=40, ay=-40, font_size=12,
                        bordercolor="black", borderwidth=1, bgcolor="white")

    fig2.update_layout(title_text=f"<b>The Practice: {n_to_plot} Simulated Experiments</b>", yaxis_visible=False,
                      xaxis_title="Measured Value", showlegend=True, legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.7)'))
    
    return fig1, fig2, capture_count, n_sims, avg_width

@st.cache_data
def plot_core_validation_params(bias_pct=1.5, repeat_cv=1.5, intermed_cv=2.5, interference_effect=8.0):
    """
    Generates enhanced, more realistic dynamic plots for the core validation module.
    """
    # --- 1. Accuracy (Bias) Data ---
    # SME Enhancement: The accuracy simulation now incorporates the precision (random error)
    # parameters, which is more realistic.
    np.random.seed(42)
    true_values = np.array([50, 100, 150])
    df_accuracy_list = []
    
    # Calculate overall bias and recovery for annotation
    all_measured = []
    all_true = []

    for val in true_values:
        # Simulate data with both systematic bias and random error (%CV)
        # Use the intermediate precision CV as it represents the total expected random error
        random_error_sd = val * (intermed_cv / 100)
        measurements = np.random.normal(val * (1 + bias_pct / 100), random_error_sd, 10)
        all_measured.extend(measurements)
        all_true.extend([val] * 10)
        for m in measurements:
            df_accuracy_list.append({'True Value': f'Level {val}', 'Measured Value': m, 'Nominal': val})
    
    df_accuracy = pd.DataFrame(df_accuracy_list)
    
    # Calculate overall %Bias
    overall_bias = (np.mean(all_measured) - np.mean(all_true)) / np.mean(all_true) * 100
    
    fig1 = px.box(df_accuracy, x='True Value', y='Measured Value', 
                  title=f'<b>1. Accuracy (Systematic Error) | Overall Bias: {overall_bias:.2f}%</b>',
                  points='all', color_discrete_sequence=['#1f77b4'])
    
    for val in true_values:
        fig1.add_hline(y=val, line_dash="dash", line_color="black", annotation_text=f"True Value = {val}", annotation_position="bottom right")
    fig1.update_layout(xaxis_title="Reference Material Concentration", yaxis_title="Measured Concentration")

    # --- 2. Precision (Random Error) Data ---
    # SME Enhancement: Simulate a more realistic intermediate precision study with multiple
    # days and analysts to better visualize the components of random error.
    np.random.seed(123)
    n_days, n_analysts_per_day, n_reps = 3, 2, 5
    center_val = 100.0
    
    # Define variance components from CV sliders
    var_repeat = (center_val * repeat_cv / 100)**2
    # Intermediate precision includes repeatability PLUS between-analyst/day variance
    var_between_cond = max(0, (center_val * intermed_cv / 100)**2 - var_repeat)
    
    precision_data = []
    for day in range(1, n_days + 1):
        for analyst in range(1, n_analysts_per_day + 1):
            # Each day/analyst combination has its own small random bias
            condition_bias = np.random.normal(0, np.sqrt(var_between_cond))
            # Measurements within that condition have repeatability error
            measurements = np.random.normal(center_val + condition_bias, np.sqrt(var_repeat), n_reps)
            for m in measurements:
                precision_data.append({'value': m, 'condition': f'Day {day}, Analyst {analyst}'})
    
    df_precision = pd.DataFrame(precision_data)
    
    # Calculate overall precision from the simulated data
    total_cv_calc = df_precision['value'].std() / df_precision['value'].mean() * 100
    
    fig2 = px.box(df_precision, x='condition', y='value', points="all",
                     title=f'<b>2. Intermediate Precision (Random Error) | Overall %CV: {total_cv_calc:.2f}%</b>',
                     labels={'value': 'Measured Value @ 100', 'condition': 'Run Condition (Day, Analyst)'})
    fig2.update_xaxes(tickangle=45)

    # --- 3. Specificity (Interference Error) Data ---
    np.random.seed(2023)
    analyte_signal = np.random.normal(1.0, 0.05, 15)
    matrix_blank_signal = np.random.normal(0.02, 0.01, 15)
    
    # The signal of the combined sample is now controlled by the interference slider
    interfered_signal = analyte_signal * (1 + interference_effect / 100)
    
    # SME Enhancement: Add pass/fail color and KPI to the title
    is_significant_interference = abs(interference_effect) > 5 # Example threshold
    interference_color = '#EF553B' if is_significant_interference else '#00CC96'
    
    df_specificity = pd.DataFrame({
        'Analyte Only': analyte_signal,
        'Matrix Blank': matrix_blank_signal,
        'Analyte + Interferent': interfered_signal
    }).melt(var_name='Sample Type', value_name='Signal Response')

    fig3 = px.box(df_specificity, x='Sample Type', y='Signal Response', points='all',
                  title=f'<b>3. Specificity (Interference Error) | Effect: {interference_effect:.1f}%</b>',
                  color='Sample Type',
                  color_discrete_map={
                      'Analyte Only': '#636EFA',
                      'Matrix Blank': 'grey',
                      'Analyte + Interferent': interference_color
                  })
    fig3.update_layout(xaxis_title="Sample Composition", yaxis_title="Assay Signal (e.g., Absorbance)", showlegend=False)

    return fig1, fig2, fig3

@st.cache_data
def plot_diagnostic_dashboard(sensitivity, specificity, prevalence, n_total=10000):
    """
    Generates a comprehensive, multi-plot dashboard for diagnostic test validation,
    and calculates a full suite of 24 performance metrics.
    """
    # 1. --- Calculate the Confusion Matrix values from inputs ---
    n_diseased = int(n_total * prevalence)
    n_healthy = n_total - n_diseased
    
    tp = int(n_diseased * sensitivity)
    fn = n_diseased - tp
    
    tn = int(n_healthy * specificity)
    fp = n_healthy - tn
    
    # 2. --- Calculate ALL 24 derived metrics systematically ---
    tpr, tnr = (tp / (tp + fn) if (tp + fn) > 0 else 0), (tn / (tn + fp) if (tn + fp) > 0 else 0)
    fpr, fnr = 1 - tnr, 1 - tpr
    ppv, npv = (tp / (tp + fp) if (tp + fp) > 0 else 0), (tn / (tn + fn) if (tn + fn) > 0 else 0)
    fdr, for_val = 1 - ppv, 1 - npv
    accuracy = (tp + tn) / n_total
    f1_score = 2 * (ppv * tpr) / (ppv + tpr) if (ppv + tpr) > 0 else 0
    youdens_j = tpr + tnr - 1
    p_obs = accuracy
    p_exp = ((tp + fp) / n_total) * ((tp + fn) / n_total) + ((fn + tn) / n_total) * ((fp + tn) / n_total)
    kappa = (p_obs - p_exp) / (1 - p_exp) if (1 - p_exp) != 0 else 0
    mcc_denom = np.sqrt(float(tp + fp) * float(tp + fn) * float(tn + fp) * float(tn + fn))
    mcc = (tp * tn - fp * fn) / mcc_denom if mcc_denom > 0 else 0
    lr_plus, lr_minus = (tpr / fpr if fpr > 0 else float('inf')), (fnr / tnr if tnr > 0 else 0)
    
    sens_clipped = np.clip(sensitivity, 0.00001, 0.99999)
    spec_clipped = np.clip(specificity, 0.00001, 0.99999)
    separation = norm.ppf(sens_clipped) + norm.ppf(spec_clipped)
    scores_diseased = np.random.normal(separation, 1, 5000)
    scores_healthy = np.random.normal(0, 1, 5000)
    y_true_roc, y_scores_roc = np.concatenate([np.ones(5000), np.zeros(5000)]), np.concatenate([scores_diseased, scores_healthy])
    fpr_roc, tpr_roc, _ = roc_curve(y_true_roc, y_scores_roc)
    auc_val = auc(fpr_roc, tpr_roc)
    prob_scores = 1 / (1 + np.exp(-y_scores_roc + separation/2))
    log_loss = -np.mean(y_true_roc * np.log(prob_scores + 1e-15) + (1 - y_true_roc) * np.log(1 - prob_scores + 1e-15))

    # --- PLOTTING ---
    # Plot 1: Professional Confusion Matrix
    fig_cm = go.Figure(data=go.Heatmap(z=[[fn, tp], [tn, fp]], x=['Actual Diseased', 'Actual Healthy'], y=['Predicted Negative', 'Predicted Positive'],
                                       colorscale=[[0, '#e3f2fd'], [1, '#0d47a1']], showscale=False, textfont={"size":16}))
    z_values = [[f'<b>FN</b><br>{fn}', f'<b>TP</b><br>{tp}'], [f'<b>TN</b><br>{tn}', f'<b>FP</b><br>{fp}']]
    for i, row in enumerate(z_values):
        for j, val in enumerate(row):
            fig_cm.add_annotation(x=j, y=i, text=val, showarrow=False, font=dict(color='white' if [[fn, tp], [tn, fp]][i][j] > n_total/3 else 'black'))
    fig_cm.add_annotation(x=0, y=1.15, text=f"Sensitivity (TPR)<br><b>{tpr:.1%}</b>", showarrow=False, yshift=30)
    fig_cm.add_annotation(x=1, y=-0.15, text=f"Specificity (TNR)<br><b>{tnr:.1%}</b>", showarrow=False, yshift=-30)
    fig_cm.add_annotation(x=-0.2, y=1, text=f"PPV<br><b>{ppv:.1%}</b>", showarrow=False, xshift=-30)
    fig_cm.add_annotation(x=-0.2, y=0, text=f"NPV<br><b>{npv:.1%}</b>", showarrow=False, xshift=-30)
    fig_cm.update_layout(title="<b>1. Confusion Matrix & Key Rates</b>", xaxis_title="Actual Condition", yaxis_title="Predicted Outcome", margin=dict(t=50, l=50, r=10, b=50))

    # Plot 2: Professional ROC Curve
    fig_roc = go.Figure()
    fig_roc.add_trace(go.Scatter(x=fpr_roc, y=tpr_roc, mode='lines', name=f'AUC = {auc_val:.3f}', line=dict(color=PRIMARY_COLOR, width=3)))
    fig_roc.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random Chance', line=dict(color='grey', width=2, dash='dash')))
    fig_roc.add_trace(go.Scatter(x=[fpr], y=[tpr], mode='markers', name="Current Threshold", marker=dict(color=SUCCESS_GREEN, size=15, symbol='star', line=dict(width=2, color='black'))))
    youden_idx = np.argmax(tpr_roc - fpr_roc)
    fig_roc.add_trace(go.Scatter(x=[fpr_roc[youden_idx]], y=[tpr_roc[youden_idx]], mode='markers', name="Optimal (Youden's J)", marker=dict(color='#FFBF00', size=12, symbol='diamond')))
    fig_roc.update_layout(title="<b>2. Receiver Operating Characteristic (ROC)</b>", xaxis_title="False Positive Rate (1 - Specificity)", yaxis_title="True Positive Rate (Sensitivity)", legend=dict(x=0.01, y=0.01, yanchor='bottom'))

    # Plot 3: Predictive Values vs. Prevalence
    prevalence_range = np.linspace(0.001, 1, 100)
    ppv_curve = (sensitivity * prevalence_range) / (sensitivity * prevalence_range + (1 - specificity) * (1 - prevalence_range))
    npv_curve = (specificity * (1 - prevalence_range)) / (specificity * (1 - prevalence_range) + (1 - sensitivity) * prevalence_range)
    fig_pv = go.Figure()
    fig_pv.add_trace(go.Scatter(x=prevalence_range, y=ppv_curve, mode='lines', name='PPV (Precision)', line=dict(color='red', width=3)))
    fig_pv.add_trace(go.Scatter(x=prevalence_range, y=npv_curve, mode='lines', name='NPV', line=dict(color='blue', width=3)))
    fig_pv.add_vline(x=prevalence, line=dict(color='black', dash='dash'), annotation_text=f"Current Prevalence ({prevalence:.1%})")
    fig_pv.update_layout(title="<b>3. The Prevalence Effect on Predictive Values</b>", xaxis_title="Disease Prevalence in Population", yaxis_title="Predictive Value", xaxis_tickformat=".0%", yaxis_tickformat=".0%", legend=dict(x=0.01, y=0.01, yanchor='bottom'))
    
    metrics = {
        "True Positive (TP)": tp, "True Negative (TN)": tn, "False Positive (FP)": fp, "False Negative (FN)": fn,
        "Prevalence": prevalence, "Sensitivity (TPR / Power)": tpr, "Specificity (TNR)": tnr,
        "False Positive Rate (Œ±)": fpr, "False Negative Rate (Œ≤)": fnr,
        "PPV (Precision)": ppv, "NPV": npv, "False Discovery Rate (FDR)": fdr, "False Omission Rate (FOR)": for_val,
        "Accuracy": accuracy, "F1 Score": f1_score, "Youden's Index (J)": youdens_j,
        "Matthews Correlation Coefficient (MCC)": mcc, "Cohen‚Äôs Kappa (Œ∫)": kappa,
        "Positive Likelihood Ratio (LR+)": lr_plus, "Negative Likelihood Ratio (LR-)": lr_minus,
        "Area Under Curve (AUC)": auc_val, "Log-Loss (Cross-Entropy)": log_loss
    }
    other_concepts = {
        "Bias": "Systematic deviation from a true value. In diagnostics, this could be a technology that consistently over- or under-estimates a biomarker value.",
        "Error": "The difference between a measured value and the true value. Comprises both random error (imprecision) and systematic error (bias).",
        "Precision": "The closeness of repeated measurements to each other. Poor precision widens the score distributions, reducing the separation between healthy and diseased populations.",
        "Prevalence Threshold": "A policy decision, not a calculation. It's the prevalence at which you decide the PPV is too low for the test to be useful as a screening tool."
    }
    
    return fig_cm, fig_roc, fig_pv, metrics, other_concepts

@st.cache_data
def plot_attribute_agreement(n_parts, n_replicates, prevalence, skilled_accuracy, uncertain_accuracy, biased_accuracy, bias_strength):
    """
    Generates a professional-grade dashboard for Attribute Agreement Analysis with realistic inspector archetypes.
    """
    np.random.seed(42)
    # 1. Create the "gold standard" reference parts, including "borderline" cases
    n_defective = int(n_parts * prevalence)
    n_good = n_parts - n_defective
    reference = np.array([1] * n_defective + [0] * n_good) # 1=Defective, 0=Good
    
    # Add borderline parts that are harder to classify
    n_borderline = int(n_parts * 0.2)
    borderline_indices = np.random.choice(n_parts, n_borderline, replace=False)
    is_borderline = np.zeros(n_parts, dtype=bool)
    is_borderline[borderline_indices] = True
    
    # 2. Simulate inspector assessments with different archetypes
    inspectors = {
        'Inspector A (Skilled)': {'acc': skilled_accuracy, 'bias': 0.5},
        'Inspector B (Uncertain)': {'acc': uncertain_accuracy, 'bias': 0.5},
        'Inspector C (Biased)': {'acc': biased_accuracy, 'bias': bias_strength}
    }
    
    assessments = []
    for name, params in inspectors.items():
        for part_idx in range(n_parts):
            for _ in range(n_replicates):
                true_status = reference[part_idx]
                accuracy = params['acc']
                # The "Uncertain" inspector is less accurate on borderline parts
                if "Uncertain" in name and is_borderline[part_idx]:
                    accuracy *= 0.7 
                
                is_correct = np.random.rand() < accuracy
                if is_correct:
                    assessment = true_status
                else: # They got it wrong
                    assessment = 1 - true_status
                    # The "Biased" inspector is more likely to make a false positive error
                    if "Biased" in name and true_status == 0:
                        assessment = 1 if np.random.rand() < params['bias'] else 0
                
                assessments.append([name, f'Part_{part_idx+1}', true_status, assessment])

    df = pd.DataFrame(assessments, columns=['Inspector', 'Part', 'Reference', 'Assessment'])

    # 3. Calculate Key Metrics
    # Fleiss' Kappa for overall agreement
    # Create a contingency table: rows are parts, columns are Good/Defective ratings by inspectors
    contingency_table = pd.crosstab(df['Part'], df['Assessment'])
    if 0 not in contingency_table.columns: contingency_table[0] = 0
    if 1 not in contingency_table.columns: contingency_table[1] = 0
    
    N = len(contingency_table) # Number of subjects
    n = contingency_table.sum(axis=1).iloc[0] # Number of ratings per subject
    p_j = contingency_table.sum(axis=0) / (N * n) # Proportions of each category
    P_i = ( (contingency_table**2).sum(axis=1) - n ) / (n * (n-1))
    P_bar = P_i.mean()
    P_e_bar = (p_j**2).sum()
    kappa = (P_bar - P_e_bar) / (1 - P_e_bar) if (1-P_e_bar) != 0 else 0


    # Individual inspector effectiveness
    effectiveness = {}
    for name in df['Inspector'].unique():
        sub = df[df['Inspector'] == name]
        cm = pd.crosstab(sub['Reference'], sub['Assessment'])
        tn = cm.loc[0,0] if 0 in cm.index and 0 in cm.columns else 0
        fp = cm.loc[0,1] if 0 in cm.index and 1 in cm.columns else 0
        fn = cm.loc[1,0] if 1 in cm.index and 0 in cm.columns else 0
        tp = cm.loc[1,1] if 1 in cm.index and 1 in cm.columns else 0
        
        miss_rate = fn / (fn + tp) if (fn + tp) > 0 else 0
        false_alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
        accuracy = (tp + tn) / (tp + tn + fp + fn)
        effectiveness[name] = {'Miss Rate': miss_rate, 'False Alarm Rate': false_alarm_rate, 'Accuracy': accuracy}
    
    df_eff = pd.DataFrame(effectiveness).T.reset_index().rename(columns={'index':'Inspector'})

    # 4. Generate Plots
    fig_eff = go.Figure()
    fig_eff.add_trace(go.Scatter(
        x=df_eff['False Alarm Rate'], y=df_eff['Miss Rate'],
        mode='markers+text', text=df_eff['Inspector'], textposition='top center',
        marker=dict(
            size=(df_eff['Miss Rate'] + df_eff['False Alarm Rate']) * 200 + 15, # Bubble size reflects total error
            color=df_eff['Accuracy'], colorscale='RdYlGn', cmin=0.7, cmax=1.0,
            showscale=True, colorbar_title='Accuracy'
        ),
    ))
    fig_eff.add_shape(type="rect", x0=0, y0=0, x1=0.05, y1=0.1, fillcolor='rgba(44, 160, 44, 0.2)', line_width=0, layer='below')
    fig_eff.add_annotation(x=0.025, y=0.05, text="<b>Ideal Zone</b>", showarrow=False)
    fig_eff.update_layout(title="<b>1. Inspector Effectiveness Report</b>",
                          xaxis_title="False Alarm Rate (Good parts failed)", yaxis_title="Miss Rate (Bad parts passed)",
                          xaxis_tickformat=".1%", yaxis_tickformat=".1%",
                          xaxis_range=[-0.02, max(0.2, df_eff['False Alarm Rate'].max()*1.2)],
                          yaxis_range=[-0.02, max(0.2, df_eff['Miss Rate'].max()*1.2)])

    # Plot 2: Cohen's Kappa Matrix
    from sklearn.metrics import cohen_kappa_score
    kappa_matrix = pd.DataFrame(index=inspectors.keys(), columns=inspectors.keys())
    for name1 in inspectors.keys():
        for name2 in inspectors.keys():
            d1 = df[df['Inspector'] == name1]['Assessment']
            d2 = df[df['Inspector'] == name2]['Assessment']
            kappa_matrix.loc[name1, name2] = cohen_kappa_score(d1, d2)
    kappa_matrix = kappa_matrix.astype(float)
    
    fig_kappa = px.imshow(kappa_matrix, text_auto=".2f", aspect="auto",
                          color_continuous_scale='Blues',
                          title="<b>2. Inter-Inspector Agreement (Cohen's Kappa)</b>",
                          labels=dict(color="Kappa"))
    
    return fig_eff, fig_kappa, kappa, df_eff

@st.cache_data    
def plot_gage_rr(part_sd=5.0, repeatability_sd=1.5, operator_sd=0.75, interaction_sd=0.5):
    """
    Generates dynamic and more realistic plots for the Gage R&R module,
    including sorted parts and operator-part interaction.
    """
    np.random.seed(10)
    n_operators, n_samples, n_replicates = 3, 10, 3
    operators = ['Alice', 'Bob', 'Charlie']
    
    # --- SME ENHANCEMENT 1: Create structured, sorted parts that span a realistic process range ---
    # Instead of random sampling, create parts that are intentionally spread out, as in a real study.
    center = 100
    # Use part_sd to define the spread of parts, e.g., covering a +/- 2.5 sigma range
    part_spread = 2.5 * part_sd 
    true_part_values_sorted = np.linspace(center - part_spread, center + part_spread, n_samples)
    part_names_sorted = [f'Part-{i+1:02d}' for i in range(n_samples)]
    part_map = dict(zip(part_names_sorted, true_part_values_sorted))

    # Generate consistent operator biases
    operator_biases = np.random.normal(0, operator_sd, n_operators)
    operator_bias_map = {op: bias for op, bias in zip(operators, operator_biases)}
    
    # --- SME ENHANCEMENT 2: Simulate Operator-Part Interaction ---
    # Create a consistent random effect for each specific operator-part combination.
    interaction_effects = {}
    for op in operators:
        for part_name in part_names_sorted:
            interaction_effects[(op, part_name)] = np.random.normal(0, interaction_sd)

    data = []
    for operator in operators:
        for part_name, true_value in part_map.items():
            # The final "true" value for this measurement includes the main effects and the new interaction effect
            effective_true_value = true_value + operator_bias_map[operator] + interaction_effects[(operator, part_name)]
            
            # Generate measurements with repeatability (instrument noise) around this effective value
            measurements = np.random.normal(effective_true_value, repeatability_sd, n_replicates)
            
            for m in measurements:
                data.append([operator, part_name, m])
    
    df = pd.DataFrame(data, columns=['Operator', 'Part', 'Measurement'])
    
    # Perform ANOVA (the model correctly captures the new interaction term)
    model = ols('Measurement ~ C(Part) + C(Operator) + C(Part):C(Operator)', data=df).fit()
    anova_table = sm.stats.anova_lm(model, typ=2)
    
    ms_operator = anova_table.loc['C(Operator)', 'sum_sq'] / anova_table.loc['C(Operator)', 'df']
    ms_part = anova_table.loc['C(Part)', 'sum_sq'] / anova_table.loc['C(Part)', 'df']
    ms_interaction = anova_table.loc['C(Part):C(Operator)', 'sum_sq'] / anova_table.loc['C(Part):C(Operator)', 'df']
    ms_error = anova_table.loc['Residual', 'sum_sq'] / anova_table.loc['Residual', 'df']
    
    # Calculate variance components from Mean Squares
    var_repeatability = ms_error
    var_operator = max(0, (ms_operator - ms_interaction) / (n_samples * n_replicates))
    var_interaction = max(0, (ms_interaction - ms_error) / n_replicates)
    var_reproducibility = var_operator + var_interaction
    var_part = max(0, (ms_part - ms_interaction) / (n_operators * n_replicates))
    var_rr = var_repeatability + var_reproducibility
    var_total = var_rr + var_part
    
    # Calculate final KPIs
    pct_rr = (var_rr / var_total) * 100 if var_total > 0 else 0
    ndc = int(1.41 * np.sqrt(var_part / var_rr)) if var_rr > 0 else 10
    
    # Plotting (updated titles and layout for clarity)
    fig = make_subplots(rows=2, cols=2, column_widths=[0.7, 0.3], row_heights=[0.5, 0.5],
                        specs=[[{"rowspan": 2}, {}], [None, {}]],
                        subplot_titles=("<b>Measurement by Part (Grouped by Operator)</b>",
                                        "<b>Overall Variation by Operator</b>",
                                        "<b>Final Verdict: Variation Contribution</b>"))

    # Main plot: Box plot for each part, colored by operator
    fig_box = px.box(df, x='Part', y='Measurement', color='Operator', color_discrete_sequence=px.colors.qualitative.Plotly)
    for trace in fig_box.data: fig.add_trace(trace, row=1, col=1)
    
    # Add operator mean lines to show trends and interactions
    for i, operator in enumerate(operators):
        operator_df = df[df['Operator'] == operator]
        part_means = operator_df.groupby('Part')['Measurement'].mean().reindex(part_names_sorted) # Ensure correct order
        fig.add_trace(go.Scatter(x=part_means.index, y=part_means.values, mode='lines',
                                 line=dict(width=2), name=f'{operator} Mean',
                                 showlegend=False, marker_color=fig_box.data[i].marker.color), row=1, col=1)

    # Top-right plot: Box plot of overall measurements for each operator
    fig_op_box = px.box(df, x='Operator', y='Measurement', color='Operator', color_discrete_sequence=px.colors.qualitative.Plotly)
    for trace in fig_op_box.data: fig.add_trace(trace, row=1, col=2)
    
    # Bottom-right plot: Final verdict bar chart
    pct_part = (var_part / var_total) * 100 if var_total > 0 else 0
    fig.add_trace(go.Bar(x=['% Gage R&R', '% Part Var.'], y=[pct_rr, pct_part],
                         marker_color=['#EF553B' if pct_rr > 30 else ('#FECB52' if pct_rr > 10 else '#00CC96'), '#636EFA'],
                         text=[f'{pct_rr:.1f}%', f'{pct_part:.1f}%'], textposition='auto'), row=2, col=2)
    
    # Add acceptance criteria lines to the verdict plot
    fig.add_hline(y=10, line_dash="dash", line_color="darkgreen", annotation_text="Acceptable < 10%", annotation_position="bottom right", row=2, col=2)
    fig.add_hline(y=30, line_dash="dash", line_color="darkorange", annotation_text="Unacceptable > 30%", annotation_position="top right", row=2, col=2)

    fig.update_layout(title_text='<b>Gage R&R Study: Is the Measurement System Fit for Purpose?</b>', title_x=0.5, height=800,
                      boxmode='group', showlegend=True, legend_title_text='Operator')
    fig.update_xaxes(tickangle=0, row=1, col=1) # No longer need tickangle with sorted part names
    fig.update_yaxes(title_text="Measurement", row=1, col=1)
    
    return fig, pct_rr, ndc

@st.cache_data
def plot_lod_loq(slope=0.02, baseline_sd=0.01):
    """
    Generates enhanced, more realistic dynamic plots for the LOD & LOQ module.
    """
    np.random.seed(3)
    
    # --- 1. Simulate a more realistic dataset ---
    # Multiple blank lots and multiple low-concentration spikes
    n_blanks, n_spikes = 60, 20
    blank_signals = np.random.normal(0.05, baseline_sd, n_blanks)
    
    # Use the calculated LOQ (from the sliders) to determine a realistic spike level
    # This creates a circular but effective demonstration
    approx_loq_conc = (10 * baseline_sd) / slope
    spike_signals = np.random.normal(0.05 + approx_loq_conc * slope, baseline_sd * 1.5, n_spikes)

    df_dist = pd.concat([
        pd.DataFrame({'Signal': blank_signals, 'Sample Type': 'Blanks'}), 
        pd.DataFrame({'Signal': spike_signals, 'Sample Type': 'Low Conc Spikes'})
    ])
    
    # --- 2. Low-Level Calibration Curve ---
    concentrations = np.array([0, 0, 0, 0, 0, 0.5, 0.5, 1, 1, 2, 2, 5, 5, 10, 10])
    signals = 0.05 + slope * concentrations + np.random.normal(0, baseline_sd, len(concentrations))
    df_cal = pd.DataFrame({'Concentration': concentrations, 'Signal': signals})
    
    # --- 3. Fit Model and Calculate Key Values ---
    # Use the blank measurements to get a robust estimate of the blank mean and SD
    mean_blank = np.mean(blank_signals)
    sd_blank = np.std(blank_signals, ddof=1)
    
    X = sm.add_constant(df_cal['Concentration'])
    model = sm.OLS(df_cal['Signal'], X).fit()
    
    fit_slope = model.params['Concentration'] if model.params['Concentration'] > 0.001 else 0.001
    
    # Calculate the full LOB/LOD/LOQ hierarchy
    LOB = mean_blank + 1.645 * sd_blank
    LOD_signal = LOB + 1.645 * sd_blank # Signal at the LOD
    LOD_conc = (LOD_signal - model.params['const']) / fit_slope
    LOQ_conc = (10 * sd_blank) / fit_slope # ICH Signal/Noise method
    
    # --- 4. Plotting ---
    fig = make_subplots(rows=2, cols=1,
                        subplot_titles=("<b>1. Signal Distributions: Defining LOB & LOD</b>",
                                        "<b>2. Calibration Curve: Defining LOQ</b>"),
                        vertical_spacing=0.15)
    
    # Top Plot: Signal Distributions
    fig_hist = px.histogram(df_dist, x='Signal', color='Sample Type', barmode='overlay',
                            opacity=0.7, nbins=30,
                            color_discrete_map={'Blanks': 'skyblue', 'Low Conc Spikes': 'lightgreen'})
    for trace in fig_hist.data:
        fig.add_trace(trace, row=1, col=1)
    
    fig.add_vline(x=LOB, line_dash="dot", line_color="purple", row=1, col=1,
                  annotation_text=f"<b>LOB = {LOB:.3f}</b>", annotation_position="top")
    fig.add_vline(x=LOD_signal, line_dash="dash", line_color="orange", row=1, col=1,
                  annotation_text=f"<b>LOD Signal = {LOD_signal:.3f}</b>", annotation_position="top")
    
    # Bottom Plot: Calibration Curve with Prediction Intervals
    fig.add_trace(go.Scatter(x=df_cal['Concentration'], y=df_cal['Signal'], mode='markers',
                             name='Calibration Points', marker=dict(color='darkblue', size=8)), row=2, col=1)
    
    x_pred = pd.DataFrame({'const': 1, 'Concentration': np.linspace(-1, df_cal['Concentration'].max()*1.1, 100)})
    pred_summary = model.get_prediction(x_pred).summary_frame(alpha=0.05)
    
    fig.add_trace(go.Scatter(x=x_pred['Concentration'], y=pred_summary['mean'], mode='lines',
                             name='Regression Line', line=dict(color='red', dash='dash')), row=2, col=1)
    # SME Enhancement: Show Prediction Interval, which is key for LOQ
    fig.add_trace(go.Scatter(x=x_pred['Concentration'], y=pred_summary['obs_ci_upper'],
                             fill=None, mode='lines', line_color='rgba(255,0,0,0.2)', showlegend=False), row=2, col=1)
    fig.add_trace(go.Scatter(x=x_pred['Concentration'], y=pred_summary['obs_ci_lower'],
                             fill='tonexty', mode='lines', line_color='rgba(255,0,0,0.2)',
                             name='95% Prediction Interval'), row=2, col=1)

    fig.add_vline(x=LOD_conc, line_dash="dash", line_color="orange", row=2, col=1,
                  annotation_text=f"<b>LOD = {LOD_conc:.2f}</b>", annotation_position="bottom")
    fig.add_vline(x=LOQ_conc, line_dash="dot", line_color="red", row=2, col=1,
                  annotation_text=f"<b>LOQ = {LOQ_conc:.2f}</b>", annotation_position="top")
    
    fig.update_layout(title_text='<b>Assay Sensitivity Analysis: The LOB, LOD, & LOQ Hierarchy</b>',
                      title_x=0.5, height=800, legend=dict(x=0.01, y=0.45))
    fig.update_yaxes(title_text="Assay Signal (e.g., Absorbance)", row=1, col=1)
    fig.update_xaxes(title_text="Signal", row=1, col=1)
    fig.update_yaxes(title_text="Assay Signal", row=2, col=1)
    fig.update_xaxes(title_text="Concentration (ng/mL)", row=2, col=1, range=[-1, df_cal['Concentration'].max()*1.1])
    
    return fig, LOD_conc, LOQ_conc
    
# ==============================================================================
# HELPER & PLOTTING FUNCTION (Linearity) - SME ENHANCED
# ==============================================================================
@st.cache_data
def plot_linearity(curvature=-1.0, random_error=1.0, proportional_error=2.0, use_wls=False):
    """
    Generates enhanced, more realistic dynamic plots for the Linearity module,
    including replicates and optional Weighted Least Squares (WLS) regression.
    """
    np.random.seed(42)
    nominal_levels = np.array([10, 25, 50, 100, 150, 200, 250])
    n_replicates = 3 # SME Enhancement: Real studies use replicates
    
    data = []
    for nom in nominal_levels:
        for _ in range(n_replicates):
            curvature_effect = curvature * (nom / 150)**3
            error = np.random.normal(0, random_error + nom * (proportional_error / 100))
            measured = nom + curvature_effect + error
            data.append({'Nominal': nom, 'Measured': measured})
    
    df = pd.DataFrame(data)
    
    # --- SME Enhancement: Implement OLS vs. WLS Regression ---
    X = sm.add_constant(df['Nominal'])
    
    if use_wls and proportional_error > 0:
        # For WLS, weights are typically the inverse of the variance at each level.
        # We approximate variance from the nominal concentration.
        df['weights'] = 1 / (random_error + df['Nominal'] * (proportional_error / 100))**2
        model = sm.WLS(df['Measured'], X, weights=df['weights']).fit()
        model_type = "Weighted Least Squares (WLS)"
    else:
        model = sm.OLS(df['Measured'], X).fit()
        model_type = "Ordinary Least Squares (OLS)"

    df['Predicted'] = model.predict(X)
    df['Residual'] = model.resid
    df['Recovery'] = (df['Measured'] / df['Nominal']) * 100
    
    # --- Plotting ---
    fig = make_subplots(
        rows=2, cols=2,
        specs=[[{}, {}], [{"colspan": 2}, None]],
        subplot_titles=(f"<b>1. Linearity Plot (R¬≤ = {model.rsquared:.4f})</b>",
                        "<b>2. Residuals vs. Nominal</b>",
                        "<b>3. Percent Recovery vs. Nominal</b>"),
        vertical_spacing=0.2
    )
    
    # Plot 1: Linearity Plot
    fig.add_trace(go.Scatter(x=df['Nominal'], y=df['Measured'], mode='markers', name='Measured Values',
                             marker=dict(size=8, color='blue', opacity=0.7)), row=1, col=1)
    
    # Plot the regression line using a smooth range
    x_range = np.linspace(0, 260, 100)
    y_range = model.predict(sm.add_constant(x_range))
    fig.add_trace(go.Scatter(x=x_range, y=y_range, mode='lines', name=f'{model_type} Fit',
                             line=dict(color='red')), row=1, col=1)
    fig.add_trace(go.Scatter(x=[0, 260], y=[0, 260], mode='lines', name='Line of Identity (y=x)',
                             line=dict(dash='dash', color='black')), row=1, col=1)
    
    # Plot 2: Residual Plot (SME Enhancement: Use box plots for clarity)
    fig.add_trace(px.box(df, x='Nominal', y='Residual').data[0].update(marker_color='green', name='Residuals'), row=1, col=2)
    fig.add_hline(y=0, line_dash="dash", line_color="black", row=1, col=2)
    
    # Plot 3: Recovery Plot (SME Enhancement: Use box plots for clarity)
    fig.add_trace(px.box(df, x='Nominal', y='Recovery').data[0].update(marker_color='purple', name='Recovery'), row=2, col=1)
    fig.add_hrect(y0=80, y1=120, fillcolor="green", opacity=0.1, layer="below", line_width=0, row=2, col=1)
    fig.add_hline(y=100, line_dash="dash", line_color="black", row=2, col=1)
    fig.add_hline(y=80, line_dash="dot", line_color="red", row=2, col=1, annotation_text="80% Limit")
    fig.add_hline(y=120, line_dash="dot", line_color="red", row=2, col=1, annotation_text="120% Limit")
    
    fig.update_layout(title_text=f'<b>Assay Linearity Dashboard (Model: {model_type})</b>',
                      title_x=0.5, height=800, showlegend=True,
                      legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.7)'))
    fig.update_xaxes(title_text="Nominal Concentration", row=1, col=1); fig.update_yaxes(title_text="Measured Concentration", row=1, col=1)
    fig.update_xaxes(title_text="Nominal Concentration", row=1, col=2); fig.update_yaxes(title_text="Residual (Error)", row=1, col=2)
    fig.update_xaxes(title_text="Nominal Concentration", row=2, col=1); fig.update_yaxes(title_text="% Recovery", range=[70, 130], row=2, col=1)
    
    return fig, model

# ==============================================================================
# HELPER & PLOTTING FUNCTION (4PL) - SME ENHANCED
# ==============================================================================
@st.cache_data
def plot_4pl_regression(a_true=1.5, b_true=1.2, c_true=10.0, d_true=0.05, noise_sd=0.05, proportional_noise=1.0, use_irls=True):
    """
    Generates enhanced, more realistic dynamic plots for the 4PL regression module,
    including heteroscedastic noise and weighted fitting (IRLS).
    """
    # 4PL logistic function
    def four_pl(x, a, b, c, d):
        return d + (a - d) / (1 + (x / c)**b)

    # SME Enhancement: More realistic data generation with replicates and proportional noise
    np.random.seed(42)
    conc_levels = np.logspace(-2, 3, 8)
    conc_replicates = np.repeat(conc_levels, 2) # Duplicates for replicates

    signal_true = four_pl(conc_replicates, a_true, b_true, c_true, d_true)
    
    # Noise model: combination of constant and signal-proportional noise
    total_noise_sd = noise_sd + (signal_true * (proportional_noise / 100))
    signal_measured = signal_true + np.random.normal(0, total_noise_sd)
    
    df = pd.DataFrame({'Concentration': conc_replicates, 'Signal': signal_measured})

    # SME Enhancement: Implement Iteratively Reweighted Least Squares (IRLS) for fitting
    model_type = "Least Squares"
    try:
        p0 = [a_true, b_true, c_true, d_true]
        if use_irls and proportional_noise > 0:
            model_type = "Weighted (IRLS)"
            # Use sigma parameter in curve_fit for weighting (inverse of variance)
            # Estimate variance at each point
            sigma_weights = noise_sd + (df['Signal'] * (proportional_noise / 100))
            params, cov = curve_fit(four_pl, df['Concentration'], df['Signal'], p0=p0, sigma=sigma_weights, absolute_sigma=True, maxfev=10000)
        else:
            params, cov = curve_fit(four_pl, df['Concentration'], df['Signal'], p0=p0, maxfev=10000)
    except RuntimeError:
        params, cov = p0, np.full((4, 4), np.inf)
        
    a_fit, b_fit, c_fit, d_fit = params
    perr = np.sqrt(np.diag(cov)) # Standard errors of parameters

    # Calculate residuals
    df['Predicted'] = four_pl(df['Concentration'], *params)
    df['Residual'] = df['Signal'] - df['Predicted']

    # --- Plotting ---
    fig = make_subplots(rows=2, cols=1,
                        subplot_titles=(f"<b>1. 4-Parameter Logistic Fit (Model: {model_type})</b>",
                                        "<b>2. Residuals vs. Concentration</b>"),
                        vertical_spacing=0.15)

    # Plot 1: 4PL Curve
    fig.add_trace(go.Scatter(x=df['Concentration'], y=df['Signal'], mode='markers',
                             name='Measured Data', marker=dict(size=8, color='blue')), row=1, col=1)
    
    x_fit = np.logspace(-2, 3, 100)
    y_fit = four_pl(x_fit, *params)
    fig.add_trace(go.Scatter(x=x_fit, y=y_fit, mode='lines',
                             name='4PL Fit', line=dict(color='red', dash='dash')), row=1, col=1)
    
    # Add annotations for key fitted parameters
    fig.add_hline(y=d_fit, line_dash='dot', annotation_text=f"d={d_fit:.2f}", row=1, col=1)
    fig.add_hline(y=a_fit, line_dash='dot', annotation_text=f"a={a_fit:.2f}", row=1, col=1)
    fig.add_vline(x=c_fit, line_dash='dot', annotation_text=f"EC50={c_fit:.2f}", row=1, col=1)
    
    # Plot 2: Residuals
    fig.add_trace(go.Scatter(x=df['Concentration'], y=df['Residual'], mode='markers',
                             name='Residuals', marker=dict(color='green', size=8)), row=2, col=1)
    fig.add_hline(y=0, line_dash='dash', line_color='black', row=2, col=1)

    fig.update_layout(height=800, title_text='<b>Non-Linear Regression for Bioassay Potency</b>', title_x=0.5,
                      legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.7)'))
    fig.update_xaxes(type="log", title_text="Concentration (log scale)", row=1, col=1)
    fig.update_yaxes(title_text="Signal Response", row=1, col=1)
    fig.update_xaxes(type="log", title_text="Concentration (log scale)", row=2, col=1)
    fig.update_yaxes(title_text="Residual (Signal - Predicted)", row=2, col=1)
    
    return fig, params, perr
    
# ==============================================================================
# HELPER & PLOTTING FUNCTION (ROC Curve) - SME ENHANCED
# ==============================================================================
@st.cache_data
def plot_roc_curve(diseased_mean=65, population_sd=10, cutoff=55):
    """
    Generates enhanced, more realistic, and interactive plots for the ROC curve module,
    including a dynamic cutoff point and visualized confusion matrix.
    """
    np.random.seed(0)
    
    healthy_mean = 45
    scores_diseased = np.random.normal(loc=diseased_mean, scale=population_sd, size=200)
    scores_healthy = np.random.normal(loc=healthy_mean, scale=population_sd, size=200)
    
    y_true = np.concatenate([np.ones(200), np.zeros(200)]) # 1 for diseased, 0 for healthy
    y_scores = np.concatenate([scores_diseased, scores_healthy])
    
    fpr, tpr, thresholds = roc_curve(y_true, y_scores)
    auc_value = auc(fpr, tpr)

    # --- Calculations for dynamic cutoff point ---
    TP = np.sum((scores_diseased >= cutoff))
    FN = np.sum((scores_diseased < cutoff))
    TN = np.sum((scores_healthy < cutoff))
    FP = np.sum((scores_healthy >= cutoff))
    
    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0
    specificity = TN / (TN + FP) if (TN + FP) > 0 else 1
    current_fpr = 1 - specificity
    
    ppv = TP / (TP + FP) if (TP + FP) > 0 else 0 # Positive Predictive Value
    npv = TN / (TN + FN) if (TN + FN) > 0 else 0 # Negative Predictive Value
    
    # --- Plotting ---
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=("<b>1. Score Distributions & Interactive Cutoff</b>",
                        f"<b>2. Receiver Operating Characteristic (ROC) Curve | AUC = {auc_value:.3f}</b>"),
        vertical_spacing=0.15,
        row_heights=[0.6, 0.4]
    )

    # Plot 1: Distributions
    # Use density plots for a smoother look
    from scipy.stats import gaussian_kde
    x_range = np.linspace(min(scores_healthy.min(), scores_diseased.min()), max(scores_healthy.max(), scores_diseased.max()), 200)
    healthy_kde = gaussian_kde(scores_healthy)
    diseased_kde = gaussian_kde(scores_diseased)

    fig.add_trace(go.Scatter(x=x_range, y=healthy_kde(x_range), fill='tozeroy', name='Healthy', line=dict(color='blue')), row=1, col=1)
    fig.add_trace(go.Scatter(x=x_range, y=diseased_kde(x_range), fill='tozeroy', name='Diseased', line=dict(color='red')), row=1, col=1)
    
    # SME Enhancement: Shade the TP/FP/TN/FN areas
    fig.add_vrect(x0=cutoff, x1=x_range.max(), fillcolor="rgba(255, 0, 0, 0.2)", layer="below", line_width=0,
                  annotation_text="Positive Calls", annotation_position="top right", row=1, col=1)
    fig.add_vrect(x0=x_range.min(), x1=cutoff, fillcolor="rgba(0, 0, 255, 0.2)", layer="below", line_width=0,
                  annotation_text="Negative Calls", annotation_position="top left", row=1, col=1)

    fig.add_vline(x=cutoff, line_width=3, line_color='black', name='Cutoff',
                  annotation_text=f"Cutoff = {cutoff}", annotation_position="bottom right", row=1, col=1)

    # Plot 2: ROC Curve
    fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name='ROC Curve', line=dict(color='darkorange', width=3)), row=2, col=1)
    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='No-Discrimination Line', line=dict(color='grey', width=2, dash='dash')), row=2, col=1)
    
    # SME Enhancement: Add the dynamic point for the selected cutoff
    fig.add_trace(go.Scatter(x=[current_fpr], y=[sensitivity], mode='markers',
                             marker=dict(color='black', size=15, symbol='x', line=dict(width=3)),
                             name='Current Cutoff Performance'), row=2, col=1)

    fig.update_layout(height=800, title_text="<b>Diagnostic Assay Performance Dashboard</b>", title_x=0.5,
                      legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))
    fig.update_xaxes(title_text="Assay Score", row=1, col=1)
    fig.update_yaxes(title_text="Density", row=1, col=1)
    fig.update_xaxes(title_text="False Positive Rate (1 - Specificity)", range=[-0.05, 1.05], row=2, col=1)
    fig.update_yaxes(title_text="True Positive Rate (Sensitivity)", range=[-0.05, 1.05], row=2, col=1)
    
    return fig, auc_value, sensitivity, specificity, ppv, npv
    
@st.cache_data
def plot_tost(delta=5.0, true_diff=1.0, std_dev=5.0, n_samples=50):
    """
    Generates an enhanced, 3-plot dashboard for the TOST module that visually
    connects the raw data to the final statistical conclusion.
    """
    np.random.seed(1)
    data_A = np.random.normal(loc=100, scale=std_dev, size=n_samples)
    data_B = np.random.normal(loc=100 + true_diff, scale=std_dev, size=n_samples)
    
    mean_A, var_A = np.mean(data_A), np.var(data_A, ddof=1)
    mean_B, var_B = np.mean(data_B), np.var(data_B, ddof=1)
    diff_mean = mean_B - mean_A
    
    if n_samples <= 1:
        return go.Figure(), 1.0, False, 0, 0, 0, 0, 0

    std_err_diff = np.sqrt(var_A/n_samples + var_B/n_samples)
    df_welch = (std_err_diff**4) / (((var_A/n_samples)**2 / (n_samples-1)) + ((var_B/n_samples)**2 / (n_samples-1)))
    
    t_lower = (diff_mean - (-delta)) / std_err_diff
    t_upper = (diff_mean - delta) / std_err_diff
    p_lower, p_upper = stats.t.sf(t_lower, df_welch), stats.t.cdf(t_upper, df_welch)
    p_tost = max(p_lower, p_upper)
    is_equivalent = p_tost < 0.05
    
    ci_margin = t.ppf(0.95, df_welch) * std_err_diff
    ci_lower, ci_upper = diff_mean - ci_margin, diff_mean + ci_margin
    
    # --- THIS LINE WAS MOVED HERE ---
    ci_color = '#00CC96' if is_equivalent else '#EF553B' # Green for pass, Red for fail
    # --- END OF MOVE ---
    
    # --- PLOTTING ---
    fig = make_subplots(
        rows=3, cols=1,
        row_heights=[0.4, 0.4, 0.2],
        vertical_spacing=0.1,
        subplot_titles=("<b>1. Raw Data Distributions (The Samples)</b>",
                        "<b>2. Distribution of the Difference in Means (The Evidence)</b>",
                        "<b>3. Equivalence Test (The Verdict)</b>")
    )

    # Plot 1: Raw Data Distributions
    from scipy.stats import gaussian_kde
    x_range1 = np.linspace(min(data_A.min(), data_B.min()), max(data_A.max(), data_B.max()), 200)
    kde_A, kde_B = gaussian_kde(data_A), gaussian_kde(data_B)
    fig.add_trace(go.Scatter(x=x_range1, y=kde_A(x_range1), fill='tozeroy', name='Method A', line=dict(color='blue')), row=1, col=1)
    fig.add_trace(go.Scatter(x=x_range1, y=kde_B(x_range1), fill='tozeroy', name='Method B', line=dict(color='green')), row=1, col=1)
    fig.add_vline(x=mean_A, line=dict(color='royalblue', dash='dash'), row=1, col=1)
    fig.add_vline(x=mean_B, line=dict(color='darkgreen', dash='dash'), row=1, col=1)
    fig.update_yaxes(showticklabels=False, row=1, col=1)
    
    # Plot 2: THE VISUAL BRIDGE - Distribution of the Difference
    x_range2 = np.linspace(diff_mean - 4*std_err_diff, diff_mean + 4*std_err_diff, 200)
    diff_pdf = stats.t.pdf(x_range2, df=df_welch, loc=diff_mean, scale=std_err_diff)
    fig.add_trace(go.Scatter(x=x_range2, y=diff_pdf, fill='tozeroy', name='Sampling Dist.', line=dict(color='grey')), row=2, col=1)
    
    # Shade the 90% CI area
    x_fill = np.linspace(ci_lower, ci_upper, 100)
    y_fill = stats.t.pdf(x_fill, df=df_welch, loc=diff_mean, scale=std_err_diff)
    fig.add_trace(go.Scatter(x=x_fill, y=y_fill, fill='tozeroy', name='90% CI', line=dict(color=ci_color), fillcolor=ci_color), row=2, col=1)
    fig.add_vrect(x0=-delta, x1=delta, fillcolor="rgba(0,128,0,0.1)", layer="below", line_width=0, row=2, col=1)
    fig.add_vline(x=-delta, line=dict(color="red", dash="dash"), row=2, col=1)
    fig.add_vline(x=delta, line=dict(color="red", dash="dash"), row=2, col=1)
    fig.update_yaxes(showticklabels=False, row=2, col=1)

    # Plot 3: The Verdict CI Bar
    fig.add_trace(go.Scatter(
        x=[diff_mean], y=[0], error_x=dict(type='data', array=[ci_upper-diff_mean], arrayminus=[diff_mean-ci_lower], thickness=15),
        mode='markers', name='90% CI for Diff.', marker=dict(color=ci_color, size=18, line=dict(width=2, color='black'))
    ), row=3, col=1)
    fig.add_vrect(x0=-delta, x1=delta, fillcolor="rgba(0,128,0,0.1)", layer="below", line_width=0, row=3, col=1)
    fig.add_vline(x=-delta, line=dict(color="red", dash="dash"), row=3, col=1)
    fig.add_vline(x=delta, line=dict(color="red", dash="dash"), row=3, col=1)
    fig.update_yaxes(showticklabels=False, range=[-1, 1], row=3, col=1)
    
    fig.update_layout(height=800, title_text="<b>Equivalence Testing: From Raw Data to Verdict</b>", title_x=0.5, showlegend=False)
    fig.update_xaxes(title_text="Measured Value", row=1, col=1)
    fig.update_xaxes(title_text="Difference in Means", row=2, col=1)
    fig.update_xaxes(title_text="Difference in Means", row=3, col=1)
    
    return fig, p_tost, is_equivalent, ci_lower, ci_upper, mean_A, mean_B, diff_mean

# SNIPPET: Replace your entire plot_reliability_weibull function with this final, correct version.

@st.cache_data
def plot_reliability_weibull(n, beta, eta, test_duration):
    """
    Simulates a censored life test and fits a Weibull distribution to the results.
    This version is updated for maximum stability using lifelines for fitting and scipy for plotting.
    """
    from lifelines import WeibullFitter
    
    np.random.seed(42)
    # Simulate time-to-failure data from a true Weibull distribution
    failures = stats.weibull_min.rvs(c=beta, scale=eta, size=n)
    
    # Censor the data based on the test duration
    durations = np.minimum(failures, test_duration)
    event_observed = (failures <= test_duration)

    # Use lifelines for its robust fitting of censored data
    wf = WeibullFitter()
    wf.fit(durations, event_observed=event_observed)
    
    # Extract the fitted parameters (lifelines uses lambda_ and rho_)
    fitted_alpha = wf.lambda_ # This is the Scale parameter (eta)
    fitted_beta = wf.rho_    # This is the Shape parameter (beta)

    # Generate plots
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=("<b>Weibull Probability Plot</b>", "<b>Reliability Curve (Survival Function)</b>")
    )
    
    # --- START OF THE FIX: Use scipy.stats.probplot for a robust probability plot ---
    # 1. Probability Plot
    # We only plot the points that actually failed to check the fit.
    failed_durations = durations[event_observed]
    if len(failed_durations) < 2: # Need at least 2 points to plot a line
        return go.Figure().update_layout(title_text="Not enough failures to plot. Increase test duration or sample size."), 0, 0, 0

    # `probplot` generates theoretical quantiles vs. ordered data values.
    # For a Weibull plot, we plot against the weibull_min distribution.
    (osm, osr), (slope, intercept, r) = stats.probplot(failed_durations, dist=stats.weibull_min, sparams=(fitted_beta, 0, fitted_alpha))
    
    # Plot the ordered data against the theoretical quantiles
    fig.add_trace(go.Scatter(x=osm, y=osr, mode='markers', name='Failure Data', marker=dict(color=PRIMARY_COLOR)), row=1, col=1)
    
    # Plot the best-fit line
    fig.add_trace(go.Scatter(x=osm, y=intercept + slope * osm, mode='lines', name='Weibull Fit', line=dict(color='red', dash='dash')), row=1, col=1)
    # --- END OF THE FIX ---

    # 2. Reliability Curve (Survival Function) - this part is correct and robust
    time_grid = np.linspace(0, test_duration * 1.2, 200)
    survival_prob = wf.survival_function_at_times(time_grid)
    ci_df = wf.confidence_interval_survival_function_
    
    fig.add_trace(go.Scatter(x=survival_prob.index, y=survival_prob.values, mode='lines', name='Reliability', line=dict(color=PRIMARY_COLOR, width=3)), row=1, col=2)
    fig.add_trace(go.Scatter(x=ci_df.index, y=ci_df.iloc[:, 1], mode='lines', line=dict(width=0), showlegend=False), row=1, col=2)
    fig.add_trace(go.Scatter(x=ci_df.index, y=ci_df.iloc[:, 0], mode='lines', line=dict(width=0), fill='tonexty', fillcolor='rgba(0,104,201,0.2)', name='95% CI'), row=1, col=2)

    try:
        b10_life = wf.predict(0.90) # Time at which 90% are still surviving
        fig.add_hline(y=0.90, line=dict(color='red', dash='dash'), row=1, col=2)
        fig.add_vline(x=b10_life, line=dict(color='red', dash='dash'), row=1, col=2)
        fig.add_annotation(x=b10_life, y=0.90, text=f"B10 Life: {b10_life:.1f} hrs", showarrow=True, arrowhead=2, ax=50, ay=-40, row=1, col=2)
    except Exception:
        b10_life = 0

    fig.update_xaxes(title_text="Weibull Theoretical Quantiles", row=1, col=1)
    fig.update_yaxes(title_text="Ordered Failure Times", row=1, col=1)
    fig.update_xaxes(title_text="Operating Time (Hours)", row=1, col=2)
    fig.update_yaxes(title_text="Probability of Survival", range=[0, 1.05], row=1, col=2)
    fig.update_layout(title="<b>Weibull Analysis of Life Test Data</b>", showlegend=False)

    return fig, fitted_beta, fitted_alpha, b10_life
# ==============================================================================
# HELPER & PLOTTING FUNCTION (DOE/RSM) - SME ENHANCED
# ==============================================================================
@st.cache_data
def plot_doe_robustness(ph_effect=2.0, temp_effect=5.0, interaction_effect=0.0, ph_quad_effect=-5.0, temp_quad_effect=-5.0, noise_sd=1.0):
    """
    Generates enhanced, more realistic dynamic RSM plots for the DOE module,
    including a Pareto plot of effects and ANOVA table.
    """
    np.random.seed(42)
    
    # 1. Design the experiment in coded units (-alpha to +alpha)
    alpha = 1.414
    design_coded = {
        'pH_coded':  [-1, 1, -1, 1, -alpha, alpha, 0, 0, 0, 0, 0, 0, 0],
        'Temp_coded':[-1, -1, 1, 1, 0, 0, -alpha, alpha, 0, 0, 0, 0, 0]
    }
    df = pd.DataFrame(design_coded)
    
    # Map coded units to realistic "real" units
    df['pH'] = df['pH_coded'] * 0.5 + 7.0   # e.g., pH 6.5 to 7.5
    df['Temp'] = df['Temp_coded'] * 5 + 30  # e.g., Temp 25 to 35 C

    # 2. Simulate the response using the full quadratic model in CODED units
    true_response = 100 + \
                    ph_effect * df['pH_coded'] + \
                    temp_effect * df['Temp_coded'] + \
                    interaction_effect * df['pH_coded'] * df['Temp_coded'] + \
                    ph_quad_effect * (df['pH_coded']**2) + \
                    temp_quad_effect * (df['Temp_coded']**2)
    
    df['Response'] = true_response + np.random.normal(0, noise_sd, len(df))

    # 3. Analyze the results with a quadratic OLS model using coded variables
    model = ols('Response ~ pH_coded + Temp_coded + I(pH_coded**2) + I(Temp_coded**2) + pH_coded:Temp_coded', data=df).fit()
    
    # SME Enhancement: Create a user-friendly ANOVA summary table
    anova_table = sm.stats.anova_lm(model, typ=2)
    anova_summary = anova_table[['sum_sq', 'df', 'F', 'PR(>F)']].reset_index()
    anova_summary.columns = ['Term', 'Sum of Squares', 'df', 'F-value', 'p-value']
    anova_summary['Term'] = anova_summary['Term'].str.replace('_coded', '') # Clean up names
    
    # 4. Create the prediction grid for the surfaces (in coded units)
    x_range_coded = np.linspace(-1.5, 1.5, 50)
    y_range_coded = np.linspace(-1.5, 1.5, 50)
    xx, yy = np.meshgrid(x_range_coded, y_range_coded)
    grid = pd.DataFrame({'pH_coded': xx.ravel(), 'Temp_coded': yy.ravel()})
    pred = model.predict(grid).values.reshape(xx.shape)

    # 5. Find the optimum point from the model
    max_idx = np.unravel_index(np.argmax(pred), pred.shape)
    opt_ph_coded = x_range_coded[max_idx[1]]
    opt_temp_coded = y_range_coded[max_idx[0]]
    max_response = np.max(pred)
    # Convert optimum back to real units for reporting
    opt_ph_real = opt_ph_coded * 0.5 + 7.0
    opt_temp_real = opt_temp_coded * 5 + 30
    
    # 6. Create the 2D Contour Plot (in real units for interpretation)
    x_range_real = x_range_coded * 0.5 + 7.0
    y_range_real = y_range_coded * 5 + 30
    fig_contour = go.Figure(data=[
        go.Contour(z=pred, x=x_range_real, y=y_range_real, colorscale='Viridis',
                    contours=dict(coloring='lines', showlabels=True, labelfont=dict(size=12, color='white'))),
        go.Scatter(x=df['pH'], y=df['Temp'], mode='markers',
                   marker=dict(color='red', size=12, line=dict(width=2, color='black')), name='Design Points')
    ])
    fig_contour.add_trace(go.Scatter(x=[opt_ph_real], y=[opt_temp_real], mode='markers',
                                     marker=dict(color='gold', size=18, symbol='star', line=dict(width=2, color='black')),
                                     name='Predicted Optimum'))
    fig_contour.update_layout(title='<b>2D Response Surface (Contour Plot)</b>',
                              xaxis_title="pH (Real Units)", yaxis_title="Temperature (¬∞C, Real Units)")

    # 7. Create the 3D Surface Plot
    fig_3d = go.Figure(data=[
        go.Surface(z=pred, x=x_range_real, y=y_range_real, colorscale='Viridis', opacity=0.8),
        go.Scatter3d(x=df['pH'], y=df['Temp'], z=df['Response'], mode='markers', 
                      marker=dict(color='red', size=5, line=dict(width=2, color='black')), name='Design Points')
    ])
    fig_3d.update_layout(title='<b>3D Response Surface Plot</b>',
                         scene=dict(xaxis_title='pH', yaxis_title='Temp (¬∞C)', zaxis_title='Response'),
                         margin=dict(l=0, r=0, b=0, t=40))

    # 8. SME Enhancement: Create a Pareto Plot of Standardized Effects
    effects = model.params[1:] # Exclude intercept
    std_errs = model.bse[1:]
    t_values = np.abs(effects / std_errs)
    
    # Get p-values from ANOVA table to color the bars
    p_values_map = anova_summary.set_index('Term')['p-value']
    effect_names = ['pH', 'Temp', 'I(pH**2)', 'I(Temp**2)', 'pH:Temp']
    p_values = [p_values_map.get(name, 1.0) for name in effect_names]
    
    effects_df = pd.DataFrame({'Effect': effect_names, 't-value': t_values, 'p-value': p_values})
    effects_df = effects_df.sort_values(by='t-value', ascending=False)
    
    fig_pareto = px.bar(effects_df, x='Effect', y='t-value',
                        title='<b>Pareto Plot of Standardized Effects</b>',
                        labels={'Effect': 'Model Term', 't-value': 'Absolute t-value (Effect Magnitude)'},
                        color=effects_df['p-value'] < 0.05,
                        color_discrete_map={True: '#00CC96', False: '#636EFA'},
                        template='plotly_white')
    # Add significance threshold line
    t_crit_pareto = stats.t.ppf(1 - 0.05 / 2, df=model.df_resid)
    fig_pareto.add_hline(y=t_crit_pareto, line_dash="dash", line_color="red",
                         annotation_text=f"Significance (p=0.05)", annotation_position="bottom right")
    fig_pareto.update_layout(showlegend=False)

    return fig_contour, fig_3d, fig_pareto, anova_summary, opt_ph_real, opt_temp_real, max_response

@st.cache_data
def plot_mixture_design(a_effect, b_effect, c_effect, ab_interaction, ac_interaction, bc_interaction, noise_sd, response_threshold):
    """
    Generates a professional-grade dashboard for a mixture design of experiments.
    """
    # 1. Define the experimental design points (Simplex-Lattice Design)
    points = [[1,0,0], [0,1,0], [0,0,1], [0.5,0.5,0], [0.5,0,0.5], [0,0.5,0.5], [1/3,1/3,1/3]]
    df = pd.DataFrame(points, columns=['A', 'B', 'C'])

    # 2. Simulate the response using the Scheff√© quadratic model
    a, b, c = df['A'], df['B'], df['C']
    true_response = (a_effect * a) + (b_effect * b) + (c_effect * c) + \
                    (ab_interaction * a * b) + (ac_interaction * a * c) + (bc_interaction * b * c)
    df['Response'] = true_response + np.random.normal(0, noise_sd, len(df))

    # 3. Fit the model
    model = ols('Response ~ A + B + C + A:B + A:C + B:C - 1', data=df).fit()
    
    # 4. Create a dense grid of points for the ternary plot surface
    @st.cache_data
    def generate_ternary_grid(n=40):
        import itertools
        s = itertools.product(range(n + 1), repeat=3)
        points = np.array(list(s))
        points = points[points.sum(axis=1) == n] / n
        return pd.DataFrame(points, columns=['A', 'B', 'C'])
        
    grid = generate_ternary_grid()
    grid['Predicted_Response'] = model.predict(grid)
    
    # 5. Generate Plot 1: Model Effects
    effects = model.params
    effect_df = pd.DataFrame({'Term': effects.index, 'Coefficient': effects.values})
    effect_df['Type'] = ['Interaction' if ':' in term else 'Main Effect' for term in effect_df['Term']]
    fig_effects = px.bar(effect_df, x='Coefficient', y='Term', color='Type', orientation='h',
                         title='<b>1. Model Effects Plot</b>',
                         labels={'Coefficient': 'Coefficient Value (Impact on Response)'})
    fig_effects.update_layout(yaxis={'categoryorder':'total ascending'}, showlegend=False)

    # 6. Generate Plot 2: Professional Ternary Map
    opt_idx = grid['Predicted_Response'].idxmax()
    opt_blend = grid.loc[opt_idx]
    
    fig_ternary = go.Figure()

    # --- THIS IS THE CORRECTED PLOTTING LOGIC ---
    # Layer 1: The heatmap surface, created with a scatterternary trace
    fig_ternary.add_trace(go.Scatterternary(
        a=grid['A'], b=grid['B'], c=grid['C'],
        mode='markers',
        marker=dict(
            color=grid['Predicted_Response'],
            colorscale='Viridis',
            showscale=True,
            colorbar=dict(title='Response<br>(e.g., Solubility)'),
            size=5
        ),
        hoverinfo='none'
    ))

    # Layer 2: The "Sweet Spot" / Design Space boundary
    sweet_spot_grid = grid[grid['Predicted_Response'] >= response_threshold]
    fig_ternary.add_trace(go.Scatterternary(
        a=sweet_spot_grid['A'], b=sweet_spot_grid['B'], c=sweet_spot_grid['C'],
        mode='markers',
        marker=dict(color=SUCCESS_GREEN, size=5, opacity=0.6),
        hoverinfo='none',
        name='Design Space'
    ))
    # --- END OF CORRECTION ---

    # Layer 3: The experimental points
    fig_ternary.add_trace(go.Scatterternary(
        a=df['A'], b=df['B'], c=df['C'], mode='markers',
        marker=dict(symbol='circle', color='red', size=12, line=dict(width=2, color='black')),
        name='DOE Runs'
    ))
    
    # Layer 4: The predicted optimum
    fig_ternary.add_trace(go.Scatterternary(
        a=[opt_blend['A']], b=[opt_blend['B']], c=[opt_blend['C']], mode='markers',
        marker=dict(symbol='star', color='white', size=18, line=dict(width=2, color='black')),
        name='Predicted Optimum'
    ))
    
    fig_ternary.update_layout(
        title='<b>2. Formulation Design Space Map</b>',
        ternary_sum=1,
        ternary_aaxis_title_text='<b>Component A (%)</b>',
        ternary_baxis_title_text='<b>Component B (%)</b>',
        ternary_caxis_title_text='<b>Component C (%)</b>',
        margin=dict(l=40, r=40, b=40, t=60), showlegend=False
    )
    
    return fig_effects, fig_ternary, model, opt_blend
    
@st.cache_data
def plot_doe_optimization_suite(ph_effect, temp_effect, interaction_effect, ph_quad_effect, temp_quad_effect, asymmetry_effect, noise_sd, yield_threshold):
    """
    Generates a full suite of professional-grade plots: Pareto, 3D RSM, 2D RSM Map, and 2D ML PDP.
    """
    np.random.seed(42)
    # 1. Simulate a realistic, asymmetric process
    design_coded = {'pH_coded': [-1, 1, -1, 1, -1.414, 1.414, 0, 0, 0, 0, 0], 'Temp_coded': [-1, -1, 1, 1, 0, 0, -1.414, 1.414, 0, 0, 0]}
    df = pd.DataFrame(design_coded)
    df['pH_real'] = df['pH_coded'] * 0.5 + 7.2
    df['Temp_real'] = df['Temp_coded'] * 5 + 37
    true_response = 100 + ph_effect*df['pH_coded'] + temp_effect*df['Temp_coded'] + interaction_effect*df['pH_coded']*df['Temp_coded'] + \
                    ph_quad_effect*(df['pH_coded']**2) + temp_quad_effect*(df['Temp_coded']**2) + asymmetry_effect*(df['pH_coded']**3)
    df['Response'] = true_response + np.random.normal(0, noise_sd, len(df))

    # 2. Fit both RSM and ML models
    X = df[['pH_coded', 'Temp_coded']]
    y = df['Response']
    rsm_model = ols('Response ~ pH_coded + Temp_coded + I(pH_coded**2) + I(Temp_coded**2) + pH_coded:Temp_coded', data=df).fit()
    ml_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42).fit(X, y)

    # 3. Create prediction grids
    x_range_coded = np.linspace(-2, 2, 100); y_range_coded = np.linspace(-2, 2, 100)
    xx_c, yy_c = np.meshgrid(x_range_coded, y_range_coded)
    grid = pd.DataFrame({'pH_coded': xx_c.ravel(), 'Temp_coded': yy_c.ravel()})
    pred_rsm = rsm_model.predict(grid).values.reshape(xx_c.shape)
    pred_ml = ml_model.predict(grid).reshape(xx_c.shape)
    x_range_real = x_range_coded * 0.5 + 7.2; y_range_real = y_range_coded * 5 + 37

    # 4. Find optimum and define NOR
    max_idx = np.unravel_index(np.argmax(pred_rsm), pred_rsm.shape)
    opt_temp_real, opt_ph_real = y_range_real[max_idx[1]], x_range_real[max_idx[0]]
    max_response = np.max(pred_rsm)
    nor = {'x0': opt_temp_real - 1, 'y0': opt_ph_real - 0.1, 'x1': opt_temp_real + 1, 'y1': opt_ph_real + 0.1}

    # 5. Generate Pareto Plot
    anova_for_pareto = sm.stats.anova_lm(rsm_model, typ=2)
    anova_filtered = anova_for_pareto.drop('Residual', errors='ignore')
    p_values_map = anova_filtered['PR(>F)']
    effects = rsm_model.params[1:]; std_errs = rsm_model.bse[1:]
    t_values = np.abs(effects / std_errs)
    effect_names = ['pH', 'Temp', 'pH¬≤', 'Temp¬≤', 'pH:Temp']
    effects_df = pd.DataFrame({'Effect': effect_names, 't-value': t_values.values, 'p-value': p_values_map.values}).sort_values('t-value', ascending=False)
    
    fig_pareto = px.bar(effects_df, x='Effect', y='t-value', title='<b>1. Pareto Plot of Effects</b>',
                        color=effects_df['p-value'] < 0.05,
                        color_discrete_map={True: SUCCESS_GREEN, False: PRIMARY_COLOR}, template='plotly_white')
    t_crit = stats.t.ppf(1 - 0.05 / 2, df=rsm_model.df_resid)
    fig_pareto.add_hline(y=t_crit, line_dash="dash", line_color="red", annotation_text="Significance (p=0.05)")
    fig_pareto.update_layout(showlegend=False)

    # 6. Generate Professional 3D Surface Plot (Code is correct)
    fig_rsm_3d = go.Figure(data=[go.Surface(z=pred_rsm, x=y_range_real, y=x_range_real, colorscale='Plasma', cmin=80, cmax=100, opacity=0.9, colorbar_title='Yield')])
    fig_rsm_3d.add_trace(go.Scatter3d(x=df['Temp_real'], y=df['pH_real'], z=df['Response'], mode='markers', marker=dict(color='red', size=5, line=dict(width=2, color='black')), name='DOE Runs'))
    fig_rsm_3d.update_layout(title='<b>2a. RSM Model (3D Surface)</b>', scene=dict(xaxis_title='Temp', yaxis_title='pH', zaxis_title='Yield'), margin=dict(l=0,r=0,b=0,t=40))

    # 7. Generate Professional 2D Topographic Map (Code is correct)
    fig_rsm_2d = go.Figure()
    fig_rsm_2d.add_trace(go.Contour(z=pred_rsm, x=y_range_real, y=x_range_real, colorscale='Geyser', contours_coloring='fill', showscale=False))
    fig_rsm_2d.add_trace(go.Contour(z=(pred_rsm >= yield_threshold).astype(int), x=y_range_real, y=x_range_real, contours_coloring='fill', showscale=False, colorscale=[[0, 'rgba(239, 83, 80, 0.4)'], [1, 'rgba(44, 160, 44, 0.4)']], line_width=0))
    fig_rsm_2d.add_shape(type="rect", x0=nor['x0'], y0=nor['y0'], x1=nor['x1'], y1=nor['y1'], line=dict(color='white', width=3, dash='dash'))
    fig_rsm_2d.add_trace(go.Scatter(x=[opt_temp_real], y=[opt_ph_real], mode='markers', marker=dict(color='white', size=18, symbol='star', line=dict(width=2, color='black'))))
    fig_rsm_2d.add_trace(go.Scatter(x=df['Temp_real'], y=df['pH_real'], mode='markers', marker=dict(color='red', size=8, line=dict(width=1, color='black'))))
    fig_rsm_2d.add_annotation(x=np.mean([nor['x0'], nor['x1']]), y=np.mean([nor['y0'], nor['y1']]), text="<b>NOR</b>", showarrow=False, font=dict(color='white', size=16))
    fig_rsm_2d.update_layout(title='<b>2b. RSM Topographic Map (PAR & NOR)</b>', xaxis_title='Temperature (¬∞C)', yaxis_title='pH', margin=dict(l=0,r=0,b=0,t=40), showlegend=False)
    
    # 8. Generate 2D PDP Heatmap from ML Model (Code is correct)
    fig_pdp, ax_pdp = plt.subplots(figsize=(8, 6))
    display = PartialDependenceDisplay.from_estimator(estimator=ml_model, X=X, features=[(0, 1)], feature_names=['pH (coded)', 'Temp (coded)'], kind="average", ax=ax_pdp, contour_kw={"cmap": "viridis"})
    ax_pdp.set_title("3. ML Model (2D Partial Dependence)", fontsize=16)
    pdp_buffer = io.BytesIO()
    fig_pdp.savefig(pdp_buffer, format='png', bbox_inches='tight')
    plt.close(fig_pdp)
    pdp_buffer.seek(0)

    # --- THIS IS THE SECOND KEY FIX ---
    # Create the final anova_table for display with the correct column names
    anova_display_table = sm.stats.anova_lm(rsm_model, typ=2).reset_index()
    anova_display_table.columns = ['Term', 'Sum of Squares', 'df', 'F-value', 'p-value']
    # --- END OF FIX ---
    
    return fig_pareto, fig_rsm_3d, fig_rsm_2d, pdp_buffer, anova_display_table, opt_ph_real, opt_temp_real, max_response

# SNIPPET: Replace your entire plot_bayesian_optimization_step function with this correct version.

@st.cache_data
def plot_bayesian_optimization_step(history, x_range, acquisition_func_type):
    """
    Visualizes a single step of Bayesian Optimization.
    The unhashable true_func has been moved inside this function to make it cacheable.
    This version fixes the NumPy truth ambiguity error.
    """
    from sklearn.gaussian_process import GaussianProcessRegressor
    from sklearn.gaussian_process.kernels import Matern
    
    def true_func(x):
        return (-(x - 15)**2 / 20) + 2 * np.sin(x) + 85
    
    x_hist = np.array(history['x']).reshape(-1, 1)
    y_hist = np.array(history['y'])
    
    kernel = Matern(nu=2.5)
    gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True).fit(x_hist, y_hist)
    
    x_pred = x_range.reshape(-1, 1)
    y_pred, y_std = gpr.predict(x_pred, return_std=True)
    
    # --- START OF THE FIX ---
    # Calculate the acquisition function safely
    if acquisition_func_type == 'Upper Confidence Bound (UCB)':
        kappa = 1.96
        acquisition = y_pred + kappa * y_std
    else: # Expected Improvement (EI)
        y_best = np.max(y_hist)
        # Add a small epsilon to y_std to prevent division by zero and handle the array logic correctly.
        y_std_safe = y_std + 1e-9 
        z = (y_pred - y_best) / y_std_safe
        acquisition = (y_pred - y_best) * norm.cdf(z) + y_std_safe * norm.pdf(z)
    # --- END OF THE FIX ---

    next_point_x = x_pred[np.argmax(acquisition)][0]

    # Plotting
    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1,
                        subplot_titles=("<b>Surrogate Model (Gaussian Process)</b>", "<b>Acquisition Function</b>"))

    fig.add_trace(go.Scatter(x=x_range, y=true_func(x_range), name='True Function (Hidden)', line=dict(color='grey', dash='dash')), row=1, col=1)
    fig.add_trace(go.Scatter(x=x_hist.flatten(), y=y_hist, mode='markers', name='Sampled Points', marker=dict(color='red', size=10, symbol='x')), row=1, col=1)
    fig.add_trace(go.Scatter(x=x_pred.flatten(), y=y_pred, name='GP Mean (Belief)', line=dict(color=PRIMARY_COLOR, width=3)), row=1, col=1)
    fig.add_trace(go.Scatter(x=x_pred.flatten(), y=y_pred + 1.96 * y_std, line=dict(width=0), showlegend=False), row=1, col=1)
    fig.add_trace(go.Scatter(x=x_pred.flatten(), y=y_pred - 1.96 * y_std, line=dict(width=0), fill='tonexty', fillcolor='rgba(0,104,201,0.2)', name='GP 95% CI'), row=1, col=1)
    
    fig.add_trace(go.Scatter(x=x_pred.flatten(), y=acquisition, name='Acquisition Function', line=dict(color=SUCCESS_GREEN, width=3), fill='tozeroy'), row=2, col=1)
    fig.add_vline(x=next_point_x, line=dict(color='red', dash='dash'), row=2, col=1)
    fig.add_annotation(x=next_point_x, y=np.max(acquisition), text="<b>Next Point to Sample!</b>", showarrow=True, arrowhead=2, ax=0, ay=-60, row=2, col=1)

    fig.update_layout(height=600, showlegend=False)
    fig.update_yaxes(title_text="Process Yield", row=1, col=1)
    fig.update_yaxes(title_text="Utility Score", row=2, col=1)
    fig.update_xaxes(title_text="Process Parameter (e.g., Temperature)", row=2, col=1)
    
    return fig, next_point_x
# ==============================================================================
# HELPER & PLOTTING FUNCTION (Split-Plot) - SME ENHANCED
# ==============================================================================
@st.cache_data
def plot_split_plot_doe(lot_variation_sd=0.5, interaction_effect=0.0):
    """
    Generates enhanced, more realistic dynamic plots for a Split-Plot DOE,
    including a controllable interaction effect and a dedicated interaction plot.
    """
    np.random.seed(42)
    
    # --- Define the Experimental Design ---
    lots = ['Lot A', 'Lot B']
    concentrations = [10, 20, 30] # mg/L
    n_replicates = 4 

    # --- Dynamic Data Generation ---
    data = []
    # Simulate a "true" effect for the lots
    # To make the effect consistent, we'll use a fixed shift, scaled by the SD slider
    lot_effects = {'Lot A': 0, 'Lot B': -2 * lot_variation_sd}
    
    # SME Enhancement: Add a controllable interaction effect
    # The effect of the supplement is now different for Lot B
    for lot in lots:
        for conc in concentrations:
            supplement_effect = (conc - 10) * 0.5
            current_interaction = 0
            if lot == 'Lot B':
                # Interaction term: scales with supplement concentration
                current_interaction = interaction_effect * (conc - 10) / 10 

            true_mean = 100 + supplement_effect + lot_effects[lot] + current_interaction
            measurements = np.random.normal(true_mean, 1.5, n_replicates)
            for m in measurements:
                data.append([lot, conc, m])

    df = pd.DataFrame(data, columns=['Lot', 'Supplement', 'Response'])

    # --- Analyze the data with ANOVA ---
    # Note: A proper split-plot uses a mixed model, but for visualization and p-values,
    # a standard ANOVA is a reasonable approximation here.
    model = ols('Response ~ C(Lot) * C(Supplement)', data=df).fit() # Use '*' for interaction
    anova_table = sm.stats.anova_lm(model, typ=2).reset_index()
    anova_table.columns = ['Term', 'Sum of Squares', 'df', 'F-value', 'p-value']
    
    # --- Plotting ---
    fig_main = px.box(df, x='Lot', y='Response', color=df['Supplement'].astype(str),
                 title='<b>1. Split-Plot Experimental Results</b>',
                 labels={
                     "Lot": "Base Media Lot (Hard-to-Change)",
                     "Response": "Cell Viability (%)",
                     "color": "Supplement Conc. (mg/L)"
                 },
                 points='all')
    
    # SME Enhancement: Add mean lines to the box plot for clarity
    mean_data = df.groupby(['Lot', 'Supplement'])['Response'].mean().reset_index()
    for conc in concentrations:
        subset = mean_data[mean_data['Supplement'] == conc]
        fig_main.add_trace(go.Scatter(x=subset['Lot'], y=subset['Response'], mode='lines',
                                      line=dict(width=3, dash='dash'), showlegend=False))

    fig_main.update_layout(legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01))

    # SME Enhancement: Create a dedicated Interaction Plot
    fig_interaction = px.line(mean_data, x='Supplement', y='Response', color='Lot',
                              title='<b>2. Interaction Plot</b>',
                              labels={'Supplement': 'Supplement Conc. (mg/L)', 'Response': 'Mean Cell Viability (%)'},
                              markers=True)
    fig_interaction.update_layout(xaxis=dict(tickvals=concentrations),
                                  legend=dict(yanchor="top", y=0.99, xanchor="right", x=0.99))
    fig_interaction.add_annotation(x=20, y=mean_data['Response'].min(),
                                   text="<i>Parallel lines = No Interaction<br>Non-parallel lines = Interaction</i>",
                                   showarrow=False, yshift=-40)

    return fig_main, fig_interaction, anova_table

# SNIPPET: Add or replace the plot_causal_inference function in the "ALL HELPER & PLOTTING FUNCTIONS" section.

@st.cache_data
def plot_causal_inference(confounding_strength=5.0):
    """
    Generates enhanced, more realistic dynamic plots for the Causal Inference module,
    featuring a professionally redesigned Directed Acyclic Graph (DAG).
    """
    import statsmodels.api as sm
    from statsmodels.formula.api import ols
    
    # --- 1. The Causal Map (DAG) - Redesigned for professional rendering ---
    fig_dag = go.Figure()

    # Node positions and properties
    nodes = {
        'Sensor Reading': {'pos': (0, 0), 'color': '#636EFA'},
        'Product Purity': {'pos': (3, 0), 'color': '#00CC96'},
        'Calibration Age': {'pos': (1.5, 1.5), 'color': '#EF553B'}
    }
    
    # Add Edges (Arrows) with color-coding for paths
    # Causal Path (Green)
    fig_dag.add_annotation(x=nodes['Product Purity']['pos'][0] - 0.45, y=nodes['Product Purity']['pos'][1], # Arrow head
                           ax=nodes['Sensor Reading']['pos'][0] + 0.45, ay=nodes['Sensor Reading']['pos'][1], # Arrow tail
                           xref='x', yref='y', axref='x', ayref='y', showarrow=True,
                           arrowhead=2, arrowwidth=4, arrowcolor='#00CC96')
    
    # Backdoor Path (Red)
    fig_dag.add_annotation(x=nodes['Sensor Reading']['pos'][0] + 0.15, y=nodes['Sensor Reading']['pos'][1] + 0.4,
                           ax=nodes['Calibration Age']['pos'][0] - 0.15, ay=nodes['Calibration Age']['pos'][1] - 0.4,
                           xref='x', yref='y', axref='x', ayref='y', showarrow=True,
                           arrowhead=2, arrowwidth=3, arrowcolor='#EF553B')
    fig_dag.add_annotation(x=nodes['Product Purity']['pos'][0] - 0.15, y=nodes['Product Purity']['pos'][1] + 0.4,
                           ax=nodes['Calibration Age']['pos'][0] + 0.15, ay=nodes['Calibration Age']['pos'][1] - 0.4,
                           xref='x', yref='y', axref='x', ayref='y', showarrow=True,
                           arrowhead=2, arrowwidth=3, arrowcolor='#EF553B')

    # Add Nodes (Circles)
    for name, attrs in nodes.items():
        fig_dag.add_shape(type="circle", xref="x", yref="y",
                          x0=attrs['pos'][0] - 0.5, y0=attrs['pos'][1] - 0.5,
                          x1=attrs['pos'][0] + 0.5, y1=attrs['pos'][1] + 0.5,
                          line_color="Black", fillcolor=attrs['color'], line_width=2)
        fig_dag.add_annotation(x=attrs['pos'][0], y=attrs['pos'][1], text=f"<b>{name.replace(' ', '<br>')}</b>",
                               showarrow=False, font=dict(color='white', size=12))

    # Add Path Labels
    fig_dag.add_annotation(x=1.5, y=-0.3, text="<b><span style='color:#00CC96'>Direct Causal Path</span></b>",
                           showarrow=False, font_size=14)
    fig_dag.add_annotation(x=1.5, y=0.8, text="<b><span style='color:#EF553B'>Confounding 'Backdoor' Path</span></b>",
                           showarrow=False, font_size=14)

    fig_dag.update_layout(
        title="<b>1. The Causal Map (DAG): Calibration Drift Scenario</b>",
        showlegend=False, xaxis=dict(visible=False, showgrid=False, range=[-1, 4]),
        yaxis=dict(visible=False, showgrid=False, range=[-1, 2.5]),
        height=400, margin=dict(t=50, b=20), plot_bgcolor='rgba(0,0,0,0)'
    )

    # --- 2. Simulate data demonstrating Simpson's Paradox ---
    np.random.seed(42)
    n_samples = 200
    cal_age = np.random.randint(0, 2, n_samples)
    
    true_causal_effect_sensor_on_purity = 0.8
    true_effect_age_on_purity = -confounding_strength
    true_effect_age_on_sensor = confounding_strength
    
    sensor = 50 + true_effect_age_on_sensor * cal_age + np.random.normal(0, 5, n_samples)
    purity = 90 + true_causal_effect_sensor_on_purity * (sensor - 50) + true_effect_age_on_purity * cal_age + np.random.normal(0, 5, n_samples)
    
    df = pd.DataFrame({'SensorReading': sensor, 'Purity': purity, 'CalibrationAge': cal_age})
    df['calibration_status'] = df['CalibrationAge'].apply(lambda x: 'Old' if x == 1 else 'New')

    # --- 3. Calculate effects ---
    naive_model = ols('Purity ~ SensorReading', data=df).fit()
    naive_effect = naive_model.params['SensorReading']
    
    # Check if we have both old and new calibration data
    if len(df['calibration_status'].unique()) > 1:
        adjusted_model = ols('Purity ~ SensorReading + C(calibration_status)', data=df).fit()
        adjusted_effect = adjusted_model.params['SensorReading']
    else:
        # If only one calibration status, the adjusted effect is just the naive effect.
        adjusted_effect = naive_effect

    # --- 4. Create the scatter plot ---
    fig_scatter = px.scatter(df, x='SensorReading', y='Purity', color='calibration_status',
                             title="<b>2. Simpson's Paradox: The Danger of Confounding</b>",
                             color_discrete_map={'New': 'blue', 'Old': 'red'},
                             labels={'SensorReading': 'In-Process Sensor Reading', 'calibration_status': 'Calibration Status'})
    
    x_range = np.array([df['SensorReading'].min(), df['SensorReading'].max()])
    
    fig_scatter.add_trace(go.Scatter(x=x_range, y=naive_model.predict({'SensorReading': x_range}), mode='lines', 
                                     name='Naive Correlation (Misleading)', line=dict(color='orange', width=4, dash='dash')))
    
    # Only plot the adjusted causal effect if both groups are present
    if len(df['calibration_status'].unique()) > 1:
        intercept_new = adjusted_model.params['Intercept']
        intercept_old = intercept_new + adjusted_model.params['C(calibration_status)[T.Old]']
        fig_scatter.add_trace(go.Scatter(x=x_range, y=intercept_new + adjusted_effect * x_range, mode='lines', 
                                         name='True Causal Effect (Within Groups)', line=dict(color='darkgreen', width=4)))
        fig_scatter.add_trace(go.Scatter(x=x_range, y=intercept_old + adjusted_effect * x_range, mode='lines', 
                                         showlegend=False, line=dict(color='darkgreen', width=4)))

    fig_scatter.update_layout(height=500, legend=dict(x=0.01, y=0.99))
    
    return fig_dag, fig_scatter, naive_effect, adjusted_effect

# ============================================ Casual ML ==================================================
# SNIPPET: Replace your entire plot_causal_ml_comparison function with this optimized version.

@st.cache_data
def plot_causal_ml_comparison(confounding_strength):
    """
    Compares a standard model's correlation to a Causal ML estimate.
    OPTIMIZED for speed and CORRECTED for all data shape and return type issues.
    """
    from econml.dml import LinearDML
    from sklearn.ensemble import RandomForestRegressor
    import statsmodels.api as sm

    # Simulate data
    np.random.seed(42)
    n = 500
    W = np.random.uniform(0, 10, size=(n, 1)) # Confounder
    T_1d = np.random.uniform(0, 5, n) + W.flatten() * confounding_strength # Treatment
    T = T_1d.reshape(-1, 1) # Ensure Treatment is 2D: (n_samples, 1)
    
    # The true causal effect of T is a constant +2
    Y = 2 * T.flatten() + 5 * np.sin(W.flatten()) + np.random.normal(0, 1, n)

    # --- Causal ML Model (LinearDML) ---
    est = LinearDML(
        model_y=RandomForestRegressor(n_estimators=30, min_samples_leaf=10, random_state=42), 
        model_t=RandomForestRegressor(n_estimators=30, min_samples_leaf=10, random_state=42),
        random_state=42
    )
    
    # --- THIS IS THE DEFINITIVE FIX ---
    # Ensure Y is a 1D array (`.ravel()`) to prevent the scikit-learn DataConversionWarning.
    est.fit(Y.ravel(), T, W=W)
    # Extract the final estimate as a simple float using .item()
    causal_effect_ate = est.const_marginal_effect().item()
    # --- END OF DEFINITIVE FIX ---

    # --- Standard OLS Model for Naive Correlation ---
    # This provides a simple coefficient to display as the biased effect
    naive_model = sm.OLS(Y, sm.add_constant(T)).fit()
    naive_effect_coeff = naive_model.params[1]

    # --- PLOTTING ---
    fig = go.Figure()
    
    # Plot the naive (biased) regression line
    x_range_plot = np.linspace(T.min(), T.max(), 100)
    fig.add_trace(go.Scatter(x=x_range_plot, y=naive_model.predict(sm.add_constant(x_range_plot)), name=f'Naive Correlation (Slope ‚âà {naive_effect_coeff:.2f})', line=dict(color='red', dash='dash', width=3)))
    
    # Plot the true causal effect line
    intercept_causal = Y.mean() - (causal_effect_ate * T.mean())
    fig.add_trace(go.Scatter(x=x_range_plot, y=intercept_causal + causal_effect_ate * x_range_plot, name=f'Causal ML Effect (ATE ‚âà {causal_effect_ate:.2f})', line=dict(color=SUCCESS_GREEN, width=4)))
    
    # Plot raw data for context
    fig.add_trace(go.Scatter(x=T.flatten(), y=Y, mode='markers', name='Raw Data', marker=dict(opacity=0.1, color='grey')))
    
    fig.update_layout(title="<b>Standard Correlation vs. Causal ML: Uncovering the True Effect</b>",
                      xaxis_title="Process Parameter Value", yaxis_title="Impact on Process Output",
                      legend=dict(x=0.01, y=0.99))
                      
    # Return the figure and the two simple float values
    return fig, causal_effect_ate, naive_effect_coeff
    
#=====================================================================================================================================================================
##=============================================================================================END ACT I ===================================================================================
##==========================================================================================================================================================================================
@st.cache_data
def plot_sample_size_curves(confidence_level, reliability, lot_size, calc_method, required_n):
    """
    Generates a plot showing the trade-off between sample size and achievable reliability.
    """
    c = confidence_level / 100
    r_req = reliability / 100
    
    # Define the range of sample sizes to plot
    max_n = 3 * required_n if isinstance(required_n, int) and required_n > 50 else 300
    n_range = np.arange(1, max_n)

    # --- Calculate Achievable Reliability for each model ---
    # Binomial Calculation (inverse of the main formula)
    r_binomial = (1 - c)**(1 / n_range)

    # Hypergeometric Calculation (iterative solve for R at each n)
    @st.cache_data
    def solve_hypergeometric_r(n, M, C):
        log_alpha = math.log(1 - C)
        # Iterate backwards from the max possible defects to find the highest D that works
        for D in range(int(0.5 * M), -1, -1):
            if n > M - D: continue # Cannot sample more than the number of good items
            log_prob_zero_defect = (
                math.lgamma(M - D + 1) - math.lgamma(n + 1) - math.lgamma(M - D - n + 1)
            ) - (
                math.lgamma(M + 1) - math.lgamma(n + 1) - math.lgamma(M - n + 1)
            )
            if log_prob_zero_defect <= log_alpha:
                return (M - D) / M
        return 0.5 # Return a baseline if no solution found
    
    r_hypergeometric = [solve_hypergeometric_r(n, lot_size, c) for n in n_range] if lot_size else None

    # --- Create the Plot ---
    fig = go.Figure()

    # Add Binomial Curve
    fig.add_trace(go.Scatter(
        x=n_range, y=r_binomial, mode='lines', name='Binomial Model (Infinite Lot)',
        line=dict(color=PRIMARY_COLOR, width=3)
    ))

    # Add Hypergeometric Curve if applicable
    if r_hypergeometric and "Hypergeometric" in calc_method:
        fig.add_trace(go.Scatter(
            x=n_range, y=r_hypergeometric, mode='lines', name=f'Hypergeometric (Lot Size={lot_size})',
            line=dict(color='#FF7F0E', width=3, dash='dash')
        ))
        
    # Add the user's requirement point and lines
    if isinstance(required_n, int):
        fig.add_trace(go.Scatter(
            x=[required_n], y=[r_req], mode='markers', name='Your Requirement',
            marker=dict(color=SUCCESS_GREEN, size=15, symbol='star', line=dict(width=2, color='black'))
        ))
        # Add dashed lines to the axes
        fig.add_shape(type="line", x0=0, y0=r_req, x1=required_n, y1=r_req, line=dict(color="grey", width=2, dash="dash"))
        fig.add_shape(type="line", x0=required_n, y0=0, x1=required_n, y1=r_req, line=dict(color="grey", width=2, dash="dash"))

    fig.update_layout(
        title="<b>Sample Size vs. Achievable Reliability</b>",
        xaxis_title="Sample Size (n) with Zero Failures",
        yaxis_title=f"Achievable Reliability at {confidence_level:.1f}% Confidence",
        yaxis=dict(tickformat=".2%", range=[min(r_binomial) - 0.01, 1.01]),
        xaxis=dict(range=[0, max_n]),
        legend=dict(yanchor="bottom", y=0.01, xanchor="right", x=0.99)
    )

    return fig

@st.cache_data
def plot_stability_design_comparison(strengths, containers, design_type):
    """
    Generates a professional-grade dashboard comparing full, bracketing, and matrixing stability study designs.
    """
    time_points = ["0", "3", "6", "9", "12", "18", "24", "36"]
    
    # --- Generate the full design matrix ---
    full_design = []
    for s in strengths:
        for c in containers:
            full_design.append({'Strength': s, 'Container': c, 'Timepoints': len(time_points)})
    df_full = pd.DataFrame(full_design)
    
    # --- Determine the reduced design based on user selection ---
    df_reduced = df_full.copy()
    pulls_saved = 0
    
    if design_type == "Bracketing":
        min_s, max_s = min(strengths), max(strengths)
        df_reduced = df_full[df_full['Strength'].isin([min_s, max_s])].copy()
    
    elif design_type == "Matrixing":
        # A simple half-matrix design for demonstration
        df_reduced['Timepoints'] = 0
        for i, row in df_full.iterrows():
            if (i % 2) == 0: # Test every other combination at all timepoints
                df_reduced.loc[i, 'Timepoints'] = len(time_points)
            else: # For the others, test only at key timepoints
                df_reduced.loc[i, 'Timepoints'] = 3 # e.g., 0, 12, 36
    
    total_pulls_full = df_full['Timepoints'].sum()
    total_pulls_reduced = df_reduced['Timepoints'].sum()
    pulls_saved = total_pulls_full - total_pulls_reduced

    # --- Create the Visualization (Heatmap Table) ---
    def create_heatmap_table(df, title):
        pivot = df.pivot(index='Container', columns='Strength', values='Timepoints').fillna(0).astype(int)
        fig = px.imshow(pivot, text_auto=True, aspect="auto",
                        labels=dict(x="Strength (mg)", y="Container Type", color="Pulls"),
                        title=f"<b>{title} (Total Pulls: {df['Timepoints'].sum()})</b>",
                        color_continuous_scale='Greens')
        fig.update_xaxes(side="top")
        return fig

    fig_full = create_heatmap_table(df_full, "Full Study Design")
    fig_reduced = create_heatmap_table(df_reduced, f"Reduced Study Design ({design_type})")
    
    return fig_full, fig_reduced, pulls_saved, total_pulls_full, total_pulls_reduced

@st.cache_data
def plot_spc_charts(scenario='Stable'):
    """
    Generates dynamic SPC charts based on a selected process scenario.
    """
    np.random.seed(42)
    n_points = 25
    
    # --- Generate Base Data ---
    data_i = np.random.normal(loc=100.0, scale=2.0, size=n_points)
    data_xbar = np.random.normal(loc=100, scale=5, size=(n_points, 5))
    data_p_defects = np.random.binomial(n=200, p=0.02, size=n_points)
    
    # --- Inject Special Cause based on Scenario ---
    if scenario == 'Sudden Shift':
        data_i[15:] += 8
        data_xbar[15:, :] += 6
        data_p_defects[15:] = np.random.binomial(n=200, p=0.08, size=10)
    elif scenario == 'Gradual Trend':
        trend = np.linspace(0, 10, n_points)
        data_i += trend
        data_xbar += trend[:, np.newaxis]
        data_p_defects += np.random.binomial(n=200, p=trend/200, size=n_points)
    elif scenario == 'Increased Variability':
        data_i[15:] = np.random.normal(loc=100.0, scale=6.0, size=10)
        data_xbar[15:, :] = np.random.normal(loc=100, scale=15, size=(10, 5))
        data_p_defects[15:] = np.random.binomial(n=200, p=0.02, size=10) # Less obvious on p-chart

    # --- I-MR Chart ---
    x_i = np.arange(1, len(data_i) + 1)
    limit_data_i = data_i[:15] if scenario != 'Stable' else data_i
    mean_i = np.mean(limit_data_i)
    mr = np.abs(np.diff(data_i))
    mr_mean = np.mean(np.abs(np.diff(limit_data_i)))
    sigma_est_i = mr_mean / 1.128
    UCL_I, LCL_I = mean_i + 3 * sigma_est_i, mean_i - 3 * sigma_est_i
    UCL_MR = mr_mean * 3.267
    
    fig_imr = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1, subplot_titles=("I-Chart", "MR-Chart"))
    fig_imr.add_trace(go.Scatter(x=x_i, y=data_i, mode='lines+markers', name='Value'), row=1, col=1)
    fig_imr.add_hline(y=mean_i, line=dict(dash='dash', color='black'), row=1, col=1); fig_imr.add_hline(y=UCL_I, line=dict(color='red'), row=1, col=1); fig_imr.add_hline(y=LCL_I, line=dict(color='red'), row=1, col=1)
    fig_imr.add_trace(go.Scatter(x=x_i[1:], y=mr, mode='lines+markers', name='Range'), row=2, col=1)
    fig_imr.add_hline(y=mr_mean, line=dict(dash='dash', color='black'), row=2, col=1); fig_imr.add_hline(y=UCL_MR, line=dict(color='red'), row=2, col=1)
    fig_imr.update_layout(title_text='<b>1. I-MR Chart</b>', showlegend=False)
    
    # --- X-bar & R Chart ---
    subgroup_means = np.mean(data_xbar, axis=1)
    subgroup_ranges = np.max(data_xbar, axis=1) - np.min(data_xbar, axis=1)
    x_xbar = np.arange(1, n_points + 1)
    limit_data_xbar_means = subgroup_means[:15] if scenario != 'Stable' else subgroup_means
    limit_data_xbar_ranges = subgroup_ranges[:15] if scenario != 'Stable' else subgroup_ranges
    mean_xbar, mean_r = np.mean(limit_data_xbar_means), np.mean(limit_data_xbar_ranges)
    UCL_X, LCL_X = mean_xbar + 0.577 * mean_r, mean_xbar - 0.577 * mean_r
    UCL_R = 2.114 * mean_r; LCL_R = 0 * mean_r

    fig_xbar = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1, subplot_titles=("X-bar Chart", "R-Chart"))
    fig_xbar.add_trace(go.Scatter(x=x_xbar, y=subgroup_means, mode='lines+markers'), row=1, col=1)
    fig_xbar.add_hline(y=mean_xbar, line=dict(dash='dash', color='black'), row=1, col=1); fig_xbar.add_hline(y=UCL_X, line=dict(color='red'), row=1, col=1); fig_xbar.add_hline(y=LCL_X, line=dict(color='red'), row=1, col=1)
    fig_xbar.add_trace(go.Scatter(x=x_xbar, y=subgroup_ranges, mode='lines+markers'), row=2, col=1)
    fig_xbar.add_hline(y=mean_r, line=dict(dash='dash', color='black'), row=2, col=1); fig_xbar.add_hline(y=UCL_R, line=dict(color='red'), row=2, col=1)
    fig_xbar.update_layout(title_text='<b>2. X-bar & R Chart</b>', showlegend=False)

    # --- P-Chart ---
    proportions = data_p_defects / 200
    limit_data_p = proportions[:15] if scenario != 'Stable' else proportions
    p_bar = np.mean(limit_data_p)
    sigma_p = np.sqrt(p_bar * (1-p_bar) / 200)
    UCL_P, LCL_P = p_bar + 3 * sigma_p, max(0, p_bar - 3 * sigma_p)

    fig_p = go.Figure()
    fig_p.add_trace(go.Scatter(x=np.arange(1, n_points+1), y=proportions, mode='lines+markers'))
    fig_p.add_hline(y=p_bar, line=dict(dash='dash', color='black')); fig_p.add_hline(y=UCL_P, line=dict(color='red')); fig_p.add_hline(y=LCL_P, line=dict(color='red'))
    fig_p.update_layout(title_text='<b>3. P-Chart</b>', yaxis_tickformat=".0%", showlegend=False, xaxis_title="Batch Number", yaxis_title="Proportion Defective")
    
    return fig_imr, fig_xbar, fig_p
    
@st.cache_data
def plot_capability(scenario='Ideal'):
    """
    Generates enhanced, more realistic dynamic plots for the process capability module,
    including multiple 'out of control' types and a KDE overlay.
    """
    np.random.seed(42)
    n = 150
    LSL, USL, Target = 90, 110, 100
    
    # --- Data Generation based on scenario ---
    mean, std = 100, 1.5
    is_stable = True
    phase1_end = 75

    if scenario == 'Ideal (High Cpk)':
        mean, std = 100, 1.5
    elif scenario == 'Shifted (Low Cpk)':
        mean, std = 104, 1.5
    elif scenario == 'Variable (Low Cpk)':
        mean, std = 100, 3.5
    elif scenario == 'Out of Control (Shift)':
        is_stable = False
        data = np.random.normal(mean, std, n)
        data[phase1_end:] += 6 # Add a shift
    elif scenario == 'Out of Control (Trend)':
        is_stable = False
        data = np.random.normal(mean, std, n)
        data += np.linspace(0, 8, n) # Add a gradual trend
    elif scenario == 'Out of Control (Bimodal)':
        is_stable = False
        data1 = np.random.normal(97, 1.5, n // 2)
        data2 = np.random.normal(103, 1.5, n // 2)
        data = np.concatenate([data1, data2])
        np.random.shuffle(data)

    if is_stable:
        data = np.random.normal(mean, std, n)
        
    # --- Control Chart Calculations ---
    # Use only stable part for limits if a known instability is introduced
    limit_data = data[:phase1_end] if scenario in ['Out of Control (Shift)', 'Out of Control (Trend)'] else data
    center_line = np.mean(limit_data)
    mr_mean = np.mean(np.abs(np.diff(limit_data)))
    sigma_est = mr_mean / 1.128 # d2 for n=2
    UCL_I, LCL_I = center_line + 3 * sigma_est, center_line - 3 * sigma_est
    
    ooc_indices = np.where((data > UCL_I) | (data < LCL_I))[0]

    # --- Capability Calculation ---
    if not is_stable or len(ooc_indices) > 0:
        cpk_val = np.nan # Invalid if not stable
    else:
        # Use overall mean and std for capability calculation if stable
        data_mean, data_std = np.mean(data), np.std(data, ddof=1)
        cpk_upper = (USL - data_mean) / (3 * data_std)
        cpk_lower = (data_mean - LSL) / (3 * data_std)
        cpk_val = min(cpk_upper, cpk_lower)

    # --- Plotting ---
    fig = make_subplots(
        rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1,
        subplot_titles=("<b>1. Control Chart (Is the process stable?)</b>",
                        "<b>2. Capability Histogram (Does it meet specs?)</b>")
    )
    # Control Chart
    fig.add_trace(go.Scatter(x=np.arange(n), y=data, mode='lines+markers', name='Process Data',
                             marker=dict(color='#636EFA')), row=1, col=1)
    fig.add_trace(go.Scatter(x=ooc_indices, y=data[ooc_indices], mode='markers', name='Out of Control',
                             marker=dict(color='#EF553B', size=10, symbol='x')), row=1, col=1)
    fig.add_hline(y=center_line, line_dash="dash", line_color="black", row=1, col=1)
    fig.add_hline(y=UCL_I, line_color="red", row=1, col=1)
    fig.add_hline(y=LCL_I, line_color="red", row=1, col=1)
    if scenario in ['Out of Control (Shift)', 'Out of Control (Trend)']:
        fig.add_vrect(x0=phase1_end - 0.5, x1=n - 0.5, fillcolor="rgba(255,150,0,0.15)", line_width=0,
                      annotation_text="Process Change", annotation_position="top left", row=1, col=1)
    
    # Histogram
    fig.add_trace(go.Histogram(x=data, name='Distribution', nbinsx=25, histnorm='probability density'), row=2, col=1)
    
    # SME Enhancement: Use KDE instead of normal curve for flexibility
    from scipy.stats import gaussian_kde
    kde = gaussian_kde(data)
    x_curve = np.linspace(min(data.min(), LSL-5), max(data.max(), USL+5), 200)
    y_curve = kde(x_curve)
    fig.add_trace(go.Scatter(x=x_curve, y=y_curve, mode='lines', name='Process Voice (KDE)',
                             line=dict(color='darkblue', width=3)), row=2, col=1)
    
    # SME Enhancement: Highlight OOC points on the histogram
    if len(ooc_indices) > 0:
        fig.add_trace(go.Scatter(x=data[ooc_indices], y=np.zeros_like(ooc_indices), mode='markers',
                                 name='OOC Points', marker=dict(color='#EF553B', size=8, symbol='circle')), row=2, col=1)

    # Add Spec Limits
    fig.add_vline(x=LSL, line_dash="dot", line_color="darkred", annotation_text="<b>LSL</b>", row=2, col=1)
    fig.add_vline(x=USL, line_dash="dot", line_color="darkred", annotation_text="<b>USL</b>", row=2, col=1)
    fig.add_vline(x=Target, line_dash="dash", line_color="grey", annotation_text="Target", row=2, col=1)
    
    # SME Enhancement: Add a Cpk Verdict annotation
    if np.isnan(cpk_val):
        verdict_text, verdict_color = "INVALID (Process Unstable)", "#EF553B"
    elif cpk_val < 1.0:
        verdict_text, verdict_color = f"POOR (Cpk = {cpk_val:.2f})", "#EF553B"
    elif cpk_val < 1.33:
        verdict_text, verdict_color = f"MARGINAL (Cpk = {cpk_val:.2f})", "#FECB52"
    else:
        verdict_text, verdict_color = f"GOOD (Cpk = {cpk_val:.2f})", "#00CC96"
        
    fig.add_annotation(x=0.98, y=0.98, xref="x2 domain", yref="y2 domain",
                       text=f"<b>Capability Verdict:<br>{verdict_text}</b>",
                       showarrow=False, font=dict(size=16, color='white'),
                       bgcolor=verdict_color, borderpad=10, bordercolor='black', borderwidth=2)

    fig.update_layout(height=700, showlegend=False, xaxis2_title="Measured Value")
    return fig, cpk_val

@st.cache_data
def plot_proportion_cis(n_samples, n_successes, prior_alpha, prior_beta):
    """
    Calculates and plots a comparison of multiple confidence interval methods for a binomial proportion.
    """
    if n_samples == 0:
        return go.Figure(), {} # Return empty objects if no data

    k, n = n_successes, n_samples
    p_hat = k / n
    
    # --- Calculate all 7 intervals ---
    metrics = {}
    
    # 1. Wald (for demonstration of its flaws)
    if n > 0 and p_hat > 0 and p_hat < 1:
        wald_se = np.sqrt(p_hat * (1 - p_hat) / n)
        metrics['Wald (Approximate)'] = (p_hat - 1.96 * wald_se, p_hat + 1.96 * wald_se)
    else:
        metrics['Wald (Approximate)'] = (0, 0) # Fails at extremes
        
    # 2. Wilson Score
    metrics['Wilson Score'] = wilson_score_interval(p_hat, n)
    
    # 3. Agresti-Coull
    n_adj, k_adj = n + 4, k + 2
    p_adj = k_adj / n_adj
    ac_se = np.sqrt(p_adj * (1 - p_adj) / n_adj)
    metrics['Agresti‚ÄìCoull'] = (p_adj - 1.96 * ac_se, p_adj + 1.96 * ac_se)
    
    # 4. Clopper-Pearson (Exact)
    metrics['Clopper‚ÄìPearson (Exact)'] = stats.beta.interval(0.95, k, n - k + 1)
    
    # 5. Jeffreys Interval (Bayesian)
    metrics['Jeffreys Interval (Bayesian)'] = stats.beta.interval(0.95, k + 0.5, n - k + 0.5)
    
    # 6. Bayesian with Beta Priors
    metrics['Bayesian with Custom Prior'] = stats.beta.interval(0.95, k + prior_alpha, n - k + prior_beta)
    
    # 7. Bootstrapped CI
    @st.cache_data
    def bootstrap_ci(num_samples, num_successes):
        data = np.array([1] * num_successes + [0] * (num_samples - num_successes))
        if len(data) == 0: return (0, 0)
        boot_means = [np.random.choice(data, size=len(data), replace=True).mean() for _ in range(2000)]
        return np.percentile(boot_means, [2.5, 97.5])
    metrics['Bootstrapped CI'] = bootstrap_ci(n, k)
    
    # --- Create the Plot ---
    fig = go.Figure()
    
    # Order the intervals for logical presentation
    order = ['Wald (Approximate)', 'Agresti‚ÄìCoull', 'Wilson Score', 'Clopper‚ÄìPearson (Exact)', 
             'Jeffreys Interval (Bayesian)', 'Bayesian with Custom Prior', 'Bootstrapped CI']
    
    for i, name in enumerate(order):
        lower, upper = metrics[name]
        color = '#EF553B' if name.startswith('Wald') else PRIMARY_COLOR
        fig.add_trace(go.Scatter(
            x=[lower, upper], y=[name, name],
            mode='lines', line=dict(color=color, width=10),
            hovertemplate=f"<b>{name}</b><br>95% CI: [{lower:.3f}, {upper:.3f}]<extra></extra>"
        ))
    
    fig.add_vline(x=p_hat, line=dict(color='black', dash='dash'), annotation_text=f"Observed Rate = {p_hat:.2%}")
    fig.add_vrect(x0=0.95, x1=1.0, fillcolor="rgba(44, 160, 44, 0.1)", layer="below", line_width=0,
                  annotation_text="Target >95% Success", annotation_position="bottom left")

    fig.update_layout(
        title=f'<b>Comparing 95% CIs for {k} Successes in {n} Samples</b>',
        xaxis_title='Success Rate (Proportion)',
        yaxis_title='Confidence Interval Method',
        xaxis_range=[-0.05, 1.05],
        xaxis_tickformat=".0%",
        showlegend=False
    )
    
    return fig, metrics

@st.cache_data
def plot_process_equivalence(cpk_site_a, mean_shift, var_change_factor, n_samples, margin):
    """
    Generates a professional, multi-plot dashboard for demonstrating statistical equivalence between two processes.
    """
    np.random.seed(42)
    lsl, usl = 90, 110
    
    # 1. Define Site A (Original Process) from its Cpk
    mean_a = 100
    std_a = (usl - lsl) / (6 * cpk_site_a)
    data_a = np.random.normal(mean_a, std_a, n_samples)
    
    # 2. Define Site B (New Process) based on shifts
    mean_b = mean_a + mean_shift
    std_b = std_a * var_change_factor
    data_b = np.random.normal(mean_b, std_b, n_samples)
    
    # 3. Calculate Cpk for both samples
    def calculate_cpk(data, lsl, usl):
        m, s = np.mean(data), np.std(data, ddof=1)
        if s == 0: return 10.0 # Handle case of no variation
        return min((usl - m) / (3 * s), (m - lsl) / (3 * s))
    
    cpk_a_sample = calculate_cpk(data_a, lsl, usl)
    cpk_b_sample = calculate_cpk(data_b, lsl, usl)
    diff_cpk = cpk_b_sample - cpk_a_sample

    # 4. Perform Equivalence Test (Bootstrap CI for Cpk difference)
    # --- THIS DECORATOR HAS BEEN REMOVED ---
    def bootstrap_cpk_diff(d1, d2, l, u, n_boot=1000):
        boot_diffs = []
        for _ in range(n_boot):
            s1 = np.random.choice(d1, len(d1), replace=True)
            s2 = np.random.choice(d2, len(d2), replace=True)
            boot_cpk1 = calculate_cpk(s1, l, u)
            boot_cpk2 = calculate_cpk(s2, l, u)
            boot_diffs.append(boot_cpk2 - boot_cpk1)
        return np.array(boot_diffs)
    # --- END OF FIX ---

    boot_diffs = bootstrap_cpk_diff(data_a, data_b, lsl, usl)
    ci_lower, ci_upper = np.percentile(boot_diffs, [5, 95]) # 90% CI for TOST
    is_equivalent = (ci_lower >= -margin) and (ci_upper <= margin)

    # 5. Generate Plots
    fig = make_subplots(
        rows=3, cols=1,
        subplot_titles=("<b>1. Process Capability Comparison</b>", "<b>2. Statistical Evidence (Bootstrap Distribution of Difference)</b>", "<b>3. Equivalence Verdict</b>"),
        row_heights=[0.5, 0.3, 0.2], vertical_spacing=0.15
    )
    
    # Plot 1: Smoothed Capability Distributions (KDE)
    x_range = np.linspace(lsl-10, usl+10, 300)
    kde_a = stats.gaussian_kde(data_a)
    kde_b = stats.gaussian_kde(data_b)
    fig.add_trace(go.Scatter(x=x_range, y=kde_a(x_range), fill='tozeroy', name=f'Site A (Cpk={cpk_a_sample:.2f})', line=dict(color=PRIMARY_COLOR)), row=1, col=1)
    fig.add_trace(go.Scatter(x=x_range, y=kde_b(x_range), fill='tozeroy', name=f'Site B (Cpk={cpk_b_sample:.2f})', line=dict(color=SUCCESS_GREEN)), row=1, col=1)
    fig.add_vline(x=lsl, line_dash="dot", line_color="darkred", annotation_text="<b>LSL</b>", row=1, col=1)
    fig.add_vline(x=usl, line_dash="dot", line_color="darkred", annotation_text="<b>USL</b>", row=1, col=1)
    fig.update_layout(barmode='overlay', legend=dict(yanchor="top", y=0.98, xanchor="left", x=0.01))
    fig.update_traces(opacity=0.7, row=1, col=1)
    fig.update_yaxes(showticklabels=False, row=1, col=1)

    # Plot 2: Bootstrap Distribution of the Difference (The "Bridge Plot")
    ci_color = SUCCESS_GREEN if is_equivalent else '#EF553B'
    fig.add_trace(go.Histogram(x=boot_diffs, name='Bootstrap Results', marker_color='grey', histnorm='probability density'), row=2, col=1)
    fig.add_vrect(x0=ci_lower, x1=ci_upper, fillcolor=ci_color, opacity=0.3, line_width=0, row=2, col=1)
    fig.add_vline(x=-margin, line_dash="dash", line_color="red", row=2, col=1)
    fig.add_vline(x=margin, line_dash="dash", line_color="red", row=2, col=1)
    fig.update_yaxes(showticklabels=False, row=2, col=1)

    # Plot 3: Equivalence Verdict Bar
    fig.add_vrect(x0=-margin, x1=margin, fillcolor="rgba(44,160,44,0.1)", layer="below", line_width=0, row=3, col=1)
    fig.add_trace(go.Scatter(x=[ci_lower, ci_upper], y=[1, 1], mode='lines', line=dict(color=ci_color, width=10)), row=3, col=1)
    fig.add_trace(go.Scatter(x=[diff_cpk], y=[1], mode='markers', marker=dict(color='white', size=10, line=dict(color='black', width=2))), row=3, col=1)
    fig.add_annotation(x=0, y=1.5, text=f"Equivalence Zone (¬±{margin})", showarrow=False, row=3, col=1)
    fig.update_yaxes(showticklabels=False, row=3, col=1)
    fig.update_xaxes(title_text="Difference in Cpk (Site B - Site A)", row=3, col=1)
    
    return fig, is_equivalent, diff_cpk, cpk_a_sample, cpk_b_sample, ci_lower, ci_upper

# Place this helper function inside the get_plot_functions function
## =====================================================================================================================================================================================================================================
##================================================================================== SPECIAL SET / METHOD COMPARISON ANOVA, t-test, TOST, Wassertein DISTANCE ==========================================================================
#=======================================================================================================================================================================================================================================
@st.cache_data
def plot_two_process_wasserstein(df_a, df_b, lsl, usl, threshold):
    mean_a, mean_b = df_a['value'].mean(), df_b['value'].mean()
    ttest_p = stats.ttest_ind(df_a['value'], df_b['value'], equal_var=False).pvalue
    _, f_p = stats.levene(df_a['value'], df_b['value'])
    emd = stats.wasserstein_distance(df_a['value'], df_b['value'])
    is_equivalent = emd < threshold
    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1, subplot_titles=("<b>1. Process Distribution Comparison (PDF)</b>", "<b>2. Cumulative Distribution Comparison (CDF)</b>"))
    from scipy.stats import gaussian_kde
    x_range = np.linspace(min(df_a['value'].min(), df_b['value'].min()) - 5, max(df_a['value'].max(), df_b['value'].max()) + 5, 400)
    kde_a, kde_b = gaussian_kde(df_a['value']), gaussian_kde(df_b['value'])
    fig.add_trace(go.Scatter(x=x_range, y=kde_a(x_range), fill='tozeroy', name='Site A (Reference)', line=dict(color=PRIMARY_COLOR, width=3)), row=1, col=1)
    fig.add_trace(go.Scatter(x=x_range, y=kde_b(x_range), fill='tozeroy', name='Site B (New)', line=dict(color=SUCCESS_GREEN, width=3), opacity=0.7), row=1, col=1)
    fig.add_vline(x=lsl, line_dash="dot", line_color="darkred", annotation_text="<b>LSL</b>", row=1, col=1); fig.add_vline(x=usl, line_dash="dot", line_color="darkred", annotation_text="<b>USL</b>", row=1, col=1)
    cdf_a = np.array([np.mean(df_a['value'] <= x) for x in x_range]); cdf_b = np.array([np.mean(df_b['value'] <= x) for x in x_range])
    fig.add_trace(go.Scatter(x=x_range, y=cdf_a, name='CDF Site A', line=dict(color=PRIMARY_COLOR, width=3)), row=2, col=1)
    fig.add_trace(go.Scatter(x=x_range, y=cdf_b, name='CDF Site B', line=dict(color=SUCCESS_GREEN, width=3), fill='tonexty', fillcolor='rgba(255, 193, 7, 0.3)', hovertemplate=None), row=2, col=1)
    fig.add_annotation(x=np.median(x_range), y=0.5, text="<b>Area between curves ‚âà<br>Wasserstein Distance</b>", showarrow=False, font=dict(color=DARK_GREY, size=14), row=2, col=1)
    fig.update_layout(height=700, legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))
    fig.update_yaxes(title_text="Density", showticklabels=False, row=1, col=1); fig.update_yaxes(title_text="Cumulative Probability", range=[0,1.05], row=2, col=1); fig.update_xaxes(title_text="Process Output Value", row=2, col=1)
    return fig, emd, ttest_p, f_p, is_equivalent

@st.cache_data
def plot_two_process_dashboard(data_a, data_b, lsl, usl, wasserstein_dist, paired_df=None):
    """
    Generates a multi-panel dashboard for deep comparison of TWO processes.
    Includes distributional plots and a Bland-Altman plot for paired data.
    """
    # This spec creates a large plot on the left and two smaller plots on the right
    specs = [
        [{"rowspan": 2}, {}],
        [None, {}]
    ]
    fig = make_subplots(
        rows=2, cols=2,
        column_widths=[0.6, 0.4],
        subplot_titles=(
            "<b>1. Process Distributions (PDF & CDF)</b>",
            "<b>2. Bland-Altman Agreement Plot</b>",
            None # Title for 2,1 is handled by the main title
        ),
        specs=specs, vertical_spacing=0.15, horizontal_spacing=0.1
    )

    # --- Plot 1 (Left, Spanning Two Rows) ---
    # PDF
    x_range = np.linspace(min(data_a.min(), data_b.min()) - 5, max(data_a.max(), data_b.max()) + 5, 400)
    kde_a = stats.gaussian_kde(data_a)
    kde_b = stats.gaussian_kde(data_b)
    fig.add_trace(go.Scatter(x=x_range, y=kde_a(x_range), fill='tozeroy', name='Process A (Reference)', line=dict(color=PRIMARY_COLOR)), row=1, col=1)
    fig.add_trace(go.Scatter(x=x_range, y=kde_b(x_range), fill='tozeroy', name='Process B (New)', line=dict(color=SUCCESS_GREEN), opacity=0.7), row=1, col=1)
    
    # CDF on secondary y-axis
    cdf_a = np.array([np.mean(data_a <= x) for x in x_range])
    cdf_b = np.array([np.mean(data_b <= x) for x in x_range])
    fig.add_trace(go.Scatter(x=x_range, y=cdf_a, name='CDF A', line=dict(color=PRIMARY_COLOR, width=2, dash='dash'), yaxis="y2"), row=1, col=1)
    fig.add_trace(go.Scatter(x=x_range, y=cdf_b, name='CDF B', line=dict(color=SUCCESS_GREEN, width=2, dash='dash'), yaxis="y2"), row=1, col=1)
    
    fig.add_vline(x=lsl, line_dash="dot", line_color="darkred", annotation_text="<b>LSL</b>", row=1, col=1)
    fig.add_vline(x=usl, line_dash="dot", line_color="darkred", annotation_text="<b>USL</b>", row=1, col=1)
    
    # --- Plot 2 (Top Right): Bland-Altman ---
    if paired_df is not None:
        mean_diff = paired_df['Difference'].mean()
        std_diff = paired_df['Difference'].std(ddof=1)
        upper_loa = mean_diff + 1.96 * std_diff
        lower_loa = mean_diff - 1.96 * std_diff
        fig.add_trace(go.Scatter(x=paired_df['Average'], y=paired_df['Difference'], mode='markers', name='Samples'), row=1, col=2)
        fig.add_hline(y=mean_diff, line=dict(color='blue'), name='Mean Bias', row=1, col=2, annotation_text=f"Bias={mean_diff:.2f}")
        fig.add_hline(y=upper_loa, line=dict(color='red', dash='dash'), name='Upper LoA', row=1, col=2)
        fig.add_hline(y=lower_loa, line=dict(color='red', dash='dash'), name='Lower LoA', row=1, col=2)
    else:
        fig.add_annotation(text="Bland-Altman requires paired data.<br>Data is independent.", showarrow=False, row=1, col=2)
        # Add invisible trace to render the empty plot
        fig.add_trace(go.Scatter(x=[0], y=[0], mode='markers', marker=dict(opacity=0)), row=1, col=2)


    # --- Plot 3 (Bottom Right): Wasserstein Visualization ---
    # We will use an annotation here instead of a full plot to keep it clean
    fig.add_annotation(text=f"<b>Wasserstein Distance</b><br>(Area between CDFs)<br><span style='font-size: 24px;'>{wasserstein_dist:.3f}</span>",
                       font=dict(size=16), showarrow=False, row=2, col=2)
    # Add invisible trace to render the empty plot
    fig.add_trace(go.Scatter(x=[0], y=[0], mode='markers', marker=dict(opacity=0)), row=2, col=2)
    
    # --- Layout Updates ---
    fig.update_layout(
        height=700,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        yaxis=dict(title="Density", showticklabels=False),
        yaxis2=dict(title="Cumulative Prob.", overlaying='y', side='right', range=[0, 1.05]),
        xaxis=dict(title="Process Output Value"),
        yaxis3=dict(title="Difference (B - A)"),
        xaxis3=dict(title="Average of Methods"),
        yaxis4=dict(visible=False),
        xaxis4=dict(visible=False)
    )
    return fig

@st.cache_data
def plot_multi_process_comparison(df_all, lsl, usl):
    """
    Generates a violin plot to compare the distributions of three or more processes.
    """
    fig = go.Figure()
    line_names = df_all['Line'].unique()
    colors = px.colors.qualitative.Plotly
    
    for i, line in enumerate(line_names):
        subset = df_all[df_all['Line'] == line]['value']
        fig.add_trace(go.Violin(
            x=subset, 
            y0=line, 
            name=f'Line {line}', 
            orientation='h', 
            side='positive', 
            width=1.5, 
            points='all', 
            pointpos=0, 
            jitter=0.1, 
            line_color=colors[i % len(colors)]
        ))
        
    fig.add_vline(x=lsl, line_dash="dot", line_color="darkred", annotation_text="<b>LSL</b>")
    fig.add_vline(x=usl, line_dash="dot", line_color="darkred", annotation_text="<b>USL</b>")
    fig.update_layout(
        title="<b>Process Distribution Comparison</b>", 
        xaxis_title="Process Output Value", 
        yaxis_title="Production Line",
        legend_title="Production Line"
    )
    fig.update_traces(meanline_visible=True)
    return fig
# ==============================================================================
# HELPER & PLOTTING FUNCTION (Process & Method Comparability Suite) - ROBUST FIX
# ==============================================================================
@st.cache_data
def plot_comparability_dashboard(data_a, data_b, lsl, usl, wasserstein_dist,
                                 is_multi_process_mode=False,
                                 tukey_p_adj=None, tukey_group_pairs=None,
                                 qq_data_list=None, line_names=None):
    """
    Generates a multi-panel dashboard for process comparison.
    Accepts only simple, hashable data types for robust caching.
    """
    if not is_multi_process_mode: # Two-process mode
        fig = make_subplots(
            rows=2, cols=1, shared_xaxes=True,
            subplot_titles=("<b>1. Visual Evidence: Process Distributions (PDFs)</b>", "<b>2. Visual Evidence: Cumulative Distributions (CDFs)</b>"),
            vertical_spacing=0.1
        )
        x_range = np.linspace(min(data_a) - 5, max(data_a) + 5, 400)
        kde_a, kde_b = stats.gaussian_kde(data_a), stats.gaussian_kde(data_b)
        fig.add_trace(go.Scatter(x=x_range, y=kde_a(x_range), fill='tozeroy', name='Site A (Reference)', line=dict(color=PRIMARY_COLOR)), row=1, col=1)
        fig.add_trace(go.Scatter(x=x_range, y=kde_b(x_range), fill='tozeroy', name='Site B (New)', line=dict(color=SUCCESS_GREEN), opacity=0.7), row=1, col=1)
        fig.add_vline(x=lsl, line_dash="dot", line_color="darkred", annotation_text="<b>LSL</b>", row=1, col=1)
        fig.add_vline(x=usl, line_dash="dot", line_color="darkred", annotation_text="<b>USL</b>", row=1, col=1)
        fig.update_yaxes(title_text="Density", showticklabels=False, row=1, col=1)

        cdf_a = np.array([np.mean(data_a <= x) for x in x_range])
        cdf_b = np.array([np.mean(data_b <= x) for x in x_range])
        fig.add_trace(go.Scatter(x=x_range, y=cdf_a, name='CDF Site A', line=dict(color=PRIMARY_COLOR, width=3)), row=2, col=1)
        fig.add_trace(go.Scatter(x=x_range, y=cdf_b, name='CDF Site B', line=dict(color=SUCCESS_GREEN, width=3), fill='tonexty', fillcolor='rgba(255, 193, 7, 0.3)'), row=2, col=1)
        fig.add_annotation(x=np.median(x_range), y=0.5, text=f"<b>Area between curves ‚âà<br>Wasserstein Distance: {wasserstein_dist:.2f}</b>",
                           showarrow=False, font=dict(color=DARK_GREY, size=14), bgcolor='rgba(255, 193, 7, 0.5)', borderpad=4, row=2, col=1)
        fig.update_yaxes(title_text="Cumulative Prob.", range=[0, 1.05], row=2, col=1)
        fig.update_xaxes(title_text="Process Output Value", row=2, col=1)
        fig.update_layout(height=600, legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))
        return fig

    else: # Three-plus-process mode
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("<b>ANOVA Post-Hoc: Tukey's HSD</b>", "<b>Distributional Diagnostics: Q-Q Plots</b>")
        )
        # Plot 1: Tukey's HSD
        colors = ['#EF553B' if p < 0.05 else SUCCESS_GREEN for p in tukey_p_adj]
        fig.add_trace(go.Bar(
            x=tukey_p_adj, y=tukey_group_pairs, orientation='h', marker_color=colors,
            text=[f"p={p:.3f}" for p in tukey_p_adj], textposition='auto'
        ), row=1, col=1)
        fig.add_vline(x=0.05, line_dash="dash", line_color="red", row=1, col=1, annotation_text="p=0.05")
        fig.update_xaxes(title_text="Adjusted p-value", range=[0,1], row=1, col=1)
        fig.update_yaxes(title_text="Pairwise Comparison", categoryorder='total ascending', row=1, col=1)
        
        # Plot 2: Q-Q Plots
        qq_a_sorted = np.sort(qq_data_list[0])
        for i in range(1, len(line_names)):
            qq_b_sorted = np.sort(qq_data_list[i])
            interp_func = np.interp(np.linspace(0, 1, len(qq_a_sorted)), np.linspace(0, 1, len(qq_b_sorted)), qq_b_sorted)
            fig.add_trace(go.Scatter(x=qq_a_sorted, y=interp_func, mode='markers', name=f'{line_names[i]} vs. {line_names[0]}'), row=1, col=2)
        
        all_qq_data = np.concatenate(qq_data_list)
        min_val, max_val = all_qq_data.min(), all_qq_data.max()
        fig.add_shape(type='line', x0=min_val, y0=min_val, x1=max_val, y1=max_val, line=dict(color='red', dash='dash'), row=1, col=2)
        fig.update_xaxes(title_text=f"Quantiles of Reference ({line_names[0]})", row=1, col=2)
        fig.update_yaxes(title_text="Quantiles of Comparison Lines", scaleanchor="x2", scaleratio=1, row=1, col=2)
        
        fig.update_layout(height=400, showlegend=True, legend=dict(yanchor="bottom", y=0.01))
        return fig
# ==============================================================================
# HELPER & PLOTTING FUNCTION (Tolerance Intervals) - SME ENHANCED
# ==============================================================================
@st.cache_data
def plot_tolerance_intervals(n=30, coverage_pct=99.0):
    """
    Generates enhanced, more realistic, and illustrative dynamic plots for the
    Tolerance Interval module, including a visualization of the true population.
    """
    np.random.seed(42)
    pop_mean, pop_std = 100, 5
    
    # Simulate a single sample from the true population
    data = np.random.normal(pop_mean, pop_std, n)
    mean, std = np.mean(data), np.std(data, ddof=1)
    
    # 95% CI for the mean (n-dependent)
    sem = std / np.sqrt(n) if n > 0 else 0
    ci_margin = t.ppf(0.975, df=n-1) * sem if n > 1 else 0
    ci = (mean - ci_margin, mean + ci_margin)
    
    # Tolerance Interval (n and coverage dependent)
    # Using a more precise calculation instead of a lookup table
    from scipy.stats import chi2
    g = (1 - (1 - 0.95) / 2) # 95% confidence
    p = coverage_pct / 100.0 # e.g., 99% coverage
    
    # Chi-square factor for standard deviation uncertainty
    chi2_val = chi2.ppf(1-g, n-1)
    
    # Z-score for coverage proportion
    z_val = norm.ppf((1+p)/2)
    
    # Combine for the k-factor (Howe's method approximation)
    k_factor = z_val * np.sqrt((n-1)*(1 + 1/n) / chi2_val)
    
    ti_margin = k_factor * std
    ti = (mean - ti_margin, mean + ti_margin)

    # --- Plotting ---
    fig = go.Figure()

    # SME Enhancement: Plot the true population distribution in the background
    x_range = np.linspace(pop_mean - 5 * pop_std, pop_mean + 5 * pop_std, 400)
    pop_pdf = norm.pdf(x_range, pop_mean, pop_std)
    fig.add_trace(go.Scatter(x=x_range, y=pop_pdf, mode='lines',
                             line=dict(color='grey', dash='dash'), name='True Population Distribution'))

    # SME Enhancement: Shade the area of the true population covered by the TI
    x_fill = np.linspace(ti[0], ti[1], 100)
    y_fill = norm.pdf(x_fill, pop_mean, pop_std)
    fig.add_trace(go.Scatter(x=x_fill, y=y_fill, fill='tozeroy',
                             mode='none', fillcolor='rgba(0,128,0,0.3)',
                             name=f'Area Covered by TI'))
    
    # Plot the sample data histogram
    fig.add_trace(go.Histogram(x=data, name='Sample Data', histnorm='probability density',
                               marker_color='#636EFA'))

    # Add annotations for the two intervals
    # Confidence Interval
    fig.add_shape(type="rect", xref="x", yref="paper", x0=ci[0], y0=0.6, x1=ci[1], y1=0.7,
                  fillcolor="rgba(255,165,0,0.7)", line_width=1, line_color="black")
    fig.add_annotation(x=(ci[0]+ci[1])/2, y=0.65, yref="paper",
                       text=f"<b>CI for Mean</b><br>[{ci[0]:.2f}, {ci[1]:.2f}]",
                       showarrow=False, font=dict(color="black"))

    # Tolerance Interval
    fig.add_shape(type="rect", xref="x", yref="paper", x0=ti[0], y0=0.4, x1=ti[1], y1=0.5,
                  fillcolor="rgba(0,128,0,0.7)", line_width=1, line_color="black")
    fig.add_annotation(x=(ti[0]+ti[1])/2, y=0.45, yref="paper",
                       text=f"<b>Tolerance Interval</b><br>[{ti[0]:.2f}, {ti[1]:.2f}]",
                       showarrow=False, font=dict(color="white"))
    
    # Calculate the actual coverage for display
    actual_coverage = norm.cdf(ti[1], pop_mean, pop_std) - norm.cdf(ti[0], pop_mean, pop_std)
    
    fig.update_layout(
        title=f"<b>Confidence vs. Tolerance Interval | Actual Coverage: {actual_coverage:.2%}</b>",
        xaxis_title="Measured Value", yaxis_title="Density", showlegend=False,
        annotations=[
            dict(x=0.02, y=0.98, xref='paper', yref='paper',
                 text="<b>CI asks:</b> Where is the <u>mean</u>?<br><b>TI asks:</b> Where are the <u>individuals</u>?",
                 showarrow=False, align='left', font=dict(size=14),
                 bgcolor='rgba(255,255,255,0.7)')
        ]
    )
    
    return fig, ci, ti

@st.cache_data
def plot_method_comparison(constant_bias=2.0, proportional_bias=3.0, random_error_sd=3.0):
    """
    Generates an enhanced, more realistic, and integrated dashboard for method comparison,
    using Passing-Bablok regression and adding CIs to the Bland-Altman plot.
    """
    np.random.seed(1)
    n_samples = 50
    true_values = np.linspace(20, 200, n_samples)
    
    # Simulate data with different error types
    error_ref = np.random.normal(0, random_error_sd, n_samples)
    error_test = np.random.normal(0, random_error_sd, n_samples)
    
    ref_method = true_values + error_ref
    test_method = constant_bias + true_values * (1 + proportional_bias / 100) + error_test
    
    df = pd.DataFrame({'Reference': ref_method, 'Test': test_method})

    # --- SME Enhancement: Implement Passing-Bablok Regression ---
    # This is a simplified version for demonstration. A real implementation is more complex.
    slopes = []
    for i in range(n_samples):
        for j in range(i + 1, n_samples):
            if (df['Reference'][i] - df['Reference'][j]) != 0:
                slope = (df['Test'][i] - df['Test'][j]) / (df['Reference'][i] - df['Reference'][j])
                slopes.append(slope)
    pb_slope = np.median(slopes)
    pb_intercept = np.median(df['Test'] - pb_slope * df['Reference'])
    
    # --- Bland-Altman Calculations with Confidence Intervals ---
    df['Average'] = (df['Reference'] + df['Test']) / 2
    df['Difference'] = df['Test'] - df['Reference']
    mean_diff = df['Difference'].mean()
    std_diff = df['Difference'].std(ddof=1)
    
    # CIs for the mean bias and Limits of Agreement
    ci_bias_margin = 1.96 * std_diff / np.sqrt(n_samples)
    ci_loa_margin = 1.96 * std_diff * np.sqrt(3 / n_samples)
    
    upper_loa = mean_diff + 1.96 * std_diff
    lower_loa = mean_diff - 1.96 * std_diff
    
    # --- Plotting: Integrated 2x2 Dashboard ---
    fig = make_subplots(
        rows=2, cols=2,
        specs=[[{"rowspan": 2}, {}], [None, {}]],
        subplot_titles=("<b>1. Method Agreement (Passing-Bablok)</b>",
                        "<b>2. Bland-Altman Plot</b>",
                        "<b>3. Residuals vs. Reference</b>"),
        vertical_spacing=0.15, horizontal_spacing=0.1
    )

    # Plot 1 (Main): Passing-Bablok Regression
    fig.add_trace(go.Scatter(x=df['Reference'], y=df['Test'], mode='markers', name='Samples',
                             marker=dict(color='#636EFA')), row=1, col=1)
    x_range = np.array([0, df['Reference'].max() * 1.05])
    y_fit = pb_intercept + pb_slope * x_range
    fig.add_trace(go.Scatter(x=x_range, y=y_fit, mode='lines', name='Passing-Bablok Fit',
                             line=dict(color='red', width=3)), row=1, col=1)
    fig.add_trace(go.Scatter(x=x_range, y=x_range, mode='lines', name='Line of Identity (y=x)',
                             line=dict(color='black', dash='dash')), row=1, col=1)
    fig.add_annotation(x=0.05, y=0.95, xref="x domain", yref="y domain",
                       text=f"<b>y = {pb_slope:.2f}x + {pb_intercept:.2f}</b>",
                       showarrow=False, font=dict(size=14, color='red'), row=1, col=1)

    # Plot 2 (Top-Right): Bland-Altman
    fig.add_trace(go.Scatter(x=df['Average'], y=df['Difference'], mode='markers', name='Difference',
                             marker=dict(color='#1f77b4')), row=1, col=2)
    # Mean Bias with CI
    fig.add_hrect(y0=mean_diff - ci_bias_margin, y1=mean_diff + ci_bias_margin,
                  fillcolor='rgba(0,0,255,0.1)', line_width=0, row=1, col=2)
    fig.add_hline(y=mean_diff, line=dict(color='blue'), name='Mean Bias', row=1, col=2,
                  annotation_text=f"Bias: {mean_diff:.2f}", annotation_position="bottom right")
    # Limits of Agreement with CIs
    fig.add_hrect(y0=upper_loa - ci_loa_margin, y1=upper_loa + ci_loa_margin,
                  fillcolor='rgba(255,0,0,0.1)', line_width=0, row=1, col=2)
    fig.add_hrect(y0=lower_loa - ci_loa_margin, y1=lower_loa + ci_loa_margin,
                  fillcolor='rgba(255,0,0,0.1)', line_width=0, row=1, col=2)
    fig.add_hline(y=upper_loa, line=dict(color='red', dash='dash'), name='Upper LoA', row=1, col=2)
    fig.add_hline(y=lower_loa, line=dict(color='red', dash='dash'), name='Lower LoA', row=1, col=2)

    # Plot 3 (Bottom-Right): Residuals vs. Reference (to diagnose proportional bias)
    df['Residuals'] = df['Test'] - (pb_intercept + pb_slope * df['Reference'])
    fig.add_trace(go.Scatter(x=df['Reference'], y=df['Residuals'], mode='markers', name='Residuals',
                             marker=dict(color='#ff7f0e')), row=2, col=2)
    fig.add_hline(y=0, line=dict(color='black', dash='dash'), row=2, col=2)

    fig.update_layout(height=800, title_text="<b>Method Comparison Dashboard</b>", title_x=0.5,
                      legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.7)'))
    fig.update_xaxes(title_text="Reference Method", row=1, col=1); fig.update_yaxes(title_text="Test Method", row=1, col=1)
    fig.update_xaxes(title_text="Average of Methods", row=1, col=2); fig.update_yaxes(title_text="Difference (Test - Ref)", row=1, col=2)
    fig.update_xaxes(title_text="Reference Method", row=2, col=2); fig.update_yaxes(title_text="Residuals from Fit", row=2, col=2)
    
    return fig, pb_slope, pb_intercept, mean_diff, upper_loa, lower_loa

@st.cache_data
def plot_bayesian(prior_type, n_qc=20, k_qc=18, spec_limit=0.90):
    """
    Generates enhanced, more realistic, and interactive plots for the Bayesian inference module,
    including interactive data and visualization of the credible interval.
    """
    # Define Priors based on selection
    if prior_type == "Strong R&D Prior":
        # Corresponds to ~98 successes in 100 trials
        a_prior, b_prior = 98, 2
    elif prior_type == "Skeptical/Regulatory Prior":
        # Weakly centered around 80%, wide uncertainty
        a_prior, b_prior = 4, 1
    else: # "No Prior (Uninformative)"
        # Uninformative Jeffreys prior
        a_prior, b_prior = 0.5, 0.5
        
    # Bayesian Update (Posterior calculation)
    a_post = a_prior + k_qc
    b_post = b_prior + (n_qc - k_qc)
    
    # Calculate key metrics
    prior_mean = a_prior / (a_prior + b_prior)
    mle = k_qc / n_qc if n_qc > 0 else 0
    posterior_mean = a_post / (a_post + b_post)

    # --- Plotting ---
    x = np.linspace(0, 1, 500)
    fig = go.Figure()

    # Plot Prior
    prior_pdf = beta.pdf(x, a_prior, b_prior)
    fig.add_trace(go.Scatter(x=x, y=prior_pdf, mode='lines', name='Prior Belief',
                             line=dict(color='green', dash='dash', width=3)))

    # Plot Posterior
    posterior_pdf = beta.pdf(x, a_post, b_post)
    fig.add_trace(go.Scatter(x=x, y=posterior_pdf, mode='lines', name='Posterior Belief (Updated)',
                             line=dict(color='blue', width=4), fill='tozeroy',
                             fillcolor='rgba(0,0,255,0.1)'))
    
    # SME Enhancement: Calculate and shade the 95% Credible Interval (HDI)
    ci_lower, ci_upper = beta.ppf([0.025, 0.975], a_post, b_post)
    x_fill = np.linspace(ci_lower, ci_upper, 100)
    y_fill = beta.pdf(x_fill, a_post, b_post)
    fig.add_trace(go.Scatter(x=x_fill, y=y_fill, fill='tozeroy', mode='none',
                             fillcolor='rgba(0,0,255,0.3)', name='95% Credible Interval'))

    # SME Enhancement: Show the data/likelihood as a point estimate
    fig.add_trace(go.Scatter(x=[mle], y=[0], mode='markers', name=f'Data Likelihood (k/n = {mle:.2f})',
                             marker=dict(color='red', size=15, symbol='diamond', line=dict(width=2, color='black'))))

    # SME Enhancement: Calculate probability of meeting a spec
    prob_gt_spec = 1.0 - beta.cdf(spec_limit, a_post, b_post)
    fig.add_vline(x=spec_limit, line_dash="dot", line_color="black",
                  annotation_text=f"Spec Limit ({spec_limit:.0%})", annotation_position="top")

    fig.update_layout(
        title=f"<b>Bayesian Update for QC Pass Rate</b>",
        xaxis_title="True Pass Rate Parameter (Œ∏)", yaxis_title="Probability Density",
        legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.7)'),
        xaxis=dict(range=[0,1], tickformat=".0%"),
        yaxis=dict(showticklabels=False)
    )
    
    return fig, prior_mean, mle, posterior_mean, (ci_lower, ci_upper), prob_gt_spec

#============================================== ODE ================================================================
@st.cache_data
def plot_line_sync_ode(rates):
    """
    Simulates and plots buffer levels in a production line using a fast, discrete-time model.
    This replaces the slow ODE solver with a direct NumPy-based simulation for instantaneous results.
    """
    # 1. --- Simulation Parameters ---
    total_time = 40  # hours
    n_steps = 200    # Number of time steps in the simulation
    dt = total_time / n_steps  # Size of each time step

    r0, r1, r2, r3 = rates
    
    # 2. --- Initialize Arrays ---
    time_points = np.linspace(0, total_time, n_steps)
    buffer1_levels = np.zeros(n_steps)
    buffer2_levels = np.zeros(n_steps)

    # 3. --- Run the Discrete-Time Simulation Loop ---
    for i in range(1, n_steps):
        # Buffer 1 (between Step 1 and 2)
        inflow_1 = r0
        outflow_1 = r1 if buffer1_levels[i-1] > 0 else 0
        d_buffer1 = (inflow_1 - outflow_1) * dt
        buffer1_levels[i] = max(0, buffer1_levels[i-1] + d_buffer1)

        # Buffer 2 (between Step 2 and 3)
        inflow_2 = outflow_1 
        outflow_2 = r2 if buffer2_levels[i-1] > 0 else 0
        d_buffer2 = (inflow_2 - outflow_2) * dt
        # --- THIS IS THE CORRECTED LINE ---
        # The variable name is corrected from the likely typo 'els' to 'buffer2_levels'.
        buffer2_levels[i] = max(0, buffer2_levels[i-1] + d_buffer2)
        # --- END OF CORRECTION ---
        
    # 4. --- Plotting ---
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=time_points, y=buffer1_levels, name='Buffer 1 Level (Upstream)', line=dict(width=3)))
    fig.add_trace(go.Scatter(x=time_points, y=buffer2_levels, name='Buffer 2 Level (Downstream)', line=dict(width=3)))

    bottleneck_idx = np.argmin(rates)
    bottleneck_rate = rates[bottleneck_idx]
    
    fig.update_layout(
        title=f"<b>Line Synchronization Dynamics (Bottleneck Rate: {bottleneck_rate} units/hr)</b>",
        xaxis_title="Time (Hours)",
        yaxis_title="Work-in-Progress (WIP) Units in Buffer",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )
    
    return fig, np.max(buffer1_levels), np.max(buffer2_levels)

#======================================================= L E A N =======================================================================
@st.cache_data
def plot_value_stream_map(process_times, wait_times):
    """
    Generates a professional, multi-layered, and educational Value Stream Map.
    """
    steps = ["Receipt", "Dispensing", "Granulation", "Compression", "QC Testing", "Packaging"]
    total_value_added = sum(process_times)
    total_lead_time = total_value_added + sum(wait_times)
    pce = (total_value_added / total_lead_time) * 100 if total_lead_time > 0 else 0

    fig = go.Figure()

    # --- LAYOUT CONSTANTS ---
    y_process = 2.0
    y_info = 4.0
    y_timeline = -0.5

    # --- 1. RENDER MAIN PROCESS & INVENTORY FLOW ---
    x_pos = 10.0
    x_positions = [x_pos]
    for i, step in enumerate(steps):
        # Process Box
        fig.add_shape(type="rect", x0=x_pos, y0=y_process-0.6, x1=x_pos+12, y1=y_process+0.6,
                      fillcolor=SUCCESS_GREEN, line=dict(color="black", width=2))
        fig.add_annotation(x=x_pos+6, y=y_process, text=f"<b>{step}</b>", showarrow=False, font_color="white", font_size=14)
        
        # Data Box
        fig.add_shape(type="rect", x0=x_pos, y0=y_process-1.8, x1=x_pos+12, y1=y_process-0.6,
                      fillcolor="white", line=dict(color="black"))
        fig.add_annotation(x=x_pos+6, y=y_process-1.2, text=f"C/T: {process_times[i]} hrs<br>Yield: {np.random.uniform(0.95, 0.99):.1%}<br>Uptime: 98%",
                           showarrow=False, align="left", xanchor="center", font_size=11)
        
        x_pos += 12
        # Inventory Triangle (Wait Time)
        if i < len(steps) - 1:
            fig.add_shape(type="path", path=f"M {x_pos}, {y_process-0.7} L {x_pos+6}, {y_process} L {x_pos}, {y_process+0.7} Z",
                          fillcolor="#EF553B", line_color="black")
            fig.add_annotation(x=x_pos+3, y=y_process-1, text=f"{wait_times[i]} hrs", showarrow=False, font_size=12)
            fig.add_annotation(x=x_pos+3, y=y_process+1, text="üí•", font=dict(size=24, color="red"), showarrow=False) # Kaizen Burst
            x_pos += 6
        x_positions.append(x_pos)

    # --- 2. RENDER INFORMATION FLOW ---
    fig.add_shape(type="rect", x0=45, y0=y_info-0.4, x1=60, y1=y_info+0.4, fillcolor="lightblue", line_color="black")
    fig.add_annotation(x=52.5, y=y_info, text="<b>Production Control</b>", showarrow=False)
    # Push signal to first process
    fig.add_annotation(ax=52.5, ay=y_info-0.4, x=x_positions[0]+6, y=y_process+0.6, arrowhead=2, arrowwidth=2, text="Weekly Schedule")
    
    # --- THIS IS THE CORRECTED LINE ---
    # The invalid 'line=dict(dash="dash")' argument has been removed.
    fig.add_annotation(ax=x_positions[2]+6, ay=y_process+0.6, x=x_positions[1]+12, y=y_process+0.6, arrowhead=2, arrowwidth=2, arrowside="start", text="Kanban Signal")
    # --- END OF CORRECTION ---

    # --- 3. RENDER TIMELINE ---
    timeline_width = x_positions[-1] - x_positions[0]
    fig.add_shape(type="line", x0=x_positions[0], x1=x_positions[-1], y0=y_timeline, y1=y_timeline, line=dict(color="black", width=3))
    
    current_time_pos = x_positions[0]
    for i in range(len(process_times)):
        # Value-add time
        va_width = (process_times[i] / total_lead_time) * timeline_width
        fig.add_shape(type="line", x0=current_time_pos, x1=current_time_pos+va_width, y0=y_timeline, y1=y_timeline, line=dict(color=SUCCESS_GREEN, width=20))
        current_time_pos += va_width
        # Wait time
        if i < len(wait_times):
            nva_width = (wait_times[i] / total_lead_time) * timeline_width
            fig.add_shape(type="line", x0=current_time_pos, x1=current_time_pos+nva_width, y0=y_timeline, y1=y_timeline, line=dict(color="#EF553B", width=20))
            current_time_pos += nva_width
    
    fig.add_annotation(x=x_positions[0], y=y_timeline-0.5, text=f"<b>Value-Added Time: {total_value_added:.1f} Hours</b>", showarrow=False, xanchor="left", font_color=SUCCESS_GREEN)
    fig.add_annotation(x=x_positions[-1], y=y_timeline-0.5, text=f"<b>Total Lead Time: {total_lead_time:.1f} Hours</b>", showarrow=False, xanchor="right", font_color="black")
    
    fig.update_layout(
        title=f"<b>Value Stream Map (Process Cycle Efficiency: {pce:.1f}%)</b>",
        xaxis=dict(visible=False, range=[0, x_positions[-1]+10]),
        yaxis=dict(visible=False, range=[-2.5, 5]),
        showlegend=False,
        height=600,
        plot_bgcolor='#F0F2F6',
        margin=dict(l=20, r=20, t=50, b=20)
    )
    
    return fig, total_lead_time, pce
#===================================================== MONTE CARLO ========================================================
@st.cache_data
def plot_monte_carlo_simulation(dist_params, n_trials, lsl, usl):
    """
    Runs and plots a Monte Carlo simulation for process output.
    """
    np.random.seed(42)
    # Simulate the inputs
    api_potency = np.random.normal(dist_params['api_mean'], dist_params['api_sd'], n_trials)
    excipient_purity = np.random.normal(dist_params['excipient_mean'], dist_params['excipient_sd'], n_trials)
    process_loss = np.random.uniform(dist_params['loss_min'], dist_params['loss_max'], n_trials)
    
    # Calculate the output based on a model
    final_concentration = (api_potency / 100) * (excipient_purity / 100) * (100 - process_loss)
    
    # Calculate results
    failures = np.sum((final_concentration < lsl) | (final_concentration > usl))
    failure_rate = failures / n_trials
    
    # Plotting
    fig = px.histogram(x=final_concentration, nbins=100, title=f"<b>Monte Carlo Simulation Results ({n_trials:,} Trials)</b>")
    fig.add_vline(x=lsl, line=dict(color='red', dash='dash'), name='LSL')
    fig.add_vline(x=usl, line=dict(color='red', dash='dash'), name='USL')
    
    fig.add_annotation(
        x=0.98, y=0.95, xref='paper', yref='paper',
        text=f"<b>Predicted Failure Rate: {failure_rate:.2%}</b>",
        showarrow=False, font=dict(size=16, color='white'),
        bgcolor=SUCCESS_GREEN if failure_rate == 0 else ('orange' if failure_rate < 0.01 else 'red'),
        borderpad=10
    )
    fig.update_layout(xaxis_title="Final Product Concentration (%)", yaxis_title="Frequency")
    return fig, failure_rate

@st.cache_data
def plot_fty_coq(project_type, improvement_effort):
    """
    Generates a professional-grade, 4-plot dashboard for FTY and COQ for multiple project types.
    """
    profiles = {
        "Pharma Process (MAb)": {
            'steps': ["Cell Culture", "Harvest", "Purification", "Formulation", "Fill/Finish"],
            'base_fty': [0.98, 0.99, 0.92, 0.97, 0.99],
            'cost_factors': {'internal': 80000, 'external': 200000}
        },
        "Analytical Assay (ELISA)": {
            'steps': ["Coating", "Blocking", "Sample Add", "Detection", "Analysis"],
            'base_fty': [0.99, 0.98, 0.95, 0.96, 0.97],
            'cost_factors': {'internal': 5000, 'external': 20000}
        },
        "Instrument Qualification": {
            'steps': ["URS/FS", "IQ", "OQ", "PQ", "Final Report"],
            'base_fty': [0.95, 0.99, 0.92, 0.96, 0.98],
            'cost_factors': {'internal': 10000, 'external': 40000}
        },
        "Software System (CSV)": {
            'steps': ["Requirements", "Design", "Coding", "Testing", "Deployment"],
            'base_fty': [0.90, 0.95, 0.88, 0.94, 0.99],
            'cost_factors': {'internal': 25000, 'external': 100000}
        }
    }
    profile = profiles[project_type]
    steps, base_fty = profile['steps'], profile['base_fty']
    
    # --- Calculate the Improved ("After") Process ---
    improved_fty = base_fty.copy()
    temp_fty = np.array(improved_fty)
    effort_applied_per_step = np.zeros(len(steps))
    effort_remaining = improvement_effort
    while effort_remaining > 0 and np.min(temp_fty) < 0.999:
        worst_step_idx = np.argmin(temp_fty)
        effort_to_apply = 1
        effort_applied_per_step[worst_step_idx] += effort_to_apply
        improvement = (1 - temp_fty[worst_step_idx]) * 0.2 * effort_to_apply
        temp_fty[worst_step_idx] += improvement
        effort_remaining -= effort_to_apply
    improved_fty = list(temp_fty)

    rty_base, rty_improved = np.prod(base_fty), np.prod(improved_fty)

    # --- Calculate Cost of Quality (COQ) ---
    base_coq = {
        "Prevention": 20000 + (improvement_effort * 2000), "Appraisal": 30000 + (improvement_effort * 1500),
        "Internal Failure": profile['cost_factors']['internal'] * (1 - rty_base) / (1-0.9),
        "External Failure": profile['cost_factors']['external'] * (1 - rty_base)**2 / (1-0.9)**2
    }
    improved_coq = {
        "Prevention": 20000 + (improvement_effort * 2000), "Appraisal": 30000 + (improvement_effort * 1500),
        "Internal Failure": profile['cost_factors']['internal'] * (1 - rty_improved) / (1-0.9),
        "External Failure": profile['cost_factors']['external'] * (1 - rty_improved)**2 / (1-0.9)**2
    }

    # --- PLOTTING ---
    # Plot 1: Pareto Chart of Scrap/Rework
    base_scrap = [1000 * (1 - fty) * np.prod(base_fty[:i]) for i, fty in enumerate(base_fty)]
    improved_scrap = [1000 * (1 - fty) * np.prod(improved_fty[:i]) for i, fty in enumerate(improved_fty)]
    df_pareto = pd.DataFrame({'Step': steps, 'Baseline': base_scrap, 'Optimized': improved_scrap}).sort_values('Baseline', ascending=False)
    
    fig_pareto = go.Figure()
    fig_pareto.add_trace(go.Bar(name='Baseline Loss', x=df_pareto['Step'], y=df_pareto['Baseline'], marker_color='grey'))
    fig_pareto.add_trace(go.Bar(name='Optimized Loss', x=df_pareto['Step'], y=df_pareto['Optimized'], marker_color=SUCCESS_GREEN))
    fig_pareto.update_layout(title="<b>1. Pareto Chart of Yield Loss</b>", yaxis_title="Units Lost per 1000 Started", barmode='group', legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))

    # Plot 2: SPC Chart of the Weakest Step
    worst_step_name = df_pareto['Step'].iloc[0]
    worst_step_idx_orig = steps.index(worst_step_name)
    
    np.random.seed(worst_step_idx_orig)
    base_mean_offset = (1 - base_fty[worst_step_idx_orig]) * 10
    base_std_dev = (1 - base_fty[worst_step_idx_orig]) * 5 + 1
    
    improvement_factor = 1 - (effort_applied_per_step[worst_step_idx_orig] / 10.0) * 0.8
    improved_mean_offset = base_mean_offset * improvement_factor
    improved_std_dev = base_std_dev * improvement_factor
    
    data = np.random.normal(100 - improved_mean_offset, improved_std_dev, 25)
    mean, std = 100, 2
    ucl, lcl = mean + 3*std, mean - 3*std
    
    fig_spc = go.Figure()
    fig_spc.add_trace(go.Scatter(y=data, mode='lines+markers', name='CPP Data', line=dict(color=PRIMARY_COLOR)))
    fig_spc.add_hline(y=ucl, line=dict(color='red', dash='dash')); fig_spc.add_hline(y=lcl, line=dict(color='red', dash='dash'))
    fig_spc.add_hline(y=mean, line=dict(color='black', dash='dot'))
    fig_spc.update_layout(title=f"<b>2. SPC of Critical Parameter for '{worst_step_name}'</b>", yaxis_title="CPP Value", xaxis_title="Batch Number")

    # Plot 3: Yield Funnel Sankey Diagram
    fig_sankey = go.Figure()
    scenarios = {'Baseline': base_fty, 'Optimized': improved_fty}
    for i, (name, ftys) in enumerate(scenarios.items()):
        labels = ["Input"] + steps + ["Final Output"] + [f"Scrap/Rework {j+1}" for j in range(len(steps))]
        sources, targets, values = [], [], []
        units_in = 1000
        for j, fty in enumerate(ftys):
            units_out, scrap = units_in * fty, units_in * (1 - fty)
            sources.extend([j, j]); targets.extend([j + 1, len(steps) + 1 + j]); values.extend([units_out, scrap])
            units_in = units_out
        
        fig_sankey.add_trace(go.Sankey(
            domain={'x': [i*0.5, i*0.5+0.48]},
            node=dict(pad=15, thickness=20, line=dict(color="black", width=0.5), label=labels, color=PRIMARY_COLOR),
            link=dict(source=sources, target=targets, value=values)
        ))
    rty_text = "Right First Time" if "Software" in project_type else "Rolled Throughput Yield"
    fig_sankey.update_layout(title_text=f"<b>3. Process Yield Funnel ({rty_text})</b>",
                             annotations=[dict(x=0.24, y=1.1, text=f"<b>Baseline (RTY: {rty_base:.1%})</b>", showarrow=False),
                                          dict(x=0.74, y=1.1, text=f"<b>Optimized (RTY: {rty_improved:.1%})</b>", showarrow=False)])

    # Plot 4: Cost of Quality "Iceberg" Chart
    fig_iceberg = go.Figure()
    good_quality_base = base_coq['Prevention'] + base_coq['Appraisal']
    poor_quality_base = base_coq['Internal Failure'] + base_coq['External Failure']
    good_quality_improved = improved_coq['Prevention'] + improved_coq['Appraisal']
    poor_quality_improved = improved_coq['Internal Failure'] + improved_coq['External Failure']
    
    fig_iceberg.add_trace(go.Bar(x=['Baseline', 'Optimized'], y=[good_quality_base, good_quality_improved], name='Cost of Good Quality (Visible Investment)', marker_color='skyblue'))
    fig_iceberg.add_trace(go.Bar(x=['Baseline', 'Optimized'], y=[-poor_quality_base, -poor_quality_improved], name='Cost of Poor Quality (Hidden Losses)', marker_color='salmon'))
    fig_iceberg.add_hline(y=0, line_color="darkblue", line_width=3)
    fig_iceberg.update_layout(title="<b>4. The Cost of Quality 'Iceberg'</b>", yaxis_title="Cost (Relative Cost Units)", barmode='relative',
                              legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))
    
    return fig_pareto, fig_spc, fig_sankey, fig_iceberg, rty_base, rty_improved, base_coq, improved_coq
##=================================================================================================================================================================================================
##=======================================================================================END ACT II ===============================================================================================
##=================================================================================================================================================================================================
@st.cache_data
def plot_oee_breakdown(availability, performance, quality, oee):
    """
    Generates a waterfall chart to visualize OEE losses.
    """
    measures = ["Initial Capacity", "Availability Loss", "Performance Loss", "Quality Loss", "Valuable Time (OEE)"]
    
    # Calculate the deltas for the waterfall chart
    base_values = [100, 100 * availability, 100 * availability * performance, 100 * availability * performance * quality]
    deltas = [
        100,
        -100 * (1 - availability),
        -100 * availability * (1 - performance),
        -100 * availability * performance * (1 - quality),
        oee * 100
    ]
    
    fig = go.Figure(go.Waterfall(
        name = "OEE Breakdown", orientation = "v",
        measure = ["absolute", "relative", "relative", "relative", "total"],
        x = measures,
        textposition = "outside",
        text = [f"{v:.1f}%" for v in deltas],
        y = deltas,
        connector = {"line":{"color":"rgb(63, 63, 63)"}},
        decreasing = {"marker":{"color":"#EF553B"}},
        increasing = {"marker":{"color":SUCCESS_GREEN}},
        totals = {"marker":{"color":PRIMARY_COLOR}}
    ))

    fig.update_layout(
            title = "<b>OEE Waterfall: Visualizing the Six Big Losses</b>",
            yaxis_title="Percentage of Total Capacity (%)",
            waterfallgap = 0.3,
    )
    return fig

@st.cache_data
def plot_control_plan_dashboard(cpp_data, sample_size, frequency, spc_tool):
    """
    Generates a professional-grade Control Plan dashboard including a simulated SPC chart.
    """
    # 1. --- Generate the Control Plan Table ---
    control_plan_data = {
        'Parameter (CPP)': [cpp_data['name']],
        'Specification': [f"{cpp_data['lsl']} - {cpp_data['usl']}"],
        'Measurement System': [cpp_data['method']],
        'Sample Size / Freq.': [f"n={sample_size}, {frequency}"],
        'Control Method': [spc_tool],
        'Reaction Plan': ['Follow OCAP-001']
    }
    df = pd.DataFrame(control_plan_data)
    fig_table = go.Figure(data=[go.Table(
        header=dict(values=list(df.columns), fill_color=PRIMARY_COLOR, font=dict(color='white', size=14), align='left', height=40),
        cells=dict(values=[df[col] for col in df.columns], fill_color='lavender', align='left', font_size=12, height=30)
    )])
    fig_table.update_layout(title_text="<b>1. Process Control Plan Document</b>", margin=dict(l=10, r=10, t=40, b=10))

    # 2. --- Simulate and Plot the SPC Chart ---
    np.random.seed(42)
    process_mean = (cpp_data['usl'] + cpp_data['lsl']) / 2
    process_std = (cpp_data['usl'] - cpp_data['lsl']) / 8 # Simulate a Cpk of 1.33
    
    # More frequent sampling means faster detection of shifts
    shift_detection_point = 25 - int(np.log2(1 if "batch" in frequency else (5 if "hour" in frequency else 10)))
    
    data = np.random.normal(process_mean, process_std, 30)
    data[15:] += process_std * 0.5 # Introduce a small 0.5 sigma shift
    
    fig_spc = go.Figure()
    fig_spc.add_trace(go.Scatter(y=data, mode='lines+markers', name='CPP Data'))
    mean_line = np.mean(data[:15])
    std_line = np.std(data[:15])
    fig_spc.add_hline(y=mean_line + 3*std_line, line=dict(color='red', dash='dash'))
    fig_spc.add_hline(y=mean_line - 3*std_line, line=dict(color='red', dash='dash'))
    fig_spc.add_hline(y=mean_line, line=dict(color='black', dash='dot'))
    fig_spc.add_vrect(x0=14.5, x1=29.5, fillcolor="rgba(255,165,0,0.1)", line_width=0, annotation_text="Process Shift")
    if 15 < shift_detection_point < 30:
        fig_spc.add_vline(x=shift_detection_point, line=dict(color='purple', width=2), annotation_text="Predicted Detection")

    fig_spc.update_layout(title=f"<b>2. Simulated Performance of '{spc_tool}'</b>", yaxis_title=cpp_data['name'])

    # 3. --- Generate the OCAP Flowchart ---
    fig_flowchart = go.Figure(go.Sankey(
        arrangement = "snap",
        node = {"label": ["Process In-Control", f"Signal on {spc_tool}?", "Stop Process", "Notify QA", "Continue Process", "Investigate Cause"], "color": [SUCCESS_GREEN, 'orange', 'red', 'red', SUCCESS_GREEN, 'lightblue']},
        link = {"source": [0, 1, 1, 3], "target": [1, 2, 4, 5], "value": [1, 0.5, 0.5, 0.5], "label": ["Monitor", "YES", "NO", "Action"]}
    ))
    fig_flowchart.update_layout(title_text="<b>3. Out-of-Control Action Plan (OCAP) Flowchart</b>", font_size=12)

    return fig_table, fig_spc, fig_flowchart

@st.cache_data
def plot_westgard_scenario(scenario='Stable'):
    """
    Generates an enhanced, more realistic dynamic Westgard chart, including
    algorithmic rule detection and a Power Functions plot.
    """
    # Establish historical process parameters
    mean, std = 100, 2
    n_points = 30
    
    # --- Generate data based on the selected scenario ---
    np.random.seed(101) # Use a consistent seed for base data
    data = np.random.normal(mean, std, n_points)
    
    # Inject more realistic failures
    if scenario == 'Large Random Error':
        data[20] = 107.5 # A single blunder
    elif scenario == 'Systematic Shift':
        data[18:] = np.random.normal(mean + 2.2*std, std, n_points - 18) # A true shift in the mean
    elif scenario == 'Increased Imprecision':
        data[22:] = np.random.normal(mean, std * 2.5, n_points - 22) # Increased noise
    
    # --- SME ENHANCEMENT: Algorithmic Westgard Rule detection ---
    violations = {}
    limits = {i: mean + i * std for i in [-3, -2, -1, 1, 2, 3]}

    for i in range(1, n_points):
        # 1-3s rule
        if data[i] > limits[3] or data[i] < limits[-3]:
            violations[i] = "1-3s Violation"
        # 2-2s rule
        if (data[i] > limits[2] and data[i-1] > limits[2]) or \
           (data[i] < limits[-2] and data[i-1] < limits[-2]):
            violations[i] = violations.get(i, "") + " 2-2s Violation"
            violations[i-1] = violations.get(i-1, "") + " 2-2s Violation"
        # R-4s rule
        if abs(data[i] - data[i-1]) > 4 * std:
            violations[i] = violations.get(i, "") + " R-4s Violation"
            violations[i-1] = violations.get(i-1, "") + " R-4s Violation"
    # 4-1s rule
    for i in range(3, n_points):
        if (all(d > limits[1] for d in data[i-3:i+1])) or \
           (all(d < limits[-1] for d in data[i-3:i+1])):
            for j in range(i-3, i+1):
                violations[j] = violations.get(j, "") + " 4-1s Violation"
    # 10-x rule (simplified check)
    if n_points >= 10:
        for i in range(9, n_points):
            if (all(d > mean for d in data[i-9:i+1])) or \
               (all(d < mean for d in data[i-9:i+1])):
                for j in range(i-9, i+1):
                    violations[j] = violations.get(j, "") + " 10-x Violation"

    # --- Plotting ---
    fig = go.Figure()
    
    # Add shaded regions for control zones
    for i, color in zip([3, 2, 1], ['rgba(239,83,80,0.1)', 'rgba(254,203,82,0.1)', 'rgba(0,204,150,0.1)']):
        fig.add_hrect(y0=mean - i*std, y1=mean + i*std, line_width=0, fillcolor=color, layer='below')
    
    # Add SD lines with labels
    for i in [-3, -2, -1, 1, 2, 3]:
        fig.add_hline(y=mean + i*std, line=dict(color='grey', dash='dot'),
                      annotation_text=f"{i:+}œÉ", annotation_position="bottom right")
    fig.add_hline(y=mean, line=dict(color='black', dash='dash'), annotation_text='Mean')

    # Add data trace
    fig.add_trace(go.Scatter(x=np.arange(1, n_points + 1), y=data, mode='lines+markers', name='Control Data',
                             line=dict(color='#636EFA', width=3),
                             marker=dict(size=10, symbol='circle', line=dict(width=2, color='black'))))
    
    # Add violation highlights and hover text
    violation_indices = sorted(violations.keys())
    fig.add_trace(go.Scatter(
        x=np.array(violation_indices) + 1,
        y=data[violation_indices],
        mode='markers', name='Violation',
        marker=dict(color='red', size=16, symbol='diamond', line=dict(width=2, color='black')),
        hoverinfo='text',
        text=[f"<b>Point {i+1}</b><br>Value: {data[i]:.2f}<br>Rules: {violations[i].strip()}" for i in violation_indices]
    ))
        
    fig.update_layout(title=f"<b>Westgard Rules Diagnostic Chart: {scenario} Scenario</b>",
                      xaxis_title="Measurement Number", yaxis_title="Control Value",
                      showlegend=False, height=600)

    # --- SME Enhancement: Power Functions Plot ---
    shifts = np.linspace(0, 4, 50) # Shift size in sigma units
    power = {
        '1-3s': 1 - (norm.cdf(3 - shifts) - norm.cdf(-3 - shifts)),
        '2-2s': 1 - (norm.cdf(2 - shifts)**2 - norm.cdf(-2-shifts)**2), # Approximation
        '4-1s': 1 - (norm.cdf(1 - shifts)**4 - norm.cdf(-1-shifts)**4), # Approximation
        '10-x': 1 - (norm.cdf(0-shifts)**10 - norm.cdf(-0-shifts)**10) # Approximation
    }
    
    fig_power = go.Figure()
    for rule, p_detect in power.items():
        fig_power.add_trace(go.Scatter(x=shifts, y=p_detect, mode='lines', name=rule))
    
    fig_power.update_layout(
        title="<b>Rule Power Functions: Probability of Error Detection</b>",
        xaxis_title="Systematic Shift Size (in multiples of œÉ)",
        yaxis_title="Probability of Detection (Power)",
        yaxis_tickformat=".0%",
        legend_title_text="Westgard Rule"
    )
    
    return fig, fig_power, violations
    
# ==============================================================================
# HELPER & PLOTTING FUNCTION (Multivariate SPC) - SME ENHANCED
# ==============================================================================
@st.cache_data
def plot_multivariate_spc(scenario='Stable', n_train=100, n_monitor=30, random_seed=42):
    """
    Generates enhanced, more realistic MSPC analysis and plots, including
    labeled confidence ellipses and a more subtle correlation break.
    """
    if scenario == 'Stable':
        np.random.seed(101)
    else:
        np.random.seed(random_seed)

    # 1. --- Data Generation ---
    mean_train = [25, 150]
    cov_train = [[5, 12], [12, 40]] # Strong positive correlation
    df_train = pd.DataFrame(np.random.multivariate_normal(mean_train, cov_train, n_train), columns=['Temperature', 'Pressure'])

    if scenario == 'Stable':
        df_monitor = pd.DataFrame(np.random.multivariate_normal(mean_train, cov_train, n_monitor), columns=['Temperature', 'Pressure'])
    elif scenario == 'Shift in Y Only':
        mean_shift = [25, 165] # Shifted up in Pressure
        df_monitor = pd.DataFrame(np.random.multivariate_normal(mean_shift, cov_train, n_monitor), columns=['Temperature', 'Pressure'])
    elif scenario == 'Correlation Break':
        # More subtle break - correlation weakens but doesn't disappear
        cov_break = [[5, 4], [4, 40]] 
        df_monitor = pd.DataFrame(np.random.multivariate_normal(mean_train, cov_break, n_monitor), columns=['Temperature', 'Pressure'])

    df_full = pd.concat([df_train, df_monitor], ignore_index=True)
    mean_vec = df_train.mean().values

    # 2. --- MSPC Calculations (T¬≤ and SPE) ---
    S_inv = np.linalg.inv(df_train.cov())
    diff = df_full[['Temperature', 'Pressure']].values - mean_vec
    df_full['T2'] = [d.T @ S_inv @ d for d in diff]
    
    # SPE is only meaningful if we reduce dimensions (e.g., n_components=1)
    pca_spe = PCA(n_components=1).fit(df_train)
    X_hat_spe = pca_spe.inverse_transform(pca_spe.transform(df_full[['Temperature', 'Pressure']]))
    residuals = df_full[['Temperature', 'Pressure']].values - X_hat_spe
    df_full['SPE'] = np.sum(residuals**2, axis=1)

    # 3. --- Calculate Control Limits ---
    alpha = 0.01
    p = df_train.shape[1]
    t2_ucl = (p * (n_train - 1) / (n_train - p)) * f.ppf(1 - alpha, p, n_train - p)
    spe_ucl = np.percentile(df_full['SPE'].iloc[:n_train], (1 - alpha) * 100)

    # 4. --- OOC Check and Error Type Determination ---
    monitor_data = df_full.iloc[n_train:]
    t2_ooc_points = monitor_data[monitor_data['T2'] > t2_ucl]
    spe_ooc_points = monitor_data[monitor_data['SPE'] > spe_ucl]
    alarm_detected = not t2_ooc_points.empty or not spe_ooc_points.empty
    
    if scenario == 'Stable':
        error_type_str = "Type I Error (False Alarm)" if alarm_detected else "Correct In-Control"
    else:
        error_type_str = "Correct Detection" if alarm_detected else "Type II Error (Missed Signal)"

    # --- PLOTTING ---
    # 5. --- Create Process State Space Plot ---
    fig_scatter = go.Figure()
    
    # Helper for ellipse calculation and plotting
    def get_ellipse_path(mean, cov, conf_level):
        vals, vecs = np.linalg.eigh(cov)
        order = vals.argsort()[::-1]; vals, vecs = vals[order], vecs[:, order]
        theta = np.arctan2(*vecs[:, 0][::-1])
        scale_factor = np.sqrt(stats.chi2.ppf(conf_level, df=2))
        width, height = 2 * scale_factor * np.sqrt(vals)
        
        t = np.linspace(0, 2 * np.pi, 100)
        ellipsis_x_circ = width/2 * np.cos(t)
        ellipsis_y_circ = height/2 * np.sin(t)

        x_rotated = ellipsis_x_circ * np.cos(theta) - ellipsis_y_circ * np.sin(theta)
        y_rotated = ellipsis_x_circ * np.sin(theta) + ellipsis_y_circ * np.cos(theta)
        
        return x_rotated + mean[0], y_rotated + mean[1]

    x99, y99 = get_ellipse_path(mean_vec, cov_train, 0.99)
    x95, y95 = get_ellipse_path(mean_vec, cov_train, 0.95)

    # Plot Ellipses as filled polygons
    fig_scatter.add_trace(go.Scatter(x=x99, y=y99, fill="toself", fillcolor='rgba(239,83,80,0.1)', line=dict(color='rgba(239,83,80,0.5)'), name='99% CI'))
    fig_scatter.add_trace(go.Scatter(x=x95, y=y95, fill="toself", fillcolor='rgba(0,204,150,0.1)', line=dict(color='rgba(0,204,150,0.5)'), name='95% CI'))

    # Add annotations
    fig_scatter.add_annotation(x=np.mean(x95), y=np.max(y95), text="<b>95% CI (Normal Zone)</b>", showarrow=False, font=dict(color='darkgreen'))
    fig_scatter.add_annotation(x=np.mean(x99), y=np.max(y99) + 2, text="<b>99% CI (Control Limit)</b>", showarrow=False, font=dict(color='darkred'))

    # Add data points
    fig_scatter.add_trace(go.Scatter(x=df_train['Temperature'], y=df_train['Pressure'], mode='markers', marker=dict(color='#636EFA', opacity=0.7), name='In-Control (Training)'))
    fig_scatter.add_trace(go.Scatter(x=df_monitor['Temperature'], y=df_monitor['Pressure'], mode='markers', marker=dict(color='black', size=8, symbol='star'), name=f'Monitoring ({scenario})'))
    fig_scatter.update_layout(title=f"<b>Process State Space: Normal Operating Region</b>", xaxis_title="Temperature (¬∞C)", yaxis_title="Pressure (kPa)", legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01))
    
    # 6. --- Create Control Charts Plot ---
    fig_charts = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=("<b>Hotelling's T¬≤ Chart (Distance to Center)</b>", "<b>SPE Chart (Distance to Model)</b>"))
    chart_indices = np.arange(1, len(df_full) + 1)
    fig_charts.add_trace(go.Scatter(x=chart_indices, y=df_full['T2'], mode='lines+markers', name='T¬≤ Value'), row=1, col=1)
    fig_charts.add_hline(y=t2_ucl, line_dash="dash", line_color="red", row=1, col=1)
    if not t2_ooc_points.empty: fig_charts.add_trace(go.Scatter(x=t2_ooc_points.index + 1, y=t2_ooc_points['T2'], mode='markers', marker=dict(color='red', size=10, symbol='x')), row=1, col=1)
    
    fig_charts.add_trace(go.Scatter(x=chart_indices, y=df_full['SPE'], mode='lines+markers', name='SPE Value'), row=2, col=1)
    fig_charts.add_hline(y=spe_ucl, line_dash="dash", line_color="red", row=2, col=1)
    if not spe_ooc_points.empty: fig_charts.add_trace(go.Scatter(x=spe_ooc_points.index + 1, y=spe_ooc_points['SPE'], mode='markers', marker=dict(color='red', size=10, symbol='x')), row=2, col=1)
    
    fig_charts.add_vrect(x0=n_train+0.5, x1=n_train+n_monitor+0.5, fillcolor="rgba(255,150,0,0.15)", line_width=0, annotation_text="Monitoring Phase", annotation_position="top left", row='all', col=1)
    fig_charts.update_layout(height=500, title_text="<b>Multivariate Control Charts</b>", showlegend=False, yaxis_title="T¬≤ Statistic", yaxis2_title="SPE Statistic", xaxis2_title="Observation Number")

    # 7. --- Create Contribution Plot for Diagnosis ---
    fig_contrib = None
    if alarm_detected:
        if not t2_ooc_points.empty:
            first_ooc_point = df_full.loc[t2_ooc_points.index[0]]
            contributions = (first_ooc_point[['Temperature', 'Pressure']] - mean_vec)**2
            title_text = "<b>T¬≤ Alarm Diagnosis: Likely Mean Shift</b>"
        else: # SPE alarm
            first_ooc_idx = spe_ooc_points.index[0]
            contributions = pd.Series(residuals[first_ooc_idx]**2, index=['Temperature', 'Pressure'])
            title_text = "<b>SPE Alarm Diagnosis: Likely Correlation Change</b>"
        
        fig_contrib = px.bar(x=contributions.index, y=contributions.values, title=title_text,
                             labels={'x':'Process Variable', 'y':'Contribution Value'},
                             color=contributions.index, color_discrete_sequence=px.colors.qualitative.Plotly)

    return fig_scatter, fig_charts, fig_contrib, not t2_ooc_points.empty, not spe_ooc_points.empty, error_type_str
    
# ==============================================================================
# HELPER & PLOTTING FUNCTION (Small Shift) - SME ENHANCED & CORRECTED
# ==============================================================================
@st.cache_data
def plot_ewma_cusum_comparison(shift_size=0.75, scenario='Sudden Shift'):
    """
    Generates enhanced, more realistic dynamic I, EWMA, and CUSUM charts,
    including multiple scenarios and a "Time to Detect" KPI.
    """
    np.random.seed(123)
    n_points = 50
    shift_point = 25
    mean, std = 100, 2
    
    # --- Data Generation with dynamic shift scenarios ---
    data = np.random.normal(mean, std, n_points)
    
    if scenario == 'Sudden Shift':
        actual_shift_value = shift_size * std
        data[shift_point:] += actual_shift_value
        scenario_desc = f"{shift_size}œÉ Sudden Shift"
    elif scenario == 'Gradual Drift':
        drift = np.linspace(0, shift_size * std, n_points - shift_point)
        data[shift_point:] += drift
        scenario_desc = f"{shift_size}œÉ Gradual Drift"

    # --- Calculations ---
    # I-Chart
    i_ucl, i_lcl = mean + 3 * std, mean - 3 * std
    i_ooc = np.where((data[shift_point:] > i_ucl) | (data[shift_point:] < i_lcl))[0]
    i_detect_time = i_ooc[0] + 1 if len(i_ooc) > 0 else np.nan

    # EWMA
    lam = 0.2
    
    # --- THIS IS THE CORRECTED LINE ---
    ewma = pd.Series(data).ewm(alpha=lam, adjust=False).mean().values
    # --- END OF CORRECTION ---
    
    ewma_ucl = mean + 3 * (std * np.sqrt(lam / (2-lam)))
    ewma_lcl = mean - 3 * (std * np.sqrt(lam / (2-lam)))
    ewma_ooc = np.where((ewma[shift_point:] > ewma_ucl) | (ewma[shift_point:] < ewma_lcl))[0]
    ewma_detect_time = ewma_ooc[0] + 1 if len(ewma_ooc) > 0 else np.nan
    
    # CUSUM
    target = mean; k = 0.5 * std # Slack parameter tuned for 1-sigma shifts
    h = 5 * std # Decision interval
    sh, sl = np.zeros(n_points), np.zeros(n_points)
    for i in range(1, n_points):
        sh[i] = max(0, sh[i-1] + (data[i] - target) - k)
        sl[i] = max(0, sl[i-1] + (target - data[i]) - k)
    cusum_ooc = np.where((sh[shift_point:] > h) | (sl[shift_point:] > h))[0]
    cusum_detect_time = cusum_ooc[0] + 1 if len(cusum_ooc) > 0 else np.nan
        
    # --- Plotting ---
    fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.08,
                        subplot_titles=("<b>I-Chart: The Beat Cop</b> (Detects large, obvious events)",
                                        "<b>EWMA: The Sentinel</b> (Detects small, sustained shifts)",
                                        "<b>CUSUM: The Bloodhound</b> (Fastest detection for specific shifts)"))

    # Plot I-Chart
    fig.add_trace(go.Scatter(x=np.arange(1, n_points+1), y=data, mode='lines+markers', name='Data'), row=1, col=1)
    fig.add_hline(y=i_ucl, line_color='red', line_dash='dash', row=1, col=1)
    fig.add_hline(y=i_lcl, line_color='red', line_dash='dash', row=1, col=1)
    if not np.isnan(i_detect_time):
        idx = shift_point + int(i_detect_time) - 1
        fig.add_trace(go.Scatter(x=[idx+1], y=[data[idx]], mode='markers', name='First Detect',
                                 marker=dict(color='red', size=12, symbol='x')), row=1, col=1)

    # Plot EWMA
    fig.add_trace(go.Scatter(x=np.arange(1, n_points+1), y=ewma, mode='lines+markers', name='EWMA'), row=2, col=1)
    fig.add_hline(y=ewma_ucl, line_color='red', line_dash='dash', row=2, col=1)
    fig.add_hline(y=ewma_lcl, line_color='red', line_dash='dash', row=2, col=1)
    if not np.isnan(ewma_detect_time):
        idx = shift_point + int(ewma_detect_time) - 1
        fig.add_trace(go.Scatter(x=[idx+1], y=[ewma[idx]], mode='markers', name='First Detect',
                                 marker=dict(color='red', size=12, symbol='x')), row=2, col=1)
    
    # Plot CUSUM
    fig.add_trace(go.Scatter(x=np.arange(1, n_points+1), y=sh, mode='lines+markers', name='CUSUM High'), row=3, col=1)
    fig.add_trace(go.Scatter(x=np.arange(1, n_points+1), y=sl, mode='lines+markers', name='CUSUM Low'), row=3, col=1)
    fig.add_hline(y=h, line_color='red', line_dash='dash', row=3, col=1)
    if not np.isnan(cusum_detect_time):
        idx = shift_point + int(cusum_detect_time) - 1
        fig.add_trace(go.Scatter(x=[idx+1], y=[sh[idx] if sh[idx]>0 else sl[idx]], mode='markers', name='First Detect',
                                 marker=dict(color='red', size=12, symbol='x')), row=3, col=1)

    # Add annotation for the process change
    fig.add_vrect(x0=shift_point + 0.5, x1=n_points + 0.5, 
                  fillcolor="rgba(255,150,0,0.15)", line_width=0,
                  annotation_text="Process Change Begins", annotation_position="top left",
                  row='all', col=1)

    fig.update_layout(title=f"<b>Performance Comparison: Detecting a {scenario_desc}</b>",
                      height=800, showlegend=False)
    fig.update_xaxes(title_text="Data Point Number", row=3, col=1)
    
    return fig, i_detect_time, ewma_detect_time, cusum_detect_time
    
# ==============================================================================
# HELPER & PLOTTING FUNCTION (Time Series) - SME ENHANCED
# ==============================================================================
def fit_prophet_model(train_df, n_forecast):
    """A non-cached helper function to safely fit and predict with Prophet."""
    # Context manager to suppress stdout for noisy libraries
    @contextlib.contextmanager
    def suppress_stdout():
        with open(os.devnull, 'w') as fnull:
            import sys
            saved_stdout = sys.stdout
            sys.stdout = fnull
            try:
                yield
            finally:
                sys.stdout = saved_stdout
    
    try:
        with suppress_stdout():
            m_prophet = Prophet().fit(train_df)
        future = m_prophet.make_future_dataframe(periods=n_forecast, freq='W')
        fc_prophet = m_prophet.predict(future)
        return fc_prophet['yhat'].iloc[-n_forecast:].values
    except Exception:
        return pd.Series(np.nan, index=pd.to_datetime(train_df['ds'].tail(n_forecast)))

def plot_forecasting_suite(models_to_run, trend_type, seasonality_type, noise_level, changepoint_strength):
    """
    Generates a dynamic time series and fits a user-selected subset of FAST forecasting models.
    This version is optimized for speed and stability by excluding heavy, compiled-backend models.
    """
    np.random.seed(42)
    periods = 96
    n_forecast = 12
    dates = pd.date_range(start='2015-01-01', periods=periods, freq='MS')
    
    # 1. Generate Data
    if trend_type == 'Multiplicative':
        trend = 50 * np.exp(np.arange(periods) * 0.02)
    else: 
        trend = 100 + 0.5 * np.arange(periods)
    seasonality = np.zeros(periods)
    seasonal_period_1 = 12
    if seasonality_type == 'Single (Yearly)':
        seasonality = 15 * np.sin(np.arange(periods) * (2 * np.pi / seasonal_period_1))
    
    changepoint_loc = 72
    trend[changepoint_loc:] += changepoint_strength * np.arange(periods - changepoint_loc)
    noise = np.random.normal(0, noise_level, periods)
    y = trend + seasonality + noise
    df = pd.DataFrame({'ds': dates, 'y': y})
    train, test = df.iloc[:-n_forecast], df.iloc[-n_forecast:]
    
    # 2. Fit ONLY SELECTED, FAST Models & Forecast
    forecasts = {}
    mae_scores = {}

    if 'Holt-Winters' in models_to_run:
        try:
            hw_trend = 'mul' if trend_type == 'Multiplicative' else 'add'
            hw_seasonal = 'mul' if trend_type == 'Multiplicative' else 'add' if seasonality_type != 'None' else None
            hw = ExponentialSmoothing(train['y'], trend=hw_trend, seasonal=hw_seasonal, seasonal_periods=seasonal_period_1 if hw_seasonal else None).fit()
            forecasts['Holt-Winters'] = hw.forecast(n_forecast)
        except Exception: forecasts['Holt-Winters'] = pd.Series(np.nan, index=pd.to_datetime(test['ds']))

    if 'SARIMA' in models_to_run:
        try:
            seasonal_order = (1,1,0,seasonal_period_1) if seasonality_type != 'None' else (0,0,0,0)
            sarima = SARIMAX(train['y'], order=(1,1,1), seasonal_order=seasonal_order).fit(disp=False)
            forecasts['SARIMA'] = sarima.get_forecast(steps=n_forecast).predicted_mean
        except Exception: forecasts['SARIMA'] = pd.Series(np.nan, index=pd.to_datetime(test['ds']))
        
    if 'ETS' in models_to_run:
        try:
            ets_trend = 'mul' if trend_type == 'Multiplicative' else 'add'
            ets_seasonal = 'mul' if trend_type == 'Multiplicative' else 'add' if seasonality_type != 'None' else None
            ets = ETSModel(train['y'], error="add", trend=ets_trend, seasonal=ets_seasonal, seasonal_periods=seasonal_period_1 if ets_seasonal else None).fit(disp=False)
            forecasts['ETS'] = ets.forecast(n_forecast)
        except Exception: forecasts['ETS'] = pd.Series(np.nan, index=pd.to_datetime(test['ds']))

    # 3. Calculate MAE and Plot
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df['ds'], y=df['y'], mode='lines', name='Actual Data', line=dict(color='black')))
    forecast_start_date = train['ds'].iloc[-1]
    fig.add_shape(type="line", x0=forecast_start_date, y0=0, x1=forecast_start_date, y1=1, yref="paper", line=dict(color="grey", dash="dash"))
    fig.add_annotation(x=forecast_start_date, y=1, yref="paper", text="Forecast Start", showarrow=False, yshift=10)
    colors = px.colors.qualitative.Plotly
    for i, (name, fc) in enumerate(forecasts.items()):
        fc_series = pd.Series(fc, index=test['ds'])
        if not fc_series.isna().all() and len(fc_series) == len(test['y']):
            mae_scores[name] = mean_absolute_error(test['y'], fc_series)
            fig.add_trace(go.Scatter(x=test['ds'], y=fc_series, mode='lines', name=name, line=dict(dash='dot', color=colors[i])))
    fig.update_layout(title="<b>Competitive Forecasts vs. Actual Data</b>", xaxis_title="Date", yaxis_title="Value",
                      legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))
    return fig, mae_scores
#=============================================================================================== ADVANCED TIME SERIES =========================================================================================================

def plot_prophet_only(trend_type, seasonality_type, noise_level, changepoint_strength):
    """
    Fits and plots ONLY the Prophet model. This function is designed to be stable
    and handle data conversions explicitly to prevent plotting errors.
    """
    np.random.seed(42)
    periods = 96
    n_forecast = 12
    dates = pd.date_range(start='2015-01-01', periods=periods, freq='MS')
    
    # 1. Generate Data
    if trend_type == 'Multiplicative':
        trend = 50 * np.exp(np.arange(periods) * 0.02)
    else: 
        trend = 100 + 0.5 * np.arange(periods)
    seasonality = np.zeros(periods)
    if seasonality_type == 'Single (Yearly)':
        seasonality = 15 * np.sin(np.arange(periods) * (2 * np.pi / 12))
    elif seasonality_type == 'Multiple (Yearly + Quarterly)':
        seasonality = 15 * np.sin(np.arange(periods) * (2 * np.pi / 12)) + 7 * np.sin(np.arange(periods) * (2 * np.pi / 3))
    
    changepoint_loc = 72
    trend[changepoint_loc:] += changepoint_strength * np.arange(periods - changepoint_loc)
    noise = np.random.normal(0, noise_level, periods)
    y = trend + seasonality + noise
    df = pd.DataFrame({'ds': dates, 'y': y})
    train, test = df.iloc[:-n_forecast], df.iloc[-n_forecast:]
    
    mae_score = None
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df['ds'], y=df['y'], mode='lines', name='Actual Data', line=dict(color='black')))
    
    # Context manager to suppress stdout
    @contextlib.contextmanager
    def suppress_stdout():
        with open(os.devnull, 'w') as fnull:
            import sys
            saved_stdout = sys.stdout
            sys.stdout = fnull
            try:
                yield
            finally:
                sys.stdout = saved_stdout

    try:
        with suppress_stdout():
            m_prophet = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)
            if seasonality_type == 'Multiple (Yearly + Quarterly)':
                m_prophet.add_seasonality(name='quarterly', period=365.25/4, fourier_order=5)
            m_prophet.fit(train)
        
        future = m_prophet.make_future_dataframe(periods=n_forecast, freq='MS')
        fc_prophet = m_prophet.predict(future)
        
        # Robustly extract forecast and calculate MAE
        forecast_values = fc_prophet['yhat'].iloc[-n_forecast:]
        mae_score = mean_absolute_error(test['y'], forecast_values)
        
        # Explicitly plot the forecast data
        fig.add_trace(go.Scatter(x=test['ds'], y=forecast_values, mode='lines', name='Prophet Forecast', line=dict(dash='dot', color=px.colors.qualitative.Plotly[3])))
        
        # Plot confidence interval
        fig.add_trace(go.Scatter(x=fc_prophet['ds'], y=fc_prophet['yhat_upper'], mode='lines', line=dict(width=0), showlegend=False))
        fig.add_trace(go.Scatter(x=fc_prophet['ds'], y=fc_prophet['yhat_lower'], mode='lines', line=dict(width=0), fill='tonexty', fillcolor='rgba(255,0,0,0.1)', name='Prophet 80% CI'))

    except Exception as e:
        st.error(f"Prophet model failed to run: {e}")

    forecast_start_date = train['ds'].iloc[-1]
    fig.add_shape(type="line", x0=forecast_start_date, y0=0, x1=forecast_start_date, y1=1, yref="paper", line=dict(color="grey", dash="dash"))
    fig.add_annotation(x=forecast_start_date, y=1, yref="paper", text="Forecast Start", showarrow=False, yshift=10)
    
    fig.update_layout(title="<b>Prophet Forecast vs. Actual Data</b>", xaxis_title="Date", yaxis_title="Value",
                      legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))
                      
    return fig, mae_score
    
# ==============================================================================
# HELPER & PLOTTING FUNCTION (Stability Analysis) - SME ENHANCED
# ==============================================================================
@st.cache_data
def plot_stability_analysis(degradation_rate=-0.4, noise_sd=0.5, batch_to_batch_sd=0.1):
    """
    Generates enhanced, more realistic dynamic plots for stability analysis,
    including a formal ANCOVA test for poolability and individual batch trend lines,
    as per ICH Q1E guideline principles.
    """
    np.random.seed(1)
    time_points = np.array([0, 3, 6, 9, 12, 18, 24])
    n_batches = 3
    
    # --- Dynamic Data Generation with Batch-to-Batch Variation ---
    df_list = []
    for i in range(n_batches):
        # SME Enhancement: Control batch-to-batch variation for intercepts and slopes
        initial_potency = np.random.normal(102, batch_to_batch_sd * 5)
        batch_degradation_rate = np.random.normal(degradation_rate, batch_to_batch_sd)
        
        noise = np.random.normal(0, noise_sd, len(time_points))
        potency = initial_potency + batch_degradation_rate * time_points + noise
        
        batch_df = pd.DataFrame({'Time': time_points, 'Potency': potency, 'Batch': f'Batch {i+1}'})
        df_list.append(batch_df)
    
    df_melt = pd.concat(df_list, ignore_index=True)

    # --- SME Enhancement: Formal ANCOVA for Poolability ---
    # Fit a full model with interactions to test if slopes are different
    ancova_model = ols('Potency ~ Time * C(Batch)', data=df_melt).fit()
    anova_table = sm.stats.anova_lm(ancova_model, typ=2)
    poolability_p_value = anova_table.loc['Time:C(Batch)', 'PR(>F)']
    is_poolable = poolability_p_value > 0.25 # ICH Q1E criterion

    # --- Fit Pooled Model and Calculate Shelf Life ---
    # This is done regardless of poolability to show the user the result they WOULD get.
    pooled_model = ols('Potency ~ Time', data=df_melt).fit()
    LSL = 95.0
    
    x_pred = pd.DataFrame({'Time': np.linspace(0, 48, 100)})
    predictions = pooled_model.get_prediction(x_pred).summary_frame(alpha=0.05)
    
    shelf_life_df = predictions[predictions['mean_ci_lower'] >= LSL]
    shelf_life = x_pred['Time'][shelf_life_df.index[-1]] if not shelf_life_df.empty else 0
    shelf_life_val = shelf_life if not shelf_life_df.empty else 0
    
    if shelf_life > 47:
        shelf_life_str = ">48 Months"
    else:
        shelf_life_str = f"{shelf_life:.1f} Months"

    # --- Plotting ---
    fig = go.Figure()
    
    # Add shaded acceptable region
    fig.add_hrect(y0=LSL, y1=105, fillcolor="rgba(0,204,150,0.1)", layer="below", line_width=0)

    colors = px.colors.qualitative.Plotly
    # SME Enhancement: Plot individual batch data and trend lines
    for i, batch in enumerate(df_melt['Batch'].unique()):
        batch_df = df_melt[df_melt['Batch'] == batch]
        color = colors[i % len(colors)]
        # Raw data points
        fig.add_trace(go.Scatter(x=batch_df['Time'], y=batch_df['Potency'], mode='markers',
                                 name=batch, marker=dict(color=color, size=8)))
        # Individual batch fit line from ANCOVA model
        pred_batch = ancova_model.predict(batch_df)
        fig.add_trace(go.Scatter(x=batch_df['Time'], y=pred_batch, mode='lines',
                                 line=dict(color=color, dash='dash'), showlegend=False))

    # Plot results from the pooled model
    fig.add_trace(go.Scatter(x=x_pred['Time'], y=predictions['mean'], mode='lines',
                             name='Pooled Mean Trend', line=dict(color='black', width=3)))
    fig.add_trace(go.Scatter(x=x_pred['Time'], y=predictions['mean_ci_lower'], mode='lines',
                             name='95% Lower CI (Pooled)', line=dict(color='red', dash='dot', width=3)))
    
    # Add Limit and Shelf-Life lines
    fig.add_hline(y=LSL, line=dict(color='red', width=2), annotation_text="<b>Specification Limit</b>")
    
    if shelf_life_val > 0 and shelf_life_val < 48:
        fig.add_vline(x=shelf_life_val, line=dict(color='blue', dash='dash'),
                      annotation_text=f'<b>Shelf-Life = {shelf_life_str}</b>', annotation_position="bottom")
    
    title_color = '#00CC96' if is_poolable else '#EF553B'
    title_text = f"<b>Stability Analysis for Shelf-Life | Batches are {'Poolable' if is_poolable else 'NOT Poolable'} (p={poolability_p_value:.3f})</b>"
    
    fig.update_layout(
        title={'text': title_text, 'font': {'color': title_color}},
        xaxis_title="Time (Months)", yaxis_title="Potency (%)",
        xaxis_range=[-1, 48], yaxis_range=[max(85, df_melt['Potency'].min()-2), 105],
        legend=dict(x=0.01, y=0.01, yanchor='bottom', bgcolor='rgba(255,255,255,0.7)')
    )
    
    return fig, shelf_life_str, pooled_model.params['Time'], poolability_p_value

# The @st.cache_data decorator has been removed to allow for dynamic updates from sliders.
# ==============================================================================
# HELPER & PLOTTING FUNCTION (Survival Analysis) - SME ENHANCED & CORRECTED
# ==============================================================================
@st.cache_data
def plot_survival_analysis(group_b_lifetime=30, censor_rate=0.2):
    """
    Generates an enhanced, more realistic survival analysis dashboard, including
    confidence intervals, an "at risk" table, and a proper log-rank test.
    """
    np.random.seed(42)
    n_samples = 50
    
    # Generate time-to-event data from a Weibull distribution
    time_A = stats.weibull_min.rvs(c=1.5, scale=20, size=n_samples)
    time_B = stats.weibull_min.rvs(c=1.5, scale=group_b_lifetime, size=n_samples)
    
    # Generate censoring status
    event_observed_A = 1 - np.random.binomial(1, censor_rate, n_samples)
    event_observed_B = 1 - np.random.binomial(1, censor_rate, n_samples)

    # Use the lifelines library for robust calculations
    kmf_A = KaplanMeierFitter()
    kmf_A.fit(time_A, event_observed=event_observed_A, label='Group A (Old Component)')
    
    kmf_B = KaplanMeierFitter()
    kmf_B.fit(time_B, event_observed=event_observed_B, label='Group B (New Component)')

    # Perform the log-rank test for statistical significance
    results = logrank_test(time_A, time_B, event_observed_A, event_observed_B)
    p_value = results.p_value

    fig = make_subplots(
            rows=2, cols=1,
            shared_xaxes=True,
            vertical_spacing=0.05,
            row_heights=[0.8, 0.2],
            specs=[[{"type": "xy"}],        # The first plot is a standard xy plot
                   [{"type": "table"}]],    # The second plot is explicitly a table
            subplot_titles=("<b>Kaplan-Meier Survival Estimates</b>",)
        )
    # --- END OF CORRECTION ---
    
    # Plot curves with confidence intervals
    colors = px.colors.qualitative.Plotly
    for kmf, color_hex in zip([kmf_A, kmf_B], [colors[0], colors[1]]):
        kmf_df = kmf.survival_function_.join(kmf.confidence_interval_)
        fig.add_trace(go.Scatter(x=kmf_df.index, y=kmf_df[kmf.label], mode='lines',
                                 name=kmf.label, line=dict(color=color_hex, shape='hv', width=3)), row=1, col=1)
        
        fill_color_rgba = f'rgba({",".join(str(c) for c in px.colors.hex_to_rgb(color_hex))}, 0.2)'
        
        fig.add_trace(go.Scatter(x=kmf_df.index, y=kmf_df[f'{kmf.label}_upper_0.95'], mode='lines',
                                 line=dict(width=0), showlegend=False), row=1, col=1)
        fig.add_trace(go.Scatter(x=kmf_df.index, y=kmf_df[f'{kmf.label}_lower_0.95'], mode='lines',
                                 line=dict(width=0), fill='tonexty', fillcolor=fill_color_rgba,
                                 name=f'{kmf.label} 95% CI'), row=1, col=1)

    # Add markers for censored data
    censored_A = time_A[event_observed_A == 0]
    censored_B = time_B[event_observed_B == 0]
    fig.add_trace(go.Scatter(x=censored_A, y=kmf_A.predict(censored_A), mode='markers',
                             marker=dict(color=colors[0], symbol='line-ns-open', size=10),
                             name='Censored (Group A)'), row=1, col=1)
    fig.add_trace(go.Scatter(x=censored_B, y=kmf_B.predict(censored_B), mode='markers',
                             marker=dict(color=colors[1], symbol='line-ns-open', size=10),
                             name='Censored (Group B)'), row=1, col=1)

    # Add "At Risk" table to the second row
    time_bins = np.linspace(0, max(time_A.max(), time_B.max()), 6).astype(int)
    at_risk_A = [np.sum(time_A >= t) for t in time_bins]
    at_risk_B = [np.sum(time_B >= t) for t in time_bins]
    
    fig.add_trace(go.Table(
        header=dict(values=['<b>Time Point</b>'] + [f'<b>{t}</b>' for t in time_bins], align="left"),
        cells=dict(values=[
            ['<b>Group A At Risk</b>', '<b>Group B At Risk</b>'],
            *[[at_risk_A[i], at_risk_B[i]] for i in range(len(time_bins))]
        ], align="left", font=dict(size=12)),
    ), row=2, col=1)
    
    fig.update_layout(
        title='<b>Reliability / Survival Analysis Dashboard</b>',
        xaxis_title='Time to Event (e.g., Days to Failure)',
        yaxis_title='Survival Probability',
        yaxis_range=[0, 1.05],
        legend=dict(yanchor="top", y=0.98, xanchor="right", x=0.98, bgcolor='rgba(255,255,255,0.7)'),
        height=800
    )
    
    return fig, kmf_A.median_survival_time_, kmf_B.median_survival_time_, p_value

# ==============================================================================
# HELPER & PLOTTING FUNCTION (MVA/PLS) - SME ENHANCED
# ==============================================================================
@st.cache_data
def plot_mva_pls(signal_strength=2.0, noise_sd=0.2):
    """
    Generates an enhanced, more realistic MVA dashboard with more realistic spectral data,
    a cross-validation plot, and a predicted vs. actual plot.
    """
    np.random.seed(0)
    n_samples = 50
    n_features = 200
    wavelengths = np.linspace(800, 2200, n_features)

    # --- SME Enhancement: More realistic spectral data generation ---
    def make_peak(x, center, width, height):
        return height * np.exp(-((x - center)**2) / (2 * width**2))

    # Generate a curved baseline
    baseline = 0.1 + 5e-5 * (wavelengths - 800) + 1e-7 * (wavelengths - 1500)**2
    X = np.tile(baseline, (n_samples, 1))
    
    # Generate response variable y (e.g., concentration)
    y = np.linspace(5, 25, n_samples) + np.random.normal(0, 0.1, n_samples)
    
    # Add peaks to X that are correlated with y
    for i in range(n_samples):
        # Peak 1 (positively correlated with y)
        X[i, :] += make_peak(wavelengths, 1200, 50, y[i] * 0.005 * signal_strength)
        # Peak 2 (negatively correlated with y)
        X[i, :] += make_peak(wavelengths, 1600, 60, (25 - y[i]) * 0.004 * signal_strength)
        # Add some noise peaks unrelated to y
        X[i, :] += make_peak(wavelengths, 1000, 30, np.random.rand() * 0.05)
    
    # Add random measurement noise
    X += np.random.normal(0, noise_sd * 0.01, X.shape)

    # --- Dynamic Model Fitting & KPI Calculation ---
    from sklearn.model_selection import cross_val_predict
    from sklearn.metrics import r2_score, mean_squared_error

    max_comps = 8
    r2_cal = []
    r2_cv = [] # This is Q2
    for n_comp in range(1, max_comps + 1):
        pls_cv = PLSRegression(n_components=n_comp)
        pls_cv.fit(X, y)
        y_cv = cross_val_predict(pls_cv, X, y, cv=10)
        r2_cal.append(r2_score(y, pls_cv.predict(X)))
        r2_cv.append(r2_score(y, y_cv))

    # Select the optimal number of components that maximizes Q2
    optimal_n_comp = np.argmax(r2_cv) + 1
    
    # Fit the final model
    pls = PLSRegression(n_components=optimal_n_comp)
    pls.fit(X, y)
    y_pred = pls.predict(X)
    
    model_r2 = pls.score(X, y)
    model_q2 = r2_cv[optimal_n_comp - 1]
    rmsecv = np.sqrt(mean_squared_error(y, cross_val_predict(pls, X, y, cv=10)))

    # VIP score calculation
    T = pls.x_scores_; W = pls.x_weights_; Q = pls.y_loadings_
    p, h = W.shape; VIPs = np.zeros((p,))
    s = np.diag(T.T @ T @ Q.T @ Q).reshape(h, -1)
    total_s = np.sum(s)
    for i in range(p):
        weight = np.array([(W[i,j] / np.linalg.norm(W[:,j]))**2 for j in range(h)])
        VIPs[i] = np.sqrt(p * (s.T @ weight) / total_s)

    # --- Plotting Dashboard ---
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=("<b>1. Raw Spectral Data</b> (Color mapped to Y)",
                        "<b>2. Cross-Validation: Choosing Components</b>",
                        "<b>3. Model Performance: Predicted vs. Actual</b>",
                        "<b>4. Model Interpretation: VIP Scores</b>"),
        vertical_spacing=0.2, horizontal_spacing=0.1
    )

    # Plot 1: Raw Spectra
    for i in range(n_samples):
        color = px.colors.sample_colorscale('Viridis', (y[i] - y.min()) / (y.max() - y.min()))[0]
        fig.add_trace(go.Scatter(x=wavelengths, y=X[i,:], mode='lines', line=dict(color=color), showlegend=False), row=1, col=1)

    # Plot 2: Cross-Validation
    fig.add_trace(go.Scatter(x=np.arange(1, max_comps+1), y=r2_cal, mode='lines+markers', name='R¬≤ (Fit)', line=dict(color='blue')), row=1, col=2)
    fig.add_trace(go.Scatter(x=np.arange(1, max_comps+1), y=r2_cv, mode='lines+markers', name='Q¬≤ (Predict)', line=dict(color='green')), row=1, col=2)
    fig.add_vline(x=optimal_n_comp, line_dash="dash", line_color="black", annotation_text=f"Optimal LV={optimal_n_comp}", row=1, col=2)

    # Plot 3: Predicted vs. Actual
    fig.add_trace(go.Scatter(x=y, y=y_pred.flatten(), mode='markers', name='Samples'), row=2, col=1)
    fig.add_shape(type='line', x0=y.min(), y0=y.min(), x1=y.max(), y1=y.max(),
                  line=dict(color='black', dash='dash'), row=2, col=1)

    # Plot 4: VIP Scores
    fig.add_trace(go.Bar(x=wavelengths, y=VIPs, name='VIP Score', marker_color='orange'), row=2, col=2)
    fig.add_hline(y=1, line=dict(color='red', dash='dash'), name='Significance Threshold', row=2, col=2)
    # Highlight the true peaks
    fig.add_vrect(x0=1150, x1=1250, fillcolor="rgba(0,255,0,0.15)", line_width=0, row=2, col=2, annotation_text="True Signal", annotation_position="top left")
    fig.add_vrect(x0=1550, x1=1650, fillcolor="rgba(0,255,0,0.15)", line_width=0, row=2, col=2)
    
    fig.update_layout(height=800, title_text='<b>Multivariate Analysis (PLS Regression) Dashboard</b>', title_x=0.5, showlegend=False)
    fig.update_xaxes(title_text='Wavelength (nm)', row=1, col=1); fig.update_yaxes(title_text='Absorbance', row=1, col=1)
    fig.update_xaxes(title_text='Number of Latent Variables', row=1, col=2); fig.update_yaxes(title_text='Coefficient of Determination', row=1, col=2)
    fig.update_xaxes(title_text='Actual Value', row=2, col=1); fig.update_yaxes(title_text='Predicted Value', row=2, col=1, scaleanchor="x2", scaleratio=1)
    fig.update_xaxes(title_text='Wavelength (nm)', row=2, col=2); fig.update_yaxes(title_text='VIP Score', row=2, col=2)
    
    return fig, model_r2, model_q2, optimal_n_comp, rmsecv

# ==============================================================================
# HELPER & PLOTTING FUNCTION (Clustering) - SME ENHANCED
# ==============================================================================
@st.cache_data
def plot_clustering(separation=15, spread=2.5, n_true_clusters=3):
    """
    Generates an enhanced, more realistic clustering dashboard, including a variable
    number of true clusters, a Silhouette diagnostic plot, and Voronoi boundaries.
    """
    np.random.seed(42)
    n_points_per_cluster = 50
    
    # --- Dynamic Data Generation ---
    # SME Enhancement: Generate a variable number of true clusters
    centers = []
    angle_step = 360 / n_true_clusters
    for i in range(n_true_clusters):
        angle = np.deg2rad(i * angle_step)
        x_center = 10 + separation * np.cos(angle)
        y_center = 10 + separation * np.sin(angle)
        centers.append((x_center, y_center))

    X_list, Y_list = [], []
    for x_c, y_c in centers:
        X_list.append(np.random.normal(x_c, spread, n_points_per_cluster))
        Y_list.append(np.random.normal(y_c, spread, n_points_per_cluster))
        
    X = np.concatenate(X_list)
    Y = np.concatenate(Y_list)
    df = pd.DataFrame({'X': X, 'Y': Y})
    
    # --- 1. Perform Clustering for a range of k and Diagnostics ---
    k_range = range(2, 9)
    wcss = []
    silhouette_scores = []
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto').fit(df[['X', 'Y']])
        wcss.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(df[['X', 'Y']], kmeans.labels_))
    
    # Find the optimal k based on silhouette score
    optimal_k = k_range[np.argmax(silhouette_scores)]

    # --- 2. Generate Plots based on the OPTIMAL k ---
    kmeans_opt = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto').fit(df)
    df['Cluster'] = kmeans_opt.labels_.astype(str)
    final_silhouette = silhouette_score(df[['X', 'Y']], df['Cluster'])

    # --- Plotting Dashboard ---
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=("<b>1. Discovered Process Regimes</b>",
                        "<b>2. Elbow Method for Selecting k</b>",
                        "<b>3. Silhouette Analysis for Selecting k</b>",),
        specs=[[{"rowspan": 2}, {}], [None, {}]],
        vertical_spacing=0.2
    )

    # Plot 1: Main Scatter Plot with Voronoi boundaries
    fig_scatter = px.scatter(df, x='X', y='Y', color='Cluster',
                             labels={'X': 'Process Parameter 1', 'Y': 'Process Parameter 2'})
    for trace in fig_scatter.data:
        fig.add_trace(trace, row=1, col=1)
    
    centers = kmeans_opt.cluster_centers_
    fig.add_trace(go.Scatter(x=centers[:, 0], y=centers[:, 1], mode='markers',
                             marker=dict(color='black', size=15, symbol='x'),
                             name='Centroids'), row=1, col=1)

    # Plot 2: Elbow Method
    fig.add_trace(go.Scatter(x=list(k_range), y=wcss, mode='lines+markers', name='Inertia'), row=1, col=2)
    fig.add_vline(x=n_true_clusters, line_dash="dash", line_color="red",
                  annotation_text=f"True k = {n_true_clusters}", row=1, col=2)

    # Plot 3: Silhouette Score Plot
    fig.add_trace(go.Scatter(x=list(k_range), y=silhouette_scores, mode='lines+markers', name='Silhouette Score'), row=2, col=2)
    fig.add_vline(x=optimal_k, line_dash="dash", line_color="green",
                  annotation_text=f"Optimal k = {optimal_k}", row=2, col=2)

    fig.update_layout(height=800, title_text='<b>Unsupervised Clustering & Diagnostics Dashboard</b>', title_x=0.5, showlegend=False)
    fig.update_xaxes(title_text='Process Parameter 1', row=1, col=1); fig.update_yaxes(title_text='Process Parameter 2', row=1, col=1)
    fig.update_xaxes(title_text='Number of Clusters (k)', row=1, col=2); fig.update_yaxes(title_text='Inertia (WCSS)', row=1, col=2)
    fig.update_xaxes(title_text='Number of Clusters (k)', row=2, col=2); fig.update_yaxes(title_text='Mean Silhouette Score', row=2, col=2)
                             
    return fig, final_silhouette, optimal_k

# ==============================================================================
# HELPER & PLOTTING FUNCTION (Anomaly Detection) - ROBUST STATIC TREE VERSION
# ==============================================================================
@st.cache_data
def plot_isolation_forest_static_tree(contamination_rate=0.1):
    """
    Generates a robust dashboard with a static, digitally-rendered isolation tree.
    This version is 100% stable as it removes all external dependencies like graphviz.
    """
    np.random.seed(42)
    n_inliers = 200
    n_outliers = 15
    
    # --- Data Generation ---
    angle = np.random.uniform(0, 2 * np.pi, n_inliers)
    radius = np.random.normal(5, 0.5, n_inliers)
    X_inliers = np.array([radius * np.cos(angle), radius * np.sin(angle)]).T
    X_outliers = np.random.uniform(low=-10, high=10, size=(n_outliers, 2))
    X = np.concatenate([X_inliers, X_outliers], axis=0)
    ground_truth = np.concatenate([np.zeros(n_inliers), np.ones(n_outliers)])
    
    # --- Model Fitting ---
    clf = IsolationForest(contamination=contamination_rate, random_state=42, n_estimators=100)
    y_pred = clf.fit_predict(X)
    anomaly_scores = clf.decision_function(X) * -1
    
    df = pd.DataFrame(X, columns=['X', 'Y'])
    df['Status'] = ['Anomaly' if p == -1 else 'Normal' for p in y_pred]
    df['GroundTruth'] = ['Outlier' if gt == 1 else 'Inlier' for gt in ground_truth]
    df['Score'] = anomaly_scores

    # --- FIGURE 1: Main Scatter Plot ---
    fig_scatter = px.scatter(
        df, x='X', y='Y',
        color='Status', color_discrete_map={'Normal': '#636EFA', 'Anomaly': '#EF553B'},
        symbol='Status', symbol_map={'Normal': 'circle', 'Anomaly': 'x-thin-open'},
        title="<b>1. Anomaly Detection Results</b>"
    )
    fig_scatter.update_layout(legend=dict(yanchor="top", y=0.98, xanchor="left", x=0.01))

    # --- FIGURE 2: Digital Isolation Tree Visualization ---
    fig_tree_viz = go.Figure()
    target_idx = df[df['GroundTruth'] == 'Outlier'].index[0] # Pick the first true outlier to isolate
    target_point = df.loc[target_idx]

    # Plot all points, highlighting the target
    fig_tree_viz.add_trace(go.Scatter(
        x=df['X'], y=df['Y'], mode='markers',
        marker=dict(color='lightgrey', size=5), showlegend=False
    ))
    fig_tree_viz.add_trace(go.Scatter(
        x=[target_point['X']], y=[target_point['Y']], mode='markers',
        marker=dict(color='red', size=12, symbol='star'), showlegend=False
    ))
    
    # Simulate the isolation splits
    bounds = {'x_min': df['X'].min(), 'x_max': df['X'].max(), 'y_min': df['Y'].min(), 'y_max': df['Y'].max()}
    for i in range(5): # Limit to 5 splits for clarity
        points_in_bounds = df[
            (df['X'] >= bounds['x_min']) & (df['X'] <= bounds['x_max']) &
            (df['Y'] >= bounds['y_min']) & (df['Y'] <= bounds['y_max'])
        ]
        if len(points_in_bounds) <= 1:
            break

        axis = 'x' if i % 2 == 0 else 'y' # Alternate axes for a clearer visual
        
        if axis == 'x':
            split_val = np.random.uniform(bounds['x_min'], bounds['x_max'])
            fig_tree_viz.add_shape(type='line', x0=split_val, x1=split_val, y0=bounds['y_min'], y1=bounds['y_max'], line=dict(color='orange', dash='dash'))
            fig_tree_viz.add_annotation(x=split_val, y=bounds['y_max'], text=f"Split {i+1}", showarrow=False, yshift=10)
            if target_point['X'] < split_val:
                bounds['x_max'] = split_val
            else:
                bounds['x_min'] = split_val
        else: # axis == 'y'
            split_val = np.random.uniform(bounds['y_min'], bounds['y_max'])
            fig_tree_viz.add_shape(type='line', x0=bounds['x_min'], x1=bounds['x_max'], y0=split_val, y1=split_val, line=dict(color='purple', dash='dash'))
            fig_tree_viz.add_annotation(x=bounds['x_max'], y=split_val, text=f"Split {i+1}", showarrow=False, xshift=10)
            if target_point['Y'] < split_val:
                bounds['y_max'] = split_val
            else:
                bounds['y_min'] = split_val
    
    fig_tree_viz.update_layout(title_text="<b>2. How an Isolation Tree Works</b>", xaxis_title="Parameter 1", yaxis_title="Parameter 2")

    # --- FIGURE 3: Histogram of Scores ---
    fig_hist = px.histogram(
        df, x='Score', color='GroundTruth',
        color_discrete_map={'Inlier': 'grey', 'Outlier': 'red'},
        barmode='overlay', marginal='rug',
        title="<b>3. Distribution of Anomaly Scores</b>"
    )
    score_threshold = np.percentile(anomaly_scores, 100 * (1 - contamination_rate))
    fig_hist.add_vline(x=score_threshold, line_dash="dash", line_color="black", annotation_text="Decision Threshold")
    fig_hist.update_layout(showlegend=False)

    return fig_scatter, fig_hist, fig_tree_viz, (y_pred == -1).sum()

@st.cache_data
def plot_predictive_modeling_suite(boundary_radius, mlp_params):
    """
    Trains and visualizes Logistic Regression, Random Forest, and a tuned MLP Classifier.
    """
    # Import MLPClassifier locally within the function
    from sklearn.neural_network import MLPClassifier

    np.random.seed(1)
    n_points = 200
    purity = np.random.uniform(95, 105, n_points)
    bioactivity = np.random.uniform(80, 120, n_points)
    distance_from_center_sq = (purity - 100)**2 + (bioactivity - 100)**2
    prob_of_failure = 1 / (1 + np.exp(distance_from_center_sq - boundary_radius))
    y = np.random.binomial(1, prob_of_failure)
    X = np.column_stack((purity, bioactivity))
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # --- Fit all three models ---
    lr = LogisticRegression().fit(X_train, y_train)
    rf = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train, y_train)
    
    # MLP with user-defined hyperparameters
    mlp = MLPClassifier(
        hidden_layer_sizes=mlp_params['layers'],
        activation=mlp_params['activation'],
        solver='adam',
        learning_rate_init=mlp_params['learning_rate'],
        max_iter=500, # Increased for better convergence
        random_state=42
    ).fit(X_train, y_train)

    # --- Calculate ROC curves for all models ---
    lr_probs = lr.predict_proba(X_test)[:, 1]
    rf_probs = rf.predict_proba(X_test)[:, 1]
    mlp_probs = mlp.predict_proba(X_test)[:, 1]
    
    fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probs)
    auc_lr = auc(fpr_lr, tpr_lr)
    fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)
    auc_rf = auc(fpr_rf, tpr_rf)
    fpr_mlp, tpr_mlp, _ = roc_curve(y_test, mlp_probs)
    auc_mlp = auc(fpr_mlp, tpr_mlp)

    # --- Create plots ---
    fig = make_subplots(
        rows=1, cols=4,
        subplot_titles=("<b>Logistic Regression</b>", "<b>Random Forest</b>", "<b>MLP Neural Network</b>", "<b>Performance: ROC Curves</b>"),
        column_widths=[0.3, 0.3, 0.3, 0.1]
    )
    
    # Create meshgrid for decision boundaries
    xx, yy = np.meshgrid(np.linspace(95, 105, 100), np.linspace(80, 120, 100))
    grid = np.c_[xx.ravel(), yy.ravel()]
    
    # Plot models
    models = {'lr': lr, 'rf': rf, 'mlp': mlp}
    for i, (name, model) in enumerate(models.items()):
        Z = model.predict_proba(grid)[:, 1].reshape(xx.shape)
        contour = go.Contour(x=xx[0], y=yy[:,0], z=Z, colorscale='RdBu', showscale=False, opacity=0.5,
                             contours=dict(start=0, end=1, size=0.1))
        boundary = go.Contour(x=xx[0], y=yy[:,0], z=Z, showscale=False, contours_coloring='lines', 
                              line_width=3, contours=dict(start=0.5, end=0.5, size=0))
        scatter = go.Scatter(x=X[:,0], y=X[:,1], mode='markers', showlegend=False,
                             marker=dict(color=y, colorscale=[[0, 'blue'], [1, 'red']], line=dict(width=1, color='black')))
        
        fig.add_trace(contour, row=1, col=i+1)
        fig.add_trace(boundary, row=1, col=i+1)
        fig.add_trace(scatter, row=1, col=i+1)

    # Plot ROC Curves
    fig.add_trace(go.Scatter(x=fpr_lr, y=tpr_lr, mode='lines', name=f'Logistic Reg. (AUC={auc_lr:.3f})'), row=1, col=4)
    fig.add_trace(go.Scatter(x=fpr_rf, y=tpr_rf, mode='lines', name=f'Random Forest (AUC={auc_rf:.3f})'), row=1, col=4)
    fig.add_trace(go.Scatter(x=fpr_mlp, y=tpr_mlp, mode='lines', name=f'MLP (AUC={auc_mlp:.3f})'), row=1, col=4)
    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random Chance', line=dict(color='grey', dash='dash')), row=1, col=4)

    fig.update_layout(title="<b>Predictive Modeling Suite: Linear vs. Ensemble vs. Neural Network</b>", height=450,
                      legend=dict(orientation="v", yanchor="bottom", y=0.05, xanchor="left", x=1.02))
    fig.update_xaxes(title_text="Purity", row=1, col=1); fig.update_yaxes(title_text="Bioactivity", row=1, col=1)
    fig.update_xaxes(title_text="Purity", row=1, col=2); fig.update_yaxes(title_text="Bioactivity", row=1, col=2, showticklabels=False)
    fig.update_xaxes(title_text="Purity", row=1, col=3); fig.update_yaxes(title_text="Bioactivity", row=1, col=3, showticklabels=False)
    fig.update_xaxes(title_text="1-Spec", row=1, col=4); fig.update_yaxes(title_text="Sens", row=1, col=4)
    
    return fig, auc_lr, auc_rf, auc_mlp
# ==============================================================================
# HELPER & PLOTTING FUNCTION (XAI/SHAP) - SME ENHANCED & PDP FIX
# ==============================================================================
@st.cache_data
def plot_xai_shap(case_to_explain="highest_risk", dependence_feature='Operator Experience (Months)'):
    """
    Trains an XGBoost model and generates a robust set of SHAP and PDP/ICE plots
    for comprehensive model explanation.
    """
    plt.style.use('default')
    
    # 1. Simulate Assay Data
    np.random.seed(42)
    n_runs = 200
    operator_experience = np.random.randint(1, 25, n_runs)
    cal_slope = np.random.normal(1.0, 0.05, n_runs) - (operator_experience * 0.0015)
    qc1_value = np.random.normal(50, 2, n_runs) - np.random.uniform(0, operator_experience / 8, n_runs)
    reagent_age_days = np.random.randint(5, 90, n_runs)
    instrument_id = np.random.choice(['Inst_A', 'Inst_B', 'Inst_C'], n_runs, p=[0.5, 0.3, 0.2])
    
    prob_failure = 1 / (1 + np.exp(-(-2.0 - 0.18 * operator_experience + (reagent_age_days / 20)**1.5
                                     - (cal_slope - 1.0) * 25 + (qc1_value < 48) * 0.5
                                     + (instrument_id == 'Inst_C') * 1.5)))
    run_failed = np.random.binomial(1, prob_failure)

    X_display = pd.DataFrame({
        'Operator Experience (Months)': operator_experience,
        'Reagent Age (Days)': reagent_age_days,
        'Calibrator Slope': cal_slope,
        'QC Level 1 Value': qc1_value,
        'Instrument ID': instrument_id
    })
    y = pd.Series(run_failed, name="Run Failed")
    
    X_encoded = pd.get_dummies(X_display, drop_first=True)
    X = X_encoded.astype(int)

    # Use XGBoost for better performance
    model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss').fit(X, y)
    explainer = shap.Explainer(model, X)
    shap_values = explainer(X)

    # Find the instance index for the local explanation
    failure_probabilities = model.predict_proba(X)[:, 1]
    if case_to_explain == "highest_risk":
        instance_index = np.argmax(failure_probabilities)
    elif case_to_explain == "lowest_risk":
        instance_index = np.argmin(failure_probabilities)
    else: # "most_ambiguous"
        instance_index = np.argmin(np.abs(failure_probabilities - 0.5))

    # 3. Generate Global Summary Plot (Beeswarm)
    fig_summary, ax_summary = plt.subplots()
    shap.summary_plot(shap_values, X, show=False)
    buf_summary = io.BytesIO()
    fig_summary.savefig(buf_summary, format='png', bbox_inches='tight')
    plt.close(fig_summary)
    buf_summary.seek(0)
    
    # 4. Generate Local Waterfall Plot
    fig_waterfall, ax_waterfall = plt.subplots()
    shap.waterfall_plot(shap_values[instance_index], show=False)
    buf_waterfall = io.BytesIO()
    fig_waterfall.savefig(buf_waterfall, format='png', bbox_inches='tight')
    plt.close(fig_waterfall)
    buf_waterfall.seek(0)
    
    # --- 5. SME ENHANCEMENT: Generate a robust PDP/ICE Plot instead of SHAP Dependence Plot ---
    fig_pdp, ax_pdp = plt.subplots(figsize=(8, 6))
    
    # The feature name might be different after one-hot encoding
    plot_feature = dependence_feature
    if dependence_feature == 'Instrument ID':
        inst_cols = [col for col in X.columns if 'Instrument ID_' in col]
        plot_feature = inst_cols[0] if inst_cols else 'Operator Experience (Months)'
    
    PartialDependenceDisplay.from_estimator(
        estimator=model,
        X=X,
        features=[plot_feature],
        kind="both",  # Show both PDP and ICE lines
        ice_lines_kw={"color": "lightblue", "alpha": 0.3, "linewidth": 0.5},
        pd_line_kw={"color": "red", "linewidth": 4, "linestyle": "--"},
        ax=ax_pdp
    )
    ax_pdp.set_title(f"Partial Dependence & ICE Plot for\n'{plot_feature}'", fontsize=14)
    ax_pdp.set_ylabel("Predicted Probability of Failure", fontsize=12)
    ax_pdp.set_xlabel(plot_feature, fontsize=12)
    
    buf_pdp = io.BytesIO()
    fig_pdp.savefig(buf_pdp, format='png', bbox_inches='tight')
    plt.close(fig_pdp)
    buf_pdp.seek(0)
    
    actual_outcome = "Failed" if y.iloc[instance_index] == 1 else "Passed"
    
    # Return the new PDP buffer instead of the old dependence buffer
    return buf_summary, buf_waterfall, buf_pdp, X_display.iloc[instance_index:instance_index+1], actual_outcome, instance_index
    
# ==============================================================================
# HELPER & PLOTTING FUNCTION (Advanced AI) - SME VISUAL OVERHAUL
# ==============================================================================
@st.cache_data
def plot_advanced_ai_concepts(concept, p1=0, p2=0, p3=0):
    """
    Generates professionally redesigned, interactive, and domain-specific Plotly figures
    for advanced AI topics in a V&V and Tech Transfer context.
    """
    fig = go.Figure()

    if concept == "Transformers":
        time = np.arange(14)
        vcd = 1 / (1 + np.exp(-0.8 * (time - 6))) * 15
        glucose = 5 - 0.4 * time + np.random.normal(0, 0.1, 14)
        do = 30 + 10 * np.sin(time / 2) + np.random.normal(0, 1, 14)
        
        fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.1,
                            specs=[[{"secondary_y": True}]] * 3)
        
        # Plot time series data
        fig.add_trace(go.Scatter(x=time, y=do, mode='lines', name='DO', line=dict(color='#636EFA')), row=1, col=1, secondary_y=False)
        fig.add_trace(go.Scatter(x=time, y=glucose, mode='lines', name='Glucose', line=dict(color='#00CC96')), row=2, col=1, secondary_y=False)
        fig.add_trace(go.Scatter(x=time, y=vcd, mode='lines', name='VCD', line=dict(color='#FECB52')), row=3, col=1, secondary_y=False)

        # SME Enhancement: Visualize attention as a bar chart on a secondary axis
        attention_day = p1
        importance_profile = np.exp(-0.5 * (np.abs(time - attention_day)))
        importance_profile[6] = max(importance_profile[6], 1.5) # Peak growth is always important
        importance_profile /= importance_profile.sum()

        fig.add_trace(go.Bar(x=time, y=importance_profile, name='Attention', marker_color='#EF553B', opacity=0.6), row=1, col=1, secondary_y=True)
        fig.add_trace(go.Bar(x=time, y=importance_profile, showlegend=False, marker_color='#EF553B', opacity=0.6), row=2, col=1, secondary_y=True)
        fig.add_trace(go.Bar(x=time, y=importance_profile, showlegend=False, marker_color='#EF553B', opacity=0.6), row=3, col=1, secondary_y=True)
        
        fig.update_layout(title_text=f"<b>Transformer Attention: Predicting final titer from Day {attention_day}</b>",
                          legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))
        fig.update_xaxes(title_text="Day", row=3, col=1)
        fig.update_yaxes(title_text="DO (%)", row=1, col=1, secondary_y=False)
        fig.update_yaxes(title_text="Glucose (g/L)", row=2, col=1, secondary_y=False)
        fig.update_yaxes(title_text="VCD", row=3, col=1, secondary_y=False)
        fig.update_yaxes(title_text="Attention Weight", showgrid=False, row=1, col=1, secondary_y=True)
        fig.update_yaxes(showticklabels=False, row=2, col=1, secondary_y=True)
        fig.update_yaxes(showticklabels=False, row=3, col=1, secondary_y=True)


    elif concept == "Graph Neural Networks (GNNs)":
        evidence_strength = p1
        nodes = {
            'Media Lot A1': {'pos': (1, 5)}, 'Buffer Lot B2': {'pos': (1, 3)},
            'Assay Kit C3': {'pos': (1, 1)}, 'Batch 101': {'pos': (3, 5)},
            'Batch 102': {'pos': (3, 3)}, 'Batch 103': {'pos': (3, 1)},
            'Final QC Test': {'pos': (5, 3)}
        }
        edges = [('Media Lot A1', 'Batch 101'), ('Media Lot A1', 'Batch 102'), ('Buffer Lot B2', 'Batch 101'),
                 ('Buffer Lot B2', 'Batch 102'), ('Buffer Lot B2', 'Batch 103'), ('Assay Kit C3', 'Batch 103'),
                 ('Batch 101', 'Final QC Test'), ('Batch 102', 'Final QC Test'), ('Batch 103', 'Final QC Test')]
        
        # Propagate "guilt" probability backwards
        node_probs = {name: 0.0 for name in nodes}
        node_probs['Final QC Test'] = 0.99
        node_probs['Batch 102'] = min(0.99, node_probs['Final QC Test'] * 0.8 * evidence_strength)
        node_probs['Media Lot A1'] = min(0.99, node_probs['Batch 102'] * 0.7 * evidence_strength)
        node_probs['Buffer Lot B2'] = min(0.99, node_probs['Batch 102'] * 0.9 * evidence_strength)

        for start, end in edges:
            fig.add_trace(go.Scatter(x=[nodes[start]['pos'][0], nodes[end]['pos'][0]],
                                     y=[nodes[start]['pos'][1], nodes[end]['pos'][1]],
                                     mode='lines', line=dict(color="lightgrey", width=2)))
        
        for name, props in nodes.items():
            prob = node_probs[name]
            color = f'rgb(255, {220 - prob*200}, {220 - prob*200})'
            fig.add_trace(go.Scatter(x=[props['pos'][0]], y=[props['pos'][1]], mode='markers+text',
                                     text=f"<b>{name.replace(' ', '<br>')}</b><br>P(Cause)={prob:.2f}", textposition="middle center",
                                     marker=dict(size=120, color=color, symbol='circle',
                                                 line=dict(width=3, color='black'))))
        fig.update_layout(title_text="<b>GNN: Root Cause Analysis via Lot Genealogy</b>")

    elif concept == "Reinforcement Learning (RL)":
        cost_of_waste = p1; process_variability = p2
        
        # Agent
        fig.add_shape(type="rect", x0=1, y0=3.5, x1=3, y1=5, fillcolor='#636EFA', line=dict(width=2))
        fig.add_annotation(x=2, y=4.25, text="<b>RL Agent</b><br>(Feed Policy)", showarrow=False, font_color="white")
        # Digital Twin
        fig.add_shape(type="rect", x0=5, y0=0.5, x1=11, y1=5.5, fillcolor='rgba(128,128,128,0.1)', line=dict(width=2, dash='dash'))
        fig.add_annotation(x=8, y=5.2, text="<b>Digital Twin (Bioreactor Sim)</b>", showarrow=False)
        # Real Process
        fig.add_shape(type="rect", x0=1, y0=1, x1=3, y1=2.5, fillcolor='#00CC96', line=dict(width=2))
        fig.add_annotation(x=2, y=1.75, text="<b>Real Process</b>", showarrow=False, font_color="white")
        
        # Arrows
        fig.add_annotation(x=4.8, y=4.25, ax=3.2, ay=4.25, text="<b>Action</b>", showarrow=True, arrowhead=2)
        fig.add_annotation(x=3.2, y=3.75, ax=4.8, ay=3.75, text="<b>State, Reward</b>", showarrow=True, arrowhead=2)
        fig.add_annotation(x=2, y=2.7, ax=2, ay=3.3, text="<b>Deploy<br>Optimal Policy</b>", showarrow=True, arrowhead=5, arrowcolor='#00CC96')

        time = np.linspace(5.5, 10.5, 100)
        target_vcd = 3.5
        aggressiveness = 1 / (1 + cost_of_waste * process_variability)
        vcd_profile = target_vcd - 1 * np.exp(-aggressiveness * (time-5.5)) + np.random.normal(0, process_variability*0.1, 100)
        
        fig.add_trace(go.Scatter(x=time, y=vcd_profile, mode='lines', line=dict(color='royalblue', width=3)))
        fig.add_hline(y=target_vcd, line=dict(color='black', dash='dot'), line_width=2)
        fig.update_yaxes(range=[2.2, 4.2], title_text="VCD")
        fig.update_xaxes(title_text="Days")
        fig.update_layout(title_text="<b>RL: Learning an Optimal Feeding Strategy</b>")

    elif concept == "Generative AI":
        creativity = p1
        np.random.seed(42)
        x_real = np.random.normal(98, 0.5, 4); y_real = np.random.normal(70, 2, 4)
        x_synth = np.random.normal(98, 0.5 + creativity, 100); y_synth = np.random.normal(70, 2 + creativity * 5, 100)
        
        fig.add_trace(go.Scatter(x=x_real, y=y_real, mode='markers', name='Real OOS Data',
                                 marker=dict(color='#EF553B', size=15, symbol='x-thin', line=dict(width=3))))
        fig.add_trace(go.Scatter(x=x_synth, y=y_synth, mode='markers', name='Synthetic OOS Data',
                                 marker=dict(color='#FECB52', size=8, symbol='circle', opacity=0.7)))
        
        fig.add_vrect(x0=95, x1=105, fillcolor='rgba(0,204,150,0.1)', line_width=0)
        fig.add_hrect(y0=80, y1=120, fillcolor='rgba(0,204,150,0.1)', line_width=0,
                      annotation_text="In-Spec Region", annotation_position="top left")
        
        fig.add_annotation(x=103, y=100, text="<b>Generative<br>Model</b>", showarrow=True, arrowhead=2,
                           ax=101, ay=85, font=dict(size=14, color='white'), bgcolor='#636EFA', borderpad=10)
        
        fig.update_layout(title_text="<b>Generative AI: Augmenting Rare Assay Failure Data</b>",
                          xaxis_title="Assay Parameter 1 (e.g., Purity)",
                          yaxis_title="Assay Parameter 2 (e.g., Potency)")

    # Standardize final layout
    fig.update_layout(height=500, showlegend=False, margin=dict(l=10, r=10, t=50, b=10))
    if concept in ["Graph Neural Networks (GNNs)"]:
        fig.update_layout(xaxis=dict(visible=False, showgrid=False, range=[0, 6]), 
                          yaxis=dict(visible=False, showgrid=False, range=[0, 6]))
    elif concept in ["Reinforcement Learning (RL)"]:
        fig.update_layout(xaxis=dict(visible=False, showgrid=False), 
                          yaxis=dict(visible=False, showgrid=False))
                      
    return fig
#================================================================================================================================================================================================
#=========================================================================NEW HYBRID METHODS ==================================================================================================
#===========================================================================================================================================================================================
@st.cache_data
def plot_mewma_xgboost(drift_magnitude=0.03, lambda_mewma=0.2):
    """
    Generates an enhanced, more realistic MEWMA dashboard, including autocorrelated data,
    a gradual drift failure mode, and a clearer waterfall plot for diagnostics.
    """
    np.random.seed(42)
    n_train, n_monitor = 100, 80
    n_total = n_train + n_monitor
    drift_start_point = n_train

    # --- 1. SME Enhancement: Simulate more realistic, autocorrelated process data ---
    mean_vec = np.array([7.0, 50.0, 100.0])
    cov_matrix = np.array([[0.04, 0.5, 0.8], [0.5, 16.0, 25.0], [0.8, 25.0, 64.0]])
    
    # Generate underlying random shocks
    innovations = np.random.multivariate_normal(np.zeros(3), cov_matrix, n_total)
    
    # Create autocorrelated noise (AR(1) process)
    phi = 0.6 # Autocorrelation coefficient
    noise = np.zeros_like(innovations)
    for i in range(1, n_total):
        noise[i, :] = phi * noise[i-1, :] + innovations[i, :]
        
    data = mean_vec + noise
    
    # SME Enhancement: Inject a subtle, GRADUAL drift in Temp and Pressure
    drift_duration = n_total - drift_start_point
    drift_vector = np.zeros_like(data)
    temp_drift = np.linspace(0, drift_magnitude * np.sqrt(cov_matrix[1,1]) * drift_duration, drift_duration)
    pressure_drift = np.linspace(0, drift_magnitude * np.sqrt(cov_matrix[2,2]) * drift_duration, drift_duration)
    drift_vector[drift_start_point:, 1] = temp_drift
    drift_vector[drift_start_point:, 2] = pressure_drift
    
    data += drift_vector
    df = pd.DataFrame(data, columns=['pH', 'Temperature', 'Pressure'])

    # 2. --- MEWMA Calculation (using Phase 1 data for covariance) ---
    train_cov = df.iloc[:n_train].cov().values
    S_inv = np.linalg.inv(train_cov)
    train_mean = df.iloc[:n_train].mean().values
    
    Z = np.zeros_like(data)
    t_squared_mewma = np.zeros(n_total)
    Z[0, :] = train_mean
    for i in range(1, n_total):
        Z[i, :] = (1 - lambda_mewma) * Z[i-1, :] + lambda_mewma * data[i, :]
        diff = Z[i, :] - train_mean
        t_squared_mewma[i] = diff.T @ S_inv @ diff

    # 3. --- Control Limit (Asymptotic) ---
    p = data.shape[1]
    ucl = (p * lambda_mewma / (2 - lambda_mewma)) * f.ppf(0.9973, p, 1000) # 3-sigma equivalent
    
    ooc_points_mask = t_squared_mewma[n_train:] > ucl
    first_ooc_index = np.argmax(ooc_points_mask) + n_train if np.any(ooc_points_mask) else None
    
    # 4. --- XGBoost Diagnostic Model ---
    buf_waterfall = None
    if first_ooc_index:
        y = np.array([0] * n_train + [1] * n_monitor)
        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42).fit(df, y)
        explainer = shap.Explainer(model, df)
        shap_values = explainer(df)
        
        # SME Enhancement: Use a clearer Waterfall plot
        fig_waterfall, ax_waterfall = plt.subplots()
        shap.waterfall_plot(shap_values[first_ooc_index], show=False)
        buf_waterfall = io.BytesIO()
        fig_waterfall.savefig(buf_waterfall, format='png', bbox_inches='tight')
        plt.close(fig_waterfall)
        buf_waterfall.seek(0)

    # 5. --- Plotting Dashboard ---
    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1,
                        subplot_titles=("<b>1. Raw Process Data (The 'Stealth' Drift)</b>",
                                        "<b>2. MEWMA Chart (The Detector)</b>"))

    # Plot 1: Raw Data
    for col in df.columns:
        fig.add_trace(go.Scatter(y=df[col], name=col, mode='lines'), row=1, col=1)
    
    # Plot 2: MEWMA Chart
    fig.add_trace(go.Scatter(y=t_squared_mewma, name='MEWMA T¬≤', mode='lines+markers', line_color='#636EFA'), row=2, col=1)
    fig.add_hline(y=ucl, line=dict(color='red', dash='dash'), row=2, col=1)

    # Add annotations to both plots
    for r in [1, 2]:
        fig.add_vrect(x0=0, x1=n_train, fillcolor="rgba(0,204,150,0.1)", line_width=0,
                      annotation_text="Phase 1: Training", annotation_position="top left", row=r, col=1)
        fig.add_vrect(x0=n_train, x1=n_total, fillcolor="rgba(255,150,0,0.1)", line_width=0,
                      annotation_text="Phase 2: Monitoring", annotation_position="top right", row=r, col=1)
    if first_ooc_index:
        fig.add_vline(x=first_ooc_index, line=dict(color='red', width=2),
                      annotation_text="First Alarm", annotation_position='top', row='all', col=1)

    fig.update_layout(height=600, legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))
    fig.update_yaxes(title_text="Value", row=1, col=1)
    fig.update_yaxes(title_text="MEWMA T¬≤", row=2, col=1)
    fig.update_xaxes(title_text="Observation Number", row=2, col=1)

    return fig, buf_waterfall, first_ooc_index

# ==============================================================================
# HELPER & PLOTTING FUNCTION (Method 2) - SME ENHANCED
# ==============================================================================
@st.cache_data
def plot_bocpd_ml_features(autocorr_shift=0.4, noise_increase=2.0):
    """
    Generates an enhanced, more realistic BOCPD dashboard, monitoring the residuals
    of an AR model and visualizing the run length probability.
    """
    np.random.seed(42)
    n_points = 200
    change_point = 100
    
    # --- 1. SME Enhancement: Simulate a process with a change in dynamics ---
    data = np.zeros(n_points)
    # Phase 1: High autocorrelation, low noise
    phi1 = 0.9
    for i in range(1, change_point):
        data[i] = phi1 * data[i-1] + np.random.normal(0, 1)
    # Phase 2: Lower autocorrelation, higher noise
    phi2 = phi1 - autocorr_shift
    for i in range(change_point, n_points):
        data[i] = phi2 * data[i-1] + np.random.normal(0, noise_increase)
        
    # --- 2. Create an "ML Feature": 1-step-ahead forecast error (residuals) ---
    # A simple AR(1) model is our "ML" feature extractor
    ml_feature = np.zeros(n_points)
    for i in range(1, n_points):
        prediction = phi1 * data[i-1] # Model assumes Phase 1 dynamics
        ml_feature[i] = data[i] - prediction
    
    # --- 3. BOCPD Algorithm ---
    hazard = 1 / 250.0 # Constant hazard rate
    # Assume a Gaussian model for the residuals
    mean0, var0 = 0, np.var(ml_feature[1:change_point])
    
    R = np.zeros((n_points + 1, n_points + 1))
    R[0, 0] = 1 # Initial state
    max_run_lengths = np.zeros(n_points)
    prob_rl_zero = np.zeros(n_points) # Probability that run length is 0
    
    for t in range(1, n_points + 1):
        x = ml_feature[t-1]
        pred_probs = stats.norm.pdf(x, loc=mean0, scale=np.sqrt(var0))
        
        # Growth probabilities
        R[1:t+1, t] = R[0:t, t-1] * pred_probs * (1 - hazard)
        # Change point probability
        R[0, t] = np.sum(R[:, t-1] * pred_probs * hazard)
        
        # Normalize
        R[:, t] /= np.sum(R[:, t])
        
        # Store metrics
        prob_rl_zero[t-1] = R[0, t]
        max_run_lengths[t-1] = np.argmax(R[:, t])

    # --- 4. Plotting Dashboard ---
    fig = make_subplots(rows=4, cols=1, shared_xaxes=True, vertical_spacing=0.05,
                        subplot_titles=("<b>1. Raw Process Data (Change is subtle)</b>",
                                        "<b>2. Model Residuals (ML Feature)</b>",
                                        "<b>3. BOCPD: Most Likely Run Length (MAP)</b>",
                                        "<b>4. BOCPD: Probability of a Changepoint</b>"))
    # Plot 1: Raw Data
    fig.add_trace(go.Scatter(y=data, mode='lines', name='Raw Data'), row=1, col=1)
    
    # Plot 2: ML Feature (Residuals)
    fig.add_trace(go.Scatter(y=ml_feature, mode='lines', name='Residuals', line_color='orange'), row=2, col=1)
    
    # Plot 3: Most Likely Run Length (MAP Estimate)
    fig.add_trace(go.Scatter(y=max_run_lengths, mode='lines', name='MAP Run Length', line_color='green'), row=3, col=1)
    
    # Plot 4: Probability of a Changepoint (Run Length = 0)
    fig.add_trace(go.Scatter(y=prob_rl_zero, mode='lines', name='P(Changepoint)', line_color='purple', fill='tozeroy'), row=4, col=1)
    
    # Add changepoint line to all plots
    fig.add_vline(x=change_point, line_dash='dash', line_color='red', row='all', col=1,
                  annotation_text="True Changepoint", annotation_position="top")

    fig.update_layout(height=800, title_text="<b>Bayesian Online Change Point Detection on Model Residuals</b>", showlegend=False)
    fig.update_yaxes(title_text="Value", row=1, col=1)
    fig.update_yaxes(title_text="Residual", row=2, col=1)
    fig.update_yaxes(title_text="Run Length", row=3, col=1)
    fig.update_yaxes(title_text="Probability", row=4, col=1, range=[0, 1])
    fig.update_xaxes(title_text="Observation Number", row=4, col=1)
    
    return fig, prob_rl_zero[change_point]

@st.cache_data
def plot_kalman_nn_residual(measurement_noise=1.0, shock_magnitude=10.0, process_noise_q=0.01):
    """
    Generates an enhanced, more realistic Kalman Filter dashboard, simulating a non-linear
    process and visualizing the filter's uncertainty.
    """
    np.random.seed(123)
    n_points = 100
    
    # --- 1. SME Enhancement: Simulate a more realistic non-linear dynamic process ---
    # A damped sine wave, representing a process settling to a steady state.
    time = np.arange(n_points)
    true_state = 100 + 20 * np.exp(-time / 50) * np.sin(time / 5)
    
    # Add a sudden, unexpected shock (fault)
    shock_point = 70
    true_state[shock_point:] += shock_magnitude
    
    # Create noisy measurements
    measurements = true_state + np.random.normal(0, measurement_noise, n_points)
    
    # 2. --- Kalman Filter Implementation ---
    # The filter's internal model is a simple linear approximation of the true non-linear process
    # This mismatch is what a Neural Network would be used to correct in a real application.
    F = 1 # State transition matrix (assumes random walk)
    H = 1 # Measurement matrix
    Q = process_noise_q # Process noise (model uncertainty) - NOW A SLIDER
    R = measurement_noise**2 # Measurement noise (known from sensor)
    
    x_est = np.zeros(n_points) # Estimated state
    p_est = np.zeros(n_points) # Estimated error covariance
    residuals = np.zeros(n_points)
    
    x_est[0] = measurements[0]
    p_est[0] = 1.0
    
    for k in range(1, n_points):
        # Predict
        x_pred = F * x_est[k-1]
        p_pred = F * p_est[k-1] * F + Q
        
        # Update
        residual = measurements[k] - H * x_pred
        S = H * p_pred * H + R
        K = p_pred * H / S # Kalman Gain
        
        x_est[k] = x_pred + K * residual
        p_est[k] = (1 - K * H) * p_pred
        residuals[k] = residual

    # 3. --- Control Chart on Residuals ---
    limit_data = residuals[:shock_point-1]
    res_std = np.std(limit_data)
    ucl, lcl = 3 * res_std, -3 * res_std
    
    ooc_indices = np.where((residuals > ucl) | (residuals < lcl))[0]
    first_ooc = ooc_indices[0] if len(ooc_indices) > 0 else None
    
    # --- 4. Plotting Dashboard ---
    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1,
                        subplot_titles=("<b>1. State Estimation: Tracking a Noisy Process</b>",
                                        "<b>2. Fault Detection: Control Chart on Residuals</b>"))

    # Plot 1: State Estimation
    fig.add_trace(go.Scatter(x=time, y=measurements, mode='markers', name='Noisy Measurements',
                             marker=dict(color='grey', opacity=0.7)), row=1, col=1)
    fig.add_trace(go.Scatter(x=time, y=true_state, mode='lines', name='True Hidden State',
                             line=dict(dash='dash', color='black', width=3)), row=1, col=1)
    # SME Enhancement: Show filter's uncertainty bounds
    upper_bound = x_est + np.sqrt(p_est)
    lower_bound = x_est - np.sqrt(p_est)
    fig.add_trace(go.Scatter(x=time, y=upper_bound, mode='lines', line=dict(width=0), showlegend=False), row=1, col=1)
    fig.add_trace(go.Scatter(x=time, y=lower_bound, mode='lines', line=dict(width=0), fill='tonexty',
                             fillcolor='rgba(255,0,0,0.2)', name='Kalman Uncertainty (¬±1œÉ)'), row=1, col=1)
    fig.add_trace(go.Scatter(x=time, y=x_est, mode='lines', name='Kalman Estimate',
                             line=dict(color='red', width=3)), row=1, col=1)

    # Plot 2: Residuals Control Chart
    fig.add_trace(go.Scatter(x=time, y=residuals, mode='lines+markers', name='Residuals',
                             line=dict(color='#636EFA')), row=2, col=1)
    if first_ooc:
        fig.add_trace(go.Scatter(x=[first_ooc], y=[residuals[first_ooc]], mode='markers', name='Alarm',
                                 marker=dict(color='red', size=12, symbol='x')), row=2, col=1)
    fig.add_hline(y=ucl, line_color='red', row=2, col=1)
    fig.add_hline(y=lcl, line_color='red', row=2, col=1)
    fig.add_hline(y=0, line_color='black', line_dash='dash', row=2, col=1)
    
    fig.add_vline(x=shock_point, line_dash='dot', line_color='red',
                  annotation_text='Process Shock', annotation_position='top', row='all', col=1)
    
    fig.update_layout(height=800, title_text="<b>Kalman Filter for State Estimation & Fault Detection</b>",
                      legend=dict(yanchor="top", y=0.98, xanchor="left", x=0.01))
    fig.update_yaxes(title_text="Value", row=1, col=1)
    fig.update_yaxes(title_text="Residual (Surprise)", row=2, col=1)
    fig.update_xaxes(title_text="Time", row=2, col=1)
    
    return fig, first_ooc

# ==============================================================================
# HELPER & PLOTTING FUNCTION (Method 4) - SME ENHANCED & CORRECTED
# ==============================================================================

@st.cache_data
def plot_rl_tuning(cost_false_alarm=1.0, cost_delay_unit=5.0, shift_size=1.0):
    """
    Generates an enhanced, more realistic RL dashboard for the economic design of an EWMA chart,
    optimizing over both lambda and control limit width (L).
    Returns TWO separate figures: one for the 3D plot, one for the 2D plots.
    """
    # 1. --- Create a grid of parameters to search ---
    lambdas = np.linspace(0.05, 0.5, 20)
    Ls = np.linspace(2.5, 3.5, 20) # Control limit widths
    
    # Pre-calculate Performance (ARL) over the grid
    arl0_grid = np.zeros((len(Ls), len(lambdas)))
    arl1_grid = np.zeros((len(Ls), len(lambdas)))
    
    for i, L in enumerate(Ls):
        for j, lam in enumerate(lambdas):
            delta = shift_size * np.sqrt(lam / (2 - lam))
            L_eff = L
            
            arl0_grid[i, j] = (np.exp(-2*L_eff*delta) * (1 + 2*L_eff*delta) + np.exp(2*L_eff*delta) * (1 - 2*L_eff*delta)) / (2*delta**2) if delta != 0 else np.inf
            if arl0_grid[i,j] == np.inf or arl0_grid[i,j] < 1: arl0_grid[i, j] = 1e9

            arl1_grid[i, j] = (np.exp(-2*(L_eff-delta)*delta) + 2*(L_eff-delta)*delta - 1) / (2*delta**2) if delta != 0 else np.inf
            if arl1_grid[i, j] <= 0 or arl1_grid[i, j] == np.inf: arl1_grid[i, j] = 1

    # 2. --- Calculate Economic Cost and find optimum ---
    total_cost_grid = (cost_false_alarm / arl0_grid) + (cost_delay_unit * arl1_grid)
    min_idx = np.unravel_index(np.argmin(total_cost_grid), total_cost_grid.shape)
    optimal_L = Ls[min_idx[0]]
    optimal_lambda = lambdas[min_idx[1]]
    min_cost = total_cost_grid[min_idx]
    
    # --- 3. Create Figure 1: The 3D Surface Plot ---
    fig_3d = go.Figure()
    fig_3d.add_trace(go.Surface(x=lambdas, y=Ls, z=total_cost_grid, colorscale='Viridis', showscale=False))
    fig_3d.add_trace(go.Scatter3d(x=[optimal_lambda], y=[optimal_L], z=[min_cost], mode='markers',
                                 marker=dict(color='red', size=8, symbol='x'), name="Optimal Point"))
    fig_3d.update_layout(
        title="<b>1. Economic Cost Surface</b>",
        scene=dict(xaxis_title='Lambda (Œª)', yaxis_title='Limit Width (L)', zaxis_title='Total Cost'),
        margin=dict(l=0, r=0, b=0, t=40)
    )

    # --- 4. Create Figure 2: The 2D Diagnostic Plots ---
    fig_2d = make_subplots(
        rows=2, cols=2,
        subplot_titles=("<b>2. ARL‚ÇÄ (Time to False Alarm)</b>",
                        "<b>3. ARL‚ÇÅ (Time to Detect)</b>",
                        "<b>4. Optimal EWMA Chart</b>"),
        vertical_spacing=0.2, horizontal_spacing=0.15
    )
    
    # ARL0 Contour
    fig_2d.add_trace(go.Contour(x=lambdas, y=Ls, z=np.log10(arl0_grid), colorscale='Blues',
                               contours=dict(showlabels=True), name="log10(ARL‚ÇÄ)"), row=1, col=1)
    fig_2d.add_trace(go.Scatter(x=[optimal_lambda], y=[optimal_L], mode='markers',
                               marker=dict(color='red', size=12, symbol='x')), row=1, col=1)
    
    # ARL1 Contour
    fig_2d.add_trace(go.Contour(x=lambdas, y=Ls, z=arl1_grid, colorscale='Reds',
                               contours=dict(showlabels=True), name="ARL‚ÇÅ"), row=1, col=2)
    fig_2d.add_trace(go.Scatter(x=[optimal_lambda], y=[optimal_L], mode='markers',
                               marker=dict(color='black', size=12, symbol='x')), row=1, col=2)

    # Optimal EWMA chart
    np.random.seed(42)
    data = np.random.normal(0, 1, 50)
    data[25:] += shift_size
    ewma_opt = pd.Series(data).ewm(alpha=optimal_lambda, adjust=False).mean().values
    ucl = optimal_L * np.sqrt(optimal_lambda / (2 - optimal_lambda))
    
    fig_2d.add_trace(go.Scatter(y=ewma_opt, mode='lines+markers', name='Optimal EWMA'), row=2, col=1)
    fig_2d.add_hline(y=ucl, line_color='red', row=2, col=1)
    fig_2d.add_hline(y=-ucl, line_color='red', row=2, col=1)
    fig_2d.add_vline(x=25, line_dash='dash', line_color='red', row=2, col=1)
    
    fig_2d.update_layout(height=600, showlegend=False)
    fig_2d.update_xaxes(title_text='Lambda (Œª)', row=1, col=1); fig_2d.update_yaxes(title_text='Limit Width (L)', row=1, col=1)
    fig_2d.update_xaxes(title_text='Lambda (Œª)', row=1, col=2); fig_2d.update_yaxes(title_text='Limit Width (L)', row=1, col=2)
    fig_2d.update_xaxes(title_text='Observation Number', row=2, col=1); fig_2d.update_yaxes(title_text='EWMA Value', row=2, col=1)
    
    # Clear the unused subplot
    fig_2d.update_xaxes(visible=False, showticklabels=False, row=2, col=2)
    fig_2d.update_yaxes(visible=False, showticklabels=False, row=2, col=2)
    
    return fig_3d, fig_2d, optimal_lambda, optimal_L, min_cost

# ==============================================================================
# HELPER & PLOTTING FUNCTION (Method 5) - SME ENHANCED
# ==============================================================================
@st.cache_data
def plot_tcn_cusum(drift_magnitude=0.05, daily_cycle_strength=1.0):
    """
    Generates an enhanced, more realistic TCN-CUSUM dashboard with bioprocess-like data,
    receptive field visualization, and residual diagnostics.
    """
    np.random.seed(42)
    n_points = 200
    time = np.arange(n_points)
    
    # --- 1. SME Enhancement: Simulate more realistic bioprocess data ---
    # Logistic growth curve (S-shape)
    growth = 100 / (1 + np.exp(-0.05 * (time - 100)))
    # Diurnal (daily) cycles
    daily_cycle = daily_cycle_strength * np.sin(time * 2 * np.pi / 24) # 24-hour cycle
    # Gradual process drift
    drift = np.linspace(0, drift_magnitude * n_points, n_points)
    # Measurement noise
    noise = np.random.normal(0, 0.5, n_points)
    
    data = growth + daily_cycle + drift + noise
    
    # --- 2. Simulate a TCN Forecast ---
    # A real TCN is complex. We simulate its key property: it learns the predictable
    # non-linear components (growth and cycles), but is blind to the unexpected linear drift.
    tcn_forecast = growth + daily_cycle
    
    # --- 3. Calculate Residuals and Apply CUSUM ---
    residuals = data - tcn_forecast
    
    target = np.mean(residuals[:50]) # Target is the mean of the initial stable residuals
    k = 0.5 * np.std(residuals[:50]) # Slack parameter
    h = 5 * np.std(residuals[:50]) # Control limit
    
    sh, sl = np.zeros(n_points), np.zeros(n_points)
    for i in range(1, n_points):
        sh[i] = max(0, sh[i-1] + (residuals[i] - target) - k)
        sl[i] = max(0, sl[i-1] + (target - residuals[i]) - k)
    
    ooc_points = np.where(sh > h)[0]
    first_ooc = ooc_points[0] if len(ooc_points) > 0 else None
    
    # --- 4. Plotting Dashboard ---
    fig = make_subplots(
        rows=2, cols=2,
        column_widths=[0.7, 0.3],
        row_heights=[0.6, 0.4],
        subplot_titles=("<b>1. TCN Forecast on Bioprocess Data</b>", "<b>3. Residual Diagnostics</b>",
                        "<b>2. CUSUM Chart on Residuals (Drift Detector)</b>"),
        vertical_spacing=0.15
    )
    
    # Plot 1: Main Forecast
    fig.add_trace(go.Scatter(x=time, y=data, mode='lines', name='Actual Data', line=dict(color='black')), row=1, col=1)
    fig.add_trace(go.Scatter(x=time, y=tcn_forecast, mode='lines', name='TCN Forecast', line=dict(dash='dash', color='red')), row=1, col=1)
    # SME Enhancement: Visualize the TCN's receptive field
    receptive_field_size = 32
    fig.add_vrect(x0=150 - receptive_field_size, x1=150, fillcolor="rgba(255,150,0,0.2)", line_width=0,
                  annotation_text="TCN Receptive Field", annotation_position="bottom left", row=1, col=1)

    # Plot 2: CUSUM on Residuals
    fig.add_trace(go.Scatter(x=time, y=sh, mode='lines', name='CUSUM High', line_color='purple'), row=2, col=1)
    fig.add_hline(y=h, line_color='red', row=2, col=1)
    if first_ooc:
        fig.add_trace(go.Scatter(x=[first_ooc], y=[sh[first_ooc]], mode='markers', name='Alarm',
                                 marker=dict(color='red', size=12, symbol='x')), row=2, col=1)

    # Plot 3: Residual Diagnostics
    # Histogram
    fig.add_trace(go.Histogram(x=residuals, name='Residuals Hist', histnorm='probability density', marker_color='grey'), row=1, col=2)
    # Q-Q Plot
    qq_data = stats.probplot(residuals, dist="norm")
    fig.add_trace(go.Scatter(x=qq_data[0][0], y=qq_data[0][1], mode='markers', name='QQ Points'), row=2, col=2)
    fig.add_trace(go.Scatter(x=qq_data[0][0], y=qq_data[1][0] * qq_data[0][0] + qq_data[1][1], mode='lines', name='QQ Fit', line_color='red'), row=2, col=2)
    
    fig.update_layout(height=800, title_text="<b>TCN-CUSUM: Hybrid Model for Complex Drift Detection</b>", showlegend=False)
    fig.update_yaxes(title_text="Biomass Conc.", row=1, col=1)
    fig.update_xaxes(title_text="Time (Hours)", row=2, col=1)
    fig.update_yaxes(title_text="CUSUM Value", row=2, col=1)
    fig.update_xaxes(title_text="Residual Value", row=1, col=2)
    fig.update_yaxes(title_text="Density", row=1, col=2)
    fig.update_xaxes(title_text="Theoretical Quantiles", row=2, col=2)
    fig.update_yaxes(title_text="Sample Quantiles", row=2, col=2)

    return fig, first_ooc

# ==============================================================================
# HELPER & PLOTTING FUNCTION (Method 6) - CORRECTED
# ==============================================================================
@st.cache_data
def plot_lstm_autoencoder_monitoring(drift_rate=0.02, spike_magnitude=2.0):
    """
    Generates an enhanced, more realistic LSTM Autoencoder dashboard, simulating
    multivariate data and visualizing the model's reconstruction performance.
    """
    np.random.seed(42)
    n_points = 250
    time = np.arange(n_points)
    
    # --- 1. SME Enhancement: Simulate realistic multivariate bioprocess data ---
    # Normal process has a slight correlation and oscillation
    temp = 70 + 5 * np.sin(time / 20) + np.random.normal(0, 0.5, n_points)
    ph = 7.0 - 0.2 * np.sin(time / 20 + np.pi/2) + np.random.normal(0, 0.05, n_points)
    df = pd.DataFrame({'Temperature': temp, 'pH': ph})
    
    # --- 2. Simulate the Reconstruction and Error ---
    # A real LSTM autoencoder is complex. We simulate its key property: it reconstructs
    # normal data well, but struggles with anomalies.
    df_recon = df.copy()
    recon_error = np.random.chisquare(df=2, size=n_points) * 0.1 # Base reconstruction error

    # Inject a gradual drift anomaly (e.g., sensor drift in both signals)
    drift_start = 100
    drift_duration = n_points - drift_start
    drift = np.linspace(0, drift_rate * drift_duration, drift_duration)
    # Add drift to the real data (the model won't know how to reconstruct this)
    df['Temperature'][drift_start:] += drift
    df['pH'][drift_start:] -= drift * 0.01
    recon_error[drift_start:] += np.linspace(0, 1.5, drift_duration)**2 # Error accelerates

    # Inject a sudden spike anomaly (e.g., process shock)
    spike_point = 200
    df['Temperature'][spike_point] += spike_magnitude * 2
    df['pH'][spike_point] += spike_magnitude * 0.1
    recon_error[spike_point] += spike_magnitude**2

    # --- 3. Apply Hybrid Monitoring to the Reconstruction Error ---
    # EWMA for drift detection
    lambda_ewma = 0.1
    ewma = pd.Series(recon_error).ewm(alpha=lambda_ewma, adjust=False).mean().values
    ewma_mean = np.mean(recon_error[:drift_start])
    ewma_std = np.std(recon_error[:drift_start])
    ewma_ucl = ewma_mean + 3 * ewma_std * np.sqrt(lambda_ewma / (2 - lambda_ewma))
    ewma_ooc_mask = ewma > ewma_ucl
    first_ewma_ooc = np.argmax(ewma_ooc_mask) if np.any(ewma_ooc_mask) else None

    # BOCPD for spike detection
    hazard = 1/500.0
    mean0, var0 = np.mean(recon_error[:drift_start]), np.var(recon_error[:drift_start])
    R = np.zeros((n_points + 1, n_points + 1)); R[0, 0] = 1
    prob_rl_zero = np.zeros(n_points)
    for t in range(1, n_points + 1):
        x = recon_error[t-1]
        pred_probs = stats.norm.pdf(x, loc=mean0, scale=np.sqrt(var0)) # Simplified model
        R[1:t+1, t] = R[0:t, t-1] * pred_probs * (1 - hazard)
        R[0, t] = np.sum(R[:, t-1] * pred_probs * hazard)
        if np.sum(R[:, t]) > 0: R[:, t] /= np.sum(R[:, t])
        prob_rl_zero[t-1] = R[0, t]
    
    # Find spike detection point
    bocpd_ooc_mask = prob_rl_zero > 0.5 # Threshold for spike detection
    first_bocpd_ooc = np.argmax(bocpd_ooc_mask) if np.any(bocpd_ooc_mask) else None
    
    # --- 4. Plotting Dashboard ---
    fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.05,
                        subplot_titles=("<b>1. Multivariate Process Data & AI Reconstruction</b>",
                                        "<b>2. AI Reconstruction Error (Process Health Score)</b>",
                                        "<b>3. Hybrid Monitoring Charts on Health Score</b>"))

    # Plot 1: Raw Data and Reconstruction
    for col, color in zip(df.columns, ['red', 'blue']):
        fig.add_trace(go.Scatter(x=time, y=df[col], name=f'Actual {col}', mode='lines', line=dict(color=color)), row=1, col=1)
        fig.add_trace(go.Scatter(x=time, y=df_recon[col], name=f'Reconstructed {col}', mode='lines',
                                 line=dict(color=color, dash='dash')), row=1, col=1)
    
    # Plot 2: Reconstruction Error
    fig.add_trace(go.Scatter(x=time, y=recon_error, mode='lines', name='Recon. Error', line_color='black'), row=2, col=1)

    # Plot 3: Hybrid Monitoring
    fig.add_trace(go.Scatter(x=time, y=ewma, mode='lines', name='EWMA (for Drift)', line_color='orange'), row=3, col=1)
    fig.add_hline(y=ewma_ucl, line_color='orange', line_dash='dash', row=3, col=1)
    fig.add_trace(go.Scatter(x=time, y=prob_rl_zero, mode='lines', name='BOCPD (for Spikes)', line_color='purple', yaxis='y2'), row=3, col=1)

    # Add dynamic annotations for alarms
    if first_ewma_ooc:
        fig.add_vline(x=first_ewma_ooc, line=dict(color='orange', width=2),
                      annotation_text="EWMA Drift Alarm", annotation_position='top', row='all', col=1)
    if first_bocpd_ooc:
        fig.add_vline(x=first_bocpd_ooc, line=dict(color='purple', width=2, dash='dot'),
                      annotation_text="BOCPD Spike Alarm", annotation_position='bottom', row='all', col=1)

    fig.update_layout(
        height=800, title_text="<b>LSTM Autoencoder with Hybrid Monitoring System</b>",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        yaxis3=dict(overlaying='y', side='right', title='P(Change)', range=[0,1]), # Secondary axis for BOCPD
        yaxis_title="Value",
        yaxis2_title="Recon. Error",
        yaxis3_title="EWMA",
        xaxis3_title="Time (Hours)"
    )
    
    return fig, first_ewma_ooc, first_bocpd_ooc

#=============================================================== PSO and AUTOENCODER FINAL ========================================================================================================
# ==============================================================================
# HELPER FUNCTIONS & DATA for PSO + Autoencoder Module
# ==============================================================================

def _pharma_surface(x, y):
    return 0.5 * (x - 6.5)**2 + 0.01 * (y - 115)**2 + 10 * np.exp(-((x - 5.0)**2 * 2 + (y - 130)**2 * 0.1))

def _assay_surface(x, y):
    return 2 * (x - 30)**2 + 800 * (y - 1.0)**2 + 12 * np.exp(-((x - 34)**2 * 0.5 + (y - 0.85)**2 * 100))

def _instrument_surface(x, y):
    return 300 * (x - 1.0)**2 + 0.8 * (y - 40)**2 + 15 * np.exp(-((x - 1.15)**2 * 150 + (y - 36)**2 * 0.5))

def _software_surface(x, y):
    return 0.001 * (x - 100)**2 + 0.01 * (y - 20)**2 + 10 * np.exp(-(((x - 450)/50)**2 + ((y - 90)/10)**2))

def _ivd_surface(x, y):
    return 150 * (x - 10)**2 + 0.002 * (y - 20)**2 + 18 * np.exp(-((x - 8.5)**2 * 5 + ((y-80)/10)**2))

PSO_CONTEXTS = {
    'Pharma Process': {
        'x_label': 'Metabolic Shift Point (Day)', 'x_range': [4, 9],
        'y_label': 'Peak VCD (x10‚Å∂ cells/mL)', 'y_range': [80, 140],
        'surface_func': _pharma_surface
    },
    'Assay': {
        'x_label': 'Incubation Time (min)', 'x_range': [25, 35],
        'y_label': 'Antibody Concentration (¬µg/mL)', 'y_range': [0.8, 1.2],
        'surface_func': _assay_surface
    },
    'Instrument': {
        'x_label': 'Flow Rate (mL/min)', 'x_range': [0.8, 1.2],
        'y_label': 'Column Temperature (¬∞C)', 'y_range': [35, 45],
        'surface_func': _instrument_surface
    },
    'Software': {
        'x_label': 'Database Queries / Sec', 'x_range': [50, 500],
        'y_label': 'Concurrent API Calls', 'y_range': [10, 100],
        'surface_func': _software_surface
    },
    'IVD': {
        'x_label': 'Sample Volume (¬µL)', 'x_range': [8, 12],
        'y_label': 'Reagent Age (Days)', 'y_range': [1, 90],
        'surface_func': _ivd_surface
    }
}

@st.cache_data
def run_pso_simulation(n_particles, n_iterations, project_context, landscape_complexity=1.0, noise_level=0.5):
    """
    Runs the PSO simulation on a dynamically generated landscape controlled by sliders.
    """
    np.random.seed(42)
    context = PSO_CONTEXTS[project_context]
    
    # The surface function now uses the new interactive parameters
    def reconstruction_error_surface(x, y):
        base_surface = context['surface_func'](x, y)
        # Add complexity (more local peaks)
        complexity_term = 5 * np.sin(x * landscape_complexity) * np.cos(y * landscape_complexity / 2)
        return base_surface + complexity_term + np.random.uniform(0, noise_level, size=x.shape if hasattr(x, 'shape') else 1)

    x_range = np.linspace(context['x_range'][0], context['x_range'][1], 100)
    y_range = np.linspace(context['y_range'][0], context['y_range'][1], 100)
    xx, yy = np.meshgrid(x_range, y_range)
    zz = reconstruction_error_surface(xx, yy)

    # --- PSO Algorithm (using fixed, sensible internal parameters) ---
    inertia, cognition, social = 0.7, 1.5, 1.5
    positions = np.random.rand(n_particles, 2)
    positions[:, 0] = positions[:, 0] * (context['x_range'][1] - context['x_range'][0]) + context['x_range'][0]
    positions[:, 1] = positions[:, 1] * (context['y_range'][1] - context['y_range'][0]) + context['y_range'][0]
    velocities = np.zeros_like(positions)
    pbest_positions = positions.copy()
    pbest_scores = reconstruction_error_surface(positions[:, 0], positions[:, 1])
    gbest_idx = np.argmax(pbest_scores)
    gbest_position = pbest_positions[gbest_idx].copy()
    gbest_score = pbest_scores[gbest_idx]
    
    history_start = positions.copy()

    for _ in range(n_iterations - 1):
        r1, r2 = np.random.rand(2)
        velocities = (inertia * velocities +
                      cognition * r1 * (pbest_positions - positions) +
                      social * r2 * (gbest_position - positions))
        positions += velocities
        positions[:, 0] = np.clip(positions[:, 0], context['x_range'][0], context['x_range'][1])
        positions[:, 1] = np.clip(positions[:, 1], context['y_range'][0], context['y_range'][1])
        
        current_scores = reconstruction_error_surface(positions[:, 0], positions[:, 1])
        update_mask = current_scores > pbest_scores
        pbest_positions[update_mask] = positions[update_mask]
        pbest_scores[update_mask] = current_scores[update_mask]
        
        current_best_idx = np.argmax(pbest_scores)
        if pbest_scores[current_best_idx] > gbest_score:
            gbest_position = pbest_positions[current_best_idx].copy()
            gbest_score = pbest_scores[current_best_idx]
            
    history_end = positions.copy()
    
    # Return ONLY simple, hashable types. The context dictionary is not returned.
    return zz, x_range, y_range, history_start, history_end, gbest_position, gbest_score
@st.cache_data
def create_pso_static_figure(zz, x_range, y_range, history_start, history_end, gbest_position, context_name, x_label, y_label):
    """
    Creates a static Plotly figure from the PSO simulation results using only simple data types.
    """
    fig = go.Figure()

    fig.add_trace(go.Contour(
        z=zz, x=x_range, y=y_range, 
        colorscale='Inferno', 
        colorbar=dict(title='Anomaly Score<br>(AE Recon. Error)')
    ))
    fig.add_trace(go.Scatter(
        x=history_start[:, 0], y=history_start[:, 1], 
        mode='markers', 
        marker=dict(color='white', size=8, symbol='x'), 
        name='Initial Search Positions'
    ))
    fig.add_trace(go.Scatter(
        x=history_end[:, 0], y=history_end[:, 1], 
        mode='markers', 
        marker=dict(color='cyan', size=8, symbol='circle'), 
        name='Final Search Positions'
    ))
    fig.add_trace(go.Scatter(
        x=[gbest_position[0]], y=[gbest_position[1]], 
        mode='markers', 
        marker=dict(color='lime', size=18, symbol='star', line=dict(width=2, color='black')), 
        name='Highest-Risk Condition Found'
    ))

    fig.update_layout(
        title=f"<b>PSO Search Result: Finding Hidden Failure Modes in a {context_name}</b>",
        xaxis_title=x_label, 
        yaxis_title=y_label,
        legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.7)'),
        height=600
    )
    
    return fig 

@st.cache_data
def plot_digital_twin_dashboard(fault_type, fault_magnitude, fault_time):
    """
    Generates a dashboard simulating a digital twin monitoring a live process.
    """
    np.random.seed(1)
    n_points = 100
    time = np.arange(n_points)
    
    # True Process (with potential fault)
    true_process = 100 + 10 * np.sin(time/10) + np.random.normal(0, 0.5, n_points)
    if fault_type == 'Drift':
        true_process[fault_time:] += np.linspace(0, fault_magnitude, n_points - fault_time)
    elif fault_type == 'Shift':
        true_process[fault_time:] += fault_magnitude

    # Digital Twin's Forecast (it doesn't know about the fault)
    twin_forecast = 100 + 10 * np.sin(time/10)
    
    # Calculate residuals and anomaly score (health score)
    residuals = true_process - twin_forecast
    health_score = pd.Series(residuals).abs().rolling(window=5).mean().fillna(0)
    
    # Plotting
    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1,
                        subplot_titles=("<b>Digital Twin vs. Real Process</b>", "<b>Process Health Score (Deviation)</b>"))
    
    fig.add_trace(go.Scatter(x=time, y=true_process, name='Real Process', line=dict(color=PRIMARY_COLOR, width=3)), row=1, col=1)
    fig.add_trace(go.Scatter(x=time, y=twin_forecast, name='Digital Twin Forecast', line=dict(color='grey', dash='dash')), row=1, col=1)
    
    fig.add_trace(go.Scatter(x=time, y=health_score, name='Health Score', line=dict(color='red', width=3), fill='tozeroy'), row=2, col=1)
    fig.add_hline(y=fault_magnitude*0.75, line=dict(color='red', dash='dash'), row=2, col=1, annotation_text="Alert Threshold")
    
    fig.add_vline(x=fault_time, line=dict(color='red', dash='dot'), annotation_text="Fault Injected")
    
    fig.update_layout(height=600, showlegend=False, title_text="<b>Digital Twin Real-Time Monitoring Dashboard</b>")
    fig.update_yaxes(title_text="Process Value", row=1, col=1)
    fig.update_yaxes(title_text="Anomaly Score", row=2, col=1)
    fig.update_xaxes(title_text="Time (Minutes)", row=2, col=1)
    
    return fig

#=========================================================================================================================================================================================================================
#========================================================================================== LAST TOOLS ====================================================================================================================

# SNIPPET: Add these six new plotting functions to the end of the "ALL HELPER & PLOTTING FUNCTIONS" section of your app.py file.

@st.cache_data
def plot_mpc_simulation(disturbance_size, control_aggressiveness):
    """Simulates and plots a comparison of Reactive vs. Model Predictive Control."""
    np.random.seed(42)
    n_points = 100
    time = np.arange(n_points)
    set_point = 100.0
    
    # Simple process model with inertia
    true_process = np.zeros(n_points)
    true_process[0] = set_point
    for t in range(1, n_points):
        # Simulate a disturbance
        disturbance = disturbance_size if t == 40 else 0
        true_process[t] = 0.95 * true_process[t-1] + 0.05 * set_point + np.random.normal(0, 0.2) + disturbance

    # Reactive (PID-like) Controller
    reactive_control = np.zeros(n_points)
    reactive_control[0] = set_point
    for t in range(1, n_points):
        error = set_point - reactive_control[t-1]
        control_action = control_aggressiveness * error
        reactive_control[t] = 0.95 * reactive_control[t-1] + 0.05 * set_point + control_action * 0.1 + np.random.normal(0, 0.2)
        if t == 40: reactive_control[t] += disturbance_size

    # Model Predictive Controller (MPC) - Simulated
    mpc_control = np.zeros(n_points)
    mpc_control[0] = set_point
    for t in range(1, n_points):
        # MPC 'predicts' the disturbance and starts acting early and smoothly
        future_error_prediction = disturbance_size if 35 <= t < 40 else 0
        error = set_point - mpc_control[t-1]
        control_action = control_aggressiveness * error + future_error_prediction * 0.5
        mpc_control[t] = 0.95 * mpc_control[t-1] + 0.05 * set_point + control_action * 0.08 + np.random.normal(0, 0.2)
        if t == 40: mpc_control[t] += disturbance_size
        
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=time, y=reactive_control, name='Reactive Control', line=dict(color='orange', width=3)))
    fig.add_trace(go.Scatter(x=time, y=mpc_control, name='Model Predictive Control', line=dict(color=SUCCESS_GREEN, width=3)))
    fig.add_hline(y=set_point, line=dict(color='black', dash='dash'), name='Setpoint')
    fig.add_vline(x=40, line=dict(color='red', dash='dot'), annotation_text="Disturbance")

    fig.update_layout(title="<b>Model Predictive Control vs. Reactive Control</b>",
                      xaxis_title="Time (Minutes)", yaxis_title="Critical Process Parameter",
                      legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))
                      
    overshoot_mpc = np.max(mpc_control[40:]) - set_point
    overshoot_reactive = np.max(reactive_control[40:]) - set_point
    return fig, overshoot_mpc, overshoot_reactive

@st.cache_data
def plot_rtrt_dashboard(model_accuracy, lab_delay, lab_noise):
    """Simulates and plots a Real-Time Release Testing dashboard."""
    np.random.seed(1)
    n_batches = 20
    true_cqa = np.random.normal(100, 1.5, n_batches)
    pat_prediction = true_cqa + np.random.normal(0, (100-model_accuracy)/20, n_batches)
    lab_result = true_cqa + np.random.normal(0, lab_noise, n_batches)
    
    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1,
                        subplot_titles=("<b>Analytical Results: PAT vs. Lab</b>", "<b>Batch Status Timeline</b>"))
    
    # Plot 1: Results
    fig.add_trace(go.Scatter(y=true_cqa, name='True CQA (Hidden)', mode='lines', line=dict(color='black', dash='dash')), row=1, col=1)
    fig.add_trace(go.Scatter(y=pat_prediction, name='PAT Prediction (Instant)', mode='lines+markers', line=dict(color=SUCCESS_GREEN)), row=1, col=1)
    lab_x = np.arange(n_batches) + lab_delay
    fig.add_trace(go.Scatter(x=lab_x, y=lab_result, name=f'Lab Result ({lab_delay} day delay)', mode='markers', marker=dict(color='orange', symbol='x', size=10)), row=1, col=1)
    
    # Plot 2: Status Timeline
    df_timeline = pd.DataFrame([
        dict(Batch=f"Batch {i+1}", Start=i, Finish=i+0.9, Status="In-Process") for i in range(n_batches)
    ] + [
        dict(Batch=f"Batch {i+1}", Start=i+0.9, Finish=i+lab_delay, Status="Awaiting QC") for i in range(n_batches)
    ] + [
        dict(Batch=f"Batch {i+1}", Start=i+lab_delay, Finish=i+lab_delay+0.1, Status="Released") for i in range(n_batches)
    ])
    fig_timeline = px.timeline(df_timeline, x_start="Start", x_end="Finish", y="Batch", color="Status",
                               color_discrete_map={"In-Process": PRIMARY_COLOR, "Awaiting QC": "orange", "Released": SUCCESS_GREEN})
    for trace in fig_timeline.data:
        fig.add_trace(trace, row=2, col=1)

    fig.update_layout(height=600, legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))
    fig.update_yaxes(title_text="CQA Value", row=1, col=1)
    fig.update_xaxes(title_text="Time (Days)", row=2, col=1)
    return fig

@st.cache_data
def plot_biosimilarity_dashboard(mean_shift_potency, variance_factor_impurity):
    """Simulates and plots a multi-CQA comparability dashboard for biosimilarity."""
    np.random.seed(42)
    n = 50
    cqas = {
        "Potency (%)": {'mean': 100, 'std': 5, 'margin': 5},
        "Aggregate (%)": {'mean': 1.0, 'std': 0.2, 'margin': 0.5},
        "Main Peak Purity (%)": {'mean': 99.0, 'std': 0.3, 'margin': 1.0},
        "Glycan Profile G0F (%)": {'mean': 40, 'std': 3, 'margin': 4}
    }
    
    results = []
    for cqa, params in cqas.items():
        data_orig = np.random.normal(params['mean'], params['std'], n)
        mean_b = params['mean']
        std_b = params['std']
        if "Potency" in cqa: mean_b += mean_shift_potency
        if "Aggregate" in cqa: std_b *= variance_factor_impurity
        
        data_bio = np.random.normal(mean_b, std_b, n)
        
        # TOST Calculation
        diff_mean = np.mean(data_bio) - np.mean(data_orig)
        std_err_diff = np.sqrt(np.var(data_bio, ddof=1)/n + np.var(data_orig, ddof=1)/n)
        df_welch = (std_err_diff**4) / (((np.var(data_bio, ddof=1)/n)**2 / (n-1)) + ((np.var(data_orig, ddof=1)/n)**2 / (n-1)))
        ci_margin = stats.t.ppf(0.95, df_welch) * std_err_diff
        ci_lower, ci_upper = diff_mean - ci_margin, diff_mean + ci_margin
        
        is_equivalent = (ci_lower >= -params['margin']) and (ci_upper <= params['margin'])
        results.append({'cqa': cqa, 'ci_lower': ci_lower, 'ci_upper': ci_upper, 'margin': params['margin'], 'equivalent': is_equivalent})
    
    df_results = pd.DataFrame(results)
    
    fig = go.Figure()
    for _, row in df_results.iterrows():
        color = SUCCESS_GREEN if row['equivalent'] else 'red'
        fig.add_trace(go.Scatter(x=[row['ci_lower'], row['ci_upper']], y=[row['cqa'], row['cqa']],
                                 mode='lines', line=dict(color=color, width=8), showlegend=False))
        fig.add_trace(go.Scatter(x=[(row['ci_lower']+row['ci_upper'])/2], y=[row['cqa']],
                                 mode='markers', marker=dict(color='white', size=8, line=dict(color='black', width=2)), showlegend=False))
    
    for _, row in df_results.iterrows():
        fig.add_shape(type="line", x0=-row['margin'], x1=row['margin'], y0=row['cqa'], y1=row['cqa'],
                      line=dict(color='black', width=12, dash='dot'), opacity=0.3)

    fig.update_layout(title="<b>Analytical Comparability: Equivalence Tier Results</b>",
                      xaxis_title="Difference (Biosimilar - Original)",
                      yaxis=dict(categoryorder='total ascending'))
    return fig, df_results

@st.cache_data
def plot_nonparametric_comparison(outlier_magnitude):
    """Compares a t-test vs. a Mann-Whitney U test in the presence of an outlier."""
    np.random.seed(1)
    n = 30
    group_a = np.random.normal(10, 2, n)
    group_b = np.random.normal(10, 2, n)
    if outlier_magnitude > 0:
        group_b[0] = 10 + outlier_magnitude * 2
    
    ttest_p = stats.ttest_ind(group_a, group_b).pvalue
    mwu_p = mannwhitneyu(group_a, group_b).pvalue
    
    df = pd.concat([pd.DataFrame({'value': group_a, 'group': 'A'}),
                    pd.DataFrame({'value': group_b, 'group': 'B'})])
    
    fig = px.box(df, x='group', y='value', points='all', title="<b>Data Distributions with Outlier</b>")
    return fig, ttest_p, mwu_p

@st.cache_data
def plot_capa_effectiveness(improvement_magnitude):
    """Simulates and plots a process before and after a CAPA."""
    np.random.seed(42)
    n = 50
    # Before CAPA: Shifted and variable process
    data_before = np.random.normal(104, 2.5, n)
    # After CAPA: Mean and variance are improved
    mean_after = 104 - 2 * improvement_magnitude
    std_after = 2.5 - 1 * improvement_magnitude
    data_after = np.random.normal(mean_after, std_after, n)
    
    lsl, usl = 95, 105
    def calc_cpk(data, lsl, usl):
        m, s = np.mean(data), np.std(data, ddof=1)
        if s == 0: return 10.0
        return min((usl - m) / (3 * s), (m - lsl) / (3 * s))
    
    cpk_before = calc_cpk(data_before, lsl, usl)
    cpk_after = calc_cpk(data_after, lsl, usl)
    
    fig = make_subplots(rows=1, cols=2, subplot_titles=("<b>Process State Before CAPA</b>", "<b>Process State After CAPA</b>"))
    
    fig.add_trace(go.Histogram(x=data_before, name='Before', marker_color='red'), row=1, col=1)
    fig.add_trace(go.Histogram(x=data_after, name='After', marker_color=SUCCESS_GREEN), row=1, col=2)
    
    for col in [1, 2]:
        fig.add_vline(x=lsl, line=dict(dash='dot', color='black'), row=1, col=col)
        fig.add_vline(x=usl, line=dict(dash='dot', color='black'), row=1, col=col)

    fig.update_layout(showlegend=False, bargap=0.1, title_text="<b>CAPA Effectiveness: Before vs. After</b>")
    return fig, cpk_before, cpk_after

@st.cache_data
def plot_hfe_dashboard(design_clarity, task_complexity):
    """Simulates and plots a dashboard for Human Factors Engineering usability data."""
    np.random.seed(42)
    # Simulate SUS Scores
    sus_base_score = 25 + design_clarity * 7
    sus_scores = np.random.normal(sus_base_score, 15, 20)
    sus_scores = np.clip(sus_scores, 0, 100)
    final_sus = np.mean(sus_scores)
    
    # Simulate Task Failures
    tasks = ["1. Setup Device", "2. Enter Patient ID", "3. Run Sample", "4. Read Result", "5. Clean Device"]
    base_fail_prob = 0.2 - (design_clarity * 0.015) + (task_complexity * 0.01)
    fail_rates = [np.clip(base_fail_prob + np.random.uniform(-0.05, 0.05) + (i*0.01*task_complexity), 0.01, 0.5) for i in range(len(tasks))]
    
    fig_tasks = px.bar(x=tasks, y=fail_rates, title="<b>Task Failure Analysis</b>",
                       labels={'x': 'User Task', 'y': 'Observed Failure Rate'},
                       color=fail_rates, color_continuous_scale='Reds')
    fig_tasks.update_layout(yaxis_tickformat=".0%")
    
    return fig_tasks, final_sus

#==============================================================================================================================================================================================
#====================================================================================== ENHACEMENTS =============================================================================================
# SNIPPET 3: Replace the existing render_spc_charts function with this enhanced version.

def render_spc_charts():
    """Renders the INTERACTIVE module for Statistical Process Control (SPC) charts."""
    st.markdown("""
    #### Purpose & Application: The Voice of the Process
    **Purpose:** To serve as an **EKG for your process**‚Äîa real-time heartbeat monitor that visualizes its stability. It distinguishes between common cause (normal noise) and special cause (a real problem) variation.
    """)
    
    sim_tab, byod_tab = st.tabs(["üìä Interactive Simulation", "üìÅ Analyze Your Data"])

    with sim_tab:
        st.info("""
        **Interactive Demo:** Use the controls to inject different types of "special cause" events into a simulated stable process. Observe how the I-MR, Xbar-R, and P-Charts each respond, helping you learn to recognize the visual signatures of common process problems.
        """)
        scenario = st.radio(
            "Select a Process Scenario to Simulate:",
            ('Stable', 'Sudden Shift', 'Gradual Trend', 'Increased Variability'),
            captions=[
                "Process is behaving normally.",
                "e.g., A new raw material lot is introduced.",
                "e.g., An instrument is slowly drifting out of calibration.",
                "e.g., An operator becomes less consistent."
            ]
        )
        fig_imr, fig_xbar, fig_p = plot_spc_charts(scenario=scenario)
        
        st.plotly_chart(fig_imr, use_container_width=True)
        st.plotly_chart(fig_xbar, use_container_width=True)
        st.plotly_chart(fig_p, use_container_width=True)
        
        # --- REPORT GENERATION ---
        if st.button("üìÑ Generate Report from Simulation"):
            report_html = generate_html_report(
                title=f"SPC Simulation Report: {scenario}",
                figures={"I-MR Chart": fig_imr, "Xbar-R Chart": fig_xbar, "P-Chart": fig_p},
                kpis={"Scenario": scenario, "Status": "Simulated"}
            )
            st.download_button(label="Download HTML Report", data=report_html, file_name=f"spc_report_{scenario}.html", mime="text/html")

    with byod_tab:
        st.info("Upload a CSV or Excel file with your process data. The data should be in a single column.")
        uploaded_file = st.file_uploader("Choose a file", type=['csv', 'xlsx'])

        if uploaded_file:
            try:
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)
                
                st.dataframe(df.head())
                data_col = st.selectbox("Select the data column for analysis:", df.columns)
                
                if data_col and pd.api.types.is_numeric_dtype(df[data_col]):
                    # Simulate a stable scenario with the user's data
                    # In a real app, you might add more options for subgrouping etc.
                    user_data = df[data_col].dropna().values
                    fig_imr_user, _, _ = plot_spc_charts(scenario='Stable') # Re-purpose the function
                    
                    # Update I-MR chart with user data
                    mean_i = np.mean(user_data)
                    mr = np.abs(np.diff(user_data))
                    mr_mean = np.mean(mr)
                    sigma_est_i = mr_mean / 1.128
                    UCL_I, LCL_I = mean_i + 3 * sigma_est_i, mean_i - 3 * sigma_est_i
                    
                    fig_imr_user.data[0].y = user_data
                    fig_imr_user.data[1].y = mr
                    fig_imr_user.update_layout(
                        shapes=[
                            dict(type='line', y0=mean_i, y1=mean_i, line=dict(dash='dash', color='black')),
                            dict(type='line', y0=UCL_I, y1=UCL_I, line=dict(color='red')),
                            dict(type='line', y0=LCL_I, y1=LCL_I, line=dict(color='red')),
                        ]
                    )
                    st.plotly_chart(fig_imr_user, use_container_width=True)
                    st.success("Successfully generated I-MR chart from your data.")
                else:
                    st.warning("Please select a numeric column for analysis.")
            except Exception as e:
                st.error(f"An error occurred while processing your file: {e}")

@st.cache_data
def plot_doc_control_flow(current_status="Draft"):
    """Generates a flowchart for a typical GxP document lifecycle."""
    fig = go.Figure()
    statuses = ["Draft", "In Review", "Approved", "Effective", "Obsolete"]
    status_colors = {s: 'lightgrey' for s in statuses}
    
    # Highlight the current and past statuses
    try:
        current_index = statuses.index(current_status)
        for i in range(current_index + 1):
            status_colors[statuses[i]] = SUCCESS_GREEN
    except ValueError:
        # Handle cases where status might not be in the list, though it shouldn't happen with the slider
        pass

    nodes = {s: {'label': s, 'pos': (i*2, 5)} for i, s in enumerate(statuses)}
    edges = [("Draft", "In Review"), ("In Review", "Approved"), ("Approved", "Effective"), ("Effective", "Obsolete")]

    # Draw arrows
    for start, end in edges:
        fig.add_annotation(
            ax=nodes[start]['pos'][0] + 0.8, ay=nodes[start]['pos'][1],
            x=nodes[end]['pos'][0] - 0.8, y=nodes[end]['pos'][1],
            arrowhead=2, arrowwidth=2, showarrow=True
        )

    # Draw nodes
    for name, props in nodes.items():
        fig.add_shape(
            type="rect", 
            x0=props['pos'][0]-0.8, y0=props['pos'][1]-0.4, 
            x1=props['pos'][0]+0.8, y1=props['pos'][1]+0.4, 
            fillcolor=status_colors[name], 
            line=dict(color='black')
        )
        fig.add_annotation(
            x=props['pos'][0], y=props['pos'][1], 
            text=f"<b>{props['label']}</b>", 
            showarrow=False, 
            font_color="black" if status_colors[name]=='lightgrey' else "white"
        )

    fig.update_layout(
        title_text="<b>Document Control Lifecycle</b>",
        xaxis=dict(visible=False, range=[-1, 9]),
        yaxis=dict(visible=False, range=[4, 6]),
        height=250,
        margin=dict(l=10, r=10, t=40, b=10)
    )
    return fig
# =================================================================================================================================================================================================
# ================================================================== ALL UI RENDERING FUNCTIONS =================================================================================================
# ==================================================================================================================================================================================================

def render_introduction_content():
    """Renders the complete, all-in-one introduction and framework dashboard for V&V Sentinel."""
    st.title("üõ°Ô∏è V&V Sentinel Toolkit")
    st.markdown("### The Interactive Playbook for Biotech & MedTech Validation")
    
    st.markdown(
        """
        <p style='color: grey; margin-bottom: 0;'>Developed by<br><b>Jose Bautista, MSc, LSSBB, PMP</b></p>
        
        <div style='color: grey; margin-top: 20px;'>
        <b>Contact Information</b><br>
        üìß jbautistads@gmail.com<br>
        üîó linkedin.com/in/josebautista
        </div>
        """,
        unsafe_allow_html=True
    )
    st.divider()

    st.markdown(
        """
        Welcome to **V&V Sentinel Toolkit**, your guide through the complex landscape of Verification and Validation. 
        This application is more than a collection of tools; it's an interactive, educational journey designed for the modern scientist, engineer, and quality professional in the regulated life sciences industry. 
        
        Whether you are planning a process validation, transferring a new analytical method, or managing the lifecycle of a commercial product, the Sentinel is here to illuminate the statistical and AI-driven methods that form the backbone of a robust, compliant, and data-driven quality system.
        """
    )
    st.info("#### üëà Select a tool from the sidebar to explore an interactive module.")

    st.header("What's Inside the Sentinel Toolkit?")
    st.markdown("Each module is a self-contained deep-dive, equipped with a comprehensive set of features to provide a 360-degree understanding of the topic.")
    
    c1, c2, c3 = st.columns(3)
    c1.subheader("üî¨ Interactive Tools")
    c1.markdown("- **Gadget Controls:** Use sidebar sliders, buttons, and selectors to simulate real-world scenarios and see how the models and charts react in real-time.")
    c1.markdown("- **Statistical & ML Models:** From classical ANOVA to advanced AI like Transformers and GNNs, explore a wide array of analytical engines.")

    c2.subheader("üìä Rich Content & Insights")
    c2.markdown("- **Key Insights:** A quick-start guide to interpreting the charts and understanding the core strategic takeaway of each tool.")
    c2.markdown("- **The Business Case:** A detailed breakdown of the problem, impact, solution, and consequences, framing each tool's value in financial and strategic terms for stakeholders.")
    
    c3.subheader("üéì Deep Educational Context")
    c3.markdown('- **Glossaries & Golden Rules:** Clear definitions of key terms and actionable "best practice" advice.')
    c3.markdown("- **Theory, History & Math:** Explore the fascinating origins of each method, from WWII code-breaking to the biotech revolution, and understand the core mathematical principles.")
    c3.markdown("- **Regulatory & Compliance:** Connect each tool to specific FDA, ICH, and ISO guidelines to understand its role in a compliant GxP environment.")
    
    st.divider()

    st.header("üìñ The Scientist's Journey: A Four-Act Story")
    st.markdown(
        """
        The path from a novel idea to a stable, commercial process is a story in four acts. 
        **V&V Sentinel** is structured to follow this complete narrative, demonstrating that a successful project is not just about executing tests, but about a holistic strategy that begins with rigorous planning and continues through the entire product lifecycle.
        """
    )
    
    act0, act1, act2, act3 = st.columns(4)
    with act0:
        st.subheader("Act 0: Planning & Strategy")
        st.markdown("**The Blueprint.** Before a single experiment is run, a successful project is defined. This is the act of creating the project's 'North Star'‚Äîdefining goals, assessing risks, and creating the master plan for validation.")
    with act1: 
        st.subheader("Act I: Characterization")
        st.markdown("**The Discovery.** With a plan in place, we build a deep, data-driven understanding of the process. This act is about discovering the fundamental capabilities, limitations, and sensitivities of the new method or process.")
    with act2: 
        st.subheader("Act II: Qualification & Transfer")
        st.markdown("**The Crucible.** Here, the method or process faces its ultimate test. It must prove its performance in a new environment‚Äîa new lab, a new scale, a new team. This is about demonstrating stability, equivalence, and capability.")
    with act3: 
        st.subheader("Act III: Lifecycle Management")
        st.markdown("**The Guardianship.** Once live, the journey isn't over. This final act is about continuous vigilance: monitoring process health, detecting subtle drifts, and using advanced analytics to predict and prevent future failures.")
    
    st.divider()

    st.header("üöÄ The V&V Model: A Strategic Framework")
    st.markdown("The **Verification & Validation (V&V) Model**, shown below, provides a structured, globally accepted framework for ensuring a system meets its intended purpose, from initial requirements to final deployment.")
    fig_v_model = plot_v_model()
    st.plotly_chart(fig_v_model, use_container_width=True)

    st.markdown("The table below provides a side-by-side comparison of typical documents and activities for each stage of the V-Model across different biotech contexts.")
    summary_df = create_v_model_summary_table()
    fig_table = create_styled_v_model_table(summary_df)
    st.plotly_chart(fig_table, use_container_width=True)
    
    st.divider()
    
    st.header("üìà Project Workflow")
    st.markdown("This timeline organizes the entire toolkit by its application in a typical project lifecycle. Tools are grouped by the project phase where they provide the most value, following the four-act structure.")
    st.plotly_chart(plot_act_grouped_timeline(), use_container_width=True)

    st.header("‚è≥ A Chronological View of V&V Analytics")
    st.markdown("This timeline organizes the same tools purely by their year of invention, showing the evolution of statistical and machine learning thought over the last century.")
    st.plotly_chart(plot_chronological_timeline(), use_container_width=True)

    st.header("üó∫Ô∏è Conceptual Map of Tools")
    st.markdown("This map illustrates the relationships between the foundational concepts and the specific tools available in this application. Use it to navigate how different methods connect to broader analytical strategies.")
    st.plotly_chart(create_toolkit_conceptual_map(), use_container_width=True)

    # --- NEW SECTION FOR THE BUSINESS CASE MAP ---
    st.header("üíº Map by Business Case Theme")
    st.markdown("This map re-organizes the entire toolkit by the primary business problem it solves. Use this to communicate the value of these analytical tools to stakeholders and to build a business case for a data-driven validation strategy.")
    st.plotly_chart(create_business_case_map(), use_container_width=True)
    # --- END OF NEW SECTION ---
# ==============================================================================
# UI RENDERING FUNCTIONS (ALL DEFINED BEFORE MAIN APP LOGIC)
# ==============================================================================
# SNIPPET 3: Add this new rendering function to the "ALL UI RENDERING FUNCTIONS" section.

def render_search_page():
    """Renders the search interface and results page with an improved search parser."""
    st.title("üîé Search the V&V Sentinel Toolkit")
    
    # Create the index (will be cached and run only once)
    ix = create_search_index()
    
    # --- FIX: Updated user instructions for the new features ---
    st.info("""
    **Search Tips:**
    - The search now finds documents with **any** of your terms (OR logic).
    - Use `*` for wildcards (e.g., `valid*` finds validation, validate, etc.).
    - Use `~` after a word for fuzzy/typo search (e.g., `procss~`).
    """)

    search_query = st.text_input("Enter search terms:", label_visibility="collapsed", placeholder="Search for content (e.g., risk, validation, FMEA)")

    if search_query:
        with st.spinner("Searching..."):
            with ix.searcher() as searcher:
                # --- THIS IS THE UPGRADED SEARCH PARSER ---
                fields_to_search = ["title", "content"]
                # Give a boost to matches found in the title
                field_boosts = {"title": 2.0}
                # Use MultifieldParser, default to OR logic, and add plugins
                parser = MultifieldParser(fields_to_search, ix.schema, fieldboosts=field_boosts, group=OrGroup)
                parser.add_plugin(WildcardPlugin())
                parser.add_plugin(FuzzyTermPlugin())
                # --- END OF UPGRADE ---
                
                query = parser.parse(search_query)
                
                # Perform the search and limit results
                results = searcher.search(query, limit=20)
                
                # Configure highlighter to bold the search terms
                results.formatter = HtmlFormatter(tagname="b", classname="highlight")

                st.subheader(f"Found {len(results)} results for '{search_query}'")
                
                if not results:
                    st.warning("No matches found. Try using broader terms or a wildcard (*).")
                
                for hit in results:
                    # The path contains "Tool Name > Tab Name"
                    try:
                        tool_name, tab_name = hit['path'].split(' > ')
                    except ValueError:
                        tool_name, tab_name = hit['path'], "" # Handle cases with no tab

                    with st.container(border=True):
                        st.markdown(f"#### {hit['title']}")
                        st.caption(f"Found in: **{hit['path']}**")
                        
                        # Use hit.highlights to get the text snippet with the search term bolded
                        highlighted_snippet = hit.highlights("content", top=2)
                        if highlighted_snippet:
                            st.markdown("..." + highlighted_snippet + "...", unsafe_allow_html=True)
                        
                        # Create a button to navigate to the correct tool
                        if tool_name != "Project Framework": # Don't create button for intro page
                            if st.button(f"Go to {tool_name}", key=f"goto_{hit.docnum}"):
                                st.session_state.current_view = tool_name
                                st.rerun()
#===================================================================================================== ACT 0 Render=================================================================================================================
#===================================================================================================================================================================================================================================
def render_tpp_cqa_cascade():
    """Renders the comprehensive, interactive module for TPP & CQA Cascade, including a full QbD introduction."""
    
    # --- CASE STUDY INTEGRATION BLOCK ---
    case_study_params = {}
    is_case_study_mode = False
    active_case_key = st.session_state.get('case_study', {}).get('active_case')
    
    if active_case_key:
        current_step_index = st.session_state['case_study']['current_step']
        current_step = CASE_STUDIES[active_case_key]['steps'][current_step_index]
        
        if current_step['target_tool'] == "TPP & CQA Cascade":
            is_case_study_mode = True
            case_study_params = current_step.get('params', {})
            with st.expander("üìñ **Case Study Context**", expanded=True):
                st.info(f"**{CASE_STUDIES[active_case_key]['title']} | Step {current_step_index + 1}: {current_step['title']}**")
                st.markdown(current_step['explanation'])
    # --- END OF INTEGRATION BLOCK ---
    st.markdown("""
    #### Purpose & Application: The "Golden Thread" of QbD
    **Purpose:** To be the **"North Star" of the entire project.** This tool visualizes the "golden thread" of Quality by Design (QbD). It starts with the high-level patient or business needs (the **Target Product Profile**), translates them into measurable product requirements (the **Critical Quality Attributes**), and finally links those to the specific process parameters and material attributes that must be controlled.
    
    **Strategic Application:** This cascade is the first and most important document in a modern, science- and risk-based validation program. It provides a clear, traceable line of sight from the needs of the patient all the way down to the specific dial a process engineer needs to turn. This is the essence of **ICH Q8**.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Head of Product Development.
    1.  Select a **Project Type** to see its unique quality cascade.
    2.  Use the **TPP Target Sliders** in the sidebar to define your project's ambition. Notice how increasing a target highlights the specific CQAs (in yellow) that are critical to achieving that goal.
    """)
    
    project_type = st.selectbox(
        "Select a Project Type to visualize its Quality Cascade:", 
        ["Monoclonal Antibody", "IVD Kit", "Pharma Process (Small Molecule)", "Instrument Qualification", "Computer System Validation"]
    )
    
    target1_val, target2_val = None, None
    target1_tag, target2_tag = "", ""

    with st.sidebar:
        st.subheader("TPP Target Controls")
        if project_type == "Monoclonal Antibody":
            target1_val = st.slider("Desired Efficacy Target (%)", 80, 120, 95, 1, help="Why > 100%? Potency is often measured *relative* to a Reference Standard (defined as 100%). A process improvement could yield a more potent drug.")
            target1_tag = "Efficacy"
            target2_val = st.slider("Required Shelf-Life (Months)", 12, 36, 24, 1)
            target2_tag = "Shelf-Life"
        elif project_type == "IVD Kit":
            target1_val = st.slider("Desired Clinical Sensitivity (%)", 90, 100, 98, 1)
            target1_tag = "Efficacy"
            target2_val = st.slider("Required Shelf-Life (Months)", 6, 24, 18, 1)
            target2_tag = "Shelf-Life"
        elif project_type == "Pharma Process (Small Molecule)":
            target1_val = st.slider("Target Process Yield (%)", 75, 95, 85, 1, help="Higher yield is a key business driver.")
            target1_tag = "Yield"
            target2_val = st.slider("Target Purity Level (%)", 99.0, 99.9, 99.5, 0.1, format="%.1f", help="Higher purity is critical for patient safety.")
            target2_tag = "Purity"
        elif project_type == "Instrument Qualification":
            target1_val = st.slider("Target Throughput (Plates/hr)", 10, 100, 50, 5, help="Higher throughput is a key performance requirement.")
            target1_tag = "Throughput"
            target2_val = st.slider("Target Reliability (Uptime %)", 95.0, 99.9, 99.0, 0.1, format="%.1f", help="Higher uptime is critical for operational efficiency.")
            target2_tag = "Reliability"
        elif project_type == "Computer System Validation":
            target1_val = st.slider("Target Performance (Report Time in sec)", 1, 10, 5, 1, help="Faster performance is a key user requirement. Lower is better.")
            target1_tag = "Performance"
            target1_val = 11 - target1_val # Invert so higher on slider is better
            target2_val = st.checkbox("Full 21 CFR Part 11 Compliance Required", value=True)
            target2_tag = "Compliance"

    fig = plot_tpp_cqa_cascade(project_type, target1_val, target2_val, target1_tag, target2_tag)
    st.plotly_chart(fig, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive into Quality by Design (QbD)")
    
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìñ Introduction to QbD", "üìã QbD Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **Reading the Cascade:**
        - **TPP (Left Node):** This is the contract with the patient and the business. It defines *what* the product, process, or system must do.
        - **CQAs (Middle Nodes):** These are the measurable properties the *product* must possess or performance attributes the *system* must have to fulfill the TPP. This is the translation of the "what" into the "how."
        - **CPPs/CMAs (Right Nodes):** These are the specific, controllable "knobs" on the *process* or *design* that have a direct impact on the CQAs. This is where science and engineering meet the quality target.

        **The Strategic Insight: From 'What' to 'Why' to 'How'**
        The Sankey diagram doesn't just show a list of items; it shows a **traceable chain of logic**. As you adjust the TPP sliders, the highlighted CQAs (yellow) show the immediate impact of a strategic business decision on technical requirements. This cascade is the core of a science- and risk-based approach because it forces the team to formally document the scientific evidence that links a specific process parameter to a critical product attribute, which in turn ensures patient safety and efficacy.
        """)

    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: From Ambiguous Goals to an Actionable Blueprint
    
        #### The Problem: Siloed Project Development
        A project starts with a vague goal like "create a high-yield process." The R&D team works in isolation, the Manufacturing team has different priorities, and the QC team develops methods on their own schedule. There is no single, shared, quantitative definition of success.
    
        #### The Impact: Late-Stage Failure & Massive Rework
        This lack of alignment is the #1 cause of late-stage project failure. The process fails validation because it can't meet a quality attribute that was never clearly defined. The QC method is found to be unsuitable for its purpose after months of development. This leads to millions in wasted resources, devastating project delays, and a loss of competitive advantage.
    
        #### The Solution: The "Golden Thread" as a Binding Contract
        The TPP/CQA/CPP Cascade is not a document; it's a **formal contract** between all stakeholders (R&D, Manufacturing, Quality, Regulatory, Business). It forces a rigorous, top-down conversation at the project's inception to translate the high-level business need (TPP) into specific, measurable, technical targets (CQAs) and the process controls required to hit them (CPPs).
    
        #### The Consequences: Alignment, Speed, and "Right First Time"
        - **Without This:** The project is a high-risk gamble built on assumptions and miscommunication.
        - **With This:** The entire organization is aligned on a single, data-driven blueprint. R&D develops a process to hit specific CQA targets, and QC develops methods to measure those same targets. This proactive alignment dramatically increases the probability of "right first time" validation and accelerates time-to-market.
        """)
    with tabs[2]:
        st.markdown("""
        #### The QbD Paradigm Shift
        Quality by Design (QbD) represents a fundamental shift in the philosophy of manufacturing and validation, championed by global regulators.

        **The Traditional Approach (Quality by Testing):**
        - The process is a "black box." A fixed recipe is followed, and quality is ensured by performing extensive testing on the *final product*.
        - **Analogy:** Baking a cake by rigidly following an old recipe without understanding *why* it works. If the cake comes out badly, the only option is to throw it away (reject the batch) and try again. Quality is "inspected in" at the end.

        **The Modern Approach (Quality by Design):**
        - The process is a "glass box." We use scientific investigation (like DOE) and risk management (like FMEA) to gain a deep, predictive understanding of how raw materials (CMAs) and process parameters (CPPs) impact the final product quality (CQAs).
        - **Analogy:** Being a master baker who understands the chemistry of baking. You know exactly how adjusting the oven temperature (a CPP) will affect the cake's moistness (a CQA). You design a robust recipe and process, and you monitor the critical parameters in real-time. You *know* the cake will be good every time, with minimal final testing required. Quality is "designed in" from the beginning.
        
        The TPP/CQA/CPP cascade is the central tool that documents this deep understanding.
        """)
        
    with tabs[3]:
        st.markdown("""
        ##### The Language of Quality by Design
        - **Target Product Profile (TPP):**
          - **Definition (ICH Q8):** A prospective summary of the quality characteristics of a drug product that ideally will be achieved to ensure the desired quality, taking into account safety and efficacy.
          - **SME Translation:** What does this product need to do for the patient? What is our contract with them?
          - **Example:** "A sterile, injectable liquid that safely and effectively treats rheumatoid arthritis with a 24-month shelf life."

        - **Critical Quality Attribute (CQA):**
          - **Definition (ICH Q8):** A physical, chemical, biological, or microbiological property that should be within an appropriate limit to ensure the desired product quality.
          - **SME Translation:** What measurable property must the *product* have to meet the TPP?
          - **Example:** "Purity must be > 99%" (to ensure safety and efficacy).

        - **Critical Material Attribute (CMA):**
          - **Definition (ICH Q8):** A property of an input material that should be within an appropriate limit to ensure the desired quality of the output product.
          - **SME Translation:** What property of a *raw material* must be controlled?
          - **Example:** "The pH of the cell culture media must be between 6.9 and 7.3."

        - **Critical Process Parameter (CPP):**
          - **Definition (ICH Q8):** A process parameter whose variability has an impact on a CQA and therefore should be monitored or controlled.
          - **SME Translation:** What "knob" on the *process* must be controlled?
          - **Example:** "The bioreactor temperature must be maintained at 37.0 ¬± 0.5 ¬∞C."

        - **Design Space:**
          - **Definition (ICH Q8):** The multidimensional combination of input variables (e.g., CMAs) and process parameters (e.g., CPPs) that have been demonstrated to provide assurance of quality.
          - **SME Translation:** The "safe operating zone" or the proven "recipe for success."

        - **Control Strategy:**
          - **Definition (ICH Q10):** A planned set of controls, derived from product and process understanding, that assures process performance and product quality.
          - **SME Translation:** The complete police force for your process: the combination of raw material specifications (for CMAs), in-process controls (for CPPs), and final product tests (for CQAs) that guarantee quality.
        """)
        
    with tabs[4]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Test, Fail, and Repeat" Cycle**
A team develops a process based on a few successful runs. When a batch fails during validation, a lengthy investigation begins with no clear starting point. The process is a 'black box,' and troubleshooting is reactive guesswork.
- **The Flaw:** There is no documented, scientific understanding of the process. The team doesn't know *which* parameters are critical, so they can't effectively control them or troubleshoot them.""")
        st.success("""üü¢ **THE GOLDEN RULE: Begin with the End in Mind, and Document the Links**
A compliant and robust QbD approach is a disciplined, multi-stage process.
1.  **Define the Goal (TPP):** First, formally document the "contract" with the patient.
2.  **Identify What Matters (CQAs):** Translate the TPP into a set of measurable, scientific targets for the product.
3.  **Link the Chain of Knowledge (FMEA & DOE):** Use risk assessment and designed experiments to discover and prove the links between the process/materials (CPPs/CMAs) and the product quality (CQAs).
4.  **Establish the Control Strategy:** Implement controls on the critical parameters and attributes you identified to ensure that every batch meets the CQAs and, by extension, the TPP. This cascade is the documented proof of this entire logical chain.""")

    with tabs[5]:
        st.markdown("""
        #### Historical Context: From Juran's Trilogy to ICH
        **The Problem:** For much of the 20th century, the pharmaceutical industry operated on a "quality by testing" paradigm. Processes were developed, locked down, and then extensively tested at the end to prove they worked. This was inefficient, costly, and led to a poor understanding of *why* processes sometimes failed, making continuous improvement difficult.
        
        **The 'Aha!' Moment:** The core ideas of QbD were articulated by the legendary quality pioneer **Joseph M. Juran** in his "Quality Trilogy" (Quality Planning, Quality Control, Quality Improvement). He argued forcefully that quality must be *planned* and *designed* into a product from the very beginning, not inspected in at the end. While these ideas were adopted by other high-tech industries like automotive and electronics, the pharmaceutical industry was slower to change due to its rigid regulatory structure.
            
        **The Impact (The ICH Revolution):** In the early 2000s, the FDA and other global regulators recognized that the traditional approach was stifling innovation and hindering process improvement. They launched the **"Pharmaceutical cGMPs for the 21st Century"** initiative to encourage a modern, science- and risk-based approach. This culminated in the landmark **ICH Q8(R2) Guideline on Pharmaceutical Development** in 2009. This guideline formally adopted Juran's philosophy, introducing the concepts of the **Target Product Profile (TPP)** and **Critical Quality Attributes (CQA)** to the industry. It marked a major philosophical shift away from a prescriptive, "cookbook" approach to a flexible, understanding-based framework, with the TPP/CQA cascade as its central pillar.
        """)
        
    with tabs[6]:
        st.markdown("""
        This entire framework is defined and championed by the International Council for Harmonisation (ICH) and adopted by global regulators like the FDA.
        - **ICH Q8(R2) - Pharmaceutical Development:** This is the primary guideline. It explicitly defines the **Target Product Profile (TPP)**, **Critical Quality Attributes (CQA)**, and **Critical Process Parameters (CPP)** as the foundational elements of QbD. It introduces the concepts of the **Design Space** and **Control Strategy**.
        - **ICH Q9 - Quality Risk Management:** The process of identifying which attributes and parameters are "critical" (i.e., identifying CQAs and CPPs) is a formal risk assessment activity that must be documented according to the principles of ICH Q9. The FMEA tool in this app is a direct implementation of this.
        - **FDA Guidance on Process Validation (2011):** The entire lifecycle approach is built on this foundation. **Stage 1 (Process Design)** is the activity of translating the CQAs into a robust manufacturing process by identifying and controlling the CPPs and CMAs.
        - **GAMP 5:** For instruments and software, the TPP is analogous to the **User Requirement Specification (URS)**, and the CQAs are analogous to the high-level **Functional Specifications (FS)**. This cascade provides the direct link between user needs and system design.
        """)
        
#=====================================================================2. ANALYTICAL TARGET PROFILE (ATP) BUILDER ====================================================
def render_atp_builder():
    """Renders the comprehensive, interactive module for building a Target Profile."""
    
    # --- CASE STUDY INTEGRATION BLOCK ---
    case_study_params = {}
    is_case_study_mode = False
    active_case_key = st.session_state.get('case_study', {}).get('active_case')
    
    if active_case_key:
        current_step_index = st.session_state['case_study']['current_step']
        current_step = CASE_STUDIES[active_case_key]['steps'][current_step_index]
        
        if current_step['target_tool'] == "Analytical Target Profile (ATP) Builder":
            is_case_study_mode = True
            case_study_params = current_step.get('params', {})
            with st.expander("üìñ **Case Study Context**", expanded=True):
                st.info(f"**{CASE_STUDIES[active_case_key]['title']} | Step {current_step_index + 1}: {current_step['title']}**")
                st.markdown(current_step['explanation'])
    # --- END OF INTEGRATION BLOCK ---
    st.markdown("""
    #### Purpose & Application: The Project's "Contract"
    **Purpose:** To serve as the **"Design Specification" or "Contract" for a new product, process, or system.** Before significant work begins, the Target Profile formally documents the performance characteristics that *must* be achieved for the project to be considered a success.
    
    **Strategic Application:** This is the formal translation of high-level user needs (from the TPP or URS) into a concrete set of statistical and performance-based acceptance criteria. It is the objective scorecard against which all development and validation activities are measured, preventing "goalpost moving" and ensuring the final result is truly fit-for-purpose.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Validation or Project Lead.
    1.  Select the **Project Type** you are planning.
    2.  Use the **Performance Requirement Sliders** in the sidebar to define the "contract" for this project.
    3.  Toggle on the "Simulate Results" to see how a hypothetical final result (green) performs against your targets.
    """)

    col1, col2 = st.columns([0.4, 0.6])
    
    with col1:
        st.subheader("Target Profile Requirements")
        project_type = st.selectbox("Select Project Type", [
            "Pharma Assay (HPLC)", "IVD Kit (ELISA)", "Instrument Qualification", "Software System (LIMS)", "Pharma Process (MAb)"
        ])
        
        atp_values = []
        achieved_values = None
        show_results = False
        
        with st.sidebar:
            st.subheader(f"Controls for {project_type}")
            if project_type == "Pharma Assay (HPLC)":
                atp_values.append(st.slider("Accuracy (%Rec)", 95.0, 102.0, 98.0, 0.5, help="Target: 100%. How close must the average measurement be to the true value?"))
                atp_values.append(st.slider("Precision (%CV)", 0.5, 5.0, 2.0, 0.1, help="How much random variability is acceptable? Lower is better."))
                atp_values.append(st.slider("Linearity (R¬≤)", 0.9900, 1.0000, 0.9990, 0.0001, format="%.4f", help="How well does signal correlate with concentration? Higher is better."))
                atp_values.append(st.slider("Range (Turn-down)", 10, 100, 50, 5, help="Ratio of highest to lowest quantifiable point."))
                atp_values.append(st.slider("Sensitivity (LOD)", 1, 50, 20, 1, help="Qualitative score for required Limit of Detection. Higher score = more sensitive."))
                show_results = st.toggle("Simulate Validation Results", value=True, key="hplc_results")
                if show_results: achieved_values = [99.5, 1.5, 0.9995, 80, 30]
            
            elif project_type == "IVD Kit (ELISA)":
                atp_values.append(st.slider("Clinical Sensitivity (%)", 90.0, 100.0, 98.0, 0.5, help="Ability to correctly identify true positives."))
                atp_values.append(st.slider("Clinical Specificity (%)", 90.0, 100.0, 99.0, 0.5, help="Ability to correctly identify true negatives."))
                atp_values.append(st.slider("Precision (%CV)", 10.0, 20.0, 15.0, 1.0, help="Assay repeatability. Lower is better for consistent results."))
                atp_values.append(st.slider("Robustness Score", 1, 10, 7, 1, help="Qualitative score for performance across different lots, users, and sites."))
                atp_values.append(st.slider("Shelf-Life (Months)", 6, 24, 18, 1, help="Required stability of the kit at recommended storage."))
                show_results = st.toggle("Simulate Validation Results", value=True, key="ivd_results")
                if show_results: achieved_values = [99.0, 99.5, 12.0, 9, 24]

            elif project_type == "Instrument Qualification":
                atp_values.append(st.slider("Accuracy (Max Bias %)", 0.1, 5.0, 1.0, 0.1, help="Maximum acceptable systematic error. Lower is better."))
                atp_values.append(st.slider("Precision (Max %CV)", 0.5, 5.0, 1.5, 0.1, help="Maximum acceptable random error. Lower is better."))
                atp_values.append(st.slider("Throughput (Samples/hr)", 10, 200, 100, 10, help="Required sample processing speed to meet business needs."))
                atp_values.append(st.slider("Uptime (%)", 95.0, 99.9, 99.0, 0.1, format="%.1f", help="Required operational reliability."))
                atp_values.append(st.slider("Footprint (m¬≤)", 1.0, 5.0, 2.0, 0.5, help="Maximum allowable lab space. A key logistical constraint. Lower is better."))
                show_results = st.toggle("Simulate Qualification Results", value=True, key="inst_results")
                if show_results: achieved_values = [0.8, 1.2, 120, 99.5, 1.8]

            elif project_type == "Software System (LIMS)":
                atp_values.append(st.slider("Reliability (Uptime %)", 99.0, 99.999, 99.9, 0.001, format="%.3f", help="Percentage of time the system must be available."))
                atp_values.append(st.slider("Performance (Query Time sec)", 0.5, 10.0, 2.0, 0.5, help="Maximum time for a key database query to return. Critical for user experience. Lower is better."))
                atp_values.append(st.slider("Security (Compliance Score)", 1, 10, 8, 1, help="Qualitative score for meeting all 21 CFR Part 11 requirements (e.g., audit trails, e-signatures)."))
                atp_values.append(st.slider("Usability (User Sat. Score)", 1, 10, 7, 1, help="Score from User Acceptance Testing (UAT), indicating how intuitive the system is."))
                atp_values.append(st.slider("Scalability (Concurrent Users)", 50, 5000, 500, 50, help="Maximum number of users the system must support simultaneously without performance degradation."))
                show_results = st.toggle("Simulate Validation Results", value=True, key="soft_results")
                if show_results: achieved_values = [99.99, 1.5, 10, 8, 1000]

            elif project_type == "Pharma Process (MAb)":
                atp_values.append(st.slider("Yield (g/L)", 1.0, 10.0, 5.0, 0.5, help="Grams of product per liter of bioreactor volume. A key economic driver."))
                atp_values.append(st.slider("Purity (%)", 98.0, 99.9, 99.5, 0.1, help="Final product purity via SEC-HPLC."))
                atp_values.append(st.slider("Consistency (Inter-batch Cpk)", 1, 10, 8, 1, help="Qualitative score for process predictability and low variability. Higher score means a more consistent process."))
                atp_values.append(st.slider("Robustness (PAR Size Score)", 1, 10, 6, 1, help="Qualitative score for the size of the proven acceptable range."))
                atp_values.append(st.slider("Cycle Time (Days)", 10, 20, 14, 1, help="Time from start to finish for a single batch. A key operational efficiency metric. Lower is better."))
                show_results = st.toggle("Simulate PPQ Results", value=True, key="proc_results")
                if show_results: achieved_values = [6.5, 99.7, 9, 8, 13]

    with col2:
        st.subheader("Target Profile Visualization")
        fig = plot_atp_radar_chart(project_type, atp_values, achieved_values)
        st.plotly_chart(fig, use_container_width=True)
        if show_results:
            st.success("The validation results (green) meet or exceed the target profile (blue) on all criteria.")

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    with tabs[0]:
        st.markdown("""
        **Interpreting the Radar Chart:**
        - The chart provides an immediate, holistic view of the project's required performance characteristics. The **blue polygon is your 'contract' (the Target Profile)**. The **green polygon is the 'deliverable' (the final validated performance)**.
        - **Success Criteria:** A successful validation project is one where the green "Achieved" polygon **fully encompasses** the blue "Target" polygon.
        - **Identifying Trade-offs:** This visualization makes strategic trade-offs clear. For an instrument, you might see that achieving maximum **Throughput** could require accepting a slightly larger lab **Footprint**. The ATP forces this to be a conscious, documented decision.
        - **Avoiding "Gold-Plating":** This visualization helps teams determine if their requirements are reasonable. If the blue polygon is vastly larger than what is necessary for the intended use, it signals that the project may be "gold-plating" the requirements, leading to excessive development time and cost.
        """)
    with tabs[1]:
        st.markdown("""
        ### The Business Case: From a "Science Project" to a Fit-for-Purpose Asset
    
        #### The Problem: The Method Transfer Chasm
        There is a deep, often unacknowledged, cultural and operational chasm between the worlds of Analytical Development (AD) and Quality Control (QC).
        - **The AD World:** Focuses on discovery, flexibility, and peak performance. Methods are often run by highly skilled PhDs on perfectly maintained instruments with low throughput. Their goal is to prove a scientific concept.
        - **The QC World:** Focuses on robustness, speed, and simplicity. Methods must be run 24/7 by technicians with variable skill levels on instruments with a heavy workload. Their goal is to produce a compliant, reliable result, every single time, as quickly as possible.
    
        When a method is developed in the AD world and "thrown over the wall" to QC without a formal contract, it is destined to fail in the more demanding routine environment.
    
        #### The Impact: The OOS Investigation Black Hole
        This failure is a massive, hidden cost center and a source of profound operational drag.
        - **Extended Batch Release Cycles:** An unreliable QC method generates frequent invalid assays, forcing re-tests that can add days or even weeks to the batch release cycle time, directly delaying revenue recognition.
        - **The OOS Investigation Black Hole:** A batch fails its specification. A costly Out-of-Specification (OOS) investigation is launched. After weeks of investigation, the root cause is often inconclusive or defaults to "analyst error," because no one suspects the officially "validated" method itself is the problem. This wastes hundreds of man-hours and destroys trust.
        - **Increased Headcount & Capital:** The QC lab's headcount swells to manage the high rate of re-tests and investigations. The business may even purchase more instruments to cope with the perceived "capacity issue," when the real problem is the inefficiency of the method.
    
        #### The Solution: The ATP as a Negotiated Service Level Agreement (SLA)
        The Analytical Target Profile is the tool that bridges the chasm. It is a formal, **negotiated Service Level Agreement (SLA)** between the method developer (AD) and the end user (QC), facilitated by Quality Assurance. Before significant development work begins, all parties come to the table to quantitatively define the "contract" for the new method.
    
        This is not a scientific discussion; it is a business and operational one. The questions are not "what is possible?" but "what is required?":
        - *What is the minimum precision (%CV) needed to reliably stay within the product's specification limits?*
        - *What is the maximum run time we can tolerate to meet our batch release goals?*
        - *What level of robustness is required for this method to be run successfully by any qualified analyst on any qualified instrument?*
    
        #### The Consequences: A High-Speed Engine for Batch Release
        - **Without This:** The QC lab is a perpetual bottleneck, a source of organizational friction, and a significant compliance risk. Method transfer is a high-stakes gamble based on hope.
        - **With This:** The ATP transforms method development from a high-risk R&D "science project" into a **predictable engineering discipline with guaranteed business outcomes**. The resulting method is guaranteed to be **fit-for-purpose**, leading to smooth, successful transfers. The QC lab becomes a reliable, high-speed engine for batch release, enabling faster revenue recognition and building a culture of data-driven collaboration between departments.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Performance Attributes
        - **Accuracy (%Rec / Bias):** The closeness of the measured average to the true or accepted reference value. Measures **systematic error**.
        - **Precision (%CV):** The closeness of agreement among a series of measurements from the same sample. Measures **random error**.
        - **Linearity (R¬≤):** The ability to elicit test results that are directly proportional to the concentration of the analyte.
        - **Range (Turn-down):** The interval between the upper and lower concentration of analyte for which the procedure has been demonstrated to have a suitable level of precision, accuracy, and linearity.
        - **Sensitivity (LOD):** The lowest amount of analyte in a sample which can be detected but not necessarily quantitated as an exact value.
        - **Clinical Sensitivity/Specificity:** Metrics for diagnostic tests measuring the rates of true positives and true negatives, respectively.
        - **Robustness:** A measure of a procedure's capacity to remain unaffected by small, but deliberate variations in method parameters.
        - **Throughput:** An operational metric, often measured in samples, plates, or units per unit of time.
        - **Reliability (Uptime):** The percentage of time a system is available for normal operation.
        - **Performance (Query Time):** A software metric for the time it takes to complete a specific computational task.
        - **Security:** For software, the ability to meet regulatory requirements for data integrity, such as those in 21 CFR Part 11.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: "We'll Know It When We See It"**
A development team starts working on a new assay with a vague goal like "make a good potency assay." They spend months optimizing, and then present the final data to the QC and Regulatory teams, who then inform them that the precision or range is not sufficient for its intended use in a routine environment.
- **The Flaw:** The project lacked a pre-defined definition of success. This leads to wasted work, internal friction, and significant project delays when the method is transferred.""")
        st.success("""üü¢ **THE GOLDEN RULE: The Target Profile is a Negotiated Contract**
The ATP is not just a scientific document; it's a formal **Service Level Agreement (SLA)** between all key stakeholders (Analytical Development, QC, Manufacturing, Regulatory, Quality) that is established *before* significant development work begins.
1.  **Define "Fit for Purpose" First:** All parties must negotiate and agree on the specific, numerical criteria in the Target Profile.
2.  **Develop to the Target:** The development team uses the profile as their explicit set of engineering goals.
3.  **Validate Against the Target:** The final validation protocol uses the profile's criteria as the formal, pre-approved acceptance criteria.
This ensures alignment from start to finish and guarantees the final deliverable is fit for its intended purpose.""")
        
    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Checklist to Lifecycle
        **The Problem:** For decades, analytical method validation was treated as a one-time, checklist activity performed at the end of development. This often resulted in methods that were technically valid but not practically robust or well-suited for the harsh realities of routine use in a 24/7 QC environment. The common "over-the-wall" transfer from an R&D lab to a QC lab was a frequent source of project delays and failures.
        
        **The 'Aha!' Moment:** The **Quality by Design (QbD)** movement, championed by thought leaders like Janet Woodcock at the FDA in the early 2000s, proposed a new paradigm. This was formalized in the **"Pharmaceutical cGMPs for the 21st Century"** initiative. The core idea was to apply a proactive, lifecycle approach to all aspects of manufacturing, including the analytical methods themselves.
            
        **The Impact:** The **Analytical Target Profile (ATP)** emerged from this philosophy as a best practice. It was championed by the FDA and scientific bodies like the AAPS in the 2010s. The ATP is the direct application of QbD to method development. It parallels the **Target Product Profile (TPP)**, but instead of defining the goals for a drug product, it defines the goals for the *measurement system* used to test that product. This represents a mature, proactive, and lifecycle-based approach to ensuring analytical method quality.
        """)
        
    with tabs[5]:
        st.markdown("""
        The Target Profile is a modern best-practice that directly supports several key regulatory guidelines by providing a clear, a priori definition of what will be validated.
        - **ICH Q8(R2), Q14:** The ATP is the starting point for applying QbD principles to analytical methods.
        - **FDA Process Validation Guidance:** The TPP for a process defines the goals for **Stage 1 (Process Design)**.
        - **GAMP 5:** For instruments and software, the Target Profile is a direct translation of the **User Requirement Specification (URS)** into a set of verifiable performance criteria for OQ and PQ.
        - **USP Chapter <1220> - The Analytical Procedure Lifecycle:** This new chapter champions a holistic, lifecycle approach to method management. The ATP is the foundational element of **Stage 1 (Procedure Design)**, where the requirements for the method are formally defined.
        """)


# SNIPPET: Replace the entire render_ivd_regulatory_framework function with this corrected version.

def render_ivd_regulatory_framework():
    """Renders the comprehensive module for the IVD & Medical Device Regulatory Framework."""

    # --- CASE STUDY INTEGRATION BLOCK ---
    case_study_params = {}
    is_case_study_mode = False
    active_case_key = st.session_state.get('case_study', {}).get('active_case')
    
    if active_case_key:
        current_step_index = st.session_state['case_study']['current_step']
        current_step = CASE_STUDIES[active_case_key]['steps'][current_step_index]
        
        if current_step['target_tool'] == "IVD & Medical Device Regulatory Framework":
            is_case_study_mode = True
            case_study_params = current_step.get('params', {})
            with st.expander("üìñ **Case Study Context**", expanded=True):
                st.info(f"**{CASE_STUDIES[active_case_key]['title']} | Step {current_step_index + 1}: {current_step['title']}**")
                st.markdown(current_step['explanation'])
    # --- END OF INTEGRATION BLOCK ---

    st.markdown("""
    #### Purpose & Application: The Global Regulatory Roadmap
    **Purpose:** To provide a clear, high-level overview of the major global regulatory pathways for In Vitro Diagnostics (IVDs) and Medical Devices. This module explains the internationally harmonized risk-based classification system and the corresponding submission types (510(k), PMA, CE Marking) required to bring a product to market in key regions.
    
    **Strategic Application:** This is the most critical strategic decision in a product's lifecycle. The choice of regulatory pathway, determined by the device's **Intended Use**, dictates the entire project's timeline, cost, data requirements, and ultimate business model. Understanding this global roadmap is non-negotiable for R&D, Quality, Regulatory Affairs, and business leadership.
    """)
    
    if not is_case_study_mode:
        st.info("""
        **Interactive Demo:** You are the Head of Regulatory Affairs.
        1.  Use the **"Regulatory Theater"** tabs below to select a major global market (USA, EU, Japan).
        2.  Use the **sidebar controls** to select your product concept.
        3.  The active flowchart will instantly **highlight the correct regulatory pathway** for your product in that specific region.
        """)

    with st.sidebar:
        st.subheader("Regulatory Pathway Simulator")
        product_concept_options = ["General Low-Risk Device", "Moderate-Risk Device with Predicate", "Novel Moderate-Risk Device", "High-Risk/Novel Device", "Emergency Use Device", "Point-of-Care (POC) Device", "Software as a Medical Device (SaMD)"]
        default_product_concept = case_study_params.get("product_concept", "Moderate-Risk Device with Predicate")
        product_concept_index = product_concept_options.index(default_product_concept) if default_product_concept in product_concept_options else 1
        
        product_concept = st.selectbox(
            "Select Your Product Concept (US FDA Example):",
            product_concept_options,
            index=product_concept_index,
            help="Your choice determines the device's risk level and corresponding US FDA regulatory pathway.",
            disabled=is_case_study_mode
        )

    path_map_fda = {
        "General Low-Risk Device": "class_i", "Moderate-Risk Device with Predicate": "510k", 
        "Novel Moderate-Risk Device": "denovo", "High-Risk/Novel Device": "pma", 
        "Emergency Use Device": "eua", "Point-of-Care (POC) Device": "510k", 
        "Software as a Medical Device (SaMD)": "510k"
    }
    path_map_eu = {"General Low-Risk Device": "class_i", "Moderate-Risk Device with Predicate": "class_iia", "Novel Moderate-Risk Device": "class_iib", "High-Risk/Novel Device": "class_iii", "Point-of-Care (POC) Device": "class_iia", "Software as a Medical Device (SaMD)": "class_iia"}
    path_map_jpn = {"General Low-Risk Device": "class_i", "Moderate-Risk Device with Predicate": "class_ii", "Novel Moderate-Risk Device": "class_iii", "High-Risk/Novel Device": "class_iv", "Point-of-Care (POC) Device": "class_ii", "Software as a Medical Device (SaMD)": "class_ii"}
    
    st.header("The Regulatory Theater: A Global Comparison")
    
    tab_usa, tab_eu, tab_jpn, tab_comp = st.tabs(["üá∫üá∏ USA (FDA)", "üá™üá∫ European Union (MDR/IVDR)", "üáØüáµ Japan (PMDA)", "üåê Global Comparison & Harmonization"])

    with tab_usa:
        highlight_path_fda = path_map_fda.get(product_concept, '510k')
        st.plotly_chart(plot_fda_pathway(highlight_path_fda), use_container_width=True)

    with tab_eu:
        highlight_path_eu = path_map_eu.get(product_concept, 'class_iia')
        st.plotly_chart(plot_eu_pathway(highlight_path_eu), use_container_width=True)

    with tab_jpn:
        highlight_path_jpn = path_map_jpn.get(product_concept, 'class_ii')
        st.plotly_chart(plot_jpn_pathway(highlight_path_jpn), use_container_width=True)

    with tab_comp:
        st.subheader("Global Harmonization: Speaking a Common Language of Quality")
        st.markdown("""
        While each region has its own specific laws, a massive global effort has been made to harmonize the underlying principles. This allows manufacturers to build a single, robust Quality Management System (QMS) that meets the requirements of multiple countries.

        #### The Core Difference: "Gatekeeper" vs. "Auditor"
        The most important strategic difference between the major regulatory systems is the role of the governing body.
        - **USA / Japan (The Gatekeeper Model):** The regulatory body (FDA, PMDA) acts as a direct **gatekeeper**. You must submit your entire technical file and premarket submission directly to the agency, and their scientists and reviewers must approve it before you can go to market.
        - **European Union (The Auditor Model):** The EU sets the laws (MDR/IVDR), but the "gatekeeper" role for most devices is delegated to independent, third-party organizations called **Notified Bodies**. You pay a Notified Body to audit your Quality System and review your Technical File. If they agree you are compliant, they grant you a **CE Mark** certificate, which allows you to sell your product anywhere in the EU.
        """)
        
        st.markdown("""
        **Comparison of Major Regulatory Systems**
        <style>
            .reg-table { width: 100%; border-collapse: collapse; } .reg-table th, .reg-table td { border: 1px solid #ddd; padding: 8px; text-align: left; }
            .reg-table th { background-color: #f2f2f2; } .reg-table tr:nth-child(even) { background-color: #f9f9f9; }
        </style>
        <table class="reg-table">
            <tr> <th>Feature</th> <th>USA (FDA)</th> <th>European Union (EU)</th> <th>Japan (MHLW/PMDA)</th> <th>Canada (Health Canada)</th> </tr>
            <tr> <td><b>Primary Law</b></td> <td>FD&C Act</td> <td>MDR & IVDR</td> <td>PMD Act</td> <td>Food and Drugs Act</td> </tr>
            <tr> <td><b>"Gatekeeper"</b></td> <td>FDA (Direct review)</td> <td>Notified Body (Third-party audit)</td> <td>PMDA (Direct review)</td> <td>Health Canada (Direct review)</td> </tr>
            <tr> <td><b>Quality System</b></td> <td>21 CFR 820 (QSR)</td> <td>ISO 13485 + MDR/IVDR Annexes</td> <td>MHLW Ordinance 169</td> <td>ISO 13485 (via MDSAP)</td> </tr>
            <tr> <td><b>Risk Classes</b></td> <td>I, II, III</td> <td>I, IIa, IIb, III (Devices) / A, B, C, D (IVDs)</td> <td>I, II, III, IV</td> <td>I, II, III, IV</td> </tr>
            <tr> <td><b>Key Approval Mark</b></td> <td>510(k) Clearance / PMA Approval</td> <td><b>CE Mark</b></td> <td>Shonin Approval</td> <td>Medical Device License</td> </tr>
        </table>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        ---
        ### The Pillars of Global Compliance: Harmonized Standards
        The key to an efficient global strategy is to build your QMS on internationally recognized standards. Compliance with these standards provides a "presumption of conformity" with the specific regulations in many regions.
        - **ISO 13485: Medical devices ‚Äî Quality management systems.** This is the foundational standard for a QMS.
        - **ISO 14971: Medical devices ‚Äî Application of risk management.** This is the global gold standard for risk management.
        - **IEC 62304: Medical device software ‚Äî Software life cycle processes.** If your device includes software, this standard is non-negotiable.
        """)
    
    st.divider()
    st.subheader("Deeper Dive into the Regulatory Framework")
    tabs_deep = st.tabs(["‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs_deep[0]:
        st.markdown("""
        ### The Business Case: Choosing Your Mountain
    
        #### The Problem: The "One-Size-Fits-All" Commercialization Plan
        A startup develops a new biomarker technology. The leadership team, focused on speed to market, assumes they will follow the "standard" 510(k) pathway. They build their entire business plan‚Äîfundraising, timelines, and resource allocation‚Äîaround this assumption, without deeply analyzing the implications of their intended use.
    
        #### The Impact: The Mid-Project Pivot and Business Model Failure
        Halfway through development, during a pre-submission meeting, the FDA informs them that the specific diagnostic claims they want to make classify their product as high-risk Class III, requiring a full PMA. This leads to exploding timelines and budgets, and potential business failure.
    
        #### The Solution: A Deliberate, Front-Loaded Strategic Choice
        The choice of regulatory pathway is the most important **strategic business decision** a medical device company will make. It must be made with eyes wide open at the very beginning of the project.
        - **The 510(k) Path:** Faster and cheaper, but your claims are limited by your predicate.
        - **The De Novo Path:** For novel, low-risk devices. More work than a 510(k) but avoids a PMA. You get to be the first.
        - **The PMA Path:** Incredibly long and expensive, but if you succeed, you have a powerful, defensible monopoly.
    
        #### The Consequences: A Predictable Journey and Aligned Investment
        A company that makes a **deliberate, informed, and strategic decision** on its regulatory pathway from Day 1 aligns the entire organization on a single, realistic plan, dramatically increasing the probability of success.
        """)
        
    with tabs_deep[1]:
        st.markdown("""
        ##### Glossary of Global Regulatory Terms
        - **510(k):** US pathway for Class II devices based on **Substantial Equivalence**.
        - **PMA:** US pathway for Class III devices requiring proof of **Safety and Efficacy**.
        - **De Novo:** US pathway for novel, low-risk devices with no predicate.
        - **EUA:** US temporary authorization during a public health emergency.
        - **QSR (21 CFR 820):** The US FDA's cGMP requirements for medical devices.
        - **CE Mark:** The mandatory conformity mark for products sold in the European Economic Area.
        - **Notified Body:** A third-party organization designated by an EU country to audit a manufacturer's QMS and technical documentation and grant the CE Mark for most devices.
        - **MDR/IVDR:** The new, more stringent regulations for medical devices and IVDs in the EU.
        - **IMDRF:** The current global harmonization body that develops standardized guidance.
        - **PMDA & MHLW:** The primary regulatory agency and ministry in Japan.
        """)
        
    with tabs_deep[2]:
        st.error("""üî¥ **THE INCORRECT APPROACH: "RUO Creep"**
A company sells a reagent kit labeled "For Research Use Only" (RUO). Their marketing, however, strongly implies that the kit can be used by clinical labs to help in diagnosis.
- **The Flaw:** This is a major regulatory violation. They are illegally marketing an unapproved medical device.""")
        st.success("""üü¢ **THE GOLDEN RULE: Your Claims Define Your Device, and the Device Defines the Controls**
The entire regulatory framework is built on a clear, logical cascade.
1.  **Your words (marketing, labeling) define the Intended Use.**
2.  **The Intended Use defines the Risk Class.**
3.  **The Risk Class defines the Regulatory Pathway.**
4.  **The Regulatory Pathway defines the required Controls.**
This chain is unbreakable and must be managed from the start.""")

    with tabs_deep[3]:
        st.markdown("""
        #### Historical Context: From Elixirs of Death to a Risk-Based Framework
        The US regulatory framework for medical devices was forged in response to public health crises.
        - **1938 FD&C Act:** Passed after the Elixir Sulfanilamide tragedy, it gave the FDA authority over drugs but left devices largely unregulated.
        - **1976 Medical Device Amendments:** The pivotal moment. Passed in response to catastrophic failures like the **Dalkon Shield IUD**, this act created the modern, risk-based framework: the **three-tiered classification system (Class I, II, III)** and the corresponding **510(k)** and **PMA** pathways.
        - **1990 Safe Medical Devices Act:** Strengthened the FDA's authority and led to the creation of the mandatory **Design Controls** regulation.
        """)
        
    with tabs_deep[4]:
        st.markdown("""
        This framework is built on a foundation of specific regulations in major global markets, which are harmonized by international standards.
        - **USA:** The **FD&C Act** and **21 CFR Parts 800-1299** (especially **Part 820**, the Quality System Regulation).
        - **Europe:** The **MDR (EU 2017/745)** and **IVDR (EU 2017/746)** are the primary laws.
        - **Japan:** The **PMD Act** is the primary law, administered by the Ministry of Health, Labour and Welfare (MHLW).
        - **International Standards:** **ISO 13485** (QMS), **ISO 14971** (Risk Management), and **IEC 62304** (Software) are key for global compliance.
        """)
#============================================================================== 3. QUALITY RISK MANAGEMENT (FMEA) ========================================================
def render_qrm_suite():
    """Renders the comprehensive, interactive module for the Quality Risk Management (QRM) Suite."""
    st.markdown("""
    This workbench provides a suite of formal, structured tools for **Quality Risk Management (QRM)**. These tools are used to systematically identify, analyze, evaluate, and control potential risks. The output of a risk assessment is the direct, auditable justification for the entire validation strategy, focusing resources where they matter most.
    """)
    
    st.info("""
    **How to Use:**
    1.  Select a **Project Type** and a **Risk Management Tool**.
    2.  The dashboard will load a realistic, contextual example.
    3.  Review the main plot and interact with the **"Workbench Gadgets"** below. For FMEA, use the sliders in the sidebar to see the charts update in real-time. Use the `(?)` for details on each control.
    """)

    col1, col2 = st.columns(2)
    with col1:
        project_type = st.selectbox(
            "Select a Scenario / Project Type:", 
            ["Pharma Process (MAb)", "IVD Assay (ELISA)", "Instrument Qualification (HPLC)", "Software System (LIMS)"]
        )
    with col2:
        tool_choice = st.selectbox(
            "Select a Quality Risk Management Tool:", 
            ["Preliminary Hazard Analysis (PHA)", "Failure Mode and Effects Analysis (FMEA)", "Fault Tree Analysis (FTA)", "Event Tree Analysis (ETA)"]
        )
    
    st.header(f"Dashboard: {tool_choice} for {project_type}")
    st.divider()

    # --- Master Data Dictionary for ALL Scenarios ---
    # This remains unchanged, but the FMEA data now assumes a 1-10 scale.
    qrm_templates = {
        "PHA": {
            "Pharma Process (MAb)": pd.DataFrame([
                {'Hazard': 'Mycoplasma Contamination in Bioreactor', 'Severity': 5, 'Likelihood': 1}, 
                {'Hazard': 'Incorrect Chromatography Buffer Leads to Impurity', 'Severity': 4, 'Likelihood': 3},
                {'Hazard': 'Filter Integrity Failure During Sterile Filtration', 'Severity': 5, 'Likelihood': 2}
            ]),
            "IVD Assay (ELISA)": pd.DataFrame([
                {'Hazard': 'Reagent Cross-Contamination gives False Positive', 'Severity': 4, 'Likelihood': 3}, 
                {'Hazard': 'Incorrect Sample Dilution gives False Negative', 'Severity': 5, 'Likelihood': 2},
                {'Hazard': 'Instrument Read Error leads to Misdiagnosis', 'Severity': 4, 'Likelihood': 4}
            ]),
            "Instrument Qualification (HPLC)": pd.DataFrame([
                {'Hazard': 'Chemical Spill from Solvent Leak', 'Severity': 3, 'Likelihood': 2}, 
                {'Hazard': 'Loss of Data due to Power Failure (No UPS)', 'Severity': 4, 'Likelihood': 1},
                {'Hazard': 'Incorrect Peak Integration leads to OOS', 'Severity': 5, 'Likelihood': 3}
            ]),
            "Software System (LIMS)": pd.DataFrame([
                {'Hazard': 'Unauthorized Access to Patient Data', 'Severity': 5, 'Likelihood': 2}, 
                {'Hazard': 'Data Corruption During Archival', 'Severity': 4, 'Likelihood': 3},
                {'Hazard': 'System Crash During Sample Login', 'Severity': 3, 'Likelihood': 4}
            ]),
        },
        "FMEA": {
            "Pharma Process (MAb)": pd.DataFrame({
                'Failure Mode': ['Contamination in Bioreactor', 'Incorrect Chromatography Buffer', 'Filter Integrity Failure'],
                'S': [10, 8, 10], 'O_Initial': [3, 5, 2], 'D_Initial': [4, 2, 3]
            }),
            "IVD Assay (ELISA)": pd.DataFrame({
                'Failure Mode': ['Degraded Capture Antibody', 'Operator Pipetting Error', 'Incorrect Incubation Time'],
                'S': [9, 7, 8], 'O_Initial': [4, 6, 5], 'D_Initial': [5, 3, 2]
            }),
            "Instrument Qualification (HPLC)": pd.DataFrame({
                'Failure Mode': ['Pump Seal Failure (Leak)', 'Detector Lamp Degradation', 'Autosampler Needle Clog'],
                'S': [6, 8, 7], 'O_Initial': [3, 4, 7], 'D_Initial': [4, 2, 3]
            }),
            "Software System (LIMS)": pd.DataFrame({
                'Failure Mode': ['Data Corruption on Database Write', 'Incorrect Calculation Logic', 'Server Downtime (No Failover)'],
                'S': [10, 10, 7], 'O_Initial': [2, 3, 4], 'D_Initial': [6, 8, 3]
            })
        },
        "FTA": {
            "Pharma Process (MAb)": {'title': 'Batch Contamination', 'nodes': {'Top':{'label':'Batch Contaminated','type':'top','shape':'square','pos':(.5,.9)}, 'OR1':{'label':'OR','type':'gate','shape':'circle','pos':(.5,.7)}, 'Filt':{'label':'Filter Failure','type':'basic','shape':'square','pos':(.25,.5),'prob':0.001}, 'Op':{'label':'Operator Error','type':'basic','shape':'square','pos':(.75,.5),'prob':0.005}}, 'links': [('OR1','Top'),('Filt','OR1'),('Op','OR1')]},
            "IVD Assay (ELISA)": {'title': 'False Negative Result', 'nodes': {'Top':{'label':'False Negative','type':'top','shape':'square','pos':(.5,.9)}, 'OR1':{'label':'OR','type':'gate','shape':'circle','pos':(.5,.7)}, 'Reag':{'label':'Inactive Reagent','type':'basic','shape':'square','pos':(.25,.5),'prob':0.02}, 'Samp':{'label':'Sample Degraded','type':'basic','shape':'square','pos':(.75,.5),'prob':0.01}}, 'links': [('OR1','Top'),('Reag','OR1'),('Samp','OR1')]},
            "Instrument Qualification (HPLC)": {'title': 'System Fails Suitability', 'nodes': {'Top':{'label':'System Fails Suitability','type':'top','shape':'square','pos':(.5,.9)}, 'AND1':{'label':'AND','type':'gate','shape':'circle','pos':(.5,.7)}, 'Peak':{'label':'Poor Peak Shape','type':'basic','shape':'square','pos':(.25,.5),'prob':0.1}, 'Press':{'label':'Unstable Pressure','type':'basic','shape':'square','pos':(.75,.5),'prob':0.05}}, 'links': [('AND1','Top'),('Peak','AND1'),('Press','AND1')]},
            "Software System (LIMS)": {'title': 'Data Integrity Loss', 'nodes': {'Top':{'label':'Data Integrity Loss','type':'top','shape':'square','pos':(.5,.9)}, 'OR1':{'label':'OR','type':'gate','shape':'circle','pos':(.5,.7)}, 'Bug':{'label':'Software Bug','type':'basic','shape':'square','pos':(.25,.5),'prob':0.001}, 'Access':{'label':'Unauthorized Edit','type':'basic','shape':'square','pos':(.75,.5),'prob':0.0005}}, 'links': [('OR1','Top'),('Bug','OR1'),('Access','OR1')]},
        },
        "ETA": {
            "Pharma Process (MAb)": {'title': 'Power Outage during Filling', 'nodes': {'IE':{'label':'Power Outage','pos':(.05,.5)}, 'UPS':{'label':'UPS Backup','pos':(.3,.5),'prob_success':0.99}, 'Gen':{'label':'Generator Starts','pos':(.6,.7),'prob_success':0.95}}, 'paths': [{'x':[.05,.3,.6,.9],'y':[.5,.7,.8,.8],'color':'green'},{'x':[.6,.9],'y':[.7,.6,.6],'color':'orange', 'dash':'dot'},{'x':[.3,.9],'y':[.5,.3,.3],'color':'red'}], 'outcomes': {'Safe Recovery':{'pos':(1,.8),'prob':0.9405,'color':'green'}, 'Partial Loss':{'pos':(1,.6),'prob':0.0495,'color':'orange'}, 'Batch Loss':{'pos':(1,.3),'prob':0.01,'color':'red'}}},
            "IVD Assay (ELISA)": {'title': 'Operator Error (Wrong Reagent)', 'nodes': {'IE':{'label':'Wrong Reagent','pos':(.05,.5)}, 'BC':{'label':'Barcode Check','pos':(.3,.5),'prob_success':0.999}, 'QC':{'label':'QC Check Fails','pos':(.6,.7),'prob_success':0.98}}, 'paths': [{'x':[.05,.3,.9],'y':[.5,.3,.3],'color':'red'},{'x':[.3,.6,.9],'y':[.5,.7,.8,.8],'color':'green', 'dash':'dot'},{'x':[.6,.9],'y':[.7,.6,.6],'color':'orange'}], 'outcomes': {'Run Aborted (Pre)':{'pos':(1,.8),'prob':0.97902,'color':'green'}, 'Run Aborted (Post)':{'pos':(1,.6),'prob':0.02,'color':'orange'}, 'Erroneous Result':{'pos':(1,.3),'prob':0.00098,'color':'red'}}},
            "Instrument Qualification (HPLC)": {'title': 'Column Pressure Spike', 'nodes': {'IE':{'label':'Pressure Spike','pos':(.05,.5)}, 'Limit':{'label':'Pressure Limit','pos':(.3,.5),'prob_success':0.9}, 'Shut':{'label':'Auto-Shutdown','pos':(.6,.7),'prob_success':0.99}}, 'paths': [{'x':[.05,.3,.6,.9],'y':[.5,.7,.8,.8],'color':'green'},{'x':[.6,.9],'y':[.7,.6,.6],'color':'orange', 'dash':'dot'},{'x':[.3,.9],'y':[.5,.3,.3],'color':'red'}], 'outcomes': {'Safe Shutdown':{'pos':(1,.8),'prob':0.891,'color':'green'}, 'Minor Damage':{'pos':(1,.6),'prob':0.009,'color':'orange'}, 'System Damage':{'pos':(1,.3),'prob':0.1,'color':'red'}}},
            "Software System (LIMS)": {'title': 'Network Loss to DB', 'nodes': {'IE':{'label':'Network Loss','pos':(.05,.5)}, 'Retry':{'label':'Retry Logic','pos':(.3,.5),'prob_success':0.8}, 'Cache':{'label':'Local Cache','pos':(.6,.7),'prob_success':0.95}}, 'paths': [{'x':[.05,.3,.6,.9],'y':[.5,.7,.8,.8],'color':'green'},{'x':[.6,.9],'y':[.7,.6,.6],'color':'orange', 'dash':'dot'},{'x':[.3,.9],'y':[.5,.3,.3],'color':'red'}], 'outcomes': {'Seamless Recovery':{'pos':(1,.8),'prob':0.76,'color':'green'}, 'Delayed Write':{'pos':(1,.6),'prob':0.04,'color':'orange'}, 'Data Lost':{'pos':(1,.3),'prob':0.2,'color':'red'}}},
        }
    }

    # --- Tool-Specific UI Rendering ---
    if tool_choice == "Preliminary Hazard Analysis (PHA)":
        pha_data = qrm_templates["PHA"][project_type]
        fig = plot_pha_matrix(pha_data, project_type)
        st.plotly_chart(fig, use_container_width=True)
        st.dataframe(pha_data, use_container_width=True)

        st.subheader("FHA Workbench Gadgets")
        # Gadgets remain the same...
        c1, c2 = st.columns(2)
        with c1:
            c1.text_input("System/Process Selector", value=project_type, help="Select the overall system, subsystem, or process being analyzed (e.g., 'Blood Analyzer XC-500', 'User Authentication Module', 'Aseptic Filling Process').")
            c1.text_input("Function Input", value="Provide accurate results", help="Define a primary operation the system is intended to perform in verb-noun format (e.g., 'Dispense Reagent', 'Encrypt User Data', 'Mix Ingredients').")
            c1.selectbox("Failure Mode", ["Loss of function", "Incorrect function", "Unintended function", "Delayed function"], help="Select the category describing how the function could fail. This provides a structured way to brainstorm potential failures.")
        with c2:
            c2.text_area("Hazardous Situation Description", value="Instrument displays wrong result to user.", height=100, help="Describe the specific scenario that could expose a person or the environment to harm (e.g., 'Operator is exposed to uncontained bio-specimen', 'Software displays wrong patient data').")
            c2.text_area("Potential Harm", value="Misdiagnosis leading to improper treatment.", height=100, help="Describe the ultimate damage or injury that could result from the hazardous situation (e.g., 'Infection with bloodborne pathogen', 'Misdiagnosis leading to improper treatment', 'Data loss').")
        c1, c2, c3 = st.columns([2,1,1])
        c1.slider("Severity Scale", 1, 5, 4, format="%d-Catastrophic", help="Rate the worst-case potential harm that could credibly occur. This rating is independent of probability.")
        c2.button("Create FTA from Hazard", help="Escalate this high-level hazard into a more detailed investigation. 'Create FTA' starts a top-down analysis of the causes.")
        c3.button("Create FMEA from Hazard", help="Escalate this high-level hazard into a more detailed investigation. 'Create FMEA' starts a bottom-up analysis of the components involved.")
        
    elif tool_choice == "Failure Mode and Effects Analysis (FMEA)":
        # --- FIX: Added full interactivity for FMEA ---
        fmea_data = qrm_templates["FMEA"][project_type].copy() # Use copy to allow modification

        # Create interactive sliders in the sidebar
        with st.sidebar:
            st.subheader("FMEA Interactive Scoring")
            st.markdown("Adjust the initial risk scores to see the dashboard update in real-time.")
            for i, mode in enumerate(fmea_data['Failure Mode']):
                st.markdown(f"--- \n **Mode {i+1}:** *{mode}*")
                # Update the DataFrame directly with the slider values
                fmea_data.loc[i, 'O_Initial'] = st.slider(f"Occurrence (O)", 1, 10, int(fmea_data.loc[i, 'O_Initial']), key=f"o_{i}")
                fmea_data.loc[i, 'D_Initial'] = st.slider(f"Detection (D)", 1, 10, int(fmea_data.loc[i, 'D_Initial']), key=f"d_{i}")

        # Simulate mitigation actions based on the *interactive* initial RPN
        fmea_data['O_Final'] = fmea_data['O_Initial']
        fmea_data['D_Final'] = fmea_data['D_Initial']
        initial_rpn = fmea_data['S'] * fmea_data['O_Initial'] * fmea_data['D_Initial']
        
        # Apply mitigation if RPN is over the action threshold
        action_threshold = 100
        for i, rpn in enumerate(initial_rpn):
            if rpn >= action_threshold:
                # Simulate a risk reduction action
                fmea_data.loc[i, 'O_Final'] = max(1, fmea_data.loc[i, 'O_Initial'] - 3)
                fmea_data.loc[i, 'D_Final'] = max(1, fmea_data.loc[i, 'D_Initial'] - 2)

        # Plot the dashboard with the now-modified DataFrame
        fig_matrix, fig_pareto = plot_fmea_dashboard(fmea_data, project_type)
        
        st.plotly_chart(fig_matrix, use_container_width=True)
        st.plotly_chart(fig_pareto, use_container_width=True)

        st.subheader("FMEA Worksheet Gadgets (Example for one Failure Mode)")
        # Gadgets are updated to a 1-10 scale
        c1, c2 = st.columns(2)
        c1.text_input("Item / Function", value="Chromatography Column", help="The specific component, process step, or software function being analyzed.")
        c2.text_input("Potential Failure Mode", value="Column Overloading", help="How could this item fail to perform its intended function? (e.g., 'Cracked', 'Leaking', 'Returns null value', 'Incorrectly calculated', 'Degraded').")
        st.text_area("Potential Effect(s) of Failure", value="Poor separation of product and impurities, leading to out-of-specification (OOS) drug substance.", help="What is the consequence if this failure occurs? Describe the impact on the system, user, or patient.")
        
        c1, c2, c3 = st.columns(3)
        s_val = c1.slider("Severity (S)", 1, 10, 8, help="How severe is the *effect* of the failure? (1 = No effect, 10 = Catastrophic failure with potential for death or serious injury).")
        o_val = c2.slider("Occurrence (O)", 1, 10, 5, help="How likely is the *cause* to occur? (1 = Extremely unlikely, 10 = Inevitable or very high frequency).")
        d_val = c3.slider("Detection (D)", 1, 10, 2, help="How well can the *current controls* detect the failure mode or its cause before it reaches the customer? (1 = Certain to be detected, 10 = Cannot be detected).")
        
        c1, c2 = st.columns(2)
        c1.text_area("Potential Cause(s)", value="Incorrect protein concentration calculation prior to loading.", help="What is the root cause or mechanism that triggers the failure mode? (e.g., 'Material fatigue', 'Incorrect algorithm', 'Operator error', 'Power surge').")
        c2.text_area("Current Controls", value="SOP for protein quantification; manual calculation verification.", help="What existing design features, tests, or procedures are in place to prevent the cause or detect the failure mode?")
        
        rpn = s_val * o_val * d_val
        st.metric("Risk Priority Number (RPN)", f"{rpn} (S x O x D)", help="A calculated value (Severity √ó Occurrence √ó Detection) used to rank and prioritize risks. High RPNs should be addressed first.")
        st.text_area("Recommended Actions", value="Implement automated loading calculation in MES. Add post-column impurity sensor.", help="What specific actions will be taken to reduce the Severity, Occurrence, or improve the Detection of this risk? Assign a responsible person and a due date.")
        # --- END OF FMEA FIX ---

    elif tool_choice == "Fault Tree Analysis (FTA)":
        fta_data = qrm_templates["FTA"][project_type]
        # Logic to calculate probability remains the same
        if 'AND1' in fta_data['nodes'] and 'AND' in fta_data['nodes']['AND1']['label']:
            prob_top = fta_data['nodes']['Peak']['prob'] * fta_data['nodes']['Press']['prob']
        else:
            p1 = fta_data['nodes'][list(fta_data['nodes'].keys())[2]]['prob']
            p2 = fta_data['nodes'][list(fta_data['nodes'].keys())[3]]['prob']
            prob_top = 1 - (1 - p1) * (1 - p2)
        fta_data['nodes']['Top']['prob'] = prob_top
        
        fig = plot_fta_diagram(fta_data, project_type)
        st.plotly_chart(fig, use_container_width=True)
        
        st.subheader("FTA Constructor Gadgets")
        # Gadgets remain the same...
        c1, c2, c3 = st.columns(3)
        c1.text_input("Top Event Definition", value=fta_data['title'], help="Define the specific, undesirable system-level failure you want to analyze. This is the starting point at the top of the fault tree (e.g., 'Inaccurate Result Displayed', 'System Overheats').")
        c2.button("Add Logic Gate (OR, AND...)", help="Insert a logic gate to define the relationship between events. **OR:** any input event causes the output. **AND:** all input events must occur to cause the output.")
        c3.button("Add Basic Event", help="Add a root cause event that cannot be broken down further. This is a terminal point in the tree (e.g., 'Resistor Fails', 'Tire Punctures', 'Power Outage'). Represented by a circle.")
        c1, c2, c3 = st.columns(3)
        c2.button("Add Intermediate Event", help="Add a sub-system failure that is caused by other events below it. This is a non-terminal event in the tree (e.g., 'Power Supply Fails', 'Cooling System Fails'). Represented by a rectangle.")
        c3.button("Calculate Minimal Cut Sets", help="Click to identify the smallest combinations of basic events that will cause the Top Event to occur. This pinpoints the system's most critical vulnerabilities.")
        c1.text_input("Assign Event Probability", value="0.005", help="Assign a probability or failure rate to a Basic Event. This enables quantitative analysis of the Top Event's overall probability.")

    elif tool_choice == "Event Tree Analysis (ETA)":
        eta_data = qrm_templates["ETA"][project_type]
        fig = plot_eta_diagram(eta_data, project_type)
        st.plotly_chart(fig, use_container_width=True)
        
        st.subheader("ETA Modeler Gadgets")
        # Gadgets remain the same...
        c1, c2 = st.columns(2)
        c1.text_input("Initiating Event", value=eta_data['title'], help="Define the single failure or error that starts the sequence. This is the root of your event tree (e.g., 'Power Surge', 'Contaminated Reagent', 'User enters invalid command').")
        c2.button("Add Safety Function / Barrier", help="Introduce a system, action, or feature designed to mitigate the event. Each barrier creates a new branching point in the tree (Success path / Failure path).")
        
        c1, c2, c3 = st.columns(3)
        c1.text_input("Success/Failure Probability", value="0.99 / 0.01", help="For each safety barrier, enter the probability of it succeeding or failing. The sum of success and failure for a single barrier must equal 1.0.")
        c2.text_input("Outcome Node", value="System Safe Shutdown", help="Define the final state at the end of a branch (e.g., 'System Safe Shutdown', 'Minor Data Corruption', 'Catastrophic Failure', 'False Negative Result').")
        c3.text_input("Path Probability Calculator", value="0.9405", disabled=True, help="Automatically calculates the total probability of reaching a specific outcome by multiplying the probabilities of all events along that path.")

    # --- Educational Tabs (remain unchanged) ---
    st.divider()
    st.subheader("A Deeper Dive into QRM Strategy")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    with tabs[0]:
        st.markdown("""
        **A validation leader must choose the right risk tool for the right question.** While all these tools analyze risk, they do so from fundamentally different perspectives. Using the wrong tool for your situation can be inefficient at best and dangerously misleading at worst.
    
        ---
        ##### A Medical Analogy for Risk Tools
        Think of your process or system as a patient, and you are the lead physician.
    
        - **PHA is the Annual Physical:** Broad, high-level, and performed early to identify the "big rock" hazards.
        - **FMEA is the Full Body System Review:** Detailed, systematic, and **bottom-up**. It asks, "What are all the ways each part could fail?"
        - **FTA is the Specialist's Diagnosis:** Forensic, focused, and **top-down (deductive)**. It asks, "For this critical symptom to occur, what must have gone wrong?"
        - **ETA is the Emergency Room Triage:** Forward-looking, sequential, and **bottom-up (inductive)**. It asks, "Now that this trauma has occurred, what are the possible outcomes?"
        
        ---
        ##### Understanding the Analytical Directions
        - **Bottom-Up (Inductive):** Start with individual **causes** and reason forward to their potential **effects**. (FMEA & ETA).
        - **Top-Down (Deductive):** Start with a known **effect** (the failure) and reason backward to deduce the potential **causes**. (FTA).
        """)
    with tabs[1]:
        st.markdown("""
        ### The Business Case: From Reactive Firefighting to Proactive Risk Elimination
    
        #### The Problem: The "Hope is a Strategy" Approach
        A team prepares for process validation. They have a process that has worked a few times in the lab, but they possess no systematic, documented understanding of its potential failure modes. The validation plan is a generic, one-size-fits-all checklist copied from a previous project, not a targeted, risk-based strategy. Key decisions are based on the "tribal knowledge" of a few senior SMEs, which is undocumented and not scalable. They are, in effect, hoping for the best.
    
        #### The Impact: Catastrophic, Preventable Failures and "Validation Theater"
        This is not just poor practice; it's a massive financial and compliance liability that manifests in two ways:
        1.  **Sudden, High-Impact Failures:** A single, unanticipated failure in a Process Performance Qualification (PPQ) batch can cost hundreds of thousands of dollars in lost materials and can delay a product launch by months, wiping out projected revenue. Worse, a failure mode that was never considered occurs in the field, leading to a product recall, irreversible reputational damage, and potential patient harm.
        2.  **"Validation Theater":** Without a risk-based focus, the validation plan becomes bloated and inefficient. The team wastes immense time, money, and resources rigorously testing process parameters that have no real impact on product quality, while potentially completely ignoring the few parameters that truly matter. They are performing "validation theater"‚Äîan expensive and time-consuming play that creates the *illusion* of control without actually reducing risk.
    
        #### The Solution: A Systematic, Cross-Functional Hunt for Failure
        The Quality Risk Management (QRM) Suite (FMEA, FTA, etc.) is not a paperwork exercise; it is a facilitated, cross-functional workshop that forces a company's best minds (from R&D, Engineering, Manufacturing, and Quality) to collaborate. The process is rigorous:
        - **Deconstruct:** Break down the process into its fundamental steps and components.
        - **Brainstorm:** Systematically brainstorm all credible ways each component could fail.
        - **Quantify:** Assess the Severity, Occurrence, and Detectability of each failure mode to quantify the risk.
        - **Prioritize & Mitigate:** Rank the risks and develop concrete action plans to eliminate or control the highest-priority items.
    
        #### The Consequences: A Resilient Process and an Intelligent Validation
        - **Without This:** Validation is a high-stakes gamble. The company is in a perpetual state of reactive "firefighting," lurching from one expensive crisis to the next. The validation program is inefficient and ineffective.
        - **With This:** The process is systematically de-risked and hardened against failure. The output of the FMEA becomes the **direct, auditable justification** for the validation strategy. The Validation Plan becomes lean, intelligent, and cost-effective, focusing resources exclusively on the parameters and attributes that were proven to carry the most risk. This leads to a higher probability of first-pass validation success and a more robust, reliable, and profitable process in the long run.
        """)

    with tabs[2]:
        st.markdown("""
        ##### Glossary of QRM Terms
        - **Risk:** The combination of the probability of occurrence of harm and the severity of that harm.
        - **Hazard:** A potential source of harm.
        - **PHA (Preliminary Hazard Analysis):** A tool to identify and prioritize hazards early in the design process.
        - **FMEA (Failure Mode and Effects Analysis):** A systematic, bottom-up method for identifying potential failure modes, their causes, and their effects.
        - **FTA (Fault Tree Analysis):** A top-down, deductive failure analysis where an undesired state is analyzed using boolean logic to combine a series of lower-level events.
        - **ETA (Event Tree Analysis):** A bottom-up, inductive logical model that explores the responses of a system to a particular initiating event.
        - **RPN (Risk Priority Number):** The product of Severity, Occurrence, and Detection scores from an FMEA, used to prioritize risks.
        """)
    with tabs[3]:
        st.success("""üü¢ **THE GOLDEN RULE:** Risk Assessment is Not a One-Time Event. Quality Risk Management is a continuous lifecycle activity. An initial PHA should inform a more detailed FMEA. The FMEA identifies critical failure modes that might warrant a deep-dive FTA. The effectiveness of the controls identified in the FMEA and FTA should be monitored throughout the product lifecycle, and the risk assessments should be updated whenever new information (e.g., from a deviation or a process change) becomes available.""")
    with tabs[4]:
        st.markdown("""
        #### Historical Context: Forged in Fire, Ice, and the Void
        These advanced risk management tools were not born in sterile laboratories but were forged in the crucibles of the 20th century's most demanding engineering challenges, where failure carried the ultimate price. They represent a migration of hard-won knowledge from high-hazard industries directly into the heart of pharmaceutical quality systems.

        ---
        
        ##### FMEA: From Munitions to the Moon (and Back to Earth)
        - **The Origin (1949):** The FMEA methodology was first formalized in a U.S. military procedure, **MIL-P-1629**. In the tense aftermath of WWII, the goal was brutally practical: to proactively analyze potential failures in complex munitions and weapon systems *before* they were deployed, preventing catastrophic failures on the battlefield.
        - **The Ultimate High-Stakes Project (1960s):** FMEA became the bedrock of **NASA's reliability engineering for the Apollo program**. For a mission where millions of components had to function perfectly, FMEA was the exhaustive, bottom-up tool used to systematically assess every single bolt, switch, and circuit, asking "How can this fail, and what happens if it does?" The consequences of missing a failure mode were famously highlighted decades later by physicist Richard Feynman in his investigation of the Space Shuttle *Challenger* disaster, where he noted that a proper FMEA of the O-rings could have identified the critical temperature-related failure mode that led to the tragedy.
        - **The Automotive Awakening (1970s):** The Ford Motor Company, reeling from the infamous Pinto fuel tank crisis and facing intense competition, championed the use of FMEA to proactively design safety and reliability into their vehicles, shifting the industry from a reactive, "test and fix" mentality to a proactive, design-centric one.

        ---

        ##### FTA: The Logic of Armageddon
        - **The Cold War Problem (1961):** At Bell Laboratories, engineers working with the U.S. Air Force on the **Minuteman I Intercontinental Ballistic Missile (ICBM)** system faced a terrifying new question: "What is the probability of an *accidental* missile launch?" A simple FMEA was insufficient. FMEA is excellent for single-point failures, but the accidental launch required a complex *combination* of events.
        - **The Insight (H.A. Watson):** Engineers led by H.A. Watson developed Fault Tree Analysis as a top-down, deductive logic tool. They started with the catastrophic top event ("Accidental Launch") and used Boolean logic gates (AND, OR) to map all the lower-level equipment failures and human errors that could lead to it. This allowed them to identify the most critical vulnerabilities‚Äîthe "minimal cut sets"‚Äîin a system where the stakes were literally global annihilation.

        ---

        ##### ETA: The Domino Effect of a Meltdown
        - **The Nuclear Age (1970s):** While FTA looked backward from a failure, engineers in the burgeoning nuclear power and chemical industries needed to look *forward*. They needed to answer a different kind of question: "A primary cooling pipe has just ruptured. **What happens next?**"
        - **The Consequence Map:** Event Tree Analysis was developed to model this sequential, cascading failure. It starts with an "initiating event" and then maps out the success or failure of each subsequent safety system (e.g., "Did the backup pump start? Did the containment doors seal?"). This creates a branching map of all possible outcomes and their probabilities. ETA was a core component of the landmark **WASH-1400 Reactor Safety Study** (1975), which, in a sense, predicted the types of failure sequences that would occur at the **Three Mile Island accident** just four years later, solidifying its importance in plant safety.

        ---
        
        ##### The Great Migration: From Engineering to Pharma
        For decades, the pharmaceutical industry relied primarily on intensive "quality by testing," often reacting to failures after they occurred. However, a series of high-profile manufacturing issues in the late 1990s and early 2000s, which led to costly recalls and drug shortages, prompted a major philosophical shift. Led by visionaries like Dr. Janet Woodcock, the FDA launched its **"Pharmaceutical cGMPs for the 21st Century"** initiative, calling for a modern, proactive, science- and risk-based approach to quality.

        This culminated in the **ICH Q9 guideline in 2005**, which formally "imported" this entire suite of battle-tested engineering tools into the pharmaceutical quality system. It was a landmark moment, signaling that the proactive, systems-thinking mindset forged in the aerospace, defense, and nuclear industries was now the regulatory expectation for ensuring patient safety, forever changing the landscape of validation and process control.
        """)
    with tabs[5]:
        st.markdown("""
        This suite of tools is the direct implementation of the principles outlined in **ICH Q9 - Quality Risk Management**.
        - **ICH Q9:** This guideline provides a framework for risk management and lists PHA, FMEA, FTA, and ETA as examples of appropriate tools.
        - **FDA Process Validation Guidance:** The guidance requires a risk-based approach. These tools provide the documented evidence for how risks were identified and controlled.
        - **ISO 14971 (Application of risk management to medical devices):** This is the international standard for risk management for medical devices, and it requires the use of a systematic process like FMEA or PHA.
        """)

# ==============================================================================
# UI RENDERING FUNCTION (V&V Strategy & Justification)
# ==============================================================================
def render_vv_strategy_justification():
    """Renders the comprehensive module explaining the strategy and purpose of V&V."""
    st.markdown("""
    #### Purpose & Application: The "Why" Behind the Work
    **Purpose:** To provide a concise, strategic, and technical rationale for why Verification & Validation is a non-negotiable cornerstone of regulated product and process development. This module answers the fundamental questions: "Why do we do this?" and "What value does it provide?"
    
    **Strategic Application:** This is the executive summary and the business case for the entire validation effort. Use this framework to justify projects to stakeholders, align cross-functional teams on a common purpose, and train new team members on the foundational principles of a robust quality system.
    """)
    
    st.info("""
    This module is a reference guide. Explore the tabs and the blueprint below to build a deep understanding of the V&V framework's core logic and its critical role in ensuring quality, safety, and compliance.
    """)
    
    st.header("The Core Logic of V&V")
    st.markdown("While the specifics depend on the context (product, manufacturing, software), the core logic is always the same: **Prove with objective evidence that what you built works as intended under real-world conditions.**")

    col1, col2 = st.columns(2)
    with col1:
        st.subheader("1Ô∏è‚É£ Verification: Did we build it right?")
        st.markdown("This is an internal-facing question. It confirms that the design, code, or process meets all the pre-defined, documented requirements and specifications that your team created.")

    with col2:
        st.subheader("2Ô∏è‚É£ Validation: Did we build the right thing?")
        st.markdown("This is an external-facing question. It confirms that the final product actually meets the user's needs and intended use in their real-world environment. A product can be perfectly verified but completely fail validation if it solves the wrong problem.")
    
    st.divider()

    st.subheader("The Three Scopes of V&V in Regulated Industries")
    st.markdown("V&V applies to more than just the final product. A complete strategy addresses three distinct layers:")
    
    scope_col1, scope_col2, scope_col3 = st.columns(3)
    with scope_col1:
        st.markdown("#### 1. V&V of the Product")
        st.markdown("**üìå Main focus on product design and requirements.**")
        st.markdown("""
        - **Verification:** Confirms the product meets its design specifications (drawings, tolerances, software requirements, etc.).
        - **Validation:** Confirms the final product meets its **intended use** under real-world conditions (usability, performance, safety).
        - **Example:** For a diagnostic device, verification ensures the sensor measures correctly; validation ensures the device delivers accurate diagnoses in an actual clinical setting.
        """)
    with scope_col2:
        st.markdown("#### 2. V&V of Processes & Instruments")
        st.markdown("**üìå Critical for ensuring reliable manufacturing and testing.**")
        st.markdown("""
        - This includes manufacturing machines, test equipment, QC lab software, and even facilities.
        - **IQ/OQ/PQ** (Installation/Operational/Performance Qualification) are the formal V&V activities for this scope.
        - **Example:** A spectrophotometer used in quality control must be validated (qualified) to prove its measurements are accurate and repeatable. Without this, product test results would be unreliable.
        """)
    with scope_col3:
        st.markdown("#### 3. V&V of Development Tools")
        st.markdown("**üìå Essential for software and complex design.**")
        st.markdown("""
        - Ensures the tools used to *design or develop* the product (e.g., CAD software, genomic analysis platforms, simulators) work as intended.
        - Required under standards like IEC 62304 (medical software) or FDA 21 CFR 820.70(i) (validation of software used in production).
        - **Example:** Validating a LIMS system that manages lab data to ensure it does not corrupt or incorrectly alter information.
        """)
    
    st.info("""
    **üí° Clear Takeaway:**
    - **Product V&V** ensures it meets requirements and user needs.
    - **Process & Instrument V&V** ensures they produce reliable, consistent results.
    - **Development Tool V&V** ensures the platforms used to design or test do not introduce errors.
    """)

    st.header("Justifications for V&V: The Pillars of a Quality System")
    st.markdown("V&V is not just a testing phase; it's a multi-faceted business strategy. Each justification represents a different lens through which to view its value.")
    
    tabs = st.tabs(["‚úÖ The Business Case", "üèõÔ∏è Regulatory", "üõ°Ô∏è Risk Management", "üí∞ Cost Avoidance", "üìà Stability & Capability", "ü§ù Customer Confidence"])
    
    with tabs[0]:
        st.markdown("""
        ### The Business Case: Investing in Certainty
    
        #### The Problem: The "It Works in the Lab" Fallacy
        A brilliant new product or process is developed in an R&D environment. It works perfectly under ideal conditions, run by the experts who designed it. Management, seeing these successful "hero runs," pushes to accelerate the launch to capture market share, viewing the formal, large-scale V&V process as a costly, time-consuming delay.
    
        #### The Impact: The High Cost of Unmanaged Risk
        This thinking exposes the company to enormous and entirely preventable financial and reputational risk.
        - **The 1-10-100 Rule:** A well-established quality principle states that if a defect costs **$1** to fix in development, it will cost **$10** to fix in production and **$100** (or more) to fix once it's in the hands of the customer. Skipping or rushing V&V is a bet against this exponential cost curve.
        - **Manufacturing Meltdown:** The process that worked in the lab fails to perform at scale. It is not robust to normal variations in raw materials, operators, or environmental conditions. This leads to low yields, high scrap rates, and an inability to meet demand.
        - **Catastrophic Field Failure:** The product fails in the hands of the user, leading to a product recall, loss of market trust, potential patient harm, and severe regulatory action. The short-term gain from an early launch is obliterated by the long-term cost of failure.
    
        #### The Solution: V&V as a Formal Risk Reduction Program
        Verification & Validation is the disciplined, systematic process of proving that a product or process is not just functional, but **robust, reliable, and scalable**. It is the formal business process for buying down risk.
        - **Verification** proves the system was built right, confirming it meets all internal specifications. This prevents costly rework during scale-up.
        - **Validation** proves it was the right system, confirming it meets the user's needs in their real-world environment. This prevents market failure.
        The documented V&V package is the objective evidence that this risk reduction was performed diligently and successfully.
    
        #### The Consequences: A Predictable and Profitable Launch
        - **Without This:** A product launch is a high-stakes gamble with unknown odds of success. The company is exposed to significant financial, regulatory, and reputational risk.
        - **With This:** V&V provides a high degree of certainty that the product will perform as expected at scale and in the field. It is a direct investment in a smooth, predictable, and profitable product launch. It's the mechanism that turns a clever invention into a reliable, commercial-grade asset.
        """)

    with tabs[1]:
        st.subheader("Regulatory Compliance")
        st.markdown("""
        In regulated industries like medical devices, aerospace, automotive, pharmaceuticals, and semiconductors, V&V isn't optional‚Äîit‚Äôs a legal requirement.
        - **Standards:** Compliance with standards such as ISO 13485, FDA 21 CFR Part 820, ISO 26262, and IEC 62304 is mandatory.
        - **The Bottom Line:** Without a complete, documented V&V package, you cannot legally ship, launch, or sell your product. It is the price of admission to the market.
        """)
    with tabs[2]:
        st.subheader("Risk Management")
        st.markdown("""
        V&V is the primary tool for proactive risk management. It is how you hunt for problems before they find you.
        - **Early Detection:** It systematically identifies failure modes early in the lifecycle (e.g., design flaws, process weaknesses, software bugs).
        - **Risk Reduction:** It reduces the probability of catastrophic, safety-related, or costly failures occurring after the product is released.
        """)
    with tabs[3]:
        st.subheader("Cost Avoidance")
        st.markdown("""
        The "Cost of Quality" is a well-established principle: the later in the lifecycle a problem is found, the more it costs to fix. V&V is a strategy to minimize this cost.
        - **Exponential Cost:** Fixing an issue in production can be 10-100x more expensive than catching it in the design or pilot stages.
        - **Tangible Savings:** A robust V&V program directly saves money by reducing rework, recalls, scrap material, and warranty claims.
        """)
    with tabs[4]:
        st.subheader("Process Stability & Capability")
        st.markdown("""
        V&V ensures that a process is not just correct once, but that it is robust enough to perform reliably over time.
        - **Control Variation:** It uses statistical methods (like SPC) to ensure the process stays within specifications run after run.
        - **Build Robustness:** It proves the process can tolerate normal, expected fluctuations (e.g., in temperature, material lots, or operators) without producing defects.
        """)
    with tabs[5]:
        st.subheader("Customer & Stakeholder Confidence")
        st.markdown("""
        The documented output of V&V is the objective proof that your product or system is fit for purpose.
        - **Builds Trust:** It provides tangible evidence to regulators, investors, and customers that your solution is safe, effective, and reliable.
        - **Protects Brand Reputation:** A strong V&V program is the best defense against the reputational damage caused by a product failure or recall.
        """)

    with st.expander("Expand to view the V&V Justification Blueprint", expanded=True):
        st.markdown("### The V&V Justification Blueprint")
        st.markdown("**Purpose:** To provide a concise, strategic and technical rationale for why Verification & Validation is non-negotiable in regulated product and process development.")
        
        st.markdown("#### Strategic Justifications")
        st.markdown("""
        | Driver                  | Why It Matters                                             | Outcome                                                    |
        | ----------------------- | ---------------------------------------------------------- | ---------------------------------------------------------- |
        | **Regulatory Compliance** | Required by ISO, FDA, IEC, and other regulations.          | Legal clearance to sell, launch, or ship.                  |
        | **Risk Management**       | Identifies and mitigates failure modes early.              | Reduced probability of safety, quality, or reliability issues. |
        | **Cost Avoidance**        | Issues found late cost 10‚Äì100√ó more to fix.                 | Reduced scrap, rework, warranty claims, and recalls.       |
        | **Market & Customer Trust** | Demonstrates safety, effectiveness, and reliability.       | Higher adoption, customer confidence, brand reputation.      |
        | **Continuous Improvement**| Feeds real-world feedback into design/process refinements. | Sustained product performance and market competitiveness.  |
        """)

        st.markdown("#### Technical Objectives")
        st.markdown("""
        - **Control Variation:**
          - Keep outputs within specifications across production runs.
          - Apply SPC, capability analysis, and tolerance stacking.
        - **Build Robustness:**
          - Design to handle normal fluctuations in materials, operators, and environment without falling out of spec.
          - Use DOE, stress testing, and worst-case analysis.
        - **Prove Reliability Under Real-World Conditions:**
          - Include aging, environmental, and stress testing.
          - Demonstrate long-term stability and performance.
        - **Document the Evidence:**
          - Ensure all test plans, results, deviations, and justifications are recorded to create an audit-ready trail.
        """)

        st.markdown("#### The ROI Equation")
        st.latex(r"\text{V\&V Investment} \ll \text{Cost of (Failure + Non-Compliance + Market Loss)}")
        st.markdown("""
        V&V reduces:
        - **Time-to-market** (by avoiding redesign cycles late in development)
        - **Financial risk** (through early issue detection)
        - **Regulatory delays** (by providing clean, audit-ready documentation)
        """)

    st.success("""
    **Executive Summary Statement:**
    
    "V&V isn‚Äôt just testing ‚Äî it‚Äôs the structured proof that our product is safe, effective, compliant, and robust enough to thrive in the real world, while controlling variation and protecting the company from avoidable risk and cost.‚Äù
    """)
# ==============================================================================
# UI RENDERING FUNCTION (Design Controls & DHF)
# ==============================================================================
def render_design_controls_dhf():
    """Renders the comprehensive, interactive module for Design Controls & DHF."""
    st.markdown("""
    #### Purpose & Application: The Project's Audit Trail
    **Purpose:** To simulate the real-world, iterative nature of a regulated project. This module demonstrates the formal **Design Controls** process and its critical partner, **Change Control**. The **Design History File (DHF)** is the living compilation of records that provides the objective, auditable evidence that this entire process was followed.
    
    **Strategic Application:** This is the master checklist for any regulated product development. For an auditor, the DHF is the single source of truth that proves a design was developed in a state of control. This tool demonstrates how the DHF is not a document you write at the end, but a binder you build concurrently throughout the project.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Project Manager.
    1. **Set the Stage:** Use the **Project Simulation Workbench** to define your project's risk profile.
    2. **Execute:** Click the **"Run..."** buttons to simulate V&V activities. Based on your risk profile, you may encounter a finding.
    3. **Manage Change:** If a finding occurs, a **Change Control Request** form will appear. Fill it out and submit to see the real-world impact on your project's cost, timeline, and the DHF.
    """)

    # Initialize session state variables for the simulation
    if 'dc_timeline_impact' not in st.session_state:
        st.session_state.dc_timeline_impact = 0
        st.session_state.dc_cost_impact = 0
        st.session_state.dc_change_controls = 0
        st.session_state.dc_completed = {'UserNeeds'}
        st.session_state.dc_dhf_docs = {'UserNeeds': ['URS-001: User Requirement Specification']}
        st.session_state.dc_finding = None
    
    # --- Project Simulation Workbench ---
    with st.expander("üî¨ Project Simulation Workbench", expanded=True):
        risk_score = 0
        col1, col2, col3 = st.columns(3)
        with col1:
            project_context = st.selectbox("Project Context", 
                ['Assay', 'Instrument', 'Software', 'Pharma Process'],
                help="Select the type of project. This will tailor the potential findings to be more realistic for the domain.")
        with col2:
            team_experience = st.select_slider("Team Experience", 
                options=['Novice', 'Experienced', 'Expert'], value='Experienced',
                help="An experienced team is less likely to make fundamental errors in design.")
            risk_score += {'Novice': 5, 'Experienced': 2, 'Expert': 0}[team_experience]
        with col3:
            tech_novelty = st.select_slider("Technical Novelty",
                options=['Proven Tech', 'New Application', 'Novel Tech'], value='New Application',
                help="Is the underlying technology well-understood or cutting-edge? 'Novel Tech' significantly increases verification risk.")
            risk_score += {'Proven Tech': 1, 'New Application': 3, 'Novel Tech': 5}[tech_novelty]
    
    # --- Project Risk Dashboard ---
    st.header("Project Risk & DHF Dashboard")
    kpi_col1, kpi_col2, kpi_col3 = st.columns(3)
    rework_risk = min(95, 5 + risk_score * 7)
    audit_readiness = max(5, 100 - risk_score * 7.5)
    kpi_col1.metric("Timeline Impact", f"+ {st.session_state.dc_timeline_impact} Days", help="Days added to the project due to rework from change controls.")
    kpi_col2.metric("Cost Impact", f"+ ${st.session_state.dc_cost_impact:,.0f}", help="Estimated cost added to the project from rework and re-testing.")
    kpi_col3.metric("Change Controls Issued", st.session_state.dc_change_controls, help="Total number of formal change controls executed.")

    # --- Main Simulation Area ---
    main_col1, dhf_col2 = st.columns([0.6, 0.4])
    with main_col1:
        prob_of_finding = min(0.9, risk_score * 0.06)
        exec_col1, exec_col2, exec_col3 = st.columns(3)
        
        if exec_col1.button("Develop Design (Inputs ‚Üí Outputs)"):
            st.session_state.dc_completed.update(['Inputs', 'Process', 'Outputs', 'Device', 'Review'])
            st.session_state.dc_dhf_docs['Inputs'] = ['FS-001: Functional Spec', 'Risk_Assessment-001']
            st.session_state.dc_dhf_docs['Outputs'] = ['Drawings_RevA', 'BOM_RevA']
            st.session_state.dc_dhf_docs['Review'] = ['Design_Review_Minutes-01']
            st.session_state.dc_finding = None
            st.rerun()

        if exec_col2.button("Run Verification Tests"):
            if 'Outputs' in st.session_state.dc_completed:
                if np.random.rand() < prob_of_finding:
                    findings = {'Assay': "...", 'Instrument': "...", 'Software': "...", 'Pharma Process': "..."} # Truncated for brevity
                    st.session_state.dc_finding = findings.get(project_context, "Generic Verification Failure.")
                else:
                    st.session_state.dc_completed.add('Verification')
                    st.session_state.dc_dhf_docs['Verification'] = ['VER-001_Report_PASS']
                    st.session_state.dc_finding = "PASS"
                st.rerun()
            else: st.warning("Cannot run Verification until Design Outputs are generated.")
        
        if exec_col3.button("Run Validation Tests"):
            if 'Device' in st.session_state.dc_completed:
                if np.random.rand() < prob_of_finding * 0.75:
                    findings = {'Assay': "...", 'Instrument': "...", 'Software': "...", 'Pharma Process': "..."} # Truncated for brevity
                    st.session_state.dc_finding = findings.get(project_context, "Generic Validation Failure.")
                else:
                    st.session_state.dc_completed.add('Validation')
                    st.session_state.dc_dhf_docs['Validation'] = ['VAL-001_Report_PASS']
                    st.session_state.dc_finding = "PASS"
                st.rerun()
            else: st.warning("Cannot run Validation until the Product/System is built.")

        if st.session_state.dc_finding:
            if st.session_state.dc_finding == "PASS":
                st.success("‚úÖ V&V Testing Passed with no findings!")
            else:
                st.error(f"**Finding Detected!** {st.session_state.dc_finding}")
                with st.form("change_control_form"):
                    st.write("**Change Control Request (CCR)**")
                    cc_desc = st.text_area("Description of Change", "Modify design specification (DS-001) to increase tolerance on component X to improve precision.")
                    cc_impact = st.multiselect("Impact Assessment", ["Specifications", "Drawings", "Code", "Risk Analysis", "Validation Plan"], default=["Specifications", "Drawings"])
                    submitted = st.form_submit_button("Submit & Approve CCR")
                    if submitted:
                        st.session_state.dc_change_controls += 1
                        st.session_state.dc_timeline_impact += 30
                        st.session_state.dc_cost_impact += 50000
                        if 'Changes' not in st.session_state.dc_dhf_docs: st.session_state.dc_dhf_docs['Changes'] = []
                        st.session_state.dc_dhf_docs['Changes'].append(f'CCR-{st.session_state.dc_change_controls:03d}_Report')
                        if "Verification" in st.session_state.dc_finding:
                            st.session_state.dc_completed.discard('Outputs'); st.session_state.dc_completed.discard('Verification')
                        if "Validation" in st.session_state.dc_finding:
                            st.session_state.dc_completed.discard('Outputs'); st.session_state.dc_completed.discard('Verification'); st.session_state.dc_completed.discard('Validation')
                        st.session_state.dc_finding = None
                        st.rerun()
        
        triggered_change_info = None
        if st.session_state.dc_finding and st.session_state.dc_finding != "PASS":
            if "Verification" in st.session_state.dc_finding: triggered_change_info = ('Verification', 'Inputs', 'CHANGE<br>CONTROL')
            elif "Validation" in st.session_state.dc_finding: triggered_change_info = ('Validation', 'UserNeeds', 'CHANGE<br>CONTROL')
        
        fig, stages = plot_design_controls_flow(st.session_state.dc_completed, triggered_change_info)
        st.plotly_chart(fig, use_container_width=True)

    with dhf_col2:
        # --- NEW: Structured DHF Table of Contents ---
        st.subheader("Design History File (DHF)")
        st.markdown("A compilation of records which describes the design history of a finished device.")

        dhf_sections = {
            "1. Design & Development Planning": ['UserNeeds'],
            "2. Design Inputs": ['Inputs'],
            "3. Design Outputs & Reviews": ['Outputs', 'Review'],
            "4. Design Verification": ['Verification'],
            "5. Design Validation": ['Validation'],
            "6. Design Transfer": ['Transfer'],
            "7. Design Changes": ['Changes']
        }
        
        for section_title, stage_keys in dhf_sections.items():
            st.markdown(f"**{section_title}**")
            found_docs = False
            for key in stage_keys:
                if key in st.session_state.dc_dhf_docs:
                    for doc in st.session_state.dc_dhf_docs[key]:
                        icon = "üìÑ"
                        if "PASS" in doc: icon = "‚úÖ"
                        if "CCR" in doc: icon = "üî¥"
                        st.markdown(f"&nbsp;&nbsp;&nbsp;{icon} {doc}")
                        found_docs = True
            if not found_docs:
                st.caption("&nbsp;&nbsp;&nbsp;*No records yet*")

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights & Significance", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])

    with tabs[0]:
        st.markdown("""
        ### The DHF: More Than Just a Binder

        The Design History File is the single most important deliverable of a regulated development project. Its significance goes far beyond mere compliance; it is a critical business asset.

        1.  **It is the Project's Story.** The DHF is the definitive, chronological narrative of the project, from the initial spark of an idea (`User Needs`) to the final handover to manufacturing (`Design Transfer`). It tells the story of every decision, every test, every failure, and every change. A well-organized DHF proves that the final product was the result of a deliberate, controlled process, not a series of happy accidents.

        2.  **It is the Auditor's Map.** During a regulatory inspection (e.g., by the FDA), the DHF is the primary "map" the auditor will use to navigate your project. Their goal is to test your **traceability**. They will pick a high-level User Need, and you must be able to use the DHF to show them the specific Design Input that captured it, the Design Output that implemented it, the Verification test that proved it was built right, and the Validation test that proved it was the right thing to build. A complete DHF makes this process smooth and painless.

        3.  **It is Corporate Memory.** The original development team will eventually move on to other projects. Years later, when a field issue arises or a product update is needed, the DHF is the only reliable source of truth for a new team to understand *why* certain design decisions were made. Without it, the company is doomed to re-learn old lessons, a costly and inefficient process.
        """)

    with tabs[1]:
        st.markdown("""
        ### The Business Case: Creating an Auditable Asset, Not an Archeological Dig
        
        #### The Problem: The "We'll Document It Later" Mentality
        A development team, under pressure to innovate quickly, operates in an ad-hoc manner. Design decisions are made in informal meetings, key data lives on individual laptops, and rationale is communicated through ephemeral chat messages. Just before the planned launch, a manager asks, "Where is the DHF for the regulatory submission?" This question triggers a panicked, multi-month "DHF archeology" project to reverse-engineer documentation for work that was done months or years ago.
        
        #### The Impact: Audit Failure and Intellectual Property Loss
        This is a critical compliance and business failure with severe consequences:
        - **Product Launch Delay:** The launch is postponed for months while the team struggles to create a plausible, albeit flawed, historical record.
        - **Guaranteed Audit Findings:** An experienced auditor can easily identify a retrospectively created DHF. The lack of contemporaneous evidence and objective proof of a controlled process will result in major findings, potentially leading to a refusal to approve the product.
        - **Loss of Corporate Memory:** The *real* reasons for key design decisions‚Äîthe dead ends, the trade-offs, the critical insights‚Äîare lost forever. When a key engineer leaves the company, their knowledge walks out the door with them. This creates massive challenges for future product support, troubleshooting, and next-generation development.
        
        #### The Solution: Concurrent Documentation as a Core Discipline
        The Design Controls process and the DHF are not an afterthought; they are the **operating system** for the project. This is not about creating more bureaucracy; it's about capturing knowledge and evidence as a natural byproduct of good engineering work. The process is disciplined and phase-gated, where the output of each stage is a formal, reviewed record that is immediately filed in the DHF.
        
        #### The Consequences: Audit Readiness and a Valuable Corporate Asset
        - **Without This:** The project is a chaotic black box. The DHF is a hastily assembled liability that increases risk.
        - **With This:** The project is a transparent, predictable, and auditable process. The DHF becomes one of the company's most valuable **corporate assets**‚Äîa complete, trustworthy, and evergreen record of the design history. It ensures permanent audit readiness, facilitates smooth knowledge transfer to new team members, and protects the company's most valuable intellectual property: the "why" behind its products.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Design & Change Control Terms

        **Core Design Control Terms:**
        - **Design Controls:** A formal, systematic process for medical device development mandated by the FDA to ensure that devices are safe and effective for their intended use.
        - **DHF (Design History File):** A compilation of records which describes the design history of a finished device. It is the complete audit trail of the entire design process.
        - **DMR (Device Master Record):** The "recipe" for building the device. It contains all the final, approved specifications, drawings, and manufacturing procedures. The DMR is an *output* of the design process, compiled from the DHF.
        - **DHR (Device History Record):** The "batch record" for a specific device or lot. It provides evidence that a specific unit or batch was manufactured in accordance with the DMR.

        ---

        **Key V&V Terms in Design Controls:**
        - **Design Inputs:** The physical and performance requirements of a device that are used as a basis for device design. (The "What").
        - **Design Outputs:** The results of a design effort at each design phase. The total finished design consists of the device, its packaging and labeling, and the device master record. (The "How").
        - **Design Verification:** Confirmation by examination and provision of objective evidence that the design **outputs** meet the design **inputs**. (Did we build the product right?)
        - **Design Validation:** Confirmation by examination and provision of objective evidence that the finished device conforms to **user needs and intended uses**. (Did we build the right product?)
        - **Design Transfer:** The process by which the device design is correctly translated into production specifications.
        
        ---

        **Related Change Control Terms:**
        - **Change Control:** A formal process used to ensure that changes to a product or system are introduced in a controlled and coordinated manner. It is a cornerstone of any GxP quality management system.
        - **Impact Assessment:** A critical part of a Change Control Request where a cross-functional team evaluates the potential effects of the proposed change on product quality, safety, regulatory filings, and validated state.
        """)

    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "DHF Archeology"**
A team develops a device in an ad-hoc, undocumented manner. Just before launch, a manager asks, "Where is the DHF?" This triggers a panicked, reverse-engineering effort to create documents for work that was done months or years ago, a process known as "DHF archeology."
- **The Flaw:** This is not just a compliance failure; it's a project management disaster. The records are often incomplete, inaccurate, and cannot prove a state of control. An auditor would easily see through this, leading to major findings.""")
        st.success("""üü¢ **THE GOLDEN RULE: The DHF is Built Concurrently, Not Consecutively**
The DHF is not a document you *write* at the end of a project; it is the *result* of the project. A disciplined project lives by this rule.
1.  **Start with the Plan:** The Design and Development Plan should outline the entire process and list the documents that will be generated for the DHF, acting as a master checklist.
2.  **Generate Evidence in Real-Time:** As each stage (Inputs, Outputs, V&V) is completed, its associated records (URS, specifications, test reports) are reviewed, approved, and immediately filed in the DHF.
3.  **The DHF is the Project Logbook:** At any point in time, the DHF should reflect the current, up-to-date status of the project, providing a complete and contemporaneous audit trail for any stakeholder or auditor to review.
""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: Learning from Tragedy
        **The Problem:** In the 1970s and 80s, a series of high-profile failures plagued the medical device industry. The most notorious was the **Dalkon Shield IUD**, which caused serious injuries, infections, and even deaths in thousands of users. Another significant case was the **Bjork-Shiley heart valve**, where fractures in the valve's struts led to catastrophic failures and patient deaths.
        
        **The 'Aha!' Moment:** Congressional and FDA investigations into these tragedies revealed a common, shocking theme: many of the device failures were not due to manufacturing errors, but were caused by fundamental **flaws in the initial design process**. The designs were not robust, had not been adequately tested for long-term performance, and the risks had not been properly assessed. Regulators realized that simply regulating manufacturing (Good Manufacturing Practices) was not enough. The design process itself needed to be brought into a state of control.
        
        **The Impact:** This led to the passage of the **Safe Medical Devices Act of 1990**. This act gave the FDA the explicit authority to regulate the design and development process for medical devices. The FDA implemented this authority by creating the **Design Controls** regulation (now codified in **21 CFR 820.30**). They adapted the systematic "waterfall" model from systems engineering and the aerospace industry, forcing the medical device industry to adopt a structured, documented, and evidence-based approach to product development. The **Design History File (DHF)** became the mandatory, auditable evidence that this disciplined process had been followed, transforming medical device development from a creative art into a rigorous engineering discipline and significantly improving patient safety.
        """)

    with tabs[5]:
        st.markdown("""
        Design Controls and the DHF are the central requirements for medical device development and are considered the gold standard for managing any complex system in a GxP environment.
        - **FDA 21 CFR 820.30 - Design Controls:** This is the primary US regulation that explicitly defines the entire process visualized in the flowchart: Design Inputs, Outputs, Review, Verification, Validation, Transfer, Changes, and the Design History File. It applies to all Class II and Class III medical devices, and certain Class I devices.
        - **ISO 13485:2016 - Medical devices ‚Äî Quality management systems:** This is the international quality management standard for medical devices. Section 7.3, "Design and development," contains requirements that are highly harmonized with 21 CFR 820.30, making the DHF a global expectation.
        - **ICH Q8, Q9, Q10 (QbD Trilogy):** While originating in pharma, the principles are identical. The DHF is the documented evidence of a **Quality by Design** approach, proving that product and process understanding were systematically developed and that risks were managed.
        - **GAMP 5:** While not a regulation, the GAMP 5 framework for validating computerized systems is built on the same principles of linking user requirements (Inputs) to specifications (Outputs) and testing (V&V). The collection of all these documents is functionally equivalent to a DHF for a software system.
        """)
        
# ==============================================================================
# UI RENDERING FUNCTION (FAT/SAT)
# ==============================================================================
def render_fat_sat():
    """Renders the comprehensive, interactive module for FAT, SAT & Qualification."""
    st.markdown("""
    #### Purpose & Application: The "Trust, but Verify" Framework
    **Purpose:** To provide a structured, two-phase framework for accepting new equipment or systems that seamlessly flows into formal GxP qualification. It ensures that what you bought is what was delivered and that it works correctly in your own environment.
    - **FAT (Factory Acceptance Test):** A dress rehearsal at the **vendor's site** to find issues early.
    - **SAT (Site Acceptance Test):** The final check-out at **your site** post-installation.
    - **IQ/OQ/PQ:** The formal **GxP qualification** that leverages the evidence from FAT/SAT.
    
    **Strategic Application:** This is a critical risk-reduction strategy for any significant capital project. A thorough FAT is one of the most powerful tools a project manager has to **prevent costly delays**. By finding and fixing problems at the vendor's factory, you avoid the massive expense and schedule impact of trying to fix them after the system has been shipped and installed in your facility, and you can significantly reduce the scope of your formal OQ.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Project Manager.
    1.  Use the **C&Q Strategy Workbench** below to define the context of your project.
    2.  Review the **SME Recommendation** that is generated based on your inputs.
    3.  Make your **Final Decision** on the FAT strategy and see the immediate impact on the project timeline, cost risk, and the detailed on-site test plan.
    """)

    # --- NEW: C&Q Strategy Workbench ---
    with st.expander("üî¨ C&Q Strategy Workbench", expanded=True):
        risk_score = 0
        col1, col2, col3 = st.columns(3)
        with col1:
            complexity = st.select_slider("System Complexity", 
                options=['Low (Off-the-shelf)', 'Medium (Configured)', 'High (Custom-built)'], 
                value='Medium (Configured)',
                help="How complex is the system? A simple, standard instrument carries less risk than a fully custom-designed manufacturing skid.")
            risk_score += {'Low': 1, 'Medium': 3, 'High': 5}[complexity.split(' ')[0]]
        with col2:
            vendor_maturity = st.select_slider("Vendor Maturity & Trust",
                options=['New Vendor', 'Known Vendor', 'Strategic Partner'],
                value='Known Vendor',
                help="What is your relationship with the vendor? A long-term strategic partner with a great track record is lower risk than a brand-new, unproven supplier.")
            risk_score += {'New': 4, 'Known': 2, 'Strategic': 1}[vendor_maturity.split(' ')[0]]
        with col3:
            schedule_pressure = st.select_slider("Project Schedule Pressure",
                options=['Relaxed', 'Standard', 'Aggressive'],
                value='Standard',
                help="What are the external pressures on the project timeline? Aggressive schedules often tempt teams into making riskier decisions, like skimping on the FAT.")
            risk_score -= {'Relaxed': 1, 'Standard': 0, 'Aggressive': -2}[schedule_pressure.split(' ')[0]]

        # Determine SME Recommendation
        if risk_score >= 7:
            recommended_strategy = "Comprehensive"
            recommendation_text = "üî¥ **Very High Risk:** A Comprehensive FAT is strongly recommended to de-risk the project."
        elif risk_score >= 4:
            recommended_strategy = "Standard"
            recommendation_text = "üü° **Moderate Risk:** A Standard FAT is appropriate. Ensure all critical functions are tested."
        else:
            recommended_strategy = "Minimal"
            recommendation_text = "üü¢ **Low Risk:** A Minimal FAT may be acceptable, focusing only on critical checks."
        
        st.info(f"**SME Recommendation:** {recommendation_text}")

        pm_decision = st.radio("Your Final Decision (as Project Manager):",
                               ["Comprehensive", "Standard", "Minimal"],
                               index=["Comprehensive", "Standard", "Minimal"].index(recommended_strategy),
                               horizontal=True,
                               help="Select the final FAT strategy you will execute. Choosing a less thorough strategy than recommended increases project risk.")

    # Calculate final KPIs based on the PM's decision
    if pm_decision == "Comprehensive":
        timeline_impact = -45  # Savings
        cost_risk = "Low"
    elif pm_decision == "Standard":
        timeline_impact = -15
        cost_risk = "Medium"
    else: # Minimal
        timeline_impact = 0
        cost_risk = "High"

    fig = plot_cq_workflow(pm_decision)
    
    st.header("Project Outcome Dashboard")
    kpi_col1, kpi_col2, kpi_col3 = st.columns(3)
    kpi_col1.metric("Project Timeline Impact", f"{timeline_impact} Days", help="Estimated days saved (-) or added (+) to the on-site qualification timeline compared to a minimal FAT strategy.")
    kpi_col2.metric("On-Site Cost Overrun Risk", cost_risk, help="The likelihood of facing significant, unplanned costs during on-site installation and qualification due to issues that could have been found at the FAT.")
    kpi_col3.metric("Executed Strategy", pm_decision)

    st.plotly_chart(fig, use_container_width=True)

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])

    with tabs[0]:
        st.markdown("""
        **Interpreting the Dashboard:**
        - **The C&Q Workbench:** This gadget simulates the real-world trade-offs. Notice how a **High Complexity** system from a **New Vendor** generates a "Very High Risk" recommendation. Ignoring this and choosing a **Minimal** FAT as your final decision results in a high-risk project with no timeline savings.
        - **The Workflow Table:** This is the core of the module. The color-coding visualizes the concept of leveraging. 
            - **Green (Leverage):** Activities successfully completed at the FAT that only need confirmation during SAT and can be formally referenced to reduce OQ scope.
            - **Yellow (Risk-Based Reduction):** A spot-check or reduced testing approach is taken on-site.
            - **Red (Full Retest):** Activities that must be performed from scratch at your site, offering no time savings and carrying the highest risk.

        **The Strategic Insight:** A FAT is not a checkbox exercise; it is the single best investment you can make in ensuring a smooth and fast site qualification. The goal is to **"Leverage, Don't Repeat."** The more thorough your FAT, the more evidence you can leverage to reduce the scope and duration of your SAT and the subsequent IQ/OQ, saving both time and money.
        """)

    with tabs[1]:
        st.markdown("""
        ### The Business Case: The Cheapest Place to Fix a Problem is at the Source

        #### The Problem: The "We'll Fix It On-Site" Fallacy
        A project team, under intense pressure to meet an aggressive deadline, decides to skip the Factory Acceptance Test (FAT) to "save a week" on the schedule and avoid travel costs. They assume the vendor will deliver a perfect system. The complex, custom-built equipment arrives at their facility, is craned into the cleanroom, and connected to utilities. During the first power-on, they discover a critical controller was incorrectly wired and a key software module is missing.

        #### The Impact: Catastrophic Delays and Exponentially Higher Costs
        The "saved" week and travel budget are instantly obliterated. The true costs are now:
        - **Project Delay:** What would have been a one-day fix at the factory now becomes a **three-month on-site delay**. The vendor must manufacture and ship a new controller and dispatch expensive software engineers to the customer's site to debug code in a live, controlled environment.
        - **Exponential Cost:** The cost of fixing the problem is now 10-100x higher. It includes not just the vendor's travel and labor, but the cost of idle facility time, the salaries of the entire project team who are now on standby, and the massive opportunity cost of the delayed product launch.
        - **Loss of Leverage:** The customer has lost all leverage. The equipment is in their facility, final payments have likely been made, and they are now completely at the mercy of the vendor's schedule to fix the problem they should have caught months earlier.

        #### The Solution: "Trust, but Verify" at the Source
        A FAT is not a line item to be cut; it is the single most powerful **risk mitigation and cost avoidance** tool for any capital project. It is a dress rehearsal of the validation in the lowest-cost environment possible: the vendor's factory, using the vendor's technicians and resources. It is the point of maximum leverage, where the customer can refuse to approve shipment until every last issue is resolved and documented.

        #### The Consequences: Predictable Timelines and Budget Adherence
        - **Without This:** On-site qualification is a high-risk, unpredictable adventure with a high probability of significant budget and schedule overruns.
        - **With This:** On-site qualification becomes a smooth, predictable confirmation of previously verified performance. A well-executed FAT is the best investment a project manager can make. It directly translates to a faster, cheaper, and less stressful on-site validation, ensuring the project meets its timeline and budget commitments to the business.
        """)
    with tabs[2]:
        st.markdown("""
        ### The Business Case: The Cheapest Place to Fix a Problem is at the Source

        #### The Problem: The "We'll Fix It On-Site" Fallacy
        A project team, under intense pressure to meet an aggressive deadline, decides to skip the Factory Acceptance Test (FAT) to "save a week" on the schedule and avoid travel costs. They assume the vendor will deliver a perfect system. The complex, custom-built equipment arrives at their facility, is craned into the cleanroom, and connected to utilities. During the first power-on, they discover a critical controller was incorrectly wired and a key software module is missing.

        #### The Impact: Catastrophic Delays and Exponentially Higher Costs
        The "saved" week and travel budget are instantly obliterated. The true costs are now:
        - **Project Delay:** What would have been a one-day fix at the factory now becomes a **three-month on-site delay**. The vendor must manufacture and ship a new controller and dispatch expensive software engineers to the customer's site to debug code in a live, controlled environment.
        - **Exponential Cost:** The cost of fixing the problem is now 10-100x higher. It includes not just the vendor's travel and labor, but the cost of idle facility time, the salaries of the entire project team who are now on standby, and the massive opportunity cost of the delayed product launch.
        - **Loss of Leverage:** The customer has lost all leverage. The equipment is in their facility, final payments have likely been made, and they are now completely at the mercy of the vendor's schedule to fix the problem they should have caught months earlier.

        #### The Solution: "Trust, but Verify" at the Source
        A FAT is not a line item to be cut; it is the single most powerful **risk mitigation and cost avoidance** tool for any capital project. It is a dress rehearsal of the validation in the lowest-cost environment possible: the vendor's factory, using the vendor's technicians and resources. It is the point of maximum leverage, where the customer can refuse to approve shipment until every last issue is resolved and documented.

        #### The Consequences: Predictable Timelines and Budget Adherence
        - **Without This:** On-site qualification is a high-risk, unpredictable adventure with a high probability of significant budget and schedule overruns.
        - **With This:** On-site qualification becomes a smooth, predictable confirmation of previously verified performance. A well-executed FAT is the best investment a project manager can make. It directly translates to a faster, cheaper, and less stressful on-site validation, ensuring the project meets its timeline and budget commitments to the business.
        """)
    with tabs[3]:
        st.markdown("""
        ##### Glossary of Commissioning & Qualification Terms

        These terms represent a logical sequence of activities, moving from the vendor's internal tests to your final process validation.

        ---
        
        **Phase 1: Vendor-Side Activities (Pre-Shipment)**
        - **VVT (Vendor Verification Testing):** The supplier's own internal testing to ensure their system meets its design specifications before they even invite the customer for the FAT.
        - **VDR (Vendor Data Review):** A formal review of the vendor's documentation package (drawings, material certs, test reports) by the customer, often done *before* traveling for the FAT to catch issues early.
        - **FAT (Factory Acceptance Test):** The formal, customer-witnessed test at the **supplier's site** to verify the system meets the pre-agreed specifications (URS/FS) before shipment.
        
        ---

        **Phase 2: User-Side Activities (Post-Installation)**
        - **SAT (Site Acceptance Test):** Testing performed at the **customer's site** after installation to confirm the system was not damaged in transit and still functions correctly in its final environment.
        - **IQ (Installation Qualification):** The first GxP qualification step. Documented verification that the system is **installed per specifications**. This often leverages FAT/SAT evidence for things like checking the Bill of Materials (BOM).
        - **OQ (Operational Qualification):** The second GxP step. Documented testing to ensure the equipment **functions as intended** across its operating ranges in the site environment. This often leverages functional tests performed during FAT/SAT.
        - **PQ (Performance Qualification):** The final GxP qualification step. Documented evidence that the system **consistently performs as intended under actual process conditions**.

        ---

        **Related Testing Concepts**
        - **SIT (System Integration Testing):** For complex systems with multiple vendors, this is testing done *before* FAT/SAT to ensure all the different components communicate and work together correctly.
        - **UAT (User Acceptance Testing):** Primarily for software and automated systems, this is testing by end-users to confirm the system meets their real-world workflow needs. It often overlaps significantly with SAT and PQ.
        - **Run-In / Burn-In Testing:** Stress testing, often performed by the vendor before FAT, where the system is run continuously for an extended period to identify and fix early-life component failures.
        - **DRT (Dry Run Testing):** An informal practice run of a test protocol (like a FAT or SAT) by the executing team to ensure the procedure is clear and all prerequisites are in place before the formal, witnessed execution.
        - **HAT (Harbor Acceptance Testing):** A specialized term used in maritime/offshore industries, functionally equivalent to a FAT/SAT but conducted at the harbor before a vessel or platform is deployed.
        """)
    with tabs[4]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "We'll Fix It Later" Fallacy**
A project team, under pressure to meet a deadline, decides to skip the FAT to "save a week" on the schedule. The equipment arrives on site, and during SAT/IQ, they discover a critical component was built with the wrong material and a key software module is missing.
- **The Flaw:** They traded a one-week trip for a three-month on-site delay, as the vendor now has to manufacture and ship a new component and dispatch software engineers to their site to fix the code. This is a classic example of being "penny wise and pound foolish." """)
        st.success("""üü¢ **THE GOLDEN RULE: Find Problems at the Factory, Not in the Cleanroom**
A robust acceptance testing plan treats the FAT as a critical, non-negotiable risk mitigation step.
1.  **Define Upfront:** The detailed requirements for the FAT and SAT must be defined in the User Requirement Specification (URS) and agreed upon with the vendor *before* the purchase order is signed.
2.  **Invest in the FAT:** Send a cross-functional team (Engineering, Quality, and the End User) to the vendor's site to execute the FAT protocol rigorously.
3.  **Document Everything:** Use a formal FAT protocol. Any deviations or failures must be documented and formally resolved by the vendor *before* you give approval to ship the equipment. This gives you maximum leverage.""")
    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Power Plants to Pharma
        The concepts of FAT and SAT did not originate in the pharmaceutical industry. They are standard **Good Engineering Practices (GEP)** developed over a century of large-scale capital project management in industries like oil & gas, chemical manufacturing, and power generation.
        
        **The Problem:** When building a billion-dollar power plant, you cannot afford to have a custom-built, 50-ton turbine arrive on site only to discover it doesn't fit or doesn't meet its performance specifications. The cost of on-site rework for such massive equipment is astronomical.
        
        **The Solution:** The engineering world developed the two-stage acceptance process to de-risk these projects. The FAT became the critical gatekeeper to ensure that complex, custom-built equipment was fully functional and met all specifications *before* it was shipped. The SAT then served as a confirmation that the equipment survived the journey and worked as expected in its final home.
        
        **The Impact in Pharma:** As pharmaceutical equipment became more complex, automated, and computer-controlled, the industry recognized the value of these engineering best practices. The **GAMP (Good Automated Manufacturing Practice)** Forum and the **ISPE (International Society for Pharmaceutical Engineering)** championed the adoption of FAT and SAT as critical commissioning steps that occur *before* formal GxP qualification (IQ/OQ/PQ), significantly improving the efficiency and success rate of validation projects.
        """)
    with tabs[5]:
        st.markdown("""
        FAT and SAT are considered **Good Engineering Practices (GEP)** and are critical inputs to the formal GxP qualification process.
        - **GAMP 5 - A Risk-Based Approach to Compliant GxP Computerized Systems:** This is the primary guidance document that formalizes this process. It positions FAT and SAT as key activities within the "Project Phase" that support the subsequent Qualification and Verification activities. A well-documented FAT report is often leveraged to reduce the scope of IQ and OQ testing.
        - **FDA Process Validation Guidance & 21 CFR 211:** While not explicitly mentioning FAT/SAT, these regulations require that equipment be suitable for its intended use and that processes are in a state of control. A robust FAT/SAT program is the industry-standard method for ensuring equipment suitability, which is a prerequisite for a successful process validation.
        - **EU Annex 15: Qualification and Validation:** Emphasizes a risk-based approach and the need for URS to be in place for equipment. FAT and SAT are the activities where the equipment is formally tested against the URS and functional specifications.
        """)
#=============================================================== DfX MODULE ===========================================================================================================

def render_dfx_dashboard():
    """Renders the comprehensive, interactive module for Design for Excellence (DfX)."""
    st.markdown("""
    #### Purpose & Application: Designing for the Entire Lifecycle
    **Purpose:** To demonstrate the strategic and economic impact of **Design for Excellence (DfX)**, a proactive engineering philosophy that focuses on optimizing a product's design for its entire lifecycle, from manufacturing to disposal.
    
    **Strategic Application:** DfX is the practical implementation of "shifting left"‚Äîaddressing downstream problems (like manufacturing costs, test time, or reliability) during the earliest stages of design, where changes are exponentially cheaper. For a validation leader, promoting DfX principles is a key strategy for ensuring that new products are not only effective but also robust, scalable, and profitable.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Head of Engineering or Validation.
    1.  Select the **Project Type** you are leading.
    2.  Use the **DfX Effort Sliders** in the sidebar to allocate engineering resources to different design philosophies.
    3.  Observe the impact in real-time on the **KPI Dashboard**, including the critical **Risk-Adjusted Cost**.
    """)
    
    profiles = {
        "Pharma Assay (ELISA)": {
            'categories': ['Robustness', 'Run Time (hrs)', 'Reagent Cost (RCU)', 'Precision (%CV)', 'Ease of Use'], 'baseline': [5, 4.0, 25.0, 18.0, 5], 'direction': [1, -1, -1, -1, 1], 'reliability_idx': 0,
            'impact': {'mfg': [0.1, -0.1, -0.2, 0, 0.1], 'quality': [0.5, -0.05, 0, -0.6, 0.2], 'sustainability': [0, 0, -0.3, 0, 0], 'ux': [0.1, -0.2, 0, 0, 0.7]}
        },
        "Instrument (Liquid Handler)": {
            'categories': ['Throughput<br>(plates/hr)', 'Uptime (%)', 'Footprint (m¬≤)', 'Service Cost<br>(RCU/yr)', 'Precision (%CV)'], 'baseline': [20, 95.0, 2.5, 5000, 5.0], 'direction': [1, 1, -1, -1, -1], 'reliability_idx': 1,
            'impact': {'mfg': [0.2, 0.1, -0.2, -0.1, 0], 'quality': [0.1, 0.8, 0, -0.2, -0.6], 'sustainability': [0, 0.1, -0.1, -0.4, 0], 'ux': [0, 0.2, 0, -0.6, 0]}
        },
        "Software (LIMS)": {
            'categories': ['Performance<br>(Query Time s)', 'Scalability<br>(Users)', 'Reliability<br>(Uptime %)', 'Compliance<br>Score', 'Dev Cost (RCU)'], 'baseline': [8.0, 100, 99.5, 6, 500], 'direction': [-1, 1, 1, 1, -1], 'reliability_idx': 2,
            'impact': {'mfg': [-0.1, 0.2, 0.2, 0, -0.4], 'quality': [-0.2, 0.1, 0.7, 0.8, 0.2], 'sustainability': [0, 0.5, 0.1, 0, -0.1], 'ux': [-0.4, 0.2, 0, 0.5, 0.1]}
        },
        "Pharma Process (MAb)": {
            'categories': ['Yield (g/L)', 'Cycle Time<br>(days)', 'COGS (RCU/g)', 'Purity (%)', 'Robustness<br>(PAR Size)'], 'baseline': [3.0, 18, 100, 98.5, 5], 'direction': [1, -1, -1, 1, 1], 'reliability_idx': 4,
            'impact': {'mfg': [0.3, -0.2, -0.4, 0.1, 0.2], 'quality': [0.1, 0, -0.1, 0.6, 0.8], 'sustainability': [0.05, -0.1, -0.2, 0, 0.1], 'ux': [0, 0, 0, 0, 0]}
        }
    }
    
    project_type = st.selectbox("Select a Project Type to Simulate DfX Impact:", list(profiles.keys()))
    selected_profile = profiles[project_type]

    with st.sidebar:
        st.subheader("DfX Effort Allocation")
        mfg_effort = st.slider("Manufacturing & Assembly Effort", 0, 10, 5, 1, help="DFM/DFA: Focus on part count reduction, standard components, and automation.")
        quality_effort = st.slider("Quality & Reliability Effort", 0, 10, 5, 1, help="DFR/DFT/DFQ: Focus on reliability, robust performance, and fast, accurate QC testing.")
        sustainability_effort = st.slider("Sustainability & Supply Chain Effort", 0, 10, 5, 1, help="DFE/DFS: Focus on standard/recyclable materials, energy use, and easy disassembly.")
        ux_effort = st.slider("Service & User Experience Effort", 0, 10, 5, 1, help="DFS/DFUX: Focus on ease of use, service, and maintenance.")

    fig_radar, fig_cost, kpis, categories, base_costs, optimized_costs = plot_dfx_dashboard(
        project_type, mfg_effort, quality_effort, sustainability_effort, ux_effort
    )

    st.header("Project KPI Dashboard")
    reliability_idx = selected_profile['reliability_idx']
    base_reliability = kpis['baseline'][reliability_idx]
    opt_reliability = kpis['optimized'][reliability_idx]
    
    base_total_cost = sum(base_costs)
    opt_total_cost = sum(optimized_costs)

    base_risk_premium = 1 + (10 - base_reliability) * 0.05 if "Score" in categories[reliability_idx] else 1 + (100 - base_reliability) * 0.02
    opt_risk_premium = 1 + (10 - opt_reliability) * 0.05 if "Score" in categories[reliability_idx] else 1 + (100 - opt_reliability) * 0.02

    base_risk_adjusted_cost = base_total_cost * base_risk_premium
    opt_risk_adjusted_cost = opt_total_cost * opt_risk_premium

    col1, col2 = st.columns(2)
    col1.metric("Total Cost (RCU)", f"{opt_total_cost:,.0f}", f"{opt_total_cost - base_total_cost:,.0f}")
    col2.metric("Risk-Adjusted Total Cost (RCU)", f"{opt_risk_adjusted_cost:,.0f}", f"{opt_risk_adjusted_cost - base_risk_adjusted_cost:,.0f}",
                help="Total Cost including a 'risk premium'. A less reliable or robust design is penalized more heavily, reflecting the long-term cost of potential failures.")
    
    st.markdown("##### Performance Profile")
    kpi_cols = st.columns(len(kpis['baseline']))
    for i, col in enumerate(kpi_cols):
        base_val = kpis['baseline'][i]
        opt_val = kpis['optimized'][i]
        delta = opt_val - base_val
        col.metric(categories[i].replace('<br>', ' '), f"{opt_val:.1f}", f"{delta:.1f}")

    st.header("Design Comparison Visualizations")
    col_radar, col_cost = st.columns(2)
    with col_radar:
        st.plotly_chart(fig_radar, use_container_width=True)
    with col_cost:
        st.plotly_chart(fig_cost, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    with tabs[0]:
        st.markdown("""
        **Interpreting the Dashboard:**
        - **KPI Dashboard:** This is your executive summary. It quantifies the return on investment for your DfX efforts in terms of performance and cost. The **Risk-Adjusted Cost** is the most important metric, as it represents the "true" cost of a design by penalizing lower quality and reliability.
        - **Performance Profile (Radar Chart):** This visualizes the multi-dimensional impact of your design choices. The goal is to create an "Optimized" profile (green) that meets or exceeds all performance targets.
        - **Cost Structure (Pie Charts):** This shows *how* you achieved cost savings. The units are **Relative Cost Units (RCU)**, focusing on proportions. The total cost is displayed in the center. A strong DFM/DFA effort will dramatically reduce the proportion of cost attributed to 'Manufacturing/Labor'.
        
        **The Strategic Insight:** The most profitable design is not always the one with the lowest initial cost. The **Risk-Adjusted Cost** demonstrates that investing in reliability and quality (DFR/DFQ) can be a financially sound decision by reducing the long-term costs associated with failures, service, and warranty claims.
        """)
    with tabs[1]:
        st.markdown("""
        ### The Business Case: Designing for Profit, Not Just Function
    
        #### The Problem: "Over-the-Wall" Engineering and the Cost Iceberg
        The R&D team designs a product in isolation, focusing exclusively on achieving the core function. They then "throw the design over the wall" to the manufacturing and quality teams. The Manufacturing team discovers it is impossibly difficult and expensive to build at scale. The Quality team finds it is slow and difficult to test reliably. The Service team finds it is a nightmare to repair in the field. The product *works*, but it is not profitable or sustainable.
    
        #### The Impact: The "Iceberg" of Hidden Lifecycle Costs
        The upfront R&D and material cost is just the visible tip of the iceberg. The true cost of a poorly designed product is hidden below the surface, in massive, ongoing operational expenses:
        - **High Manufacturing Costs:** Complex assembly, low yields, and high scrap rates erode profit margins on every unit sold.
        - **High Cost of Quality:** Long QC cycle times delay batch release, and frequent invalid results trigger costly investigations.
        - **High Service & Warranty Costs:** An unreliable product that is difficult to service leads to expensive warranty claims, customer dissatisfaction, and damage to the brand's reputation.
        This "death by a thousand cuts" can make an otherwise brilliant product a financial failure.
    
        #### The Solution: Concurrent Engineering as a Business Strategy
        Design for Excellence (DfX) is a philosophy of **concurrent engineering**. It is not about asking R&D to do more work; it is about changing *how* the work is done. It brings Manufacturing, Quality, Service, and Supply Chain experts into the design process from the very beginning, when changes are cheap. They use DfX principles as a formal checklist to ensure the design is optimized not just for function, but for:
        - **Manufacturability & Assembly (DFM/A):** To reduce unit cost and build time.
        - **Testability & Quality (DFT/Q):** To ensure fast, reliable testing and high yields.
        - **Reliability & Serviceability (DFR/S):** To minimize field failures and service costs.
    
        #### The Consequences: Higher Margins, Faster Time-to-Market, and a Stronger Brand
        - **Without This:** The company is constantly battling operational inefficiencies and hidden costs that were locked in during the design phase. Profit margins are thin, and the organization is in a state of perpetual firefighting.
        - **With This:** The company produces a product that is not only effective but also cheaper to build, faster to test, and more reliable in the field. DfX is a direct, strategic investment in **higher profit margins, faster production cycles, greater customer satisfaction, and a stronger, more defensible brand reputation.** It is the engineering foundation of a market-leading product.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of DfX Principles
        - **DfX (Design for Excellence):** An engineering philosophy that integrates lifecycle concerns into the earliest design stages.
        - **DFM (Design for Manufacturability):** Designing parts for ease of manufacturing to reduce cost and improve yield.
        - **DFA (Design for Assembly):** Designing a product to be easy to assemble, primarily by reducing part count.
        - **DFT (Design for Test):** Designing a product to make QC testing fast, automated, and reliable.
        - **DFR (Design for Reliability):** Designing a product to be robust and have a long, predictable lifespan.
        - **DFQ (Design for Quality):** Integrating quality assurance principles from concept through production, often using statistical tools like DFSS.
        - **DFSS (Design for Six Sigma):** A data-driven approach to design that aims to create products and processes that are defect-free from the start.
        - **DFE (Design for Environment):** Designing to reduce environmental footprint, including material selection and end-of-life recycling.
        - **DFS (Design for Serviceability):** Designing a product to be easy to maintain and repair.
        - **DFUX (Design for User Experience):** Optimizing the product for usability and satisfaction by incorporating human factors and ergonomic principles.
        - **RCU (Relative Cost Unit):** An abstract unit of cost used for strategic planning and comparison when precise dollar amounts are unknown or variable. It focuses on the proportions and relative magnitudes of different cost drivers.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: "Over-the-Wall" Engineering**
The R&D team designs a product in isolation, focusing only on functionality. They then "throw the design over the wall" to the manufacturing and quality teams, who discover that it is impossibly difficult to build, assemble, or test reliably at scale.
- **The Flaw:** This sequential process creates massive rework, delays, and friction between departments. Problems that would have taken minutes to fix on a CAD model now require weeks of expensive re-tooling and re-validation.""")
        st.success("""üü¢ **THE GOLDEN RULE: The Cost of a Design Change is Exponential**
The core principle of DfX is **concurrent engineering**, where design, manufacturing, quality, and other downstream teams work together as a cross-functional unit from the very beginning of the project.
1.  **Acknowledge the Cost Curve:** A design change on the whiteboard is free. A change after tooling is made costs thousands. A change after the product is in the field costs millions in recalls and reputational damage.
2.  **Shift Left:** The goal is to pull as many manufacturing, assembly, and testing considerations as far "left" into the early design phase as possible.
3.  **Use DfX as a Formal Checklist:** Proactively review designs against a formal DfX checklist at every Stage-Gate or Design Review to ensure the product is not just functional, but manufacturable, testable, and profitable.""")
        
    with tabs[4]:
        st.markdown("""
        #### Historical Context: The Birth of Concurrent Engineering
        The principles of DfX emerged from the intense manufacturing competition of the 1970s and 80s. While concepts like designing for ease of manufacturing existed for decades, they were formalized and popularized by several key forces:
        - **Japanese Manufacturing:** Companies like Toyota and Sony pioneered concepts of lean manufacturing and concurrent engineering, where design was not an isolated activity but a team sport involving all departments to eliminate "muda" (waste).
        - **Boothroyd & Dewhurst (DFA):** In the late 1970s, Geoffrey Boothroyd and Peter Dewhurst at the University of Massachusetts Amherst developed the first systematic, quantitative methodology for **Design for Assembly (DFA)**. Their work provided a structured way to analyze a design, estimate its assembly time, and identify opportunities for part reduction, transforming DFA from a vague idea into a rigorous engineering discipline.
        - **General Electric's "Bulls-eye":** In the 1980s, GE championed DFM, famously using a "bulls-eye" diagram to illustrate how the majority of a product's cost is locked in during the earliest design stages.
        
        The success of these methods led to the proliferation of the "DfX" acronym, extending the core philosophy to all aspects of a product's lifecycle.
        """)
        
    with tabs[5]:
        st.markdown("""
        DfX is the practical engineering methodology used to fulfill the requirements of formal **Design Controls**.
        - **FDA 21 CFR 820.30 (Design Controls):** This regulation for medical devices is the primary driver for DfX in the life sciences. The DfX process is how you fulfill the requirements for:
            - **Design Inputs:** Proactively considering manufacturing, assembly, and testing requirements from the very start.
            - **Design Review:** DfX checklists and scorecards are a key part of formal, documented design reviews.
            - **Design Verification & Validation:** Ensuring the design outputs meet the design inputs.
            - **Design Transfer:** A product designed with DfX principles has a much smoother and more successful design transfer into manufacturing.
        - **ICH Q8(R2) - Pharmaceutical Development:** The principles of QbD‚Äîunderstanding how product design and process parameters affect quality‚Äîare perfectly aligned with DfX.
        - **ISO 13485 (Medical Devices):** This international standard for quality management systems requires a structured design and development process, which is effectively implemented through DfX principles.
        """)
#========================================================================================= 5. VALIDATION MASTER PLAN (VMP) BUILDER =====================================================================
def render_vmp_builder():
    """Renders the comprehensive, interactive module for the Validation Master Plan Builder."""
    st.markdown("""
    #### Purpose & Application: The Project's Master Plan
    **Purpose:** To act as the **interactive project manager and compliance checklist** for any validation activity. The Validation Master Plan (VMP) is the highest-level strategic document outlining the scope, responsibilities, and methodologies for the entire validation effort.
    
    **Strategic Application:** This tool connects the strategic "why" (from the TPP and FMEA) to the tactical "how" (the specific analytical tools). It provides a clear roadmap for the project, showing which tools from this toolkit are deployed at each phase of a validation project. This is essential for project planning, resource allocation, and demonstrating a structured approach to regulators.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Validation Manager. Select a **Project Type** from the dropdown menu. The diagram below will dynamically update to show the standard validation workflow for that project, highlighting which tools from this application are used at each critical stage.
    """)

    project_type = st.selectbox(
        "Select a Validation Project Type to Plan:",
        ("Analytical Method Validation", "Instrument Qualification", "Pharma Process (PPQ)", "Software System (CSV)"),
        index=0,
        help="Choose the type of project you are planning. The diagram will update to show the standard validation workflow and the key analytical tools used at each stage."
    )

    fig = plot_vmp_flow(project_type)
    st.plotly_chart(fig, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])

    with tabs[0]:
        st.markdown(f"""
        **Connecting Strategy to Execution for: {project_type}**
        This tool demonstrates how all the other modules in the toolkit fit together to form a complete, compliant validation project.
        - The **Analytical Method Validation** workflow follows the classic V-Model, showing the direct link between defining performance specifications (like Linearity) during design and later qualifying them during PQ.
        - The **Instrument Qualification** workflow also follows the V-Model, starting with User Needs (captured in the ATP Builder) and culminating in PQ testing (using Gage R&R and SPC).
        - The **Pharma Process (PPQ)** workflow is a linear process, moving from planning (using FMEA and Sample Size) to execution and final analysis (using SPC and Capability). This represents Stage 2 of the FDA's Process Validation lifecycle.
        - The **Software System (CSV)** workflow shows how modern AI/ML tools can be integrated into the GAMP 5 V-Model. For example, **Explainable AI (XAI)** is a key activity during the "Build" phase to ensure the model is transparent.
        """)

    with tabs[1]:
        st.markdown("""
        ### The Business Case: The Blueprint for Compliance and Efficiency

        #### The Problem: The "Validation on the Fly" Approach
        A critical validation project begins without a clear, documented, and approved master plan. The scope is ill-defined, responsibilities are ambiguous, and the acceptance criteria are not finalized. Different teams validate different parts of the system with inconsistent methodologies, documentation formats, and levels of rigor.

        #### The Impact: Chaos, Inefficiency, and Guaranteed Audit Failure
        This approach is a recipe for project failure and compliance disaster.
        - **Scope Creep & Rework:** Without a defined scope, the project continuously expands. Tests are added late in the process, leading to significant rework and delays.
        - **Resource Conflicts & Inefficiency:** Teams operate in silos, leading to duplicated efforts and resource conflicts. Critical dependencies between different validation activities (e.g., the assay must be validated before the process can be) are missed, causing sequence-of-operations failures.
        - **"Testing into Compliance":** When a test fails, there is immense pressure to change the acceptance criteria on the fly to "make it pass," completely invalidating the scientific and regulatory purpose of the exercise.
        - **Audit Failure:** An auditor will immediately ask for the VMP. An inability to produce a comprehensive, pre-approved plan is a systemic failure that can call the entire validation effort into question.

        #### The Solution: The Single Source of Truth for the Project
        The Validation Master Plan (VMP) is the **single source of truth** for the entire validation project. It is the highest-level strategic document, created and approved *before* any protocols are executed. It serves as the project's constitution, formally defining:
        - **The Why:** The validation rationale and overall strategy.
        - **The What:** The precise scope of systems and processes to be validated.
        - **The Who:** Roles and responsibilities for every member of the validation team.
        - **The How:** The methodology, the required deliverables (e.g., IQ/OQ/PQ protocols), and the change control process.
        - **The When:** The high-level schedule and sequence of activities.

        #### The Consequences: A Defensible and Efficient Project
        - **Without This:** The validation project is an unpredictable, high-risk activity characterized by chaos, inefficiency, and a high probability of audit failure.
        - **With This:** The validation project becomes a well-managed, efficient, and predictable engineering process. The VMP provides a clear roadmap for the team, prevents scope creep, and ensures a consistent, harmonized approach across the entire project. For an auditor, it provides a simple, defensible summary that demonstrates the entire validation effort was conducted in a state of control from the very beginning.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of VMP & V-Model Terms
        - **Validation Master Plan (VMP):** A high-level document that describes an organization's overall philosophy, intention, and approach to validation. It governs all validation activities at a site.
        - **Validation Protocol (VP):** A detailed, written plan stating how a specific validation will be conducted, including test parameters, product characteristics, and pre-approved acceptance criteria.
        - **User Requirement Specification (URS):** Defines what the user needs the system to do, from a business perspective.
        - **Functional Specification (FS):** Translates the URS into a detailed description of what the system *must do* (its functions).
        - **Design Specification (DS):** Describes *how* the system will be built to meet the FS.
        - **Installation Qualification (IQ):** Documented verification that the system is installed correctly according to specifications and vendor recommendations.
        - **Operational Qualification (OQ):** Documented verification that the system operates as intended throughout its specified operating ranges. It is often called "testing against the Functional Spec."
        - **Performance Qualification (PQ):** Documented verification that the system, as installed and operated, can consistently perform its intended function and meet pre-determined acceptance criteria under real-world conditions. It is often called "testing against the User Requirement Spec."
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: "Validation on the Fly"**
A project team begins executing test scripts for a new instrument without a pre-approved protocol or a VMP. When a test fails, they change the acceptance criteria so it will pass.
- **The Flaw:** This is not validation; it is "testing into compliance." The acceptance criteria were not pre-defined, and the changes are undocumented. This entire activity would be invalidated during an audit.""")
        st.success("""üü¢ **THE GOLDEN RULE:** If it isn't written down, it didn't happen. The VMP is the single source of truth that defines the scope, strategy, and acceptance criteria for a validation project *before it begins*. It is the most important document for preventing ad-hoc decision making and is the first thing an auditor will ask to see.""")
    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Chaos to Control
        The concept of a formal, high-level planning document for validation grew out of the need to manage increasingly complex projects in the pharmaceutical industry in the 1980s and 90s. As systems became more integrated and regulations more stringent, the old model of simple, standalone equipment qualification was no longer sufficient.
        
        The idea was heavily influenced by project management principles and was codified by two major forces:
        1.  **Regulatory Expectation:** The FDA and other global bodies began to expect a more holistic, planned approach to validation, looking for a "master plan" that governed all qualification activities at a site.
        2.  **GAMP Guidelines:** The **GAMP (Good Automated Manufacturing Practice)** Forum, a consortium of pharmaceutical and engineering professionals, pioneered the V-Model approach for computer systems. A core part of their philosophy was the necessity of high-level planning documents, like the VMP, to manage the complexity of software validation.
        
        Regulators quickly adopted the expectation for a VMP across all types of validation to ensure projects were well-planned, structured, and auditable from the top down.
        """)
    with tabs[5]:
        st.markdown("""
        The VMP is a key document for demonstrating a compliant and well-managed quality system.
        - **EU Annex 15: Qualification and Validation:** This is one of the most explicit regulations. It states that "A Validation Master Plan (VMP) or equivalent document should be established... providing an overview of the company‚Äôs validation strategy."
        - **FDA 21 CFR 211.100 (Written procedures; deviations):** Requires that "there shall be written procedures for production and process control..." The VMP is the highest-level document in this procedural hierarchy, governing all subordinate validation protocols and reports.
        - **GAMP 5:** The VMP is a foundational document in the GAMP 5 framework, outlining the plan for validating all GxP computerized systems.
        - **PIC/S Guide to GMP (PE 009-16):** This influential international guideline, adopted by dozens of regulatory agencies worldwide, also describes the expectation for a VMP to manage the overall validation effort.
        """)

def render_rtm_builder():
    """Renders the comprehensive, interactive module for the Requirements Traceability Matrix."""
    st.markdown("""
    #### Purpose & Application: The Auditor's Golden Thread
    **Purpose:** To be the **"Auditor's Golden Thread."** The RTM is a master document that links every high-level requirement (like a URS or CQA) to the specific design elements and, most critically, to the specific Test Case(s) that prove it was met, even across different validation streams.
    
    **Strategic Application:** The RTM provides irrefutable, traceable proof that **everything that was asked for was built, and everything that was tested was required.** It is the ultimate tool for managing and ensuring the completeness of complex, multi-faceted projects like a full technology transfer.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Program Manager for a complex tech transfer.
    1.  The Sankey diagram shows the full network of requirements and dependencies.
    2.  Use the **checkboxes in the sidebar** to mark different validation streams as "complete."
    3.  Observe how the chart updates in real-time, showing the flow of completed work (green) and highlighting the remaining dependencies.
    """)

    with st.sidebar:
        st.subheader("RTM Completion Status")
        completed_streams = []
        if st.checkbox("Process Validation Complete", value=False):
            completed_streams.append("Process")
        if st.checkbox("Assay Validation Complete", value=False):
            completed_streams.append("Assay")
        if st.checkbox("Instrument Qualification Complete", value=False):
            completed_streams.append("Instrument")
        if st.checkbox("Software Validation Complete", value=False):
            completed_streams.append("Software")

    fig_sankey, _, _ = plot_rtm_sankey(completed_streams)
    st.plotly_chart(fig_sankey, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])

    with tabs[0]:
        st.markdown("""
        **Reading the Integrated RTM:**
        - **Swimlanes:** The diagram is organized into vertical "swimlanes" representing the different, parallel validation projects that must all succeed.
        - **Intra-Stream Traceability:** The horizontal links within a single swimlane (e.g., `INST-URS` ‚Üí `INST-OQ`) show the standard V-Model traceability for that specific system.
        - **Inter-Stream Dependencies (The Critical Links):** The diagonal links show the crucial dependencies *between* projects. For example, you cannot complete the `PROC-TEST` (PPQ) until the `ASSAY-VAL` (the QC test method) and the `INST-PQ` (the lab instrument) are both complete and validated.
        
        **The Strategic Insight:** This visualization reveals that a validation project is a **network of dependencies**. A delay or failure in one stream (like the Instrument Qualification) can have a direct, cascading impact on the critical path of the main process validation. The RTM is the master tool for managing these complex relationships.
        """)

    with tabs[1]:
        st.markdown("""
        ### The Business Case: The Auditor's Golden Thread & The PM's GPS

        #### The Problem: The "Did We Miss Anything?" Panic
        A complex tech transfer project, involving a new process, a new assay, a new instrument, and new software, is nearing its completion date. The project manager asks a simple, terrifying question: **"Can you prove to me that every single user requirement has been designed, built, and, most importantly, tested?"** The team scrambles, manually cross-referencing dozens of disconnected documents‚ÄîURSs, Functional Specs, Test Protocols‚Äîtrying to piece together the evidence. They are not confident they can prove 100% coverage.

        #### The Impact: Launch Delays, Compliance Gaps, and "Death by a Thousand Cuts"
        This lack of a centralized, living traceability system has severe business consequences:
        - **Critical Gaps Discovered Late:** The manual review reveals a critical user requirement that was never translated into a design specification, or a key function that was never formally tested. These late-stage discoveries are the most expensive and time-consuming to fix, often delaying a product launch by weeks or months.
        - **Audit Failure:** During a regulatory audit, the inability to quickly and accurately trace a requirement from its origin to its test case is a major red flag. It suggests a chaotic, uncontrolled process and can lead to significant findings.
        - **Inefficient Change Control:** When a requirement changes mid-project, it's nearly impossible to accurately assess the impact. Which design documents, which test cases are affected? Without a map (the RTM), this becomes a time-consuming and error-prone guessing game.

        #### The Solution: A Living Map of Project Completeness
        The RTM is not a document you create at the end of a project; it is the **living GPS and master checklist** that is built and maintained from Day 1. It is a simple but powerful matrix that formally and explicitly links every "parent" item to its "child" items:
        - `User Requirement` ‚Üí `Functional Specification` ‚Üí `Design Specification` ‚Üí `Test Case ID`
        - `Process CQA` ‚Üí `Assay ATP` ‚Üí `Instrument URS`
        It provides a real-time, unambiguous dashboard of project completeness and dependency management.

        #### The Consequences: Guaranteed Completeness and Proactive Management
        - **Without This:** Project completeness is a matter of hope and guesswork. Project managers are flying blind to critical dependencies. Audits are stressful, manual fire drills.
        - **With This:** The RTM provides a mathematical **guarantee that 100% of requirements have been verified and validated**. For project managers, it is the ultimate tool for managing complexity and understanding the true impact of delays or changes. For auditors, it is the "golden thread" that allows them to see a clear, logical, and traceable path from user need to objective evidence, proving the entire project was executed in a state of control.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of RTM & V-Model Terms
        - **Requirement:** A condition or capability needed by a user to solve a problem or achieve an objective. Can be a URS, an ATP element, or a CQA.
        - **Specification:** A document that specifies, in a complete, precise, verifiable manner, the requirements, design, behavior, or other characteristics of a system. (e.g., FS, DS).
        - **Verification:** The process of evaluating a system to determine whether the products of a given development phase satisfy the conditions imposed at the start of that phase. (Are we building the system right?)
        - **Validation:** The process of evaluating a system during or at the end of the development process to determine whether it satisfies the user requirements. (Are we building the right system?)
        - **Traceability:** The ability to trace a requirement both forwards to its implementation and testing, and backwards to its origin, even across system boundaries.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Siloed Validation" Fallacy**
The Instrument team validates their HPLC, the Assay team validates their purity method, and the Process team validates the manufacturing run, all as separate projects. They only discover at the end that the validated assay cannot achieve the required precision on the newly qualified instrument.
- **The Flaw:** The teams operated in silos, ignoring the critical interdependencies between their systems. The project lacked a holistic RTM to manage these cross-stream links.""")
        st.success("""üü¢ **THE GOLDEN RULE: One Project, One Trace Matrix**
A complex project like a tech transfer should be governed by a single, integrated Validation Master Plan (VMP) and a single, integrated Requirements Traceability Matrix (RTM).
- The RTM must capture not only the vertical traceability within each system (the V-Model) but also the **horizontal traceability between systems.**
- This integrated RTM becomes the master checklist for the entire project, ensuring that dependencies are managed, no gaps exist, and the final integrated system is truly validated as a whole.""")
        
    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Systems Engineering to Pharma
        The concept of a Requirements Traceability Matrix has its roots in **systems engineering and software development**. As systems became more complex in the 1970s and 80s, projects were often plagued by "scope creep" and mismatches between user expectations and the final product. The RTM was developed as a formal project management tool to ensure all requirements were tracked and verified.
        
        Its value was immediately recognized for regulated software, where proof of completeness is non-negotiable. The **GAMP (Good Automated Manufacturing Practice)** Forum, a consortium of pharmaceutical and engineering professionals, formally adopted traceability as a core principle. The RTM became the de facto standard method for achieving and documenting this traceability, making it a cornerstone of modern Computer System Validation (CSV) in the pharmaceutical and medical device industries.
        """)
        
    with tabs[5]:
        st.markdown("""
        The RTM is the primary document used to demonstrate compliance with a variety of regulations governing complex systems.
        - **ICH Q10 - Pharmaceutical Quality System:** This guideline emphasizes a holistic approach to quality management, including the management of outsourced activities and tech transfers. An integrated RTM is the key document for demonstrating control over these complex, multi-faceted projects.
        - **GAMP 5 - A Risk-Based Approach to Compliant GxP Computerized Systems:** The RTM is a foundational document in the GAMP 5 framework, providing the traceability that underpins the entire V-Model validation approach.
        - **FDA 21 CFR 820.30 (Design Controls):** For medical device software, the RTM is the key to demonstrating that all design inputs (user needs) have been met by the design outputs (the software) and that this has been verified through testing. It is a critical component of the Design History File (DHF).
        """)

# SNIPPET: Replace your entire render_gap_analysis_change_control function with this correct version.

def render_gap_analysis_change_control():
    """Renders the comprehensive module for Gap Analysis & Change Control."""
    st.markdown("""
    #### Purpose & Application: The Engine of Continuous Compliance
    **Purpose:** To provide the formal, structured framework for managing the evolution of a validated system. This module combines two essential processes:
    - **Gap Analysis:** A systematic comparison of a system's *current performance* against a *required standard* to identify deficiencies ("gaps").
    - **Change Control:** The formal, documented process for proposing, assessing, and implementing a change to close a gap and return a system to a state of control.
    
    **Strategic Application:** This is the "immune system" of a Quality Management System (QMS). It is the primary mechanism for managing any change, whether it's driven by an audit finding, a process improvement initiative, a system upgrade, or a CAPA. Mastering this workflow is non-negotiable for maintaining a validated state.
    """)
    st.info("""
    **Interactive Demo:** You are the System Owner for a lab's LIMS, preparing for a data integrity audit.
    1.  Use the **"Current State Assessment"** sliders in the sidebar to score your system's current capabilities.
    2.  The **Radar Chart** will instantly visualize the compliance "gaps".
    3.  The **Change Control Form** will auto-populate with a justification based on the largest gap found.
    4.  Fill out and submit the form to see a mock, official Change Control record generated below.
    """)

    if 'cc_record' not in st.session_state:
        st.session_state.cc_record = None

    with st.sidebar:
        st.subheader("Current State Assessment")
        st.markdown("Score your system's current compliance (0=Non-existent, 10=Fully Compliant).")
        
        current_scores = {
            '21 CFR Part 11<br>Compliance': st.slider("21 CFR Part 11", 0, 10, 5),
            'Data Backup &<br>Recovery': st.slider("Data Backup & Recovery", 0, 10, 6),
            'Audit Trail<br>Functionality': st.slider("Audit Trail Functionality", 0, 10, 8),
            'User Access<br>Controls': st.slider("User Access Controls", 0, 10, 7),
            'Electronic<br>Signatures': st.slider("Electronic Signatures", 0, 10, 4)
        }
    
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Gap Analysis Results")
        fig = plot_gap_analysis_radar(current_scores)
        st.plotly_chart(fig, use_container_width=True)
        
        # --- START OF THE FIX ---
        gaps = {cat: 10 - score for cat, score in current_scores.items()}
        
        # First, get the original key with the <br> tag
        original_key = max(gaps, key=gaps.get) if gaps else None
        
        if original_key:
            # Use the original key to get the value
            largest_gap_value = gaps[original_key]
            # NOW, create a clean version for display purposes
            largest_gap_category_display = original_key.replace('<br>', ' ')
            
            if largest_gap_value > 0:
                st.warning(f"**Largest Gap Identified:** A deficiency of **{largest_gap_value} points** was found in **{largest_gap_category_display}**.")
            else:
                st.success("**Fully Compliant:** No gaps were identified in the assessment.")
        else:
            largest_gap_value = 0
            largest_gap_category_display = "None"
            st.success("**Fully Compliant:** No gaps were identified in the assessment.")
        # --- END OF THE FIX ---

    with col2:
        st.subheader("Initiate Change Control")
        with st.form("change_control_form"):
            st.text_input("Change Title", "Upgrade LIMS to Address Data Integrity Gaps")
            st.text_area(
                "Justification for Change",
                f"A formal Gap Analysis was performed in preparation for an upcoming audit. A critical gap of {largest_gap_value} points was identified in the area of {largest_gap_category_display}. This change is required to close this gap and ensure full regulatory compliance.",
                height=150
            )
            st.multiselect(
                "Initial Impact Assessment",
                ["System Software", "Hardware", "Validation Documents", "SOPs & Training", "Data Migration"],
                default=["System Software", "Validation Documents", "SOPs & Training"]
            )
            c1, c2 = st.columns(2)
            s = c1.slider("Initial Risk - Severity", 1, 5, 4, help="Severity of harm if the gap is NOT addressed.")
            o = c2.slider("Initial Risk - Occurrence", 1, 5, 3, help="Likelihood of the gap causing a compliance or operational failure.")
            st.metric("Initial Risk Priority (S x O)", s * o)
            
            submitted = st.form_submit_button("Submit Change Control Request")
            if submitted:
                st.session_state.cc_record = {
                    "Title": "Upgrade LIMS to Address Data Integrity Gaps",
                    "Justification": f"A formal Gap Analysis was performed... A critical gap of {largest_gap_value} points was identified in the area of {largest_gap_category_display}...",
                    "Impact": ["System Software", "Validation Documents", "SOPs & Training"],
                    "Risk": s * o
                }
                st.rerun()

    if st.session_state.cc_record:
        st.divider()
        st.subheader("Generated Change Control Record (CC-2024-042)")
        with st.container(border=True):
            rec = st.session_state.cc_record
            st.markdown(f"**Title:** {rec['Title']}")
            st.markdown(f"**Justification:** {rec['Justification']}")
            st.markdown(f"**Systems Impacted:** {', '.join(rec['Impact'])}")
            st.markdown(f"**Initial Risk Priority:** {rec['Risk']}")
            st.success("‚úÖ **Status:** Submitted and pending QA approval.")
    
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **The Assessment (Sidebar):** The process begins by performing a formal, evidence-based assessment of a system's current state against a defined standard (e.g., a regulation, an industry best practice, or a URS).
        2.  **Visualizing the Gaps (Radar Chart):** The radar chart provides an instant, holistic visualization of the system's strengths and weaknesses. The "gaps" are the areas where the blue "Current State" polygon does not reach the green "Required State" boundary. This is a powerful communication tool for management.
        3.  **The Trigger (Gap Summary):** The analysis automatically identifies the most critical deficiency. This largest gap becomes the primary justification for initiating action.
        4.  **The Formal Action (Change Control):** The Change Control form is the official mechanism for proposing and managing the work required to close the identified gap. It is a formal, auditable process that ensures changes are made in a controlled and documented manner.

        **The Strategic Insight:** This module demonstrates that change should not be chaotic. In a regulated environment, change is a formal, data-driven process. It begins with a quantitative **Gap Analysis** to justify the need for change and is executed through a rigorous **Change Control** process to ensure the change is implemented correctly and does not have unintended consequences.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The "Immune System" of Your Quality System

        #### The Problem: The "Validation Drift"
        A company spends millions of dollars to validate a new manufacturing process and its supporting systems. The process is perfect on Day 1. Over the next two years, small, undocumented changes are made. An engineer "tweaks" a software setting to improve performance. An operator develops a "better" undocumented workaround. A new raw material is introduced that is "mostly the same." The process slowly and silently drifts away from its validated state.

        #### The Impact: The Slow Death of a Validated System
        This uncontrolled change is a cancer within a quality system.
        - **Catastrophic Audit Failure:** An auditor arrives and discovers that the system in operation bears little resemblance to the system described in the validation documents. This is a systemic failure of the Quality Management System (QMS) and can result in severe regulatory action, such as a Warning Letter.
        - **Unexplained Failures:** The process starts experiencing strange, intermittent batch failures. The root cause is impossible to find because no one has a record of the dozens of small changes that have occurred since the initial validation. The process is no longer understood or predictable.
        - **Loss of Control:** The company has lost its "state of control." It is making a product with a process that is no longer validated, a massive compliance and patient safety risk.

        #### The Solution: A Formal, Rigorous Process for Change
        Gap Analysis and Change Control are the **"immune system"** of the QMS, designed to detect and manage change to maintain the health of the validated state.
        1.  **Gap Analysis (The Sensor):** This is the tool that actively looks for problems. It is used proactively before audits or upgrades, and reactively after a deviation, to formally identify any gap between "as-is" and "should-be."
        2.  **Change Control (The Response):** This is the formal, documented immune response. It ensures that any proposed change to close a gap is rigorously assessed for its **impact** and **risk** before it is approved. It guarantees that all associated documents (SOPs, validation plans) are updated, and that the change is verified to be successful.

        #### The Consequences: A State of Perpetual Compliance and Control
        - **Without This:** The validated state is a fragile, temporary condition that will inevitably degrade over time, leading to major compliance and operational crises.
        - **With This:** The company has a robust, auditable system for managing the entire lifecycle of its validated processes. It enables **continuous improvement** without sacrificing **continuous compliance**. It ensures that the system remains in a perpetual state of control, protecting the business from audit failure and protecting patients from the risks of an unmanaged process.
        """)

    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **Gap Analysis:** A method of assessing the differences in performance between a business's information systems or software applications to determine whether business requirements are being met and, if not, what steps should be taken to ensure they are met successfully.
        - **Change Control (or Change Management):** The formal process for requesting, evaluating, approving, implementing, and reviewing changes to a controlled system or document. It is a cornerstone of any GxP Quality Management System (QMS).
        - **Validated State:** The condition of a process or system that has been proven, with a high degree of assurance, to consistently meet its pre-determined specifications and quality attributes. The goal of change control is to maintain this state.
        - **Impact Assessment:** A critical part of a change control record where a cross-functional team evaluates the potential effects of a proposed change on all other parts of the system (e.g., other software modules, validation documents, SOPs, training).
        - **Risk Assessment:** An evaluation of the risks associated with the change itself, and the risks of *not* implementing the change. This informs the priority and rigor of the change control process.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Informal Change"**
An engineer discovers a way to improve a process by tweaking a setting in a validated software system. They make the change because "it's obviously better" and inform the team via email.
- **The Flaw:** This is a catastrophic compliance failure. The system is no longer in its validated state. The change was not assessed for its impact on other parts of the system, and there is no formal, auditable record of what was changed, why it was changed, or who approved it. The validation package is now worthless.""")
        st.success("""üü¢ **THE GOLDEN RULE: No Change is Trivial. If a System is Controlled, All Changes Must Be Controlled.**
A robust quality culture lives by a simple, non-negotiable rule: you cannot informally change a validated system.
1.  **The Trigger:** Any proposed change, no matter how small, must begin with a formal Change Control Request.
2.  **The Assessment:** The change must be formally assessed by a cross-functional team for its impact and risk.
3.  **The Plan:** A plan for implementing and verifying the change must be created and approved.
4.  **The Record:** All of these steps must be documented in the change control record, creating a permanent, auditable history.
This is the only way to ensure that a system remains in a validated state throughout its entire lifecycle.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From IT Helpdesks to the GMP Floor
        The concepts of formal Change Management and Gap Analysis have their roots in two powerful management disciplines that matured in the late 20th century.
        - **IT Service Management (ITSM):** In the 1980s and 90s, as businesses became critically dependent on complex IT systems, the UK government developed the **ITIL (Information Technology Infrastructure Library)** framework. A core pillar of ITIL was **Change Management**. It was created to stop the chaos of well-meaning but uncoordinated changes that would frequently crash critical systems like payroll or email. It introduced the formal process of logging, assessing, and approving all changes to a production IT environment.
        - **Quality Management & The Deming Cycle:** The idea of a structured improvement cycle comes from quality pioneers like **W. Edwards Deming**. His **Plan-Do-Check-Act (PDCA)** cycle is, in essence, a framework for managing change. A **Gap Analysis** is a form of "Check"‚Äîcomparing the current state to the desired state. The **Change Control** process is the formal "Plan" and "Do" to implement the improvement, followed by a final "Act" to confirm its success.

        **The Impact in GxP:** Regulators in the pharmaceutical and medical device industries recognized that the same discipline ITIL brought to server rooms was desperately needed on the GxP manufacturing floor. They adopted the formal principles of Change Control as a cornerstone of a modern Quality Management System. It became the essential mechanism for ensuring that a system, once validated, *remains* in its validated state for its entire lifecycle, fulfilling the promise of continuous compliance.
        """)
        
    with tabs[5]:
        st.markdown("""
        Change Control and Gap Analysis are fundamental, explicitly required components of virtually all GxP quality system regulations.
        - **ICH Q10 - Pharmaceutical Quality System:** Section 3.2.4, "Change Management System," is entirely dedicated to this topic. It requires a formal system for managing all changes, driven by expert teams and based on risk assessment.
        - **FDA 21 CFR 211.100(a) (for Pharma):** Requires that "there shall be written procedures for production and process control... These procedures shall be drafted, reviewed, and approved by the appropriate organizational units and reviewed and approved by the quality control unit... Written production and process control procedures shall be followed... and shall be documented at the time of performance." Change Control is the mechanism for modifying these procedures.
        - **FDA 21 CFR 820.40 (for Medical Devices):** Requires the establishment and maintenance of procedures for "the control of documents," which includes the review and approval of all changes to documents.
        - **21 CFR Part 11 (Electronic Records):** Requires that changes to electronic records must not obscure previously recorded information and must be documented in a secure, computer-generated, time-stamped **audit trail**. This is a specific, technical form of change control for data.
        - **GAMP 5:** Emphasizes that a validated system must be maintained in its validated state throughout its operational life, which is achieved through a robust Change Control process.
        """)

def render_rca_suite():
    """Renders the comprehensive module for Root Cause Analysis."""
    st.markdown("""
    #### Purpose & Application: The Forensic Investigation
    **Purpose:** To provide a structured, systematic framework for conducting a **Root Cause Analysis (RCA)**. This suite combines two classic tools:
    - **Ishikawa (Fishbone) Diagram:** A visual tool for brainstorming and categorizing all potential causes of a problem.
    - **The 5 Whys:** A simple but powerful interrogative technique for drilling down past symptoms to find the true, actionable root cause.
    
    **Strategic Application:** This is the core of any effective **Corrective and Preventive Action (CAPA)** system. When a deviation or failure occurs, these tools transform a chaotic "blame game" into a disciplined, evidence-based investigation.
    """)
    st.info("""
    **Interactive Demo:** You are the Lead Investigator for a deviation.
    1.  Select a **Problem Scenario** to populate the Fishbone diagram with realistic potential causes.
    2.  Use the **5 Whys Workbench** to drill down into a key cause and practice uncovering the true root of the problem.
    """)

    scenario = st.selectbox("Select a Problem Scenario:", ["Batch Contamination", "Low Assay Signal", "Instrument Downtime"])

    cause_templates = {
        "Batch Contamination": {
            "Manpower": ["Improper Gowning", "Training Gap"], "Machine": ["Leaking Valve", "Filter Failure"],
            "Material": ["Contaminated Media", "Wrong Supplier"], "Method": ["Incorrect Sanitization", "Flawed SOP"],
            "Measurement": ["False Negative EM", "Bad Sample"], "Environment": ["HVAC Failure", "Pressure Cascade"]
        },
        "Low Assay Signal": {
            "Manpower": ["Pipetting Error", "Inexperience"], "Machine": ["Faulty Plate Reader", "Clogged Washer"],
            "Material": ["Degraded Antibody", "Wrong Buffer Lot"], "Method": ["Incorrect Incubation", "Wrong Dilution"],
            "Measurement": ["Calibration Drift", "Software Error"], "Environment": ["Lab Too Cold", "Light Exposure"]
        },
        "Instrument Downtime": {
            "Manpower": ["No Trained User", "Operator Error"], "Machine": ["Pump Seal Failure", "Software Crash"],
            "Material": ["Wrong Solvent", "No Spare Parts"], "Method": ["No PM Performed", "Incorrect Startup"],
            "Measurement": ["False Sensor Error", "Connectivity Loss"], "Environment": ["Power Surge", "High Humidity"]
        }
    }
    
    fig = plot_ishikawa_diagram(scenario, cause_templates[scenario])
    st.plotly_chart(fig, use_container_width=True)
    
    st.subheader("The 5 Whys Workbench")
    with st.container(border=True):
        st.markdown("**Problem Statement:** A sterility test for Batch 123 failed.")
        st.text_input("1. Why?", "The sterile filter used during filling was compromised.")
        st.text_input("2. Why?", "The filter was damaged by a pressure spike during startup.")
        st.text_input("3. Why?", "The startup SOP does not specify a gradual pressure ramp.")
        st.text_input("4. Why?", "The SOP was written based on the old pump, which could not ramp quickly.")
        st.text_input("5. Why? (The Process Root Cause)", "The Management of Change (MOC) process failed to trigger an SOP update when the new pump was installed.")
    
    st.divider()
    st.subheader("Deeper Dive into RCA")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Define the Problem (The Head):** The first and most critical step is to create a clear, concise, and agreed-upon problem statement. This becomes the "head" of the fish.
        2.  **Brainstorm Causes (The Bones):** Use the 6M categories (Manpower, Machine, Material, Method, Measurement, Environment) as a structured framework to brainstorm all possible causes, no matter how unlikely. This is a team activity.
        3.  **Circle & Prioritize:** Once the diagram is complete, the team reviews the map and circles the most likely or highest-risk potential causes that warrant deeper investigation.
        4.  **Drill Down (The 5 Whys):** For each high-priority cause, use the 5 Whys technique to drill down past the immediate technical symptom to find the underlying **process or systemic root cause**. Notice in the example how the investigation moves from a technical failure ("bad filter") to a systemic failure ("bad MOC process").

        **The Strategic Insight:** The goal of RCA is not just to find a technical reason for a failure. It is to find the **systemic weakness** that allowed the failure to occur, so that the corrective action can prevent an entire class of future problems. Fixing the filter is a correction; fixing the change control process is a true corrective and preventive action (CAPA).
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: From "Who Did It?" to "Why Did We Let It Happen?"

        #### The Problem: The "Blame Game" Investigation
        A critical, multi-million dollar batch fails. A high-pressure investigation is launched. The immediate focus is on finding a person or event to blame. The investigation becomes a chaotic "blame game," where departments defend their territory and individuals fear for their jobs. The final report identifies a scapegoat ("operator error") and the corrective action is to "retrain the operator."

        #### The Impact: Recurring Failures and a Culture of Fear
        This superficial, blame-oriented approach is a recipe for disaster.
        - **Recurring Failures:** The true systemic root cause (e.g., a poorly designed interface that encourages error, or a flawed SOP) is never addressed. The exact same failure inevitably happens again a few months later with a different operator, costing the company another million dollars.
        - **A Culture of Fear:** When the goal is to find someone to blame, employees become terrified of reporting problems. Small issues are hidden, near-misses are not documented, and the organization loses all of its valuable "early warning" signals. The company is blind to emerging risks.
        - **Ineffective CAPAs:** The corrective actions are weak and ineffective. "Retraining" is a temporary fix for a problem that is fundamentally embedded in the process or system itself.

        #### The Solution: A No-Blame, System-Focused Forensic Process
        A formal Root Cause Analysis (RCA) methodology, using tools like the Ishikawa Diagram and the 5 Whys, transforms the entire process. It is a **structured, no-blame forensic investigation** where the "patient" is the process, not the person. The core principle is that human error is a symptom, not a cause. The goal is to ask: **"What was wrong with our *system* that allowed this error to occur?"**
        1.  **The Ishikawa Diagram** forces a holistic, 360-degree view, ensuring no potential cause is overlooked.
        2.  **The 5 Whys** forces the team to drill down past the immediate symptom to find the broken process or flawed system that is the true, actionable root cause.

        #### The Consequences: Organizational Learning and Permanent Solutions
        - **Without This:** The company is trapped in a costly "Groundhog Day" cycle of recurring failures and ineffective, blame-oriented investigations.
        - **With This:** RCA creates a culture of **psychological safety and organizational learning**. Problems are seen as opportunities to improve the system, not to punish individuals. The resulting Corrective and Preventive Actions (CAPAs) are targeted, effective, and permanent, because they fix the underlying systemic weakness. This not only prevents the recurrence of the same failure but makes the entire operation more robust, resilient, and efficient.
        """)

    with tabs[2]:
        st.markdown("""
        ##### Glossary of RCA & CAPA Terms
        - **RCA (Root Cause Analysis):** A systematic problem-solving method used to identify the underlying causes of an incident or problem. The goal is to find the "root" cause that, if eliminated, would prevent the problem from recurring.
        - **Ishikawa / Fishbone / Cause-and-Effect Diagram:** A visual tool used to categorize and brainstorm the potential causes of a specific effect or problem.
        - **6M Categories:** The standard categories used in a manufacturing Ishikawa diagram: **M**anpower (People), **M**achine (Equipment), **M**aterial, **M**ethod (Process), **M**easurement (Inspection), and **M**other Nature (Environment).
        - **5 Whys:** An interrogative technique used to explore the cause-and-effect relationships underlying a particular problem. The primary goal is to determine the root cause by repeatedly asking the question "Why?".
        - **Symptom:** The immediately observable evidence of a problem (e.g., "the batch failed sterility").
        - **Root Cause:** The fundamental, underlying system or process failure that allowed the symptom to occur (e.g., "the change management process is flawed").
        - **CAPA (Corrective and Preventive Action):** A formal process within a Quality Management System to investigate and correct discrepancies (Corrective Action) and to prevent their recurrence (Preventive Action). A robust RCA is the foundation of an effective CAPA.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: Stopping at the Symptom**
The 5 Whys investigation concludes: "The batch failed because the filter was compromised."
- **The Flaw:** This is not a root cause; it is a restatement of the technical problem. It is a symptom. The corrective action would be to "replace the filter," which does nothing to prevent the next filter from failing in the exact same way.""")
        st.success("""üü¢ **THE GOLDEN RULE: The True Root Cause is Almost Always a Process, Not a Person or a Part**
A world-class RCA investigation relentlessly drills down until it finds a flawed **system or process**.
- **Ask the Litmus Test Question:** For any proposed "root cause," ask: "Could we write a procedure to prevent this from happening again?"
- If the cause is "bad filter," you can't write a procedure for that.
- If the cause is "the SOP is unclear," you **can** write a better procedure.
- If the cause is "the change management process is flawed," you **can** fix that process.
The goal is to find the cause that can be fixed with a **systemic improvement**, not just a local patch.
""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From the Assembly Line to the Boardroom
        The core tools of RCA were developed by the pioneers of the 20th-century quality revolution as they grappled with the complexity of modern manufacturing.
        - **Ishikawa Diagram (1960s):** This diagram was developed by **Kaoru Ishikawa**, a Japanese quality control expert, while working with the Kawasaki shipyards. He was a key figure in the development of Japan's post-war quality movement. He created the diagram as a simple, visual tool to help diverse teams (from engineers to front-line workers) collaboratively brainstorm and structure their thinking during quality improvement initiatives. Its intuitive "fishbone" shape made it easy to understand and use across all levels of an organization.
        - **The 5 Whys (1930s):** This technique was developed by **Sakichi Toyoda**, the founder of Toyota Industries. It was a core component of the scientific, problem-solving mindset he instilled in his company, which later evolved into the legendary **Toyota Production System (TPS)**. The 5 Whys was a simple but profound method for forcing engineers and workers to look beyond the obvious technical failure and find the deeper, often hidden, process-level root cause.
        
        **The Impact:** Together, these tools became foundational elements of Total Quality Management (TQM), Lean Manufacturing, and Six Sigma. They provided a structured, repeatable, and data-driven methodology for problem-solving, replacing the chaotic, blame-oriented investigations of the past. Their adoption by global industries has been a key driver of the dramatic improvements in product quality and reliability over the last 50 years.
        """)
        
    with tabs[5]:
        st.markdown("""
        A formal, documented Root Cause Analysis is a mandatory component of any compliant GxP Quality Management System. The failure to conduct a thorough and timely investigation is one of the most common findings in regulatory audits.
        - **ICH Q10 - Pharmaceutical Quality System:** Section 3.2.2, "CAPA System," explicitly requires "investigation of non-conformances... to determine the root cause." The tools in this suite are the standard methods for fulfilling this requirement.
        - **FDA 21 CFR 211.192 (for Pharma):** This regulation mandates that "any unexplained discrepancy... or the failure of a batch or any of its components to meet any of its specifications shall be **thoroughly investigated**... The investigation shall extend to other batches... that may have been associated with the specific failure or discrepancy. A written record of the investigation shall be made and shall include the conclusions and follow-up."
        - **FDA 21 CFR 820.100 (for Medical Devices):** Requires procedures for implementing corrective and preventive action, which includes "investigating the cause of nonconformities relating to product, processes, and the quality system."
        - **FDA Guidance: "Investigating Out-of-Specification (OOS) Test Results for Pharmaceutical Production":** This guidance outlines the expectation for a timely, thorough, and scientifically sound investigation, which must include a conclusion and determination of the root cause.
        """)
#====================================================================================================================================================================================================================================
#=====================================================================================================ACT 0 RENDER END ==============================================================================================================
#====================================================================================================================================================================================================================================
# ======================================== 1. EXPLORATORY DATA ANALYSIS (EDA)  ===============================================================
def render_eda_dashboard():
    """Renders the comprehensive, interactive module for Exploratory Data Analysis."""
    st.markdown("""
    #### Purpose & Application: The Data Scientist's First Look
    **Purpose:** To perform a thorough **Exploratory Data Analysis (EDA)**. Before any formal modeling or hypothesis testing, EDA is essential to understand the data's structure, identify potential quality issues, and form initial hypotheses.
    
    **Strategic Application:** This is the most critical first step in any data-driven project. Skipping EDA is like a surgeon operating without looking at the patient's chart‚Äîit's professional malpractice. This tool automates the creation of a comprehensive EDA report, allowing a validation leader or scientist to quickly assess the quality of a new dataset and discover hidden relationships that warrant formal investigation.
    """)
    
    st.info("""
    **Interactive Demo:** You are a Data Scientist receiving a new dataset.
    1.  **Select a sample dataset** to simulate a real-world analysis scenario.
    2.  Use the **Interactive Data Gadgets** to deliberately corrupt the data and see how it impacts the plots.
    3.  Review the KPIs and explore the tabs to see if you can detect the problems you introduced.
    """)

    datasets = load_datasets()
    dataset_choice = st.selectbox("Select a Sample Dataset to Analyze:", list(datasets.keys()))
    df_base = datasets[dataset_choice] # Load the pristine, base dataset

    # --- Interactive Data Gadgets Expander ---
    with st.expander("üî¨ Interactive Data Gadgets (Play with the Data!)"):
        st.markdown("Use these controls to deliberately introduce common data quality issues. Observe how they affect the plots below.")
        g_col1, g_col2, g_col3, g_col4 = st.columns(4)
        with g_col1:
            noise_level = st.slider("Add Noise", 0, 10, 0, help="Injects random noise into all numeric variables. Watch how this makes trends in the scatter plots harder to see and increases the spread in the distribution plots.")
        with g_col2:
            categorical_effect = st.slider("Categorical Effect", -10, 10, 0, help="Introduces a systematic bias for one group ('Lot B' or 'Alice'). Watch how this group separates from the others in the 'Group Analysis' box plots.")
        with g_col3:
            outlier_magnitude = st.slider("Inject Outlier", 0, 10, 0, help="Adds a single, extreme outlier to a key variable. Watch for this point appearing far from the others on the box plots and scatter plots.")
        with g_col4:
            missing_pct = st.slider("Inject Missing Data (%)", 0, 20, 0, help="Randomly removes a percentage of data from one column. Watch the 'Missing Values' KPI increase and note how some plots might change.")
    
    # Apply the transformations from the gadgets to the base dataframe
    df = apply_gadget_transformations(df_base, noise_level, missing_pct, outlier_magnitude, categorical_effect)

    st.header("Exploratory Data Analysis Report")
    st.dataframe(df.head())
    
    st.subheader("Data Quality KPIs")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Rows", df.shape[0])
    col2.metric("Columns", df.shape[1])
    missing_values = df.isnull().sum().sum()
    col3.metric("Missing Values", f"{missing_values}", help=f"Total number of empty cells. Found in: {', '.join(df.columns[df.isnull().any()].tolist()) if missing_values > 0 else 'None'}")
    col4.metric("Duplicate Rows", f"{df.duplicated().sum()}")
    
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    cat_cols = df.select_dtypes(exclude=np.number).columns.tolist()

    if numeric_cols:
        st.markdown("### EDA Visualizations")
        corr_method = st.radio("Select Correlation Method for Heatmap:", ('pearson', 'spearman'), horizontal=True,
                                   help="**Pearson:** Measures straight-line (linear) relationships. Sensitive to outliers. **Spearman:** Measures any consistent increasing/decreasing (monotonic) relationship, even if it's curved. Robust to outliers.")
        
        with st.expander("Learn More: Pearson vs. Spearman Correlation"):
            st.markdown("""
            ### Pearson vs. Spearman: The Straight Road and the Winding Trail

            Think of correlation as a way to understand how two process parameters are connected. But not all connections are the same. Choosing the right correlation method is like choosing the right map for your journey.

            #### üó∫Ô∏è The Analogy: A Tale of Two Maps

            *   **Pearson is a GPS Driving Map:** It's incredibly precise and powerful, but it works best when it assumes you can travel in a straight line from A to B. It measures the strength of a **linear** relationship. If the road is perfectly straight, the Pearson GPS is the best tool you can have. But if the road is winding, the GPS will get confused and tell you the destination is "not well correlated" with the start, even if you're consistently getting closer.

            *   **Spearman is a Topographical Hiking Map:** It's more robust and doesn't assume the path is straight. It only cares if you are consistently going **uphill or downhill**. It measures the strength of a **monotonic** relationship. It can tell you with great confidence that a winding mountain trail always leads to the summit, even if the path isn't a straight line. It's less precise about the *shape* of the path but more reliable for detecting the *trend*.

            ---

            ### In-Depth Comparison

            | Feature                    | **Pearson Correlation (r)**                                                                    | **Spearman Correlation (œÅ, rho)**                                                                 |
            | -------------------------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
            | **What It Measures**       | The strength and direction of a **linear** relationship between two continuous variables.        | The strength and direction of a **monotonic** relationship between two variables.                 |
            | **How It Works**           | Calculates the covariance of the two variables divided by the product of their standard deviations, using their **actual values**. | First, it converts the raw data into **ranks**, then calculates the Pearson correlation **on the ranks**. |
            | **Key Assumption**         | The relationship between variables is linear. The data should ideally be normally distributed. | The relationship is monotonic (as X increases, Y consistently increases or decreases, but not necessarily in a straight line). No assumption about data distribution. |
            | **Sensitivity to Outliers**| **Very sensitive.** A single outlier can dramatically skew the result.                          | **Highly robust.** Since it uses ranks, an outlier's exact value doesn't matter, only its position. |
            | **Pros**                   | ‚úÖ More powerful and statistically precise if the assumptions are met.<br>‚úÖ Directly related to the slope of a linear regression. | ‚úÖ Captures non-linear but monotonic relationships.<br>‚úÖ Robust to outliers.<br>‚úÖ Can be used with ordinal data. |
            | **Cons**                   | ‚ùå Can be misleading or completely miss strong non-linear relationships.<br>‚ùå Easily distorted by outliers. | ‚ùå Less powerful than Pearson if the relationship is truly linear.<br>‚ùå Loses information about the magnitude of the values. |

            ---

            ### When to Use Which? A Practical Guide

            This is the most important part. Your choice depends on your data and your question.

            #### Use **Pearson** When:
            *   ‚úÖ **You have visually inspected a scatter plot, and the relationship looks like a straight line.** This is the most important check!
            *   ‚úÖ Your data is continuous and at least approximately normally distributed.
            *   ‚úÖ You have checked for and handled any significant outliers.
            *   ‚úÖ Your ultimate goal is to build a **linear regression model**, as Pearson correlation is a direct measure of the goodness of fit for such a model.

            #### Use **Spearman** When:
            *   ‚úÖ **The relationship is non-linear but consistently increasing or decreasing.** Think of a saturation curve in a bioassay‚Äîit's not a line, but it's monotonic.
            *   ‚úÖ **Your data contains outliers that you cannot or do not want to remove.** Spearman will give you a much more stable and reliable result.
            *   ‚úÖ Your data is **ordinal** (e.g., rankings like "low," "medium," "high") or is not normally distributed.
            *   ‚úÖ You care more about establishing the **existence and direction of a relationship** than you do about its specific linear shape.

            ### The Golden Rule
            **Always visualize your data with a scatter plot first!** The plot is your best guide. If it looks like a straight line, Pearson is your powerful specialist. If it looks like a curve or has weird points, Spearman is your robust and trustworthy generalist.
            """)
        
        # Generate ALL figures at once using the selected method and the MODIFIED dataframe.
        figs = plot_eda_dashboard(df, tuple(numeric_cols), tuple(cat_cols), corr_method)
        
        eda_tabs = st.tabs(["üìä Relationships", "üìà Distributions", "üóÇÔ∏è Group Analysis"])

        with eda_tabs[0]:
            st.plotly_chart(figs['heatmap'], use_container_width=True)
            st.plotly_chart(figs['pairplot'], use_container_width=True)
        
        with eda_tabs[1]:
            st.plotly_chart(figs['distributions'], use_container_width=True)

        with eda_tabs[2]:
            if cat_cols:
                st.plotly_chart(figs['categorical'], use_container_width=True)
            else:
                st.warning("No categorical variables were found in this dataset to perform a group analysis.")

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Check Data Quality First:** The KPIs at the top are your first stop. A high number of **Missing Values** or **Duplicate Rows** signals a problem with data collection or integrity that must be fixed before any analysis can be trusted.
        2.  **Understand Distributions (Histograms):** Review the univariate plots. Are the data normally distributed, or are they skewed? Are there potential outliers? This informs which statistical tests will be appropriate.
        3.  **Find the Strongest Relationships (Heatmap):** The correlation heatmap is your guide to what matters most. Bright red (strong positive correlation) or bright blue (strong negative correlation) cells highlight the most powerful relationships in your process, which should be investigated further with formal tools like DOE or Regression.
        4.  **Visualize the Interactions (Pair Plot):** This is the most powerful plot. It shows every bivariate relationship in one graphic. Look for clear trends between variables. If you color by a categorical variable (like `Raw_Material_Lot`), you can often spot group differences, such as Lot A consistently producing higher yields.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: Preventing "Garbage In, Gospel Out"
    
        #### The Problem: The "Blind Analysis" Fallacy
        An analyst receives a new, critical dataset from a series of development runs or a tech transfer. Eager to provide answers, they immediately import the data into a sophisticated statistical software package, run a complex model (like an ANOVA or a Machine Learning algorithm), and present the model's p-values and predictions to management as definitive truth.
    
        #### The Impact: Dangerously Misleading, Data-Driven Disasters
        This is professional malpractice and a major business risk. The analyst never performed a basic "physical exam" on the data. The dataset was riddled with hidden problems:
        - **Missing values** from a faulty sensor were interpreted as zeros, skewing all calculations.
        - **Extreme outliers** from a simple data entry error were treated as real process events, creating false correlations.
        - **Duplicate rows** from a database glitch made the sample size appear larger than it was, leading to artificially small p-values.
        
        The sophisticated model, trying its best, interpreted this "garbage" as real process behavior. The resulting conclusions are not just wrong; they are dangerously misleading and could lead to the company investing millions of dollars in fixing the wrong problem, changing the wrong specification, or launching a product based on a complete fiction.
    
        #### The Solution: The Data Scientist's Mandatory First Step
        Exploratory Data Analysis (EDA) is the mandatory, non-negotiable first step in any serious data analysis. It is the disciplined process of using simple visual and statistical tools to **interrogate the quality and structure of the data itself** before attempting to draw conclusions from it. It's about building a relationship with the data and understanding its story, its flaws, and its secrets before you ask it to make a high-stakes decision.
    
        #### The Consequences: A Solid Foundation for High-Stakes Decisions
        - **Without This:** Any subsequent analysis is built on a foundation of sand. The entire data-driven decision-making process is a house of cards, and the company is at constant risk of making major strategic blunders based on flawed data. This is the definition of **"Garbage In, Gospel Out."**
        - **With This:** EDA provides confidence in the quality and integrity of the data. It identifies and allows for the correction of data quality issues at the source. Most importantly, it generates the initial, high-value insights that guide all subsequent, more complex analyses, ensuring that the company's most critical decisions are based on a solid, trustworthy foundation of fact.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of EDA Terms
        - **EDA (Exploratory Data Analysis):** An approach to analyzing datasets to summarize their main characteristics, often with visual methods.
        - **Data Quality:** The state of data regarding its accuracy, completeness, consistency, and reliability.
        - **Univariate Analysis:** The analysis of a single variable at a time (e.g., a histogram).
        - **Bivariate Analysis:** The analysis of the relationship between two variables (e.g., a scatter plot).
        - **Correlation:** A statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate).
        - **Outlier:** A data point that differs significantly from other observations.
        - **Missing Values:** Data points for which no value is stored (often represented as `NaN`).
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: "Garbage In, Gospel Out"**
An analyst receives a new dataset, immediately feeds it into a sophisticated machine learning model, and presents the model's predictions as truth.
- **The Flaw:** They never checked the data quality. The dataset was riddled with missing values and outliers, which the model interpreted as real patterns. The resulting predictions are statistically invalid and dangerously misleading. This is the definition of 'Garbage In, Garbage Out.'""")
        st.success("""üü¢ **THE GOLDEN RULE: Trust, but Verify Your Data**
Before performing any formal statistical analysis or building any model, you must first get to know your data.
1.  **Inspect for Quality:** Always begin by checking for fundamental issues like missing values, duplicates, and obvious errors.
2.  **Visualize the Big Picture:** Use tools like correlation heatmaps and pair plots to understand the overall structure and key relationships in your data.
3.  **Formulate Hypotheses:** Use the insights from EDA to form specific, testable hypotheses. For example, "It appears that Purity is negatively correlated with Temperature. Let's design a formal DOE to confirm this causal link."
EDA is the step that turns raw data into actionable scientific inquiry.""")
        
    with tabs[4]:
        st.markdown("""
        #### Historical Context: The Father of EDA
        While data visualization has existed for centuries, the formal discipline of **Exploratory Data Analysis (EDA)** was single-handedly championed by the brilliant American mathematician **John Tukey** in the 1970s. Tukey, a contemporary of the great quality gurus, argued that traditional statistics had become too focused on "confirmatory" analysis (hypothesis testing) and had neglected the critical first step of "exploratory" analysis.
        
        He believed that analysts should act as "data detectives," using graphical methods to uncover the hidden stories in their data. He invented several of the core visualization tools we use today, including the **box plot** and the **stem-and-leaf plot**. His 1977 book, *Exploratory Data Analysis*, is a classic that liberated statisticians from rigid formalism and encouraged a more intuitive, interactive, and curiosity-driven approach to data. With modern tools like Python and Plotly, we can now automate the powerful, interactive visualizations that Tukey could only dream of.
        """)
        
    with tabs[5]:
        st.markdown("""
        While EDA is an exploratory activity, it is a critical prerequisite for many formal validation activities and is implicitly required by several regulations.
        - **FDA Guidance on Process Validation (Stage 1 - Process Design):** The guidance states that process knowledge and understanding should be built upon a foundation of "development studies." EDA is the first step in analyzing the data from these studies to build that foundational understanding.
        - **Data Integrity (ALCOA+):** A core principle of data integrity is that data must be **Complete** and **Accurate**. The Data Quality KPIs in this dashboard are a direct check on these principles. An EDA report is often a key part of the evidence package for a new dataset, demonstrating that the data has been reviewed for quality before being used in formal GxP analysis.
        - **ICH Q9 (Quality Risk Management):** EDA is a powerful tool for risk identification. Discovering a strong, unexpected correlation in your data during EDA can highlight a previously unknown process risk that needs to be formally assessed with a tool like FMEA.
        """)

# ======================================== 2. CONFIDENCE INTERVAL CONCEPT ===============================================================
def render_ci_concept():
    """Renders the interactive module for Confidence Intervals."""
    st.markdown("""
    #### Purpose & Application: The Foundation of Inference
    **Purpose:** To build a deep, intuitive understanding of the fundamental concept of a **frequentist confidence interval** and to correctly interpret what it does‚Äîand does not‚Äîtell us.
    
    **Strategic Application:** This concept is the bedrock of all statistical inference in a frequentist framework. A misunderstanding of CIs leads to flawed conclusions and poor decision-making. This interactive simulation directly impacts resource planning and risk assessment. It allows scientists and managers to explore the crucial trade-off between **sample size (cost)** and **statistical precision (certainty)**. It provides a visual, data-driven answer to the perpetual question: "How many samples do we *really* need to run to get a reliable result and an acceptably narrow confidence interval?"
    """)
    
    st.info("""
    **Interactive Demo:** Use the **Sample Size (n)** slider below to dynamically change the number of samples in each simulated experiment. Observe how increasing the sample size dramatically narrows both the theoretical Sampling Distribution (orange curve) and the simulated Confidence Intervals (blue/red lines), directly demonstrating the link between sample size and precision.
    """)

    st.sidebar.subheader("Confidence Interval Controls")
    n_slider = st.sidebar.slider("Select Sample Size (n) for Each Simulated Experiment:", 5, 100, 30, 5,
        help="Controls the number of data points in each of the 100 simulated experiments. Notice the dramatic effect on the precision of the results.")
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        fig1_ci, fig2_ci, capture_count, n_sims, avg_width = plot_ci_concept(n=n_slider)
        st.plotly_chart(fig1_ci, use_container_width=True)
        st.plotly_chart(fig2_ci, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.metric(label=f"üìà KPI: Average CI Width (Precision) at n={n_slider}", value=f"{avg_width:.2f} units", help="A smaller width indicates higher precision. This is inversely proportional to the square root of n.")
        st.metric(label="üéØ Empirical Coverage Rate", value=f"{(capture_count/n_sims):.1%}", help=f"The % of our {n_sims} simulated CIs that successfully 'captured' the true population mean. Should be close to 95%.")
        st.markdown("""
        - **Theoretical Universe (Top Plot):**
            - The wide, light blue curve is the **true population distribution**. In real life, we *never* see this.
            - The narrow, orange curve is the **sampling distribution of the mean**. Its narrowness, guaranteed by the **Central Limit Theorem**, makes inference possible.
        - **CI Simulation (Bottom Plot):** This shows the reality we live in. We only get *one* experiment and *one* confidence interval.
        - **The n-slider is key:** As you increase `n`, the orange curve gets narrower and the CIs in the bottom plot become dramatically shorter.
        - **Diminishing Returns:** The gain in precision from n=5 to n=20 is huge. The gain from n=80 to n=100 is much smaller. This illustrates that to double your precision (halve the CI width), you must **quadruple** your sample size.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: Quantifying Uncertainty to Manage Risk
    
        #### The Problem: The Tyranny of a Single Number
        A QC manager reports that the average potency of a critical validation batch is "101.2%." A manufacturing manager reports that the average process yield was "87.3%." In most organizations, these single numbers (point estimates) are treated as absolute, infallible truth. All subsequent decisions‚Äîto release the batch, to approve the process‚Äîare based on the assumption that these numbers are perfect.
    
        #### The Impact: Poor, Risk-Blind Decisions
        Making high-stakes financial and clinical decisions based on a single number is a dangerous gamble. The number is an illusion of certainty that hides the true operational risk.
        - **False Confidence:** A batch with an average potency of 101.2% seems perfectly safe against a lower specification of 95.0%. But what if that average was based on a small, noisy sample, and the 95% confidence interval for the true mean is actually **[94.5%, 107.9%]**? This means there is a plausible chance the entire batch is actually sub-potent. Releasing this batch is a major compliance and patient safety risk.
        - **Missed Opportunities:** A pilot batch has an average yield of 84%, just missing the 85% target. The project is cancelled. However, the 95% confidence interval was **[82%, 86%]**. This means there was a plausible chance the process was already meeting its target, and the project was killed based on statistical noise, not a true failure.
    
        #### The Solution: From a Point Estimate to a Plausible Range
        The Confidence Interval is the tool that transforms a single, unreliable point estimate into a **plausible range of values**. It provides a clear, quantitative statement of the uncertainty associated with any measurement, driven by sample size and process variability. It forces a more honest and realistic conversation, changing the question from "What is the number?" to **"What is the reliable range for the true value, and what is the risk associated with that range?"**
    
        #### The Consequences: Data-Driven Risk Management and Efficient Experimentation
        - **Without This:** Decisions are based on a false sense of certainty, leading to unmanaged risk and missed opportunities.
        - **With This:** The Confidence Interval becomes a core part of the decision-making process. It allows managers to see the "error bars" around every critical metric, enabling them to make truly informed, risk-based decisions about batch release, process adjustments, and resource allocation. It also provides a direct, quantitative answer to the question "How many samples do we need to run to be sufficiently certain?", allowing for efficient and cost-effective experimental design.
        """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Core Concepts
            - **Population:** The entire group that you want to draw conclusions about (e.g., all possible measurements from a process). In reality, the true population parameters (like the mean) are unknown.
            - **Sample:** A specific group of individuals that you will collect data from (e.g., the 30 measurements you took today).
            - **Sampling Distribution:** The theoretical probability distribution of a statistic (like the sample mean) obtained through a large number of samples drawn from a specific population.
            - **Standard Error:** The standard deviation of the sampling distribution. It measures the precision of the sample statistic as an estimate of the population parameter.
            - **Confidence Level:** The percentage of all possible samples that can be expected to include the true population parameter within the calculated interval (e.g., 95%). This is a property of the *procedure*, not a single interval.
            """)
        with tabs[3]:
            st.error("""
            üî¥ **THE INCORRECT (Bayesian) INTERPRETATION:**
            *"Based on my sample, there is a 95% probability that the true mean is in this interval [X, Y]."*
            
            This is wrong because in the frequentist view, the true mean is a fixed, unknown constant. It is either in our specific interval or it is not. The probability is 1 or 0.
            """)
            st.success("""
            üü¢ **THE CORRECT (Frequentist) INTERPRETATION:**
            *"We are 95% confident that the interval [X, Y] contains the true mean."*
            
            The full meaning is: *"This interval was constructed using a procedure that, when repeated many times, will produce intervals that capture the true mean 95% of the time."* Our confidence is in the **procedure**, not in any single outcome.
            """)
        with tabs[4]:
            st.markdown("""
            #### Historical Context: The Great Debate
            **The Problem:** In the early 20th century, the field of statistics was in turmoil. The giant of the field, **Sir Ronald A. Fisher**, had developed a concept called "fiducial inference" to create intervals, but it was complex and controversial. A new school of thought, led by **Jerzy Neyman** and **Egon Pearson**, was emerging, focused on a more rigorous, decision-theoretic framework. They needed a way to define an interval estimate that was objective, mathematically sound, and had a clear, long-run performance guarantee.

            **The 'Aha!' Moment:** In a landmark 1937 paper, Neyman introduced the **confidence interval**. His revolutionary idea was to shift the probabilistic statement away from the fixed, unknown parameter (which a frequentist believes has no probability distribution) and onto the **procedure used to create the interval**. He defined the "95% confidence" not as a statement about a single interval, but as a guarantee about the long-run success rate of the method used to generate it.
            
            **The Impact:** This clever reframing was a triumph of the Neyman-Pearson school. It provided a practical and logically consistent solution that was easy to compute and understand (even if often misinterpreted!). Fisher fiercely debated against it for the rest of his life, but Neyman's confidence interval won out, becoming the dominant and most widely taught paradigm for interval estimation in the world. It is the bedrock on which most of the statistical tests in this toolkit are built.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("The general form for a two-sided confidence interval is:")
            st.latex(r"\text{Point Estimate} \pm (\text{Critical Value} \times \text{Standard Error})")
            st.markdown("""
            - **Point Estimate:** Our single best guess for the population parameter, calculated from the sample (e.g., the sample mean, `xÃÑ`).
            - **Standard Error:** The standard deviation of the sampling distribution of the point estimate (e.g., `s/‚àön`). It measures the typical error we expect in our point estimate due to random sampling.
            - **Critical Value:** A multiplier determined by our desired confidence level and the underlying distribution. For a CI for the mean with an unknown population standard deviation, this is a t-score from the t-distribution with `n-1` degrees of freedom.
            For a 95% CI for the mean, the formula is:
            """)
            st.latex(r"\bar{x} \pm t_{(0.975, n-1)} \cdot \frac{s}{\sqrt{n}}")
        with tabs[5]:
            st.markdown("""
            While not a standalone requirement, the correct application and interpretation of confidence intervals are a **foundational statistical principle** that underpins compliance with numerous guidelines.
            - **ICH Q2(R1) - Validation of Analytical Procedures:** Used to establish confidence intervals for key parameters like the slope and intercept in linearity studies.
            - **FDA Process Validation Guidance:** Used to set confidence bounds on process parameters and quality attributes during Process Performance Qualification (PPQ).
            - **21 CFR Part 211:** Implicitly required for demonstrating statistical control and for the "appropriate statistical quality control criteria" mentioned in ¬ß211.165.
            """)
# ======================================== 3. CONFIDENCE INTERVALS FOR PROPORTIONS ===============================================================
def render_proportion_cis():
    """Renders the comprehensive, interactive module for comparing binomial confidence intervals."""
    st.markdown("""
    #### Purpose & Application: Choosing the Right Statistical Ruler for Pass/Fail Data
    **Purpose:** To compare and contrast different statistical methods for calculating a confidence interval for pass/fail (binomial) data. This tool demonstrates that the choice of statistical method is not trivial and can have a significant impact on the final conclusion, especially in common validation scenarios with high success rates and limited sample sizes.
    
    **Strategic Application:** This is a critical decision point when writing a validation protocol or a statistical analysis plan. When you must prove that a process meets a high reliability target (e.g., >99% success rate) based on a limited sample, the statistical interval you choose determines your ability to make that claim. Using an overly conservative interval (like Clopper-Pearson) may require a much larger sample size, increasing project costs, while using an unreliable one (like the classic Wald interval) can lead to a false sense of confidence and significant compliance risk.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Validation Scientist.
    1.  Use the top two sliders to simulate a validation run with a specific number of samples and successes.
    2.  Pay close attention to the **"Failure Scenarios"**‚Äîwhat happens when you have **few samples** and **zero or one failures**. This is where the methods differ most dramatically.
    3.  Use the **Bayesian Prior** sliders to see how prior knowledge (e.g., from R&D) can be formally incorporated to produce a more informed interval.
    """)

    with st.sidebar:
        st.subheader("Confidence Interval Controls")
        st.markdown("**Experimental Results**")
        n_samples_slider = st.slider("Number of Validation Samples (n)", 10, 200, 50, 5, help="The total number of samples tested in your validation run. Note how interval widths shrink as you increase n.")
        n_failures_slider = st.slider("Number of Failures Observed", 0, n_samples_slider, 1, 1, help="The number of non-conforming or failing results. Scenarios with 0 or 1 failures are common and where the choice of CI method is most critical.")
        n_successes = n_samples_slider - n_failures_slider
        
        st.markdown("**Bayesian Prior Belief**")
        st.write("Simulate prior knowledge (e.g., from R&D studies).")
        prior_successes = st.slider("Prior Successes (Œ±)", 0, 100, 10, 1, help="The number of successes in your 'imaginary' prior data. A higher number represents a stronger prior belief in a high success rate.")
        prior_failures = st.slider("Prior Failures (Œ≤)", 0, 100, 1, 1, help="The number of failures in your 'imaginary' prior data. Even a small number here can make the model more conservative.")

    fig, metrics = plot_proportion_cis(n_samples_slider, n_successes, prior_successes, prior_failures)
    
    col1, col2 = st.columns([0.6, 0.4])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        st.subheader("Key Interval Results")
        st.metric("Observed Success Rate", f"{n_successes/n_samples_slider if n_samples_slider > 0 else 0:.2%}")
        st.markdown(f"**Wilson Score CI:** `[{metrics['Wilson Score'][0]:.3f}, {metrics['Wilson Score'][1]:.3f}]`")
        st.markdown(f"**Clopper-Pearson (Exact) CI:** `[{metrics['Clopper‚ÄìPearson (Exact)'][0]:.3f}, {metrics['Clopper‚ÄìPearson (Exact)'][1]:.3f}]`")
        st.markdown(f"**Custom Bayesian CI:** `[{metrics['Bayesian with Custom Prior'][0]:.3f}, {metrics['Bayesian with Custom Prior'][1]:.3f}]`")

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **Interpreting the Comparison:**
        - **The Wald Interval's Failure:** Set the "Number of Failures" to 0. Notice that the Wald interval (red) collapses to a width of zero. This is a nonsensical result that falsely claims perfect certainty from a finite sample. This is why it is blacklisted in modern statistical practice.
        - **Conservatism vs. Accuracy:** The **Clopper-Pearson** interval is often the widest. It is guaranteed to meet the 95% confidence level, but this guarantee makes it conservative (less powerful). The **Wilson Score** interval is slightly narrower and has better average performance, making it a common "best practice" choice for frequentist analysis.
        - **The Power of Priors:** Adjust the **Bayesian Prior** sliders. If you have a strong prior belief in a high success rate (e.g., 99 successes, 1 failure), notice how the "Bayesian with Custom Prior" interval is "pulled" towards that high rate, resulting in a higher lower bound than other methods. This can be a powerful way to reduce sample sizes, provided the prior is well-justified.
        
        **The Strategic Insight:** The choice of interval method directly impacts your ability to meet a pre-defined acceptance criterion. For a result of 49/50 successes (98%), the lower bound of the Wilson interval is 0.888. If your acceptance criterion is ">90% success," you fail. But for the same data, if you had a strong prior, the Bayesian lower bound might be >0.90, allowing you to pass.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The High Cost of a Bad Statistical Ruler
    
        #### The Problem: The "Zero Failures" Paradox
        A team runs a critical validation study on 50 samples to demonstrate that a process is highly reliable (e.g., >95% success rate). The results are perfect: 50 successes, 0 failures. The team celebrates, assuming they have easily passed. They use a simple, textbook statistical method (the Wald interval) to calculate the confidence interval, which nonsensically reports `[100.0%, 100.0%]`. They submit this to regulators, claiming absolute certainty of perfection.
    
        #### The Impact: Regulatory Rejection and Unnecessary Costs
        This seemingly trivial choice of statistical method has severe business consequences:
        1.  **Regulatory Rejection:** An experienced regulator or auditor will immediately reject the claim of 100% certainty from a finite sample. The use of the flawed Wald interval signals a lack of statistical maturity, calling the entire submission into question and potentially delaying product approval.
        2.  **Overly Conservative Design:** Fearing rejection, another team might use the ultra-conservative Clopper-Pearson interval. For the same 50/50 result, this interval is `[92.9%, 100.0%]`. If their acceptance criterion was "demonstrate >95% success with 95% confidence," they now believe they have failed, even with perfect results. This might trigger them to run a much larger, more expensive confirmation study that was completely unnecessary.
    
        #### The Solution: A Risk-Based Choice of Statistical Tool
        The choice of a confidence interval method is not merely academic; it is a **risk-based decision with direct financial implications**. This tool allows a team to compare different valid "statistical rulers" and understand their trade-offs *before* writing the validation plan.
        - The **Wilson Score** interval provides an accurate, industry-standard baseline.
        - The **Bayesian** interval provides a powerful, justifiable method to formally incorporate prior knowledge from development studies, potentially allowing for smaller, more efficient validation studies without sacrificing statistical rigor.
    
        #### The Consequences: A Defensible, Cost-Effective Validation Strategy
        - **Without This:** Teams either make indefensible statistical claims that risk regulatory rejection or they default to overly conservative methods that lead to unnecessarily large and expensive validation studies.
        - **With This:** The validation team can prospectively choose and justify the most appropriate statistical method for their specific situation. They can confidently defend their approach to auditors and design studies that are both statistically robust and economically efficient, saving time and money while ensuring compliance.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of CI Methods for Proportions
        - **Wald Interval:** The simplest method, based on the normal approximation. **Known to perform very poorly** with small `n` or extreme proportions and should be avoided in GxP settings.
        - **Wilson Score Interval:** A more complex method also based on the normal approximation, but it inverts the score test, giving it excellent performance across all conditions. Often the recommended default for frequentist analysis.
        - **Agresti‚ÄìCoull Interval:** A simplified version of the Wilson interval that is easier to compute by hand (it adds 2 successes and 2 failures before calculating a Wald interval). Performs similarly to Wilson but is slightly more conservative.
        - **Clopper‚ÄìPearson (Exact) Interval:** A method based directly on the binomial distribution. It guarantees that the true coverage will be *at least* 95%, but is often too wide (conservative), making it harder to pass acceptance criteria.
        - **Jeffreys Interval (Bayesian):** A Bayesian credible interval using a non-informative prior (`Beta(0.5, 0.5)`). It has excellent frequentist properties and is a good choice when no prior knowledge is available.
        - **Bayesian Credible Interval:** An interval derived from the posterior distribution. It represents a range where there is a 95% probability that the true parameter lies. Its location and width are influenced by both the data and the chosen prior.
        - **Bootstrapped CI:** A computational method that simulates thousands of new datasets by resampling from the original data. It does not rely on statistical assumptions, but can be unstable with very small sample sizes.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Textbook Default" Fallacy**
An analyst uses the simple Wald interval because it's the first one taught in many introductory statistics courses. When validating a process with a 100% success rate in 50 samples (50/50), the Wald interval is `[1.0, 1.0]`, leading them to claim with 95% confidence that the true success rate is exactly 100%.
- **The Flaw:** This is a statistically indefensible claim of absolute certainty from a finite sample. The Wilson Score interval for the same data is `[0.93, 1.0]`, which correctly communicates that the true rate could plausibly be as low as 93%.""")
        st.success("""üü¢ **THE GOLDEN RULE: Match the Method to the Risk and Justify It**
The choice of confidence interval method is a risk-based decision that must be pre-specified and justified in the validation protocol.
1.  **For General Use (Frequentist):** The **Wilson Score interval** is the recommended default, providing the best balance of accuracy and interval width.
2.  **For Absolute Guarantee:** When you absolutely must guarantee that your confidence level is not underestimated (e.g., for a critical safety claim), the **Clopper-Pearson (Exact) interval** is the most conservative and defensible choice.
3.  **When Prior Data Exists:** When you have strong, justifiable prior knowledge (e.g., from extensive R&D data), a **Bayesian credible interval** is the most powerful and efficient approach, but the prior must be explicitly defined and justified in the protocol.
**Never use the Wald interval in a formal validation report.**""")
    with tabs[4]:
        st.markdown("""
        #### Historical Context: Correcting a Century-Old Problem
        The problem of estimating an interval for a proportion seems simple, but its history is complex. The standard **Wald interval**, based on the work of Abraham Wald in the 1930s, was easy to teach and compute, so it became the default method in textbooks for decades. However, its poor performance was well-known to statisticians. A famous 1998 paper by Brown, Cai, and DasGupta, titled "Interval Estimation for a Binomial Proportion," systematically exposed the severe flaws of the Wald interval to a wider audience, calling it "persistently chaotic."
        
        The irony is that the superior solutions were much older. The **Wilson Score interval** was developed by Edwin Bidwell Wilson in **1927**, and the **Clopper-Pearson interval** was developed in **1934**. For much of the 20th century, these more accurate but computationally intensive methods were overlooked in favor of the simpler Wald interval.
        
        The "rediscovery" of these superior methods in the 1990s, driven by increased computing power and influential papers like Brown et al.'s, led to a major shift in statistical practice. Today, modern statistical software and guidelines strongly advocate for the use of Wilson, Clopper-Pearson, or other improved methods, and the simple Wald interval is largely considered obsolete for serious analysis.
        """)
    with tabs[5]:
        st.markdown("""
        The calculation of a statistically valid confidence interval for a proportion is a fundamental requirement in many validation activities where the outcome is binary (pass/fail, concordant/discordant, etc.).
        - **FDA Process Validation Guidance (Stage 2 - PPQ):** When validating a process attribute that is pass/fail (e.g., visual inspection for cosmetic defects), a confidence interval on the pass rate is used to demonstrate that the process can consistently produce conforming product. Using a robust interval is critical for making a high-confidence claim.
        - **Analytical Method Validation (ICH Q2):** For qualitative assays (e.g., a limit test), validation requires demonstrating a high rate of correct detections. For concordance studies comparing a new method to a reference, a confidence interval on the concordance rate is a key performance metric.
        - **21 CFR 820.250 (Statistical Techniques):** This regulation for medical devices explicitly requires that "Where appropriate, each manufacturer shall establish and maintain procedures for identifying valid statistical techniques..." Using a robust interval like the Wilson Score instead of the flawed Wald interval is a direct fulfillment of this requirement.
        """)
# ==================================================================================== 4. CORE VALIDATION PARAMETERS ===============================================================
def render_core_validation_params():
    """Renders the INTERACTIVE module for core validation parameters."""
    st.markdown("""
    #### Purpose & Application
    **Purpose:** To formally establish the fundamental performance characteristics of an analytical method as required by global regulatory guidelines like ICH Q2(R1). This module deconstructs the "big three" pillars of method validation:
    - **üéØ Accuracy (Bias):** How close are your measurements to the *real* value?
    - **üèπ Precision (Random Error):** How consistent are your measurements with each other?
    - **üî¨ Specificity (Selectivity):** Can your method find the target analyte in a crowded room, ignoring all the imposters?

    **Strategic Application:** These parameters are the non-negotiable pillars of any formal assay validation report. A weakness in any of these three areas is a critical deficiency that can lead to rejected submissions or flawed R&D conclusions. **Use the sliders in the sidebar to simulate different error types and see their impact on the plots.**
    """)
    
    st.info("""
    **Interactive Demo:** Now, when you navigate to the "Core Validation Parameters" tool, you will see a new set of dedicated sliders below. Changing these sliders will instantly update the three plots, allowing you to build a powerful, hands-on intuition for these critical concepts.
    """)
    
    # --- Sidebar controls for this specific module ---
    with st.sidebar:
        st.subheader("Core Validation Controls")
        bias_slider = st.slider(
            "üéØ Systematic Bias (%)", 
            min_value=-10.0, max_value=10.0, value=1.5, step=0.5,
            help="Simulates a constant positive or negative bias in the accuracy study. Watch the box plots shift."
    )
        repeat_cv_slider = st.slider(
            "üèπ Repeatability %CV", 
            min_value=0.5, max_value=10.0, value=1.5, step=0.5,
            help="Simulates the best-case random error (intra-assay precision). Watch the 'Repeatability' violin width."
    )
    # Ensure intermediate precision is always worse than or equal to repeatability
        intermed_cv_slider = st.slider(
            "üèπ Intermediate Precision %CV", 
            min_value=repeat_cv_slider, max_value=20.0, value=max(2.5, repeat_cv_slider), step=0.5,
            help="Simulates real-world random error (inter-assay). A large gap from repeatability indicates poor robustness."
    )
        interference_slider = st.slider(
            "üî¨ Interference Effect (%)", 
            min_value=-20.0, max_value=20.0, value=8.0, step=1.0,
            help="Simulates an interferent that falsely increases (+) or decreases (-) the analyte signal."
    )
    
    # Generate plots using the slider values
    fig1, fig2, fig3 = plot_core_validation_params(
        bias_pct=bias_slider, 
        repeat_cv=repeat_cv_slider, 
        intermed_cv=intermed_cv_slider, 
        interference_effect=interference_slider
    )
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig1, use_container_width=True)
        st.plotly_chart(fig2, use_container_width=True)
        st.plotly_chart(fig3, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.info("Play with the sliders in the sidebar to see how different sources of error affect the results!")
        st.markdown("""
        - **Accuracy Plot:** As you increase the **Systematic Bias** slider, watch the center of the box plots drift away from the black 'True Value' lines. This visually demonstrates what bias looks like.
        
        - **Precision Plot:** The **%CV sliders** control the width (spread) of the violin plots. Notice that Intermediate Precision must always be equal to or worse (wider) than Repeatability. A large gap between the two signals that the method is not robust to day-to-day changes.
        
        - **Specificity Plot:** The **Interference Effect** slider directly moves the 'Analyte + Interferent' box plot. A perfect assay would have this slider at 0%, making the two boxes identical. A large effect, positive or negative, indicates a failed specificity study.
    
        **The Core Strategic Insight:** This simulation shows that validation is a process of hunting for and quantifying different types of error. Accuracy is about finding *bias*, Precision is about characterizing *random error*, and Specificity is about eliminating *interference error*.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: Deconstructing and Mitigating Measurement Risk
    
        #### The Problem: The "Black Box" Measurement
        A manufacturing process relies on a key analytical method to release a multi-million dollar batch of product. The organization treats the method as a "black box"‚Äîthey put a sample in and get a number out. They have no systematic, quantitative understanding of the method's inherent error modes. Is it biased? Is it noisy? Is it susceptible to interference from a new raw material lot? They don't know.
    
        #### The Impact: The High Cost of Unreliable Data
        Operating with an uncharacterized measurement system is a massive, unmanaged business risk that creates costly and recurring problems:
        - **False Failures (Producer's Risk):** An imprecise (high %CV) method can generate a random result that falls outside the specification limits, even for a perfectly good batch. This triggers a costly OOS investigation and can lead to the **rejection of perfectly good product**, a direct hit to the bottom line.
        - **False Passes (Consumer's/Patient's Risk):** A biased (inaccurate) method might consistently under-report an impurity. This can lead to the **release of a non-conforming batch**, creating a significant patient safety issue and the risk of a product recall.
        - **Inconclusive Investigations:** When a process deviation occurs, no one can be sure if the problem is in the manufacturing process or in the measurement method itself. This ambiguity makes root cause analysis slow, inefficient, and often inconclusive.
    
        #### The Solution: A Systematic Interrogation of Error
        The validation of core parameters is the systematic process of "opening the black box" and interrogating the three fundamental types of measurement error.
        1.  **The Accuracy study** hunts for **systematic bias**. It asks: "On average, are we hitting the bullseye?"
        2.  **The Precision study** hunts for **random error**. It asks: "How tight is our shot group?" and "Does the shot group get wider when different people shoot?"
        3.  **The Specificity study** hunts for **interference error**. It asks: "Are we hitting the right target, or is something else deflecting our shots?"
    
        #### The Consequences: Trustworthy Data and Efficient Operations
        - **Without This:** The organization is flying blind. All decisions based on analytical data are built on an unknown level of uncertainty.
        - **With This:** The validation report provides a quantitative "spec sheet" for the measurement method itself. It provides **objective evidence** that the data generated by the lab is trustworthy and reliable. This builds confidence in batch release decisions, streamlines OOS investigations by ruling out the measurement method as a cause, and provides a stable foundation for all process monitoring and improvement activities. It is the fundamental prerequisite for data-driven manufacturing.
        """)
            
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Validation Parameters
            - **Accuracy (Bias):** The closeness of the average test result to the true value. It measures **systematic error**. High accuracy means low bias.
            - **Precision (%CV):** The closeness of agreement (degree of scatter) between a series of measurements. It measures **random error**. High precision means low random error.
            - **Repeatability (Intra-assay precision):** Precision under the same operating conditions over a short interval of time (e.g., one analyst, one instrument, one day).
            - **Intermediate Precision (Inter-assay precision):** Precision within the same laboratory, but under different conditions (e.g., different analysts, different days, different instruments).
            - **Specificity:** The ability to assess the analyte unequivocally in the presence of components which may be expected to be present (e.g., impurities, degradants, matrix components).
            - **Interference:** A type of error caused by a substance in the sample matrix that falsely alters the assay's response to the target analyte.
            """)

        with tabs[3]:
            st.error("""
            üî¥ **THE INCORRECT APPROACH: "Validation Theater"**
            The goal of validation is to get the protocol to pass by any means necessary.
            
            - *"My precision looks bad, so I'll have my most experienced 'super-analyst' run the experiment to guarantee a low %CV."*
            - *"The method failed accuracy at the low concentration. I'll just change the reportable range to exclude that level."*
            - *"I'll only test for interference from things I know won't be a problem and ignore the complex sample matrix."*
            
            This approach treats validation as a bureaucratic hurdle. It produces a method that is fragile, unreliable in the real world, and a major compliance risk.
            """)
            st.success("""
            üü¢ **THE GOLDEN RULE: Rigorously Prove "Fitness for Purpose"**
            The goal of validation is to **honestly and rigorously challenge the method to prove it is robust and reliable for its specific, intended analytical application.**
            
            - This means deliberately including variability (different analysts, days, instruments) to prove the method can handle it.
            - It means understanding and documenting *why* a method fails at a certain level, not just hiding the failure.
            - It means demonstrating specificity in the actual, messy matrix the method will be used for.
            
            This approach builds a truly robust method that generates trustworthy data, ensuring product quality and patient safety.
            """)

        with tabs[4]:
            st.markdown("""
            #### Historical Context & Origin
            Before the 1990s, a pharmaceutical company wishing to market a new drug globally had to prepare different, massive submission packages for each region (USA, Europe, Japan), each with slightly different technical requirements for method validation. This created enormous, costly, and scientifically redundant work.
            
            In 1990, the **International Council for Harmonisation (ICH)** was formed, bringing together regulators and industry to create a single set of harmonized guidelines. The **ICH Q2(R1) guideline, "Validation of Analytical Procedures,"** is the direct result. It is the global "bible" for this topic, and the parameters of Accuracy, Precision, and Specificity form its core. Adhering to ICH Q2(R1) ensures your data is acceptable to major regulators worldwide.
            
            #### Mathematical Basis
            The validation report is a statistical argument built on quantitative metrics.
            """)
            st.markdown("**Accuracy is measured by Percent Recovery:**")
            st.latex(r"\% \text{Recovery} = \frac{\text{Mean Experimental Value}}{\text{Known True Value}} \times 100\%")
            
            st.markdown("**Precision is measured by Percent Coefficient of Variation (%CV):**")
            st.latex(r"\% \text{CV} = \frac{\text{Standard Deviation (SD)}}{\text{Mean}} \times 100\%")
            
            st.markdown("""
            **Specificity is often assessed via Hypothesis Testing:** A Student's t-test compares the means of the "Analyte Only" and "Analyte + Interferent" groups. The null hypothesis ($H_0$) is that the means are equal. A high p-value (e.g., > 0.05) means we fail to reject $H_0$, providing evidence that the interferent has no significant effect.
            """)
        with tabs[5]:
            st.markdown("""
            The concepts of Accuracy, Precision, and Specificity are the absolute core of analytical method validation as defined by global regulators.
            - **ICH Q2(R1) - Validation of Analytical Procedures:** This is the primary global guideline that explicitly defines these parameters and provides methodologies for their assessment.
            - **FDA Guidance for Industry - Analytical Procedures and Methods Validation:** The FDA's specific guidance, which is harmonized with ICH Q2(R1).
            - **USP General Chapter <1225> - Validation of Compendial Procedures:** Provides detailed requirements for validation within the United States Pharmacopeia framework.
            """)
#====================================================== 5. LOD & LOQ =========================================================================
def render_lod_loq():
    """Renders the INTERACTIVE module for Limit of Detection & Quantitation."""
    st.markdown("""
    #### Purpose & Application
    **Purpose:** To formally establish the absolute lower performance boundaries of a quantitative assay. It determines the lowest analyte concentration an assay can reliably **detect (LOD)** and the lowest concentration it can reliably and accurately **quantify (LOQ)**.
    
    **Strategic Application:** This is a mission-critical parameter for any assay used to measure trace components, such as impurities in a drug product or biomarkers for early-stage disease diagnosis. **Use the sliders in the sidebar to simulate how assay sensitivity and noise impact the final LOD and LOQ.**
    """)
    
    st.info("""
    **Interactive Demo:** Now, when you select the "LOD & LOQ" tool, a new set of dedicated sliders will appear below. You can dynamically change the assay's slope and noise to see in real-time how these fundamental characteristics drive the final LOD and LOQ results.
    """)
    
    # --- Sidebar controls for this specific module ---
    st.subheader("LOD & LOQ Controls")
    slope_slider = st.slider(
        "üìà Assay Sensitivity (Slope)", 
        min_value=0.005, max_value=0.1, value=0.02, step=0.005, format="%.3f",
        help="How much the signal increases per unit of concentration. A steeper slope (higher sensitivity) is better."
    )
    noise_slider = st.slider(
        "üîá Baseline Noise (SD)", 
        min_value=0.001, max_value=0.05, value=0.01, step=0.001, format="%.3f",
        help="The inherent random noise of the assay at zero concentration. A lower noise floor is better."
    )
    
    # Generate plots using the slider values
    fig, lod_val, loq_val = plot_lod_loq(slope=slope_slider, baseline_sd=noise_slider)
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ Acceptance Criteria", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.metric(label="üìà KPI: Limit of Quantitation (LOQ)", value=f"{loq_val:.2f} units", help="The lowest concentration you can report with confidence in the numerical value.")
            st.metric(label="üí° Metric: Limit of Detection (LOD)", value=f"{lod_val:.2f} units", help="The lowest concentration you can reliably claim is 'present'.")
            st.info("Play with the sliders in the sidebar to see how assay parameters affect the results!")
            st.markdown("""
            - **Increase `Assay Sensitivity (Slope)`:** As the slope gets steeper, the LOD and LOQ values get **lower (better)**. A highly sensitive assay needs very little analyte to produce a strong signal that can overcome the noise.
            - **Increase `Baseline Noise (SD)`:** As the noise floor of the assay increases, the LOD and LOQ values get **higher (worse)**. It becomes much harder to distinguish a true low-level signal from random background fluctuations.
        
            **The Core Strategic Insight:** The sensitivity of an assay is a direct battle between its **signal-generating power (Slope)** and its **inherent noise (SD)**. The LOD and LOQ are simply the statistical formalization of this signal-to-noise ratio.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: The Gateway to Critical Applications
        
            #### The Problem: The "Is It Really There?" Dilemma
            A company is developing a new drug and needs to prove to regulators that a potentially toxic impurity is below a stringent safety threshold of 10 parts per million (ppm). In another case, a diagnostics company is developing a cancer screening test that needs to detect a biomarker at very low levels to enable early diagnosis. In both scenarios, the core business and clinical value depends on a single question: "Can our measurement system reliably see what it needs to see at these extremely low levels?"
        
            #### The Impact: Failed Submissions, Uncompetitive Products, and Patient Risk
            An inability to formally establish and validate a sufficiently low Limit of Quantitation (LOQ) has severe business consequences:
            - **Regulatory Rejection:** If the validated LOQ for the impurity method is 20 ppm, but the required safety limit is 10 ppm, the method is **not fit for purpose**. The regulatory submission will be rejected, delaying the entire drug approval.
            - **Lost Market Opportunity:** If a competitor's diagnostic test has a validated LOQ of 5 pg/mL while yours is only 50 pg/mL, their test will capture the market for early-stage disease detection, rendering your product uncompetitive.
            - **Patient Safety Risk:** Relying on a method to quantify a value near its limit without a formal LOQ determination means the reported numbers are statistically unreliable. This could lead to releasing a batch with an unsafe level of an impurity because the method's noise was misinterpreted as a passing result.
        
            #### The Solution: A Statistically Defensible Floor
            The formal determination of LOD and LOQ is the process of establishing a **statistically defensible "floor"** for an analytical method. It moves beyond wishful thinking and provides objective, quantifiable evidence of the method's true sensitivity. It is the official "spec sheet" for the method's performance at the lower end of its range, answering the critical questions:
            - **LOD:** At what level can we be confident a signal is real and not just random noise?
            - **LOQ:** At what level can we be confident that the numerical value we report is not just real, but also precise and accurate?
        
            #### The Consequences: Unlocking High-Value Claims
            - **Without This:** Any claims about product safety (low impurities) or diagnostic sensitivity (early detection) are scientifically unfounded and will not withstand regulatory scrutiny.
            - **With This:** A properly validated LOQ is the **gateway to making high-value claims**. It is the objective evidence that allows a pharmaceutical company to guarantee the safety of its drug product, or a diagnostics company to market a test for early disease detection. It is a direct enabler of competitive advantage and a non-negotiable requirement for ensuring patient safety.
            """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Sensitivity Terms
            - **Limit of Blank (LOB):** The highest measurement result that is likely to be observed for a blank sample. It defines the "noise floor" of the assay.
            - **Limit of Detection (LOD):** The lowest concentration of analyte that can be reliably detected above the LOB, but not necessarily quantified with acceptable precision and accuracy.
            - **Limit of Quantitation (LOQ):** The lowest concentration of analyte that can be reliably quantified with a pre-defined level of precision and accuracy. This is the lower boundary of the assay's reportable range.
            - **Slope (Sensitivity):** In a calibration curve, the slope represents the change in analytical signal per unit change in analyte concentration. A steeper slope generally indicates a more sensitive assay.
            """)
        with tabs[3]:
            st.markdown("- The primary, non-negotiable criterion is that the experimentally determined **LOQ must be ‚â§ the lowest concentration that the assay is required to measure** for its specific application (e.g., a release specification for an impurity).")
            st.markdown("- For a concentration to be formally declared the LOQ, it must be experimentally confirmed. This typically involves analyzing 5-6 independent samples at the claimed LOQ concentration and demonstrating that they meet pre-defined criteria for precision and accuracy (e.g., **%CV < 20% and %Recovery between 80-120%** for a bioassay).")
            st.warning("""
            **The LOB, LOD, and LOQ Hierarchy: A Critical Distinction**
            A full characterization involves three distinct limits:
            - **Limit of Blank (LOB):** The highest measurement expected from a blank sample.
            - **Limit of Detection (LOD):** The lowest concentration whose signal is statistically distinguishable from the LOB.
            - **Limit of Quantitation (LOQ):** The lowest concentration meeting precision/accuracy requirements, which is almost always higher than the LOD.
            """)
        with tabs[4]:
            st.markdown("""
            #### Historical Context & Origin
            The need to define analytical sensitivity is old, but definitions were inconsistent until the **International Council for Harmonisation (ICH)** guideline **ICH Q2(R1)** harmonized the methodologies. This work was heavily influenced by the statistical framework established by **Lloyd Currie at NIST** in his 1968 paper, which established the clear, hypothesis-testing basis for the modern LOB/LOD/LOQ hierarchy.

            #### Mathematical Basis
            This method is built on the relationship between the assay's signal, its sensitivity (Slope, S), and its noise (standard deviation, œÉ).
            """)
            st.latex(r"LOD \approx \frac{3.3 \times \sigma}{S}")
            st.latex(r"LOQ \approx \frac{10 \times \sigma}{S}")
            st.markdown("The factor of 10 for LOQ is the standard convention that typically yields a precision of roughly 10% CV for a well-behaved assay.")
        with tabs[5]:
            st.markdown("""
            The determination of detection and quantitation limits is a mandatory part of validating quantitative assays for impurities or trace components.
            - **ICH Q2(R1) - Validation of Analytical Procedures:** Explicitly lists "Quantitation Limit" and "Detection Limit" as key validation characteristics and provides the statistical methodologies (e.g., based on signal-to-noise or standard deviation of the response and the slope).
            - **USP General Chapter <1225>:** Mirrors the requirements of ICH Q2(R1) for the validation of analytical procedures.
            """)
#====================================================== 6.LINEARITY & RANGE =========================================================================
def render_linearity():
    """Renders the INTERACTIVE module for Linearity analysis."""
    st.markdown("""
    #### Purpose & Application
    **Purpose:** To verify that an assay's response is directly and predictably proportional to the known concentration of the analyte across its entire intended operational range.
    
    **Strategic Application:** This is a cornerstone of quantitative assay validation. A method exhibiting non-linearity might be accurate at a central control point but dangerously inaccurate at the specification limits. **Use the sliders in the sidebar to simulate different types of non-linear behavior and error.**
    """)
    
    st.info("""
    **Interactive Demo:** Use the sliders at the bottom of the sidebar to simulate different error types. The **Residual Plot** is your most important diagnostic tool! A "funnel" shape indicates proportional error, and a "U" shape indicates curvature. When you see a funnel, try switching to the WLS model to see how it can provide a better fit.
    """)
    
    # --- Sidebar controls for this specific module ---
    with st.sidebar:
        st.subheader("Linearity Controls")
        curvature_slider = st.slider("üß¨ Curvature Effect", -5.0, 5.0, -1.0, 0.5,
            help="Simulates non-linearity. A negative value creates saturation at high concentrations. Zero is perfectly linear.")
        random_error_slider = st.slider("üé≤ Random Error (Constant SD)", 0.1, 5.0, 1.0, 0.1,
            help="The baseline random noise of the assay, constant across all concentrations.")
        proportional_error_slider = st.slider("üìà Proportional Error (% of Conc.)", 0.0, 5.0, 2.0, 0.25,
            help="Error that increases with concentration. This creates a 'funnel' or 'megaphone' shape in the residual plot.")
        
        # --- NEW TOGGLE SWITCH ADDED HERE ---
        st.markdown("---")
        st.markdown("**Regression Model**")
        wls_toggle = st.toggle("Use Weighted Least Squares (WLS)", value=False,
            help="Activate WLS to give less weight to high-concentration points. Ideal for correcting the 'funnel' shape caused by proportional error.")
    
    # Generate plots using the slider values and the new toggle value
    fig, model = plot_linearity(
        curvature=curvature_slider,
        random_error=random_error_slider,
        proportional_error=proportional_error_slider,
        use_wls=wls_toggle # Pass the toggle state to the plotting function
    )
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ Acceptance Criteria", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.metric(label="üìà KPI: R-squared (R¬≤)", value=f"{model.rsquared:.4f}", help="Indicates the proportion of variance explained by the model. Note how a high R¬≤ can hide clear non-linearity!")
            st.metric(label="üí° Metric: Slope", value=f"{model.params['Nominal']:.3f}", help="Ideal = 1.0.")
            st.metric(label="üí° Metric: Y-Intercept", value=f"{model.params['const']:.2f}", help="Ideal = 0.0.")
            
            st.markdown("""
            - **The Residual Plot is Key:** This is the most sensitive diagnostic tool.
                - Add **Curvature**: Notice the classic "U-shape" or "inverted U-shape" that appears. This is a dead giveaway that your straight-line model is wrong.
                - Add **Proportional Error**: Watch the residuals form a "funnel" shape (heteroscedasticity). This means OLS is no longer valid. **Activate the WLS toggle in the sidebar** to see how a weighted model correctly handles this error structure.
            
            **The Core Strategic Insight:** A high R-squared is **not sufficient** to prove linearity. You must visually inspect the residual plot for hidden patterns. The residual plot tells the true story of your model's fit.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: The Foundation of a Trustworthy Measurement System
        
            #### The Problem: The "One-Point Wonder" Method
            A QC lab uses a single calibrator to test a critical quality attribute. The method is shown to be accurate and precise *at that one point*. However, the manufacturing process can produce material across a range of values, from the lower specification limit (LSL) to the upper specification limit (USL). The lab has no validated data to prove that their method is accurate across this full, required range.
        
            #### The Impact: Hidden Inaccuracies and Operational Inflexibility
            This "one-point calibration" approach creates significant business and compliance risks:
            - **False Passes/Failures at the Extremes:** The method's response might curve downwards at high concentrations (a saturation effect). This means a batch that is truly out-of-spec (high) might incorrectly get a passing result from the QC lab because the method can't "see" that high. This is a major patient safety and compliance risk.
            - **Operational Rigidity and Waste:** Without a validated linear range, every sample must be diluted to fall exactly at the single calibration point. This is slow, labor-intensive, and prone to dilution errors. It prevents the lab from efficiently handling process development samples, which are often at different concentrations.
            - **Invalidated Trend Analysis:** Management wants to track process trends over time. But if the measurement method is non-linear, the trends they see might be an artifact of the method's bias, not a true change in the manufacturing process, leading to flawed business decisions.
        
            #### The Solution: A Contract of Proportionality
            A linearity study is the formal process of proving that the measurement system's response is **directly and predictably proportional** to the true concentration of the substance being measured. It is a contract that guarantees the method is a trustworthy "ruler" across its entire intended range of use. By demonstrating linearity, you prove that a result of "50" means exactly half as much as a result of "100".
        
            #### The Consequences: A Versatile and Defensible Measurement System
            - **Without This:** The measurement system is a "one-trick pony" that is only trustworthy at a single point. All other results are based on an unproven assumption.
            - **With This:** The method becomes a versatile and powerful tool. It can accurately quantify samples across a wide range, supporting not just routine QC, but also process characterization, stability studies, and troubleshooting. It provides a solid, defensible foundation for all product release decisions and for the statistical process control (SPC) charts that monitor the health of the business.
            """)
        with tabs[1]:
            st.markdown("""
            ##### Glossary of Linearity Terms
            - **Linearity:** The ability of an analytical procedure to obtain test results which are directly proportional to the concentration of analyte in the sample.
            - **Range:** The interval between the upper and lower concentration of analyte for which the procedure has demonstrated a suitable level of precision, accuracy, and linearity.
            - **Residuals:** The vertical distance between an observed data point and the fitted regression line. Analyzing patterns in residuals is the best way to diagnose non-linearity.
            - **R-squared (R¬≤):** A statistical measure of how close the data are to the fitted regression line. While a high R¬≤ is necessary, it is not sufficient to prove linearity.
            - **Weighted Least Squares (WLS):** A regression method used when the variance of the errors is not constant (heteroscedasticity). It gives less weight to less precise (typically high-concentration) data points.
            """)
        with tabs[2]:
            st.markdown("These criteria are defined in the validation protocol and must be met to declare the method linear.")
            st.markdown("- **R-squared (R¬≤):** Typically > **0.995**, but for high-precision methods (e.g., HPLC), > **0.999** is often required.")
            st.markdown("- **Slope & Intercept:** The 95% confidence intervals for the slope and intercept should contain 1.0 and 0, respectively.")
            st.markdown("- **Residuals:** There should be no obvious pattern or trend. A formal **Lack-of-Fit test** can be used for objective proof (requires true replicates at each level).")
            st.markdown("- **Recovery:** The percent recovery at each concentration level must fall within a pre-defined range (e.g., 80% to 120% for bioassays).")

        with tabs[3]:
            st.markdown("""
            #### Historical Context & Origin
            The mathematical engine is **Ordinary Least Squares (OLS) Regression**, developed independently by **Adrien-Marie Legendre (1805)** and **Carl Friedrich Gauss (1809)**. The genius of OLS is that it finds the one line that **minimizes the sum of the squared vertical distances (the "residuals")** between the data points and the line.
            
            However, OLS relies on a key assumption: that the variance of the errors is constant at all levels of X (homoscedasticity). In many biological and chemical assays, this is not true; variability often increases with concentration (heteroscedasticity). **Weighted Least Squares (WLS)** is the classical solution to this problem, where each point is weighted by the inverse of its variance, giving more influence to the more precise, low-concentration points.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("The goal is to fit a simple linear model:")
            st.latex("y = \\beta_0 + \\beta_1 x + \\epsilon")
            st.markdown("""
            - **OLS** finds the `Œ≤` values that minimize: `Œ£(y·µ¢ - ≈∑·µ¢)¬≤`
            - **WLS** finds the `Œ≤` values that minimize: `Œ£w·µ¢(y·µ¢ - ≈∑·µ¢)¬≤`, where `w·µ¢` is the weight for the i-th observation, typically `1/œÉ¬≤·µ¢`.
            """)
        with tabs[4]:
            st.markdown("""
            Linearity is a fundamental characteristic required for all quantitative analytical methods.
            - **ICH Q2(R1) - Validation of Analytical Procedures:** Mandates the evaluation of Linearity and Range for quantitative tests. It specifies that a linear relationship should be evaluated across the range of the analytical procedure.
            - **FDA Guidance for Industry:** Recommends a minimum of five concentration levels to establish linearity and emphasizes the importance of visual inspection of the data and analysis of residuals.
            - **USP General Chapter <1225>:** Requires the statistical evaluation of linearity, including the calculation of the correlation coefficient, y-intercept, and slope of the regression line.
            """)
#====================================================== 7. NON-LINEAR REGRESSION (4PL/5PL) =========================================================================
def render_4pl_regression():
    """Renders the INTERACTIVE module for 4-Parameter Logistic (4PL) regression."""
    st.markdown("""
    #### Purpose & Application
    **Purpose:** To accurately model the characteristic sigmoidal (S-shaped) dose-response relationship found in most immunoassays (e.g., ELISA) and biological assays.
    
    **Strategic Application:** This is the workhorse model for potency assays and immunoassays. The 4PL model allows for the accurate calculation of critical assay parameters like the EC50. **Use the sliders in the sidebar to control the "true" shape of the curve and see how the curve-fitting algorithm performs.**
    """)
    
    st.info("""
    **Interactive Demo:** Build your own "true" 4PL curves using the sliders. The **Residual Plot** is your key diagnostic: if you see a "funnel" shape (due to `Proportional Noise`), activate the **Weighted Fit (IRLS)** toggle to see how a more advanced algorithm can produce a better, more reliable fit.
    """)
    
    # --- Sidebar controls for this specific module ---
    with st.sidebar:
        st.subheader("4PL Curve Controls (True Values)")
        d_slider = st.slider("üÖæÔ∏è Lower Asymptote (d)", 0.0, 0.5, 0.05, 0.01,
            help="The minimum signal response of the assay, typically the background signal at zero concentration. This defines the 'floor' of the curve.")
        a_slider = st.slider("üÖ∞Ô∏è Upper Asymptote (a)", 1.0, 3.0, 1.5, 0.1,
            help="The maximum signal response of the assay at infinite concentration. This represents the saturation point or the 'ceiling' of the curve.")
        c_slider = st.slider("üéØ Potency / EC50 (c)", 1.0, 100.0, 10.0, 1.0,
            help="The concentration that produces a response halfway between the lower (d) and upper (a) asymptotes. A lower EC50 value indicates a more potent substance.")
        b_slider = st.slider("üÖ±Ô∏è Hill Slope (b)", 0.5, 5.0, 1.2, 0.1,
            help="Controls the steepness of the curve at its midpoint (the EC50). A steeper slope (higher value) often indicates a more sensitive assay in its dynamic range.")
        
        st.markdown("---")
        st.subheader("Noise Model Controls")
        noise_sd_slider = st.slider("üé≤ Constant Noise (SD)", 0.0, 0.2, 0.05, 0.01,
            help="The baseline random noise, constant across all concentrations.")
        proportional_noise_slider = st.slider("üìà Proportional Noise (%)", 0.0, 5.0, 1.0, 0.5,
            help="Noise that increases with the signal. Creates a 'funnel' shape in the residuals (heteroscedasticity).")

        st.markdown("---")
        st.subheader("Fit Model Controls")
        irls_toggle = st.toggle("Use Weighted Fit (IRLS)", value=True,
            help="Activate Iteratively Reweighted Least Squares (IRLS). This is the correct model to use when proportional noise is present.")
    
    # Generate plots using the slider values
    fig, params, perr = plot_4pl_regression(
        a_true=a_slider, b_true=b_slider, c_true=c_slider, d_true=d_slider,
        noise_sd=noise_sd_slider, proportional_noise=proportional_noise_slider,
        use_irls=irls_toggle
    )
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        a_fit, b_fit, c_fit, d_fit = params
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            # Display fitted parameters with their standard errors
            st.metric(label="üÖ∞Ô∏è Fitted Upper Asymptote (a)", value=f"{a_fit:.3f} ¬± {perr[0]:.3f}")
            st.metric(label="üÖ±Ô∏è Fitted Hill Slope (b)", value=f"{b_fit:.3f} ¬± {perr[1]:.3f}")
            st.metric(label="üéØ Fitted EC50 (c)", value=f"{c_fit:.3f} ¬± {perr[2]:.2f} units")
            st.metric(label="üÖæÔ∏è Fitted Lower Asymptote (d)", value=f"{d_fit:.3f} ¬± {perr[3]:.3f}")
            
            st.markdown("""
            **The Core Strategic Insight:** The 4PL curve is a complete picture of your assay's performance. The **residuals plot is your most important diagnostic tool**. A random scatter around zero means your model is a good fit. Any pattern (like a curve or funnel) indicates a problem with the model or the data weighting.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: Unlocking the Value of Modern Biology
        
            #### The Problem: The Limits of Linearity
            The biotechnology and pharmaceutical industries are built on biological systems. Whether it's a therapeutic antibody binding to its target or a diagnostic ELISA detecting a biomarker, the underlying relationship between concentration and response is almost never a straight line. These systems exhibit **saturation effects**, producing a characteristic sigmoidal (S-shaped) curve. Attempting to fit a straight line to this reality is like trying to fit a square peg in a round hole.
        
            #### The Impact: Inaccurate Potency and Wasted Drug Development
            Using the wrong mathematical model for a bioassay has severe financial and clinical consequences:
            - **Inaccurate Potency Measurement:** A simple linear fit might seem "good enough" in the middle of the curve, but it will be wildly inaccurate at the upper and lower ends. This can lead to a company completely misstating the potency of their drug, a critical quality attribute that impacts dosing and efficacy. This is a major regulatory red flag and a patient safety risk.
            - **Failed Lot Release:** A batch of a multi-million dollar biologic might be incorrectly failed (or passed) because the simple linear model used for the QC potency assay gave a biased result.
            - **Wasted R&D Investment:** During drug discovery, promising candidate molecules might be incorrectly discarded because a poor analytical model failed to accurately measure their true biological activity.
        
            #### The Solution: The Right Tool for a Curved World
            The 4-Parameter Logistic (4PL) model is the **purpose-built mathematical tool** for the S-shaped curves that define modern biology. It is not an arbitrary choice; its four parameters (`a, b, c, d`) directly correspond to the real, physical properties of the assay: the minimum and maximum signal (floor and ceiling), the steepness of the response (Hill slope), and, most critically, the **EC50**‚Äîthe concentration that produces 50% of the maximal response, which is the universal measure of a drug's **potency**.
        
            #### The Consequences: A Foundation for Modern Drug Development
            - **Without This:** A company cannot reliably measure the most important Critical Quality Attribute of a biologic drug: its potency. It is operating with a broken ruler for its most valuable assets.
            - **With This:** The 4PL model provides the solid, scientifically and mathematically sound foundation for the entire bioassay workflow. It enables **accurate potency determination** for lot release, allows for **meaningful comparisons** between drug candidates in R&D, and provides the **stable, reliable measurement system** required for all manufacturing, validation, and regulatory activities. Mastering the 4PL model is a prerequisite for success in the modern biopharmaceutical industry.
            """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Bioassay Terms
            - **4PL (Four-Parameter Logistic) Model:** A type of non-linear regression model used to describe sigmoidal (S-shaped) dose-response curves.
            - **Upper Asymptote (a):** The maximum response of the assay at infinite concentration (the "ceiling").
            - **Lower Asymptote (d):** The response of the assay at zero concentration (the "floor" or background).
            - **EC50 / IC50 (c):** The concentration that gives a response halfway between the lower and upper asymptotes. It is the primary measure of a substance's **potency**.
            - **Hill Slope (b):** A parameter that describes the steepness of the curve at its midpoint (the EC50).
            - **Potency:** A measure of drug activity expressed in terms of the amount required to produce an effect of a given intensity. A lower EC50 means higher potency.
            """)
        with tabs[3]:
            st.error("""üî¥ **THE INCORRECT APPROACH: "Force the Fit"**
- *"My R-squared is 0.999, so the fit must be perfect."* (R-squared is easily inflated and can hide significant lack of fit).
- *"The model doesn't fit a point well. I'll delete the outlier."* (Data manipulation without statistical justification).
- *"My residuals look like a funnel, but I'll ignore it and use standard least squares."* (This leads to incorrect parameter estimates and confidence intervals).""")
            st.success("""üü¢ **THE GOLDEN RULE: Model the Biology, Weight the Variance**
- **Embrace the 'S' Shape:** Use a non-linear model for non-linear biological data. The 4PL is standard for a reason.
- **Weight Your Points:** Bioassay data is almost always heteroscedastic (non-constant variance). Use a weighted regression (like IRLS) to get the most accurate and reliable parameter estimates.
- **Inspect the Residuals:** The residuals must be visually random. Any pattern indicates your model is not correctly capturing the data's behavior.""")

        with tabs[4]:
            st.markdown("""
            #### Historical Context: Modeling Dose-Response
            **The Problem:** In the early 20th century, pharmacologists and biologists needed a mathematical way to describe the relationship between the dose of a substance and its biological response. This relationship was rarely linear; it typically showed a sigmoidal (S-shaped) curve, with a floor, a steep middle section, and a ceiling (saturation).

            **The 'Aha!' Moment:** The first major step was the **Hill Equation**, developed by physiologist Archibald Hill in 1910 to describe oxygen binding to hemoglobin. Later, A.J. Clark and others adapted these ideas into dose-response models. The 4-Parameter Logistic (4PL) model emerged as a flexible, robust, and empirically successful model for this type of data.

            **The Impact:** The proliferation of immunoassays like RIA and ELISA in the 1970s and 80s made the 4PL model a workhorse of the biotech industry. The development of accessible non-linear regression software, like that based on the **Levenberg-Marquardt algorithm**, made fitting these models routine. Today, it remains the standard model for most potency and immunogenicity assays.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("The 4-Parameter Logistic function is a sigmoidal curve defined by:")
            st.latex(r"y = d + \frac{a - d}{1 + (\frac{x}{c})^b}")
            st.markdown("""
            - **`a`**: The upper asymptote (response at infinite concentration).
            - **`b`**: The Hill slope (steepness of the curve at the midpoint).
            - **`c`**: The EC50 or IC50 (concentration at 50% of the maximal response). This is often the primary measure of potency.
            - **`d`**: The lower asymptote (response at zero concentration).
            Since this equation is non-linear in its parameters, it cannot be solved directly with linear algebra. It must be fit using an iterative numerical optimization algorithm (like Levenberg-Marquardt) that finds the parameter values `(a,b,c,d)` that minimize the sum of squared errors between the data and the fitted curve.
            """)
        with tabs[5]:
            st.markdown("""
            While the 4PL model itself is a mathematical tool, its use is governed by guidelines on the validation of bioassays, where such non-linear responses are common.
            - **USP General Chapters <111>, <1032>, <1033>:** These chapters provide extensive guidance on the design and statistical analysis of biological assays. They discuss the importance of using an appropriate non-linear model to fit dose-response curves and assess parallelism.
            - **FDA Guidance on Bioanalytical Method Validation:** Stresses the need to characterize the full concentration-response relationship and use appropriate regression models (including weighted regression for heteroscedastic data).
            """)
#=======================================================================8. GAGE R&R / VCA   ========================================================================================
def render_gage_rr():
    """Renders the INTERACTIVE module for Gage R&R."""
    st.markdown("""
    #### Purpose & Application: The Voice of the Measurement
    **Purpose:** To rigorously quantify the inherent variability (error) of a measurement system itself. It answers the fundamental question: **"Is my measurement system a precision instrument, or a random number generator?"**
    
    **Strategic Application:** This is a non-negotiable gateway in any technology transfer or process validation. An unreliable measurement system creates a "fog of uncertainty," leading to two costly errors:
    -   **Type I Error (Producer's Risk):** Rejecting good product because of measurement error.
    -   **Type II Error (Consumer's Risk):** Accepting bad product because the measurement system couldn't detect the defect.
    A Gage R&R study provides the objective evidence that your measurement system is fit for purpose.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Validation Lead. Use the sliders in the sidebar to simulate different sources of variation. Your goal is to achieve a **% Gage R&R < 10%**.
    - **`Part-to-Part Variation`**: The "true" variation you want to measure. A high value makes it *easier* to pass the Gage R&R.
    - **`Repeatability`**: The instrument's own noise. This is a primary driver of Gage R&R failure.
    - **`Operator Variation`**: Inconsistency between people. This is the other major driver of failure.
    - **`Operator-Part Interaction`**: A subtle effect where an operator's bias changes depending on the part being measured.
    """)
    
    with st.sidebar:
        st.subheader("Gage R&R Controls")
        part_sd_slider = st.slider("üè≠ Part-to-Part Variation (SD)", 1.0, 10.0, 5.0, 0.5,
            help="The 'true' variation of the product. A well-designed study uses parts that span a wide range, making this value high.")
        repeat_sd_slider = st.slider("üî¨ Repeatability / Instrument Noise (SD)", 0.1, 5.0, 1.5, 0.1,
            help="The inherent 'noise' of the instrument/assay. High values represent an imprecise measurement device.")
        operator_sd_slider = st.slider("üë§ Operator-to-Operator Variation (SD)", 0.0, 5.0, 0.75, 0.25,
            help="The systematic bias between operators. High values represent poor training or inconsistent technique.")
        # --- NEW SLIDER ADDED HERE ---
        interaction_sd_slider = st.slider("üîÑ Operator-Part Interaction (SD)", 0.0, 2.0, 0.5, 0.1,
            help="Simulates inconsistency where operators measure certain parts differently (e.g., struggling with smaller parts). This causes the operator mean lines to be non-parallel.")

    # Call the updated plot function with all four parameters
    fig, pct_rr, ndc = plot_gage_rr(
        part_sd=part_sd_slider, 
        repeatability_sd=repeat_sd_slider, 
        operator_sd=operator_sd_slider,
        interaction_sd=interaction_sd_slider
    )
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ Acceptance Criteria", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.metric(label="üìà KPI: % Gage R&R", value=f"{pct_rr:.1f}%", delta="Lower is better", delta_color="inverse")
            st.metric(label="üìä KPI: Number of Distinct Categories (ndc)", value=f"{ndc}", help="How many distinct groups of parts the system can reliably distinguish. Must be ‚â• 5.")
        
            st.markdown("""
            **Reading the Plots:**
            - **Main Plot (Left):** Now shows parts sorted by size. The colored lines represent each operator's average measurement for each part. If these lines are not parallel, it's a sign of **interaction**.
            - **Operator Plot (Top-Right):** Visualizes the overall bias between operators.
            - **Verdict (Bottom-Right):** The final bar chart. The colored bar (% Gage R&R) shows how much of the total observed variation is just measurement noise.
        
            **Core Insight:** A low % Gage R&R is achieved when measurement error is small *relative to* the true process variation. You can improve your Gage R&R by either reducing measurement error OR by testing it on parts that have a wider, more representative range of true variation.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: Escaping the "Fog of Uncertainty"
        
            #### The Problem: The Unreliable Ruler
            A manufacturing process for a critical component has tight specification limits. The process is monitored using a measurement system (a "gage") that has never been formally qualified. The operators see significant variation in their measurements day-to-day, but they don't know the source: Is the manufacturing process itself unstable, or is the ruler they are using to measure it simply unreliable?
        
            #### The Impact: The Two Most Expensive Manufacturing Errors
            Operating with an unquantified, high-error measurement system creates a "fog of uncertainty" that leads directly to two of the most costly errors a manufacturer can make:
            1.  **Producer's Risk (Scrapping Good Product):** A perfectly good part is produced. However, due to random measurement error, the gage produces a result that falls just outside the specification limits. The part is scrapped, and the company throws away a perfectly good product. This is a direct, quantifiable hit to the bottom line.
            2.  **Consumer's Risk (Shipping Bad Product):** A defective part is produced. However, due to measurement error, the gage produces a result that falls just inside the specification limits. The bad part is passed by QC and shipped to the customer. This leads to field failures, warranty claims, product recalls, and severe damage to the company's reputation and patient safety.
        
            #### The Solution: Quantifying the Voice of the Measurement
            A Gage R&R study is a systematic, statistical experiment designed to **quantify the percentage of process variation that is being consumed by the measurement system itself**. It precisely partitions the observed variability into its true sources: the actual part-to-part differences (which we want to see) versus the noise created by the instrument (Repeatability) and the operators (Reproducibility). It answers the critical question: "Is the variation I'm seeing real, or is it just the fog from my measurement system?"
        
            #### The Consequences: A Clear View of Reality and Reduced Waste
            - **Without This:** The company is flying blind. It cannot distinguish between process variation and measurement error, leading to chronic waste, high risk, and inefficient troubleshooting.
            - **With This:** The Gage R&R provides objective evidence that the measurement system is fit for purpose. It gives engineers and managers a clear, reliable view of their true process performance. This **builds confidence** in SPC charts, **streamlines** OOS investigations (by ruling out the gage as a cause), and, most importantly, **dramatically reduces** the financial losses from both scrapping good product and shipping bad product. It is a fundamental investment in operational excellence.
            """)
        with tabs[1]:
            st.markdown("""
            ##### Glossary of MSA Terms
            - **Measurement System Analysis (MSA):** A formal statistical study to evaluate the total variation present in a measurement system.
            - **Gage R&R:** The combined estimate of a measurement system's Repeatability and Reproducibility. It quantifies the inherent variability of the measurement process itself.
            - **Repeatability (Equipment Variation):** The variation observed when the same operator measures the same part multiple times with the same device. It represents the inherent "noise" of the instrument.
            - **Reproducibility (Appraiser Variation):** The variation observed when different operators measure the same part using the same device. It represents the inconsistency between people.
            - **% Gage R&R:** The percentage of the total observed process variation that is consumed by measurement system error.
            - **Number of Distinct Categories (ndc):** An index that represents the number of distinct groups of parts the measurement system can reliably distinguish. A value ‚â• 5 is considered acceptable.
            """)
        with tabs[2]:
            st.markdown("Acceptance criteria are derived from the **AIAG's Measurement Systems Analysis (MSA)** manual, the global standard.")
            st.markdown("- **< 10% Gage R&R:** The system is **acceptable**.")
            st.markdown("- **10% - 30% Gage R&R:** The system is **conditionally acceptable**, may be approved based on importance of application and cost. ")
            st.markdown("- **> 30% Gage R&R:** The system is **unacceptable** and must be improved.")
            st.error("""
            **The Part Selection Catastrophe**: The most common way to fail a Gage R&R is not bad math, but bad study design. If you select parts that are all very similar (low Part-to-Part variation), you are mathematically guaranteed to get a high % Gage R&R, even with a perfect instrument. **You must select parts that represent the full range of expected process variation.**
            """)
            
        with tabs[3]:
            st.markdown("""
            #### Historical Context: The Crisis that Forged a Standard
            **The Problem:** In the 1970s and 80s, the American automotive industry was in crisis, facing intense competition from Japanese manufacturers who had mastered statistical quality control. A major source of defects and waste was inconsistent measurement. A part might pass inspection at a supplier's factory but fail at the assembly plant simply because the two locations' measurement systems ("gages") didn't agree. There was no standardized way to qualify a measurement system.

            **The 'Aha!' Moment:** The "Big Three" US automakers‚ÄîFord, GM, and Chrysler‚Äîrealized they couldn't solve this problem alone. They formed the **Automotive Industry Action Group (AIAG)** to create common quality standards for their entire supply chain. One of their most impactful creations was the **Measurement Systems Analysis (MSA)** manual, first published in 1990.
            
            **The Impact:** The MSA manual didn't invent Gage R&R, but it codified it into a simple, repeatable procedure that became the global standard. The critical evolution it championed was the move from older, less reliable methods to the **ANOVA (Analysis of Variance) method** as the preferred approach. The ANOVA method, pioneered by **Sir Ronald A. Fisher**, is statistically superior because it can correctly partition all sources of variation, including the crucial **interaction effect** between operators and parts (e.g., if one operator struggles to measure small parts specifically). This rigorous approach became the benchmark for quality-driven industries worldwide, from aerospace to pharmaceuticals.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("The core idea is to partition the total observed variation into its components. The fundamental equation is:")
            st.latex(r"\sigma^2_{\text{Total}} = \sigma^2_{\text{Process}} + \sigma^2_{\text{Measurement System}}")
            st.markdown("The measurement system variation is further broken down:")
            st.latex(r"\sigma^2_{\text{Measurement System}} = \underbrace{\sigma^2_{\text{Repeatability}}}_\text{Equipment Variation} + \underbrace{\sigma^2_{\text{Reproducibility}}}_\text{Appraiser Variation}")
            st.markdown("ANOVA achieves this by partitioning the **Sum of Squares (SS)**:")
            st.latex(r"SS_{\text{Total}} = SS_{\text{Part}} + SS_{\text{Operator}} + SS_{\text{Interaction}} + SS_{\text{Error}}")
            st.markdown("These SS values are converted to Mean Squares (MS), and from the MS values, we can estimate the variance components (`œÉ¬≤`). For example:")
            st.latex(r"\hat{\sigma}^2_{\text{Repeatability}} = MS_{\text{Error}}")
            st.latex(r"\hat{\sigma}^2_{\text{Operator}} = \frac{MS_{\text{Operator}} - MS_{\text{Interaction}}}{n \cdot r}")
            st.markdown("The final KPI is the **% Gage R&R**, which is the percentage of the total variation that is consumed by the measurement system:")
            st.latex(r"\% \text{Gage R\&R} = \frac{\hat{\sigma}_{\text{Gage R\&R}}}{\hat{\sigma}_{\text{Total}}} \times 100")
        with tabs[4]:
            st.markdown("""
            Gage R&R is the standard methodology for Measurement Systems Analysis (MSA), a critical component of ensuring data integrity and process control.
            - **AIAG MSA Manual:** While from the automotive industry, this is considered the global "gold standard" reference for Gage R&R methodology and acceptance criteria.
            - **FDA Process Validation Guidance:** Stage 1 (Process Design) and Stage 2 (Process Qualification) require an understanding of all sources of variability, including measurement error. A Gage R&R is the formal proof that a measurement system is suitable for its intended use.
            - **21 CFR 211.160(b):** Requires that "laboratory controls shall include the establishment of scientifically sound and appropriate... standards, and test procedures... to assure that components... and drug products conform to appropriate standards of identity, strength, quality, and purity." A qualified measurement system is a prerequisite.
            """)
#=============================================================== 9. ATTRIBUTE AGREEMENT ANALYSIS ===================================================
def render_attribute_agreement():
    """Renders the comprehensive, interactive module for Attribute Agreement Analysis."""
    st.markdown("""
    #### Purpose & Application: Validating Human Judgment
    **Purpose:** To validate your **human measurement systems**. It answers the critical question: "Can our inspectors consistently and accurately distinguish good product from bad?" This is the counterpart to Gage R&R, but for subjective, pass/fail, or categorical assessments.
    
    **Strategic Application:** This analysis is essential for validating any process that relies on human visual inspection or go/no-go gauges. A failed study indicates that inspectors are either missing true defects (a risk to the patient/customer) or rejecting good product (a risk to the business), and that retraining or improved inspection aids are required.
    """)
    
    st.info("""
    **Interactive Demo:** Use the sidebar controls to create a challenging inspection scenario with different inspector archetypes.
    - The **Effectiveness Plot** (right) is your main diagnostic, showing the two critical types of error for each inspector.
    - The **Kappa Matrix** (bottom) shows who agrees with whom. Low values between two inspectors indicate they are not aligned on their decision criteria.
    """)

    with st.sidebar:
        st.subheader("Attribute Agreement Controls")
        st.markdown("**Study Design**")
        n_parts_slider = st.slider("Number of Parts in Study", 20, 100, 50, 5, help="The total number of unique parts (both good and bad) that will be assessed.")
        prevalence_slider = st.slider("True Defect Rate in Parts (%)", 10, 50, 20, 5, help="The percentage of parts in the study that are known to be defective. A good study has a high prevalence of defects to properly challenge the inspectors.")
        st.markdown("**Inspector Archetypes**")
        skilled_acc_slider = st.slider("Skilled Inspector Accuracy (%)", 85, 100, 98, 1, help="The base accuracy of your best, most experienced inspector.")
        uncertain_acc_slider = st.slider("Uncertain Inspector Accuracy (%)", 70, 100, 90, 1, help="The base accuracy of an inspector who struggles with borderline cases. Their performance will degrade on ambiguous parts.")
        biased_acc_slider = st.slider("Biased Inspector Accuracy (%)", 70, 100, 92, 1, help="The base accuracy of an inspector who is generally good but has a specific bias.")
        bias_strength_slider = st.slider("Biased Inspector 'Safe Play' Bias", 0.5, 1.0, 0.8, 0.05, help="When the Biased Inspector is unsure about a GOOD part, what is the probability they will fail it to be safe? 0.5 is no bias; 1.0 is maximum bias.")

    fig_eff, fig_kappa, kappa, df_eff = plot_attribute_agreement(
        n_parts=n_parts_slider, n_replicates=3,
        prevalence=prevalence_slider/100.0, 
        skilled_accuracy=skilled_acc_slider/100.0,
        uncertain_accuracy=uncertain_acc_slider/100.0,
        biased_accuracy=biased_acc_slider/100.0,
        bias_strength=bias_strength_slider
    )

    st.header("Results Dashboard")
    col1, col2 = st.columns([0.4, 0.6])
    with col1:
        st.subheader("Overall Study Metrics")
        st.metric("Fleiss' Kappa (Overall Agreement)", f"{kappa:.3f}", help="Measures agreement between all inspectors, corrected for chance. >0.7 is considered substantial agreement.")
        st.markdown("##### Individual Inspector Performance")
        st.dataframe(df_eff.style.format({"Miss Rate": "{:.2%}", "False Alarm Rate": "{:.2%}", "Accuracy": "{:.2%}"}), use_container_width=True)
        
    with col2:
        st.plotly_chart(fig_eff, use_container_width=True)
    
    st.plotly_chart(fig_kappa, use_container_width=True)

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Check Overall Agreement (Fleiss' Kappa):** The first KPI tells you if the inspection team, as a whole, is consistent. A low Kappa (<0.7) signals a systemic problem with the procedure or training.
        2.  **Diagnose Individual Performance (Effectiveness Plot):** This plot is the main tool for root cause analysis.
            - **Inspector A (Skilled)** should be in the bottom-left "Ideal Zone."
            - **Inspector B (Uncertain)** will drift away from the ideal zone as you increase defect prevalence, because there are more borderline parts to confuse them. This signals a need for better training or clearer defect standards.
            - **Inspector C (Biased)** will drift to the right (high False Alarm Rate). This shows they are incorrectly failing good product, indicating they are either misinterpreting a standard or are being overly cautious.
        3.  **Find Disagreements (Kappa Matrix):** This heatmap shows *who* disagrees with *whom*. A low Kappa value between Inspector B and C, for example, would be expected from the simulation. This tells you exactly which two inspectors need to sit down together with the defect library to align their criteria.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: Validating Your Most Complex Measurement System‚ÄîYour People
    
        #### The Problem: The Myth of Objective Inspection
        A critical manufacturing process relies on human visual inspection to identify and remove cosmetic defects before final packaging. Management assumes that because the inspectors are trained on a written SOP and a "defect library," their judgments are consistent and accurate. However, there is no data to support this assumption.
    
        #### The Impact: The High Cost of Human Variability
        Relying on an unvalidated human inspection process creates significant, unmanaged business risk and operational inefficiency.
        - **Producer's Risk (The Overly Cautious Inspector):** An inspector who is risk-averse or poorly trained consistently rejects borderline-acceptable product "just to be safe." This drives up the **scrap rate**, directly reducing the process yield and costing the company millions in lost product over a year.
        - **Consumer's Risk (The Inattentive Inspector):** An inspector who is fatigued or disengaged consistently misses true, critical defects. This leads to **customer complaints, product recalls, and severe brand damage**. In a medical device or pharma context, this is a major patient safety failure.
        - **The "Re-Inspect" Loop:** When two inspectors disagree on a batch, it triggers a time-consuming and costly re-inspection process involving a "tie-breaker" or a committee, creating a major bottleneck in the production flow.
    
        #### The Solution: A Gage R&R for Human Judgment
        An Attribute Agreement Analysis is, in effect, a **Gage R&R study for your human inspectors**. It is a systematic, data-driven experiment designed to quantify the performance of your inspection team. It moves beyond assumption and provides objective evidence to answer three critical questions:
        1.  **Are we consistent? (Intra-rater reliability):** Can a single inspector make the same call on the same part repeatedly?
        2.  **Are we aligned? (Inter-rater reliability):** Do different inspectors make the same call on the same part? (Measured by Kappa).
        3.  **Are we right? (Accuracy):** Does our team's judgment agree with the known "gold standard" for what is truly good and bad? (Measured by Miss Rate and False Alarm Rate).
    
        #### The Consequences: A Reliable, Data-Driven Inspection Process
        - **Without This:** The inspection process is a "black box" of subjective opinion. The company has no data to defend its quality decisions and is exposed to significant financial and compliance risk from human variability.
        - **With This:** The analysis provides a quantitative, objective performance report for the entire inspection system. It identifies specific inspectors who need targeted retraining, highlights ambiguities in the defect standards, and provides the **formal validation evidence** required by auditors. It is the essential tool for transforming a subjective art into a reliable, controlled, and data-driven business process.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Agreement Terms
        - **Attribute Data:** Data that is categorical or discrete, such as pass/fail, good/bad, or a defect classification.
        - **Miss Rate (False Negative):** The proportion of known defective parts that an inspector incorrectly classified as good. This represents **consumer's risk**.
        - **False Alarm Rate (False Positive):** The proportion of known good parts that an inspector incorrectly classified as defective. This represents **producer's risk**.
        - **Inter-Rater Reliability:** The degree of agreement among different inspectors (raters).
        - **Cohen's Kappa (Œ∫):** A statistic that measures inter-rater agreement for categorical items, while taking into account the possibility of the agreement occurring by chance.
        - **Fleiss' Kappa:** An adaptation of Cohen's Kappa for measuring agreement between a fixed number of raters (more than two).
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Percent Agreement" Trap**
An analyst simply calculates that all inspectors agreed with the standard 95% of the time and declares the system valid.
- **The Flaw:** If the study only contains 2% true defects, an inspector could pass *every single part* and still achieve 98% agreement! Simple percent agreement is dangerously misleading with imbalanced data.""")
        st.success("""üü¢ **THE GOLDEN RULE: Use Kappa for Consistency, and Effectiveness for Risk**
A robust analysis separates two key questions that must be answered.
1.  **Are the inspectors CONSISTENT? (Precision)** This is about whether the inspectors agree with **each other**. The **Kappa Matrix** is the best tool for this, as it corrects for chance agreement and pinpoints specific disagreements.
2.  **Are the inspectors ACCURATE? (Bias/Error)** This is about whether the inspectors agree with the **truth** (the gold standard). The **Effectiveness Report** is the best tool for this, as it separates the two types of business and patient risk: Miss Rate (Consumer's Risk) and False Alarm Rate (Producer's Risk).""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: Beyond Simple Percentages
        **The Problem:** For decades, researchers in social sciences and medicine struggled to quantify the reliability of subjective judgments. Simple "percent agreement" was the common method, but it had a fatal flaw: it didn't account for agreement that could happen purely by chance. Two doctors who both diagnose 90% of patients with "common cold" will have high agreement, but their skill might be no better than a coin flip if the true rate is 90%.

        **The 'Aha!' Moment:** In 1960, the psychologist **Jacob Cohen** developed **Cohen's Kappa (Œ∫)**, a statistic that brilliantly solved this problem. Kappa measures the *observed* agreement and then subtracts the *agreement expected by chance*, creating a much more robust measure of true inter-rater reliability. This concept was later extended by **Joseph L. Fleiss** in 1971 to handle cases with more than two raters, resulting in **Fleiss' Kappa**.
            
        **The Impact:** Kappa statistics became the gold standard for measuring agreement in fields from psychology to clinical diagnostics. The automotive industry, in its quest for quality, recognized that a human inspector is a "measurement system." They incorporated these advanced statistical techniques into their **Measurement Systems Analysis (MSA)** manual, which is now considered the global standard, codifying Attribute Agreement Analysis as an essential tool for any industry relying on human inspection.
        """)
        
    with tabs[5]:
        st.markdown("""
        This analysis is a key part of **Measurement Systems Analysis (MSA)**, which is a fundamental expectation of a robust quality system.
        - **FDA Process Validation Guidance & 21 CFR 820 (QSR):** Both require that all measurement systems used for process control and product release be validated and fit for purpose. This explicitly includes human inspection systems. A documented Attribute Agreement Analysis is the objective evidence that this requirement has been met.
        - **ICH Q9 (Quality Risk Management):** A poorly performing inspection system is a major quality risk. This analysis quantifies that risk (e.g., the Miss Rate is a direct measure of patient/consumer risk) and provides the data to justify mitigation, such as retraining or implementing automated inspection.
        - **Regulatory Audits:** A lack of qualification for visual inspection processes is a common finding during regulatory audits. Having a robust Attribute Agreement study in your validation package demonstrates a mature and compliant quality system.
        """)
        
#=============================================================== 10. COMPREHENSIVE DIAGNOSTIC VALIDATION ===================================================
def render_diagnostic_validation_suite():
    """Renders the comprehensive, interactive module for diagnostic test validation."""
    st.markdown("""
    #### Purpose & Application: The Definitive Diagnostic Scorecard
    **Purpose:** To provide a single, comprehensive dashboard that unites all key statistical metrics used to validate a diagnostic test. This tool moves beyond individual metrics to show how they are all interconnected and derived from the foundational **Confusion Matrix**.
    
    **Strategic Application:** This is an essential tool for R&D, clinical validation, and regulatory affairs when developing an In Vitro Diagnostic (IVD). It allows teams to simulate how a test's performance characteristics and the target population's disease rate (prevalence) will impact its real-world clinical utility.
    """)
    
    st.info("""
    **Interactive Demo:** Use the three main sliders in the sidebar to simulate any diagnostic scenario.
    - **Sensitivity & Specificity:** Control the *intrinsic quality* of your assay.
    - **Prevalence:** Control the *population* in which the test is used. Observe the powerful "Prevalence Effect" in Plot 3, and see how the predictive values in the KPI panel change.
    """)

    with st.sidebar:
        st.subheader("Diagnostic Test Controls")
        sensitivity_slider = st.slider("üéØ Sensitivity (True Positive Rate)", 0.80, 1.00, 0.98, 0.005, format="%.3f",
            help="The intrinsic ability of the test to correctly identify true positives. A value of 0.98 means it will correctly detect 98 out of every 100 diseased individuals.")
        specificity_slider = st.slider("üõ°Ô∏è Specificity (True Negative Rate)", 0.80, 1.00, 0.95, 0.005, format="%.3f",
            help="The intrinsic ability of the test to correctly identify true negatives. A value of 0.95 means it will correctly clear 95 out of every 100 healthy individuals.")
        prevalence_slider = st.slider("üìà Disease Prevalence (%)", 0.1, 50.0, 5.0, 0.1, format="%.1f%%",
            help="The percentage of the target population that actually has the disease. This is a property of the population, not the test itself, but it has a massive impact on the test's real-world predictive power.")

    fig_cm, fig_roc, fig_pv, metrics, other_concepts = plot_diagnostic_dashboard(
        sensitivity=sensitivity_slider,
        specificity=specificity_slider,
        prevalence=prevalence_slider/100.0
    )
    
    st.header("Diagnostic Performance Dashboard")
    col1, col2 = st.columns(2)
    with col1:
        st.plotly_chart(fig_cm, use_container_width=True)
    with col2:
        st.plotly_chart(fig_roc, use_container_width=True)
    
    st.plotly_chart(fig_pv, use_container_width=True)
    
    st.subheader("Comprehensive Metrics Panel")
    with st.expander("Click to view all 24 calculated and conceptual metrics", expanded=True):
        st.markdown("##### Foundational Metrics (The Building Blocks)")
        c1, c2, c3, c4, c5 = st.columns(5)
        c1.metric("True Positives (TP)", metrics["True Positive (TP)"], help="Correctly identified as diseased.")
        c2.metric("True Negatives (TN)", metrics["True Negative (TN)"], help="Correctly identified as healthy.")
        c3.metric("False Positives (FP)", metrics["False Positive (FP)"], help="Healthy individuals incorrectly identified as diseased.")
        c4.metric("False Negatives (FN)", metrics["False Negative (FN)"], help="Diseased individuals incorrectly identified as healthy.")
        c5.metric("Prevalence", f"{metrics['Prevalence']:.1%}", help="The proportion of the population that truly has the disease.")

        st.markdown("##### Core Rates & Error Types (Intrinsic Test Quality)")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("Sensitivity (TPR / Power)", f"{metrics['Sensitivity (TPR / Power)']:.2%}", help="TPR = TP / (TP + FN). The ability to detect the disease when present. Also called Recall or Power.")
        c2.metric("Specificity (TNR)", f"{metrics['Specificity (TNR)']:.2%}", help="TNR = TN / (TN + FP). The ability to correctly clear healthy individuals.")
        c3.metric("False Positive Rate (Œ±)", f"{metrics['False Positive Rate (Œ±)']:.2%}", help="FPR = FP / (TN + FP) = 1 - Specificity. The probability of a false alarm. Also called Type I Error.")
        c4.metric("False Negative Rate (Œ≤)", f"{metrics['False Negative Rate (Œ≤)']:.2%}", help="FNR = FN / (TP + FN) = 1 - Sensitivity. The probability of missing a true case. Also called Type II Error.")

        st.markdown("##### Predictive Values (Real-World Performance in this Population)")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("PPV (Precision)", f"{metrics['PPV (Precision)']:.2%}", help="PPV = TP / (TP + FP). If a patient tests positive, what is the probability they are truly diseased?")
        c2.metric("NPV", f"{metrics['NPV']:.2%}", help="NPV = TN / (TN + FN). If a patient tests negative, what is the probability they are truly healthy?")
        c3.metric("False Discovery Rate (FDR)", f"{metrics['False Discovery Rate (FDR)']:.2%}", help="FDR = FP / (TP + FP) = 1 - PPV. The proportion of positive results that are false positives.")
        c4.metric("False Omission Rate (FOR)", f"{metrics['False Omission Rate (FOR)']:.2%}", help="FOR = FN / (TN + FN) = 1 - NPV. The proportion of negative results that are false negatives.")

        st.markdown("##### Overall Performance & Agreement Scores (Single-Number Summaries)")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("Accuracy", f"{metrics['Accuracy']:.2%}", help="ACC = (TP + TN) / Total. Overall correctness. Can be misleading with imbalanced data.")
        c2.metric("F1 Score", f"{metrics['F1 Score']:.3f}", help="The harmonic mean of Precision (PPV) and Recall (Sensitivity). Best when you need a balance between them.")
        c3.metric("MCC", f"{metrics['Matthews Correlation Coefficient (MCC)']:.3f}", help="A robust correlation coefficient between -1 and +1. +1 is a perfect prediction, 0 is random, -1 is perfectly wrong. Excellent for imbalanced data.")
        c4.metric("Cohen's Kappa (Œ∫)", f"{metrics['Cohen‚Äôs Kappa (Œ∫)']:.3f}", help="Measures agreement between the prediction and reality, corrected for agreement that could happen by chance. Similar to MCC.")

        st.markdown("##### Advanced & Model-Based Metrics")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("AUC", f"{metrics['Area Under Curve (AUC)']:.3f}", help="Area Under the ROC Curve. The probability a random diseased individual has a higher score than a random healthy one. A measure of overall test separability.")
        # --- THIS IS THE DEFINITIVE FIX ---
        # Get the value from the dictionary first
        youdens_value = metrics["Youden's Index (J)"]
        # Then use the variable in the f-string, which is always safe
        c2.metric("Youden's Index (J)", f"{youdens_value:.3f}", help="J = Sensitivity + Specificity - 1. The maximum vertical distance between the ROC curve and the diagonal line. Finds a single 'optimal' cutoff.")
        # --- END OF DEFINITIVE FIX ---
        c3.metric("Positive LR (+)", f"{metrics['Positive Likelihood Ratio (LR+)']:.2f}", help="LR+ = Sens / (1 - Spec). How much more likely a positive test is to be seen in a diseased person vs. a healthy one. >10 is considered strong evidence.")
        c4.metric("Log-Loss", f"{metrics['Log-Loss (Cross-Entropy)']:.3f}", help="A measure of a probabilistic model's accuracy. It heavily penalizes being confidently wrong. Lower is better.")
        
        st.markdown("---")
        st.markdown("##### Conceptual Terms")
        for term, definition in other_concepts.items():
            st.markdown(f"**{term}:** {definition}")

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **The Prevalence Effect: The Most Important Insight**
        The most critical takeaway from this dashboard is the relationship between a test's intrinsic quality and its real-world performance, visualized in **Plot 3**.
        - **Sensitivity & Specificity** are properties of the *test itself*. They do not change based on who you test.
        - **Positive Predictive Value (PPV)** and **Negative Predictive Value (NPV)** are properties of the *test result in a specific population*. They are highly dependent on **Prevalence**.
        
        **Try this:** Set Sensitivity and Specificity to high values (e.g., 99%). Now, in the sidebar, move the **Prevalence** slider from 50% down to 1%. Watch how the PPV curve (red line in Plot 3) collapses at low prevalence. This demonstrates the **false positive paradox**: even a highly accurate test can produce a majority of false positives when used to screen a low-prevalence population.
        
        **Advanced Metrics Explained:**
        - **Likelihood Ratios (LR+ / LR-):** How much does a positive/negative result increase/decrease the odds of having the disease? They are powerful because, unlike PPV/NPV, they are independent of prevalence.
        - **MCC & Kappa:** These are advanced accuracy metrics that are robust to imbalanced data (unlike simple Accuracy). A score of +1 is perfect, 0 is random, and -1 is perfectly wrong. **MCC** is generally considered one of the most robust and informative single-number scores for a classifier.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: Defining Your Product's Clinical and Commercial Value
    
        #### The Problem: The "My Test is 99% Accurate" Fallacy
        An R&D team develops a new diagnostic test and proudly reports that its intrinsic accuracy is excellent (e.g., 99% sensitivity and 99% specificity). The marketing team begins preparing a launch campaign based on this headline number. However, no one has performed a formal analysis of how the test will actually perform in its target clinical setting.
    
        #### The Impact: Market Failure and Misleading a Clinical Community
        This failure to connect intrinsic performance to real-world application has severe consequences:
        - **The False Positive Paradox:** The test is launched as a general screening tool for a rare disease (1% prevalence). Doctors are soon overwhelmed by the number of **false positives**. Even with 99% specificity, for every 1 true positive patient, there will be 1 false positive patient (PPV ‚âà 50%). Doctors quickly lose confidence in the test, adoption plummets, and the product launch fails.
        - **Wrong Clinical Niche:** The test might be a poor screening tool but could have been an excellent **confirmatory test** for a high-prevalence, symptomatic population. By failing to do this analysis upfront, the company has pursued the wrong market with the wrong value proposition.
        - **Reimbursement and Health Economic Challenges:** Payers (insurance companies) will not reimburse a test that generates a high rate of false positives, as it leads to unnecessary and costly follow-up procedures. The company has failed to establish the health economic value of its product.
    
        #### The Solution: A Holistic, Context-Aware Validation
        This comprehensive dashboard is not just a collection of statistics; it is a **strategic planning tool**. It forces the business to move beyond simplistic accuracy claims and answer the critical questions that define a product's true value:
        1.  **Who is our target population?** (This defines the **Prevalence**).
        2.  **What is the clinical need?** Is it for screening (where high Sensitivity and NPV are paramount) or confirmation (where high Specificity and PPV are critical)?
        3.  **What is our value proposition?** How much does a positive result from our test change a doctor's certainty? (Quantified by the **Likelihood Ratio**).
    
        #### The Consequences: A Successful Launch and a Trusted Product
        - **Without This:** A diagnostic product launch is a high-risk gamble. The company is flying blind to the clinical and economic realities of its target market.
        - **With This:** The validation suite provides the complete, data-driven narrative required for success. It allows the company to **precisely define the correct clinical niche**, create a **powerful and honest value proposition** for doctors and payers, and submit a **robust, defensible data package** to regulators. It is the essential tool for turning a scientific invention into a commercially successful and clinically valuable diagnostic product.
        """)
    with tabs[2]:
        st.error("""üî¥ **THE INCORRECT APPROACH: "Accuracy is Everything"**
An analyst reports that their new test has "95% accuracy" and declares it a success.
- **The Flaw:** Accuracy is a simple but often misleading metric, especially with imbalanced data. A test for a rare disease (1% prevalence) can achieve 99% accuracy by simply calling every single patient "Healthy." It has perfect specificity but zero sensitivity, making it clinically useless.""")
        st.success("""üü¢ **THE GOLDEN RULE: Validate for the Intended Use**
A robust diagnostic validation always considers the clinical context.
1.  **Define the Intended Use:** Is this a screening test for the general population (low prevalence) or a confirmatory test for symptomatic patients (high prevalence)?
2.  **Choose the Right Metrics:** For a screening test, high **Sensitivity** and **NPV** are critical (you must not miss a case). For a confirmatory test before a risky procedure, high **Specificity** and **PPV** are paramount (you must not treat a healthy person).
3.  **Evaluate Holistically:** Use advanced, prevalence-independent metrics like **Likelihood Ratios**, **MCC**, or **AUC** to get a complete picture of test performance, but always report the PPV/NPV expected for the target population.
        """)

    with tabs[3]:
        st.markdown("""
        #### Historical Context: A Multi-Disciplinary Synthesis
        The metrics on this dashboard were not developed at once, but represent a synthesis of ideas from epidemiology, statistics, computer science, and psychology over the 20th century.
        - **Epidemiology (1940s-50s):** The concepts of **Sensitivity, Specificity, and Prevalence** were formalized to understand the performance of disease screening programs.
        - **Signal Detection Theory (1950s):** The **ROC Curve** was developed to distinguish radar signals from noise, and was later adopted by psychologists. **Youden's Index** was proposed in 1950 as a simple way to summarize a test's performance in a single number.
        - **Bayesian Statistics (1763, revived 1990s):** **Bayes' Theorem** is the mathematical engine that links prevalence (prior probability) to **PPV/NPV** (posterior probability). **Likelihood Ratios** are a direct application of this theorem.
        - **Psychology (1960s):** **Cohen's Kappa** was developed by Jacob Cohen to measure inter-rater reliability, correcting for the probability that two raters might agree simply by chance.
        - **Machine Learning (1990s-2000s):** As classifiers became common, metrics were needed to handle imbalanced datasets. The **F1 Score** emerged from information retrieval, while the **Matthews Correlation Coefficient (MCC)** was developed in bioinformatics (1980). **Log-Loss** became a standard for evaluating probabilistic forecasts.
        """)

    with tabs[4]:
        st.markdown("""
        Demonstrating the performance of a diagnostic test is a primary focus of global medical device and IVD regulations.
        - **FDA 21 CFR 820.30 (Design Controls for Medical Devices):** Requires **design validation** to ensure devices conform to defined user needs and intended uses. For an IVD, this is proven through clinical performance studies that establish the metrics on this dashboard.
        - **EU IVDR (In Vitro Diagnostic Regulation 2017/746):** Requires a comprehensive **Performance Evaluation Report (PER)**. This report must contain detailed data and analysis of the test's **analytical performance** (e.g., precision) and **clinical performance** (sensitivity, specificity, PPV, NPV, LR+). All of these metrics are explicitly mentioned.
        - **CLSI Guidelines (Clinical & Laboratory Standards Institute):** Documents like **EP12-A2** provide detailed protocols for user-based evaluation of qualitative test performance, including the calculation of sensitivity, specificity, and predictive values.
        - **GAMP 5:** If the test's result is generated by software (e.g., an algorithm that analyzes an image), that software must be validated (CSV) to ensure its calculations are correct and reliable.
        """)

#=============================================================== 11. ROC CURVE ANALYSIS ===================================================
def render_roc_curve():
    """Renders the INTERACTIVE module for Receiver Operating Characteristic (ROC) curve analysis."""
    st.markdown("""
    #### Purpose & Application
    **Purpose:** To solve **The Diagnostician's Dilemma**: a test must correctly identify patients with a disease (high **Sensitivity**) while also correctly clearing healthy patients (high **Specificity**). The ROC curve visualizes this trade-off.
    
    **Strategic Application:** This is the global standard for validating diagnostic tests. The Area Under the Curve (AUC) provides a single metric of a test's diagnostic power, while the choice of cutoff determines its real-world performance.
    """)
    
    st.info("""
    **Interactive Demo:** Use all three sliders to see how assay quality and decision-making interact.
    - **`Separation` & `Overlap`** control the fundamental quality of the assay.
    - **`Decision Cutoff`**: Move this slider to see the shaded TP/FP/TN/FN areas change in the top plot. Simultaneously, watch the black 'X' move along the ROC curve in the bottom plot, and see the real-time impact on all the performance metrics.
    """)
    
    # --- Sidebar controls for this specific module ---
    with st.sidebar:
        st.subheader("ROC Curve Controls")
        separation_slider = st.slider(
            "üìà Separation (Diseased Mean)", 
            min_value=50.0, max_value=80.0, value=65.0, step=1.0,
            help="Controls the distance between the Healthy and Diseased populations. More separation = better test."
        )
        overlap_slider = st.slider(
            "üå´Ô∏è Overlap (Population SD)", 
            min_value=5.0, max_value=20.0, value=10.0, step=0.5,
            help="Controls the 'noise' or spread of the populations. More overlap (a higher SD) = worse test."
        )
        # --- NEW SLIDER ADDED HERE ---
        cutoff_slider = st.slider(
            "üî™ Decision Cutoff",
            min_value=30, max_value=80, value=55, step=1,
            help="The threshold for calling a sample 'Positive'. Move this to trace the ROC curve and see the trade-offs."
        )

    # Generate plots using the slider values
    fig, auc_value, sensitivity, specificity, ppv, npv = plot_roc_curve(
        diseased_mean=separation_slider, 
        population_sd=overlap_slider,
        cutoff=cutoff_slider
    )
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.metric(label="üìà Overall KPI: Area Under Curve (AUC)", value=f"{auc_value:.3f}",
                      help="The overall diagnostic power of the test. 0.5 is useless, 1.0 is perfect.")
            st.markdown(f"--- \n ##### Performance at Cutoff = {cutoff_slider}")
            st.metric(label="üéØ Sensitivity (True Positive Rate)", value=f"{sensitivity:.2%}",
                      help="Of all the truly diseased patients, what percentage did we correctly identify?")
            st.metric(label="üõ°Ô∏è Specificity (True Negative Rate)", value=f"{specificity:.2%}",
                      help="Of all the truly healthy patients, what percentage did we correctly clear?")
            st.metric(label="‚úÖ Positive Predictive Value (PPV)", value=f"{ppv:.2%}",
                      help="If a patient tests positive, what is the probability they actually have the disease?")
            st.metric(label="‚ùå Negative Predictive Value (NPV)", value=f"{npv:.2%}",
                      help="If a patient tests negative, what is the probability they are actually healthy?")
            
            st.markdown("""
            **The Core Insight:** The AUC tells you how good your *assay* is. The four metrics on the right tell you how good your *decision* is at a specific cutoff. A great assay can still lead to poor outcomes if the wrong cutoff is chosen for the clinical context.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: Quantifying Your Assay's "IQ Score"
        
            #### The Problem: The Ambiguous Development Goal
            An R&D team is tasked with developing a new diagnostic assay. They are working with several different antibody pairs and formulation buffers, but they lack a single, objective metric to compare their various prototypes. Team A claims their prototype is "more sensitive," while Team B claims theirs is "more specific." Management has no clear, quantitative way to decide which prototype to advance into the expensive, multi-million dollar clinical validation phase.
        
            #### The Impact: Wasted R&D Investment and Uncompetitive Products
            This lack of a unified performance metric leads to poor strategic decisions:
            - **Investing in the Wrong Horse:** The company may spend millions developing a prototype that is ultimately inferior, only discovering this fact late in the validation process when it's too late to change course.
            - **"Death by a Thousand Tweaks":** Without a clear target, R&D can spend months in an endless cycle of "optimization," making small, incremental changes without knowing if they are truly improving the assay's fundamental diagnostic power.
            - **Launching a "B-Grade" Product:** The company might unknowingly launch a product with a mediocre AUC (e.g., 0.85) into a market where a competitor's product has a best-in-class AUC of 0.95. The market will eventually discover this, and the product will fail to gain traction.
        
            #### The Solution: The AUC as the Universal "IQ Score"
            ROC Curve analysis and its summary statistic, the **Area Under the Curve (AUC)**, provide the solution. The AUC is the single, universal "IQ score" for a diagnostic assay. It quantifies the test's overall ability to **discriminate** between the healthy and diseased populations, independent of any single decision cutoff.
            
            This allows R&D and business leaders to:
            1.  **Objectively Compare Prototypes:** An assay with an AUC of 0.95 is unequivocally better than one with an AUC of 0.85. This provides a data-driven basis for go/no-go decisions.
            2.  **Set Clear Development Targets:** The project can be given a clear, quantitative goal: "The final assay must achieve a minimum AUC of 0.92 to be commercially viable."
            3.  **Optimize the Right Thing:** The R&D team's goal is no longer a vague notion of "good performance," but the specific, measurable task of maximizing the AUC by increasing the separation and reducing the overlap of the two populations.
        
            #### The Consequences: A Data-Driven R&D Pipeline and Best-in-Class Products
            - **Without This:** Diagnostic R&D is a subjective, inefficient process with a high risk of producing uncompetitive products.
            - **With This:** ROC analysis provides the quantitative backbone for the entire diagnostic development pipeline. It enables a data-driven culture where projects are funded, advanced, and launched based on objective, quantifiable measures of their diagnostic power, leading to a higher probability of developing best-in-class products.
            """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of ROC Terms
            - **ROC Curve:** A plot of the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) for all possible cutoff values of a diagnostic test.
            - **AUC (Area Under the Curve):** A single metric (0.5 to 1.0) summarizing the overall diagnostic power of a test. 0.5 is random chance; 1.0 is a perfect test.
            - **Cutoff / Threshold:** The specific test value used to make a decision (e.g., classify a patient as "diseased" or "healthy").
            - **Sensitivity (TPR):** The ability of the test to correctly identify individuals who *have* the disease.
            - **Specificity (TNR):** The ability of the test to correctly identify individuals who do *not* have the disease.
            - **Youden's Index (J):** A statistic that captures the performance of a diagnostic test. `J = Sensitivity + Specificity - 1`. The cutoff that maximizes J is the point on the ROC curve furthest from the random chance line.
            """)
        with tabs[3]:
            st.error("""üî¥ **THE INCORRECT APPROACH: "Worship the AUC" & "Hug the Corner"**
- *"My AUC is 0.95, so we're done."* (The *chosen cutoff* might still be terrible for the clinical need).
- *"I'll just pick the cutoff closest to the top-left corner."* (This balances errors equally, which is rarely desired).""")
            st.success("""üü¢ **THE GOLDEN RULE: The Best Cutoff Depends on the Consequence of Being Wrong**
Ask: **"What is worse? A false positive or a false negative?"**
- **For deadly disease screening:** You must catch every possible case. Prioritize **maximum Sensitivity**.
- **For confirming a diagnosis for a risky surgery:** You must be certain the patient has the disease. Prioritize **maximum Specificity** to avoid unnecessary procedures.""")

        with tabs[4]:
            st.markdown("""
            #### Historical Context: From Radar Blips to Medical Labs
            **The Problem:** During World War II, engineers were developing radar to detect enemy aircraft. They faced a classic signal-detection problem: how do you set the sensitivity of the receiver? If it's too sensitive, it will pick up random noise (birds, atmospheric clutter) as enemy planes (a **false alarm**). If it's not sensitive enough, it will miss real enemy planes (a **missed hit**).

            **The 'Aha!' Moment:** Engineers needed a way to visualize this trade-off. They developed the **Receiver Operating Characteristic (ROC)** curve. It plotted the probability of a "hit" (True Positive Rate) against the probability of a "false alarm" (False Positive Rate) for every possible sensitivity setting of the receiver. This allowed them to quantify the performance of different radar systems and choose the optimal operating point.
            
            **The Impact:** After the war, the technique was adapted by psychologists for signal detection in perception experiments. In the 1970s, it was introduced to medicine, where it became the undisputed gold standard for evaluating the performance of diagnostic tests, providing a clear, graphical language to communicate the critical trade-off between sensitivity and specificity.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("The curve plots **Sensitivity (Y-axis)** versus **1 - Specificity (X-axis)**.")
            st.latex(r"\text{Sensitivity} = \frac{TP}{TP + FN} \quad , \quad \text{Specificity} = \frac{TN}{TN + FP}")
            st.markdown("Each point on the curve represents the (Sensitivity, 1-Specificity) pair for a specific cutoff value. The **Area Under the Curve (AUC)** has a powerful probabilistic interpretation: it is the probability that a randomly chosen 'Diseased' subject will have a higher test score than a randomly chosen 'Healthy' subject.")
        with tabs[5]:
            st.markdown("""
            ROC analysis is the global standard for demonstrating the clinical performance of In Vitro Diagnostics (IVDs) and medical devices.
            - **FDA 21 CFR 820 (Quality System Regulation):** The design validation section (¬ß820.30(g)) requires objective evidence that the device conforms to user needs and intended uses. For a diagnostic, this evidence is typically clinical sensitivity and specificity, which are summarized by ROC analysis.
            - **EU IVDR (In Vitro Diagnostic Regulation):** The European regulation requires a Performance Evaluation Report (PER) that includes data on clinical sensitivity, specificity, and the rationale for the chosen cutoff value.
            - **ISO 13485:2016:** The international quality management standard for medical devices, which aligns with the principles of design validation found in 21 CFR 820.
            """)

def render_component_reliability():
    """Renders the comprehensive module for Component Reliability Testing."""
    st.markdown("""
    #### Purpose & Application: The Science of Longevity
    **Purpose:** To proactively test the reliability of a critical component and model its failure characteristics using a **Weibull distribution**. This analysis allows us to make statistically sound predictions about a component's expected life and failure rate.
    
    **Strategic Application:** This is a core activity in **Design for Reliability (DfR)**. Instead of waiting for field data, we generate life data in a controlled lab test. This is essential for:
    - Setting realistic warranty periods and service contracts.
    - Qualifying new suppliers for critical components.
    - Making data-driven design choices between competing component options.
    """)
    st.info("""
    **Interactive Demo:** You are the Reliability Engineer.
    1.  Use the **"Life Test Design"** controls to define your experiment: how many units to test (`n`) and for how long (`Test Duration`).
    2.  Use the **"True Component Characteristics"** sliders to define the "ground truth" of the component you are testing.
    3.  Observe the results. The **Probability Plot** shows how well the model fits the data, while the **Reliability Curve** provides key business metrics like the **B10 Life**.
    """)

    with st.sidebar:
        st.subheader("Life Test Design")
        n = st.slider("Number of Units on Test (n)", 10, 100, 30, 5)
        test_duration = st.slider("Test Duration (Hours)", 100, 5000, 1000, 100, help="The test is stopped at this time. Any units still running are 'right-censored'.")
        
        st.subheader("True Component Characteristics")
        beta = st.slider("Shape Parameter (Œ≤)", 0.8, 5.0, 1.5, 0.1, help="Controls the failure mode. Œ≤<1: Infant mortality. Œ≤=1: Random failures. Œ≤>1: Wear-out failures.")
        eta = st.slider("Scale Parameter (Œ∑)", 500, 10000, 2000, 100, help="The characteristic life. The time at which ~63.2% of the population will have failed.")

    st.header("Reliability Test Results Dashboard")
    
    try:
        fig, fit_beta, fit_eta, b10_life = plot_reliability_weibull(n, beta, eta, test_duration)
        
        col1, col2, col3 = st.columns(3)
        col1.metric("Fitted Shape (Œ≤)", f"{fit_beta:.2f}", help="The model's estimate of the failure mode. If >1, indicates wear-out.")
        col2.metric("Fitted Life (Œ∑)", f"{fit_eta:.0f} hrs", help="The model's estimate of the characteristic life.")
        col3.metric("B10 Life", f"{b10_life:.0f} hrs", help="The time at which 10% of the population is predicted to have failed. A key reliability metric.")

        st.plotly_chart(fig, use_container_width=True)
    except Exception as e:
        st.error(f"Could not fit the model. This can happen with very high censoring. Please try increasing the Test Duration or adjusting parameters. Error: {e}")


    st.divider()
    st.subheader("Deeper Dive into Reliability Testing")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Check the Fit (Probability Plot):** The first plot on the left is your primary diagnostic. If the data points (blue dots) fall in a reasonably straight line, it confirms that the Weibull distribution is a good model for your component's failure behavior.
        2.  **Diagnose the Failure Mode (Shape Parameter Œ≤):** The fitted Beta is a critical insight.
            - **Œ≤ < 1.0 (Infant Mortality):** Failures are happening early. This points to manufacturing defects or quality control issues.
            - **Œ≤ ‚âà 1.0 (Random Failures):** Failures are constant and unpredictable, often due to external shocks.
            - **Œ≤ > 1.0 (Wear-Out):** The failure rate increases with age. This is the expected behavior for components that wear out, like bearings or seals.
        3.  **Quantify Reliability (Reliability Curve & B10 Life):** The right-hand plot is your decision-making tool. The **B10 life** is a common and critical metric: it is the time at which you can be confident that no more than 10% of your components will have failed. This is often used to set warranty periods.

        **The Strategic Insight:** Try setting a short **Test Duration**. Notice how the confidence intervals on the Reliability Curve become much wider. This demonstrates the direct trade-off: shorter, cheaper tests provide less certain predictions about long-term reliability.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: From "Break-Fix" to a Data-Driven Warranty

        #### The Problem: The "Best Guess" Warranty Period
        A company is launching a new, expensive medical device. They need to set a warranty period. Without reliability data, this is a high-stakes guessing game. If they set it too short, customers will see the product as unreliable and won't buy it. If they set it too long, the company could be exposed to millions of dollars in un-costed warranty claims.

        #### The Impact: Unmanaged Financial Risk and Brand Damage
        This "guesswork" approach to reliability creates significant financial and reputational risks.
        - **Unpredictable Warranty Costs:** The company has no accurate way to forecast future warranty expenses, a major liability that can surprise investors and impact profitability.
        - **Reputation Damage:** A string of early-life failures can quickly destroy a brand's reputation for quality, especially in the high-stakes medical device market.
        - **Inefficient Design:** Without data, engineers don't know which components are the "weakest links" in their design. They might be over-engineering robust parts while unknowingly using an unreliable, low-cost component that will cause the majority of field failures.

        #### The Solution: A "Crystal Ball" for Component Life
        Component Reliability Testing is a proactive, data-driven strategy for managing this risk. By performing a structured "life test" on critical components *before* the product is launched, the company can build a **statistical crystal ball**. The Weibull analysis provides a mathematical model of the component's failure behavior, allowing the business to answer critical financial questions with data:
        1.  "If we offer a 2-year warranty, what is the **predicted percentage of units that will fail** within that period?"
        2.  "What is the **B10 life** of this component, and is it sufficient for our customer's needs?"
        3.  "Does the new, cheaper supplier's component have the **same reliability profile** as our current, expensive one?"

        #### The Consequences: A Profitable, Defensible, and Reliable Product
        - **Without This:** The company is gambling with its warranty costs and its brand reputation.
        - **With This:** Reliability testing provides the **objective, quantitative evidence** needed to make smart, data-driven business decisions. It allows the company to **accurately forecast and provision for warranty costs**, set a warranty period that is both competitive and profitable, and **engineer reliability into the product** by focusing improvement efforts on the components with the lowest predicted lifespan.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **Reliability Engineering:** A sub-discipline of systems engineering that emphasizes the ability of a system or component to function under stated conditions for a specified period of time.
        - **Life Test:** A controlled experiment where a set of components are operated under normal or accelerated conditions to produce failure data.
        - **Weibull Distribution:** A highly flexible probability distribution that is the cornerstone of reliability analysis because it can model a wide variety of failure behaviors.
        - **Shape Parameter (Œ≤, beta):** The parameter of the Weibull distribution that determines the *nature* of the failure mode (infant mortality, random, or wear-out).
        - **Scale Parameter (Œ∑, eta):** The "characteristic life" of the Weibull distribution. It is the time at which 63.2% of the population will have failed.
        - **B-Life (e.g., B10):** A reliability metric that represents the time at which a specified percentage (X%) of the population will have failed. The B10 life is the time at which 10% of units are expected to fail.
        - **Censored Data:** Data from a life test where the unit has not failed by the end of the test. This is critical information that must be included in the analysis.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Average Life" Trap**
An engineer tests 10 components until they all fail and simply calculates the average time to failure.
- **The Flaw:** This is extremely inefficient, as you must wait for the last, most reliable unit to fail, which could take years. More importantly, the simple average provides no information about the *failure mode* (the Œ≤ parameter). An average life of 1000 hours is meaningless if 30% of the units are failing in the first 100 hours due to infant mortality.""")
        st.success("""üü¢ **THE GOLDEN RULE: Model the Distribution, Don't Just Average the Data**
A robust reliability analysis focuses on understanding the entire failure distribution.
1.  **Use Censoring:** It is not necessary to run all parts to failure. A well-designed test with a pre-defined duration and censored data can provide a highly accurate model much more quickly and cheaply.
2.  **Fit a Life Distribution:** Use a statistical model like the Weibull distribution to analyze the data. This is essential for correctly handling censored data and for making predictions.
3.  **Focus on the Shape (Œ≤) and B-Life:** The two most important outputs are the Beta parameter (which tells you *why* things are failing) and the B-Life (which tells you the early-life failure probability), as these are often more critical for business decisions than the average life.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Ball Bearings to Space Shuttles
        **The Problem:** In the 1930s and 40s, engineers faced a puzzle. The lifetime of seemingly identical components, like ball bearings, showed enormous variability. Traditional statistical methods based on the normal distribution were a poor fit for this "time-to-failure" data.
        
        **The 'Aha!' Moment:** In 1937, a Swedish engineer and mathematician named **Waloddi Weibull** was investigating the strength of materials. He was looking for a flexible statistical distribution that could describe a wide range of real-world data. He didn't derive it from first principles; he pragmatically proposed a distribution (now named after him) that simply worked remarkably well. Its key feature was the **shape parameter (Œ≤)**, which allowed a single distribution to model failures that happened early (infant mortality), randomly, or late (wear-out).
        
        **The Impact:** While initially obscure, the Weibull distribution was adopted by the US Air Force and later by the automotive and aerospace industries in the 1950s and 60s as the cornerstone of the emerging field of **Reliability Engineering**. The need to predict the reliability of components for everything from jet engines to the Apollo spacecraft made Weibull analysis an essential tool. Its ability to make accurate predictions from **censored data** (tests that are stopped before all units fail) was a massive economic breakthrough, making reliability testing practical and affordable.
        """)
        
    with tabs[5]:
        st.markdown("""
        Component Reliability testing is a core component of Design Controls and is essential for ensuring the safety and effectiveness of medical devices and other complex systems.
        - **FDA 21 CFR 820.30 - Design Controls:** The regulation requires that the design shall be **verified** and **validated** to ensure it conforms to user needs and intended uses. For any device with a defined operational life, reliability testing is the objective evidence that the device will perform as intended for that duration.
        - **Design Verification:** Reliability testing is a key input to verification. It proves that the chosen components and design outputs meet their specified reliability inputs.
        - **Design Validation:** The overall device reliability, which is a function of its component reliabilities, is a critical user need that must be validated.
        - **Risk Management (ICH Q9 / ISO 14971):** The failure of a critical component is a major hazard. Reliability testing provides the quantitative data (the failure rate over time) needed to accurately assess this risk and implement appropriate mitigations (e.g., specifying a more reliable component, defining a preventative maintenance schedule).
        """)
#====================================================================================== 12. ASSAY ROBUSTNESS (DOE)  =================================================================================================   
def render_assay_robustness_doe():
    """Renders the comprehensive, interactive module for Assay Robustness (DOE/RSM)."""
    st.markdown("""
    #### Purpose & Application: Process Cartography - The GPS for Optimization
    **Purpose:** To create a detailed topographical map of your process landscape. This analysis moves beyond simple robustness checks to full **process optimization**, using **Response Surface Methodology (RSM)** to model curvature and find the true "peak of the mountain."
    
    **Strategic Application:** This is the statistical engine for Quality by Design (QbD) and process characterization. By developing a predictive model, you can:
    - **Find Optimal Conditions:** Identify the exact settings that maximize yield, efficacy, or any other Critical Quality Attribute (CQA).
    - **Define a Design Space:** Create a multi-dimensional "safe operating zone" where the process is guaranteed to produce acceptable results.
    - **Minimize Variability:** Find a "robust plateau" on the response surface where performance is not only high, but also insensitive to small variations.
    """)
    
    st.info("""
    **Interactive Demo:** You are the process expert. Use the sliders in the sidebar to define the "true" physics of a virtual assay. The plots will show how a DOE/RSM experiment can uncover this underlying response surface, allowing you to find the optimal operating conditions.
    """)
    
    with st.sidebar:
        st.subheader("DOE / RSM Controls (True Effects)")
        st.markdown("**Linear & Interaction Effects**")
        ph_slider = st.slider("üß¨ pH Main Effect", -10.0, 10.0, 2.0, 1.0)
        temp_slider = st.slider("üå°Ô∏è Temperature Main Effect", -10.0, 10.0, 5.0, 1.0)
        interaction_slider = st.slider("üîÑ pH x Temp Interaction Effect", -10.0, 10.0, 0.0, 1.0)
        
        st.markdown("**Curvature (Quadratic) Effects**")
        ph_quad_slider = st.slider("üß¨ pH Curvature", -10.0, 10.0, -5.0, 1.0, help="A negative value creates a 'hill' (peak). A positive value creates a 'bowl' (valley).")
        temp_quad_slider = st.slider("üå°Ô∏è Temperature Curvature", -10.0, 10.0, -5.0, 1.0)

        st.markdown("**Experimental Noise**")
        noise_slider = st.slider("üé≤ Random Noise (SD)", 0.1, 5.0, 1.0, 0.1)
    
    fig_contour, fig_3d, fig_pareto, anova_summary, opt_ph, opt_temp, max_resp = plot_doe_robustness(
        ph_effect=ph_slider, temp_effect=temp_slider, interaction_effect=interaction_slider,
        ph_quad_effect=ph_quad_slider, temp_quad_effect=temp_quad_slider, noise_sd=noise_slider
    )
    
    st.header("Results Dashboard")
    
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Statistical Analysis")
        st.markdown("The Pareto plot identifies the vital few factors, while the ANOVA table provides the statistical proof.")
        
        tab1, tab2 = st.tabs(["Pareto Plot of Effects", "ANOVA Table"])
        with tab1:
            st.plotly_chart(fig_pareto, use_container_width=True)
        with tab2:
            st.dataframe(anova_summary.style.format({'p-value': '{:.4f}'}).applymap(
                lambda p: 'background-color: #C8E6C9' if p < 0.05 else '', subset=['p-value']),
                use_container_width=True)
            
    with col2:
        st.subheader("Predicted Optimum")
        st.markdown("Based on the model, these are the settings predicted to maximize the response.")
        m1, m2, m3 = st.columns(3)
        m1.metric("Optimal pH", f"{opt_ph:.2f}")
        m2.metric("Optimal Temp (¬∞C)", f"{opt_temp:.2f}")
        m3.metric("Max Response", f"{max_resp:.1f}")
        
        st.markdown("""
        **Interpreting the Visuals:**
        - **Contour Plot:** A 2D topographical map. The "bullseye" of concentric circles (if present) marks the optimal region. The gold star shows the model's predicted peak.
        - **3D Surface Plot:** A 3D view of the process landscape, helping to visualize the "mountain" you are trying to climb.
        """)

    st.divider()
    st.header("Response Surface Visualizations")
    col3, col4 = st.columns(2)
    with col3:
        st.plotly_chart(fig_contour, use_container_width=True)
    with col4:
        st.plotly_chart(fig_3d, use_container_width=True)
        
    st.divider()
    st.subheader("Deeper Dive")
    
    # --- THIS IS THE LINE THAT WAS FIXED ---
    tabs_deep = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs_deep[0]:
        st.markdown("""
        - **Pareto Plot is Key:** This is your primary diagnostic. It instantly shows you which factors (linear, interaction, quadratic) are the main drivers of the process. Green bars are statistically significant (p < 0.05).
        - **Linear Effects:** A large linear effect (e.g., `Temp`) means that factor has a strong, consistent impact.
        - **Interaction Effects:** A significant interaction (`pH:Temp`) means the factors are not independent. The effect of pH is different at high vs. low temperatures.
        - **Curvature Effects:** Significant quadratic terms (`I(pH**2)`) are the key to optimization. A negative curvature effect (as simulated by default) proves you have found a "peak" or optimal zone. A positive effect would indicate a "valley."
        """)
    
    with tabs_deep[1]:
        st.markdown("""
        ### The Business Case: From Trial-and-Error to a Predictive "GPS"
    
        #### The Problem: The One-Factor-at-a-Time (OFAT) Quagmire
        An R&D team is tasked with optimizing a complex new process with five key parameters. They use the traditional OFAT approach: vary pH while holding everything else constant, then vary temperature while holding everything else constant, and so on. After dozens of expensive, time-consuming runs, they find a set of conditions that seems to work reasonably well, but they have no real confidence that it's the true optimum.
    
        #### The Impact: Sub-Optimal Processes and "The Cliff Effect"
        This intuitive but deeply flawed OFAT approach leads to significant business and operational failures:
        - **Local Maxima Trap:** The team gets stuck on a "local hill" of performance, completely missing the true "mountain peak" that could have dramatically improved yield or quality. They have developed a sub-optimal process, leaving millions of dollars of potential profit on the table.
        - **Failure to Discover Interactions:** The biggest flaw of OFAT is its inability to detect interactions. The team never discovers that the optimal temperature is actually different at low pH vs. high pH.
        - **"The Cliff Effect":** When the process is transferred to manufacturing, it is discovered to be incredibly fragile. A tiny, normal fluctuation in two parameters simultaneously (which was never tested in OFAT) causes the process to "fall off a cliff" and fail completely. The process is not robust.
    
        #### The Solution: A Predictive "GPS" for Your Process
        Design of Experiments (DOE) and Response Surface Methodology (RSM) are a **paradigm shift** from the slow, one-dimensional crawl of OFAT. It is a highly efficient, multi-factoral approach that allows you to "map the entire territory" at once. By testing parameters in intelligent combinations, you build a **predictive mathematical model** of your process. This model is a veritable "GPS" that allows you to:
        1.  **Find the True Optimum:** Mathematically calculate the exact settings that will maximize your desired outcome.
        2.  **Understand Interactions:** Quantify how parameters work together or against each other.
        3.  **Define a Robust Operating Zone (Design Space):** Identify a "safe plateau" on the performance map where the process is not only high-performing but also insensitive to small, real-world variations.
    
        #### The Consequences: Accelerated Development and a Bulletproof Process
        - **Without This:** Process development is a slow, inefficient random walk that produces fragile, sub-optimal processes.
        - **With This:** DOE/RSM dramatically **accelerates the R&D timeline**, generating far more knowledge from far fewer experiments. It is the core engine of **Quality by Design (QbD)**. The result is a high-performing, deeply understood, and "bulletproof" process that is robust to the normal variations of a real-world manufacturing environment. The resulting Design Space provides enormous regulatory and operational flexibility, which is a major competitive advantage.
        """)
    with tabs[1]:
        st.markdown("""
        ##### Glossary of DOE/RSM Terms
        - **DOE (Design of Experiments):** A systematic method to determine the relationship between factors affecting a process and the output of that process.
        - **Factor:** An input variable that is intentionally varied during an experiment (e.g., Temperature, pH).
        - **Response:** The output variable that is measured (e.g., Yield, Purity).
        - **Main Effect:** The effect of a single factor on the response.
        - **Interaction Effect:** Occurs when the effect of one factor on the response depends on the level of another factor.
        - **RSM (Response Surface Methodology):** A collection of statistical and mathematical techniques useful for developing, improving, and optimizing processes. It uses designs (like the CCD) that can estimate curvature.
        - **Quadratic Effect:** A term in the model that describes the curvature of the response surface. A significant quadratic effect is necessary to find a true optimum (a peak or valley).
        """)
    with tabs_deep[2]:
        st.error("""üî¥ **THE INCORRECT APPROACH: One-Factor-at-a-Time (OFAT)**
Imagine trying to find the highest point on a mountain by only walking in straight lines, first due North-South, then due East-West. You will almost certainly end up on a ridge or a local hill, convinced it's the summit, while the true peak was just a few steps to the northeast.
**The Flaw:** This is what OFAT does. It is statistically inefficient and, more importantly, it is **guaranteed to miss the true optimum** if any interaction between the factors exists.""")
        st.success("""üü¢ **THE GOLDEN RULE: Map the Entire Territory at Once (DOE/RSM)**
By testing factors in combination using a dedicated design (like a Central Composite Design), you send out scouts to explore the entire landscape simultaneously. This allows you to:
1.  **Be Highly Efficient:** Gain more information from fewer experimental runs.
2.  **Understand the Terrain:** Uncover critical interaction and curvature effects that describe the true shape of the process space.
3.  **Find the True Peak:** Develop a predictive mathematical model that acts as a GPS, guiding you directly to the optimal operating conditions.""")

    with tabs_deep[3]:
        st.markdown("""
        #### Historical Context: From Screening to Optimization
        **The Problem (The Genesis):** In the 1920s, **Sir Ronald A. Fisher** invented Design of Experiments to solve agricultural problems. His factorial designs were brilliant for *screening*‚Äîefficiently figuring out *which* factors (e.g., fertilizer type, seed variety) were important.
        **The New Problem (Optimization):** The post-war chemical industry boom created a new need: not just to know *which* factors mattered, but *how to find their optimal settings*. A simple factorial design, which only tests the corners of the design space, can't model curvature and therefore can't find a peak.
        **The 'Aha!' Moment (RSM):** In 1951, **George Box** and K.B. Wilson developed **Response Surface Methodology (RSM)** to solve this. They created efficient new designs, like the **Central Composite Design (CCD)** shown here, which cleverly adds "axial" (star) points and center points to a factorial design. 
        **The Impact:** These extra points allow for the fitting of a **quadratic model**, which is the key to modeling curvature and finding the "peak of the mountain." This moved DOE from simple screening to true, powerful optimization, becoming the statistical foundation of modern process development and Quality by Design (QbD).
        """)
        st.markdown("#### Mathematical Basis")
        st.markdown("RSM typically fits a second-order (quadratic) model to the experimental data. For two factors, the model is:")
        st.latex(r"Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_{11}X_1^2 + \beta_{22}X_2^2 + \beta_{12}X_1X_2 + \epsilon")
        st.markdown(r"""
        - `Œ≤‚ÇÄ`: The intercept or baseline response.
        - `Œ≤‚ÇÅ`, `Œ≤‚ÇÇ`: The **linear main effects** (the tilt of the surface).
        - `Œ≤‚ÇÅ‚ÇÅ`, `Œ≤‚ÇÇ‚ÇÇ`: The **quadratic effects** (the curvature or "hill/bowl" shape).
        - `Œ≤‚ÇÅ‚ÇÇ`: The **interaction effect** (the twist of the surface).
        - `œµ`: The random experimental error.
        To get stable estimates of these coefficients, the analysis is performed on **coded variables**, where the high and low levels of each factor are scaled to be +1 and -1, respectively.
        """)
        
    with tabs_deep[4]:
        st.markdown("""
        DOE and RSM are core methodologies for fulfilling the principles of Quality by Design (QbD), which is strongly encouraged by regulators.
        - **ICH Q8(R2) - Pharmaceutical Development:** This guideline introduces the concept of the **Design Space**, which is defined as "the multidimensional combination and interaction of input variables... that has been demonstrated to provide assurance of quality." RSM is the primary statistical tool used to establish a Design Space.
        - **ICH Q2(R1) - Validation of Analytical Procedures:** Requires the assessment of **Robustness**, which is typically evaluated through a DOE by making small, deliberate variations in method parameters.
        - **FDA Guidance on Process Validation:** Emphasizes a lifecycle approach and process understanding, which are best achieved through the systematic study of process parameters using DOE.
        """)
#====================================================================================== 13. MIXTURE DESIGN FORMULATIONS  =================================================================================================   
def render_mixture_design():
    """Renders the comprehensive, interactive module for Mixture DOE."""
    st.markdown("""
    #### Purpose & Application: The Formulation Scientist's GPS
    **Purpose:** To act as a **Formulation Scientist's GPS**. While a standard DOE maps a process, a Mixture DOE is specifically designed to navigate the complex world of formulations where components must sum to 100%. It answers questions like: "What is the optimal blend of three excipients to maximize drug solubility?"
    
    **Strategic Application:** This is an essential tool for developing stable drug products, buffers, or cell culture media. The resulting ternary plot provides an intuitive map of all possible formulations, instantly highlighting the "sweet spot" of optimal performance, which can be filed as a regulatory Design Space.
    """)
    
    st.info("""
    **Interactive Demo:** Use the sidebar controls to define the "true" properties of your formulation components.
    - The **Model Effects Plot** is your primary statistical diagnostic, showing *why* the map has its shape.
    - The **Ternary Map** is your visual guide to the Design Space (the region bounded by the orange line).
    """)

    with st.sidebar:
        st.subheader("Mixture Design Controls")
        st.markdown("**Component Main Effects**")
        a_slider = st.slider("Component A Effect", 0, 100, 60, 5, help="The response value when the formulation is 100% Component A.")
        b_slider = st.slider("Component B Effect", 0, 100, 80, 5, help="The response value when the formulation is 100% Component B.")
        c_slider = st.slider("Component C Effect", 0, 100, 50, 5, help="The response value when the formulation is 100% Component C.")
        st.markdown("**Component Interactions (Synergy/Antagonism)**")
        ab_slider = st.slider("A x B Interaction", -50, 50, 20, 5, help="Positive values mean A and B work better together than expected (synergy). Negative values mean they interfere with each other (antagonism).")
        ac_slider = st.slider("A x C Interaction", -50, 50, 0, 5, help="Controls the synergy or antagonism between components A and C.")
        bc_slider = st.slider("B x C Interaction", -50, 50, -30, 5, help="Controls the synergy or antagonism between components B and C.")
        st.markdown("**Experimental & Quality Controls**")
        noise_slider = st.slider("Experimental Noise (SD)", 0.1, 10.0, 2.0, 0.5, help="The random measurement error in each experimental run.")
        threshold_slider = st.slider("Min. Acceptable Response", 20, 100, 75, 5, help="The quality target. Any formulation predicted to be above this value will be inside the orange Design Space boundary.")

    fig_effects, fig_ternary, model, opt_blend = plot_mixture_design(
        a_effect=a_slider, b_effect=b_slider, c_effect=c_slider,
        ab_interaction=ab_slider, ac_interaction=ac_slider, bc_interaction=bc_slider,
        noise_sd=noise_slider, response_threshold=threshold_slider
    )

    st.header("Results Dashboard")
    col1, col2 = st.columns([0.45, 0.55])
    with col1:
        st.subheader("Statistical Diagnostics")
        st.metric("Model Adj. R-squared", f"{model.rsquared_adj:.3f}", help="How well the model fits the data.")
        st.markdown("##### Predicted Optimal Blend")
        st.dataframe(opt_blend.apply(lambda x: f"{x:.1%}").to_frame(name='Proportion'))
        st.plotly_chart(fig_effects, use_container_width=True)
        
    with col2:
        st.plotly_chart(fig_ternary, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Start with the Model Effects Plot:** This is your primary statistical diagnostic. It explains *why* the ternary map has its shape.
            - **Main Effects (Blue):** The coefficients for `A`, `B`, and `C` represent the predicted response at the pure, 100% vertices of the triangle.
            - **Interactions (Red):** These are the most important terms. A large **positive** coefficient (like `A:B`) indicates **synergy**‚Äîthe blend is better than a simple average of its parts. A large **negative** coefficient (like `B:C`) indicates **antagonism**‚Äîthe components interfere with each other.
        2.  **Consult the Ternary Map:** This is your visual guide to the formulation space.
            - **Color Contours:** Show the predicted response. The "hottest" color indicates the region of optimal performance, driven by the synergistic interactions.
            - **Orange Boundary:** This is your **Design Space** or **Proven Acceptable Range (PAR)**‚Äîthe set of all formulations predicted to meet your acceptance criteria.
            - **White Star:** The single "best" blend predicted by the model.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: From "Kitchen Chemistry" to a Defensible Recipe
    
        #### The Problem: The Formulation Guessing Game
        A team is tasked with developing a stable formulation for a new biologic drug, a new cell culture media, or a new diagnostic assay buffer. They have several potential components (excipients, nutrients, etc.). The traditional approach is "kitchen chemistry"‚Äîa slow, laborious process of trial-and-error, guided by intuition and one-factor-at-a-time experiments.
    
        #### The Impact: Extended Timelines and Fragile Formulations
        This unstructured approach is a major drag on product development and a source of significant downstream risk.
        - **Massively Inefficient R&D:** The team can spend months or even years running hundreds of experiments, chasing incremental improvements without ever understanding the complete picture. This directly delays the entire product development timeline.
        - **Failure to Discover Synergy:** The most powerful formulations often rely on **synergistic interactions**, where two components together produce an effect far greater than the sum of their parts. The OFAT approach is mathematically guaranteed to miss these critical interactions.
        - **A Fragile, Indefensible "Recipe":** The team eventually lands on a single formulation that works, but they have no data on the boundaries of their success. They don't know *how close* their recipe is to the "edge of a cliff." This makes the formulation vulnerable to normal variations in raw material quality and creates a weak, indefensible position during regulatory review.
    
        #### The Solution: The GPS for Formulation Science
        A Mixture Design of Experiments is the **purpose-built, statistically rigorous tool** for navigating the complex world of formulations. It acknowledges the fundamental constraint that all components must sum to 100% and uses a specialized experimental design to efficiently map the entire formulation space. The result is a predictive mathematical model that acts as a "GPS" for the formulator, allowing them to:
        1.  **Find the True Optimum:** Identify the exact blend of components that maximizes the desired property (e.g., stability, solubility, cell growth).
        2.  **Quantify Synergy:** Discover and quantify the powerful interaction effects that are the key to innovative formulations.
        3.  **Define a Robust Design Space:** The ternary plot provides a clear, visual map of the "safe operating zone"‚Äîthe entire region of formulations that are proven to meet quality targets.
    
        #### The Consequences: Accelerated Development and a Bulletproof Formulation
        - **Without This:** Formulation is a slow, inefficient art form that produces fragile, high-risk recipes.
        - **With This:** Mixture DOE transforms formulation into a fast, predictable, and data-driven science. It **dramatically accelerates development timelines**, uncovers novel, high-performance synergistic blends, and provides the robust, defensible **Design Space** that ensures regulatory success and a resilient commercial product.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Mixture Terms
        - **Mixture Design:** A special class of DOE for experiments with ingredients or components of a mixture as the factors, where the response depends on the proportions of the ingredients, not their absolute amounts.
        - **Constraint:** The mathematical requirement that the sum of the proportions of all components must equal a constant (usually 1 or 100%).
        - **Ternary Plot:** A triangular plot used to visualize the relationship between three components of a mixture and a response variable.
        - **Simplex:** The geometric space that defines the experimental region for a mixture. For three components, the simplex is a triangle.
        - **Synergy:** A positive interaction effect where the combined effect of two or more components is greater than the sum of their individual effects.
        - **Antagonism:** A negative interaction effect where the combined effect is less than the sum of their individual effects.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: Using a Standard DOE**
        An analyst tries to use a standard factorial or response surface design to optimize a formulation.
        - **The Flaw:** Standard DOEs treat factors as independent variables that can be changed freely. In a formulation, they are not independent; increasing one component *must* decrease another. This violates the core mathematical assumptions of a standard DOE, leading to incorrect models and nonsensical predictions.""")
        st.success("""üü¢ **THE GOLDEN RULE: Use a Mixture Design for Mixture Problems**
        The experimental design must match the physical reality of the problem.
        1.  **Identify the Constraint:** If your factors are ingredients or components that must sum to a constant (e.g., 100%), you have a mixture problem.
        2.  **Choose the Right Design:** Use a specialized experimental design, like a **Simplex-Lattice** or **Simplex-Centroid** design, which efficiently places points at the vertices, edges, and center of the formulation space.
        3.  **Use the Right Model:** Analyze the results with a model designed for mixtures, like the **Scheff√© polynomial**, which correctly handles the mathematical constraints.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: Solving the Chemist's Dilemma
        **The Problem:** For the first half of the 20th century, optimizing formulations was more art than science. Chemists and food scientists relied on intuition and laborious one-factor-at-a-time experiments. The powerful Design of Experiments (DOE) tools developed by Fisher and Box were of little help, as they couldn't handle the fundamental constraint that `A + B + C = 100%`.

        **The 'Aha!' Moment:** In a landmark 1958 paper, the statistician **Henry Scheff√©** solved this problem. He recognized that the experimental space was not a cube (like in a standard DOE) but a **simplex** (a triangle for 3 components, a tetrahedron for 4, etc.). He then derived a new class of polynomial models, now known as **Scheff√© polynomials**, specifically for this geometry. These models cleverly omit the intercept and rearrange terms to perfectly suit the mixture constraint.
        
        **The Impact:** Scheff√©'s work gave scientists a systematic, statistically rigorous, and highly efficient methodology to optimize blends and formulations. It transformed formulation development from guesswork into a predictable science and is now a cornerstone of product development in industries ranging from pharmaceuticals and food science to petrochemicals and materials science.
        """)

    with tabs[5]:
        st.markdown("""
        Mixture DOE is a specialized tool for establishing a **Design Space** for formulation parameters (material attributes), a core concept of **Quality by Design (QbD)**.
        - **ICH Q8(R2) - Pharmaceutical Development:** This guideline is the primary driver for this type of work. The region inside the orange boundary on the map is a direct visualization of a formulation **Design Space**. Filing this with a regulatory agency provides significant manufacturing flexibility, as movement within this space is not considered a change.
        - **ICH Q11 - Development and Manufacture of Drug Substances:** The principles of QbD, including the use of DOE to understand the relationship between material attributes and quality, apply equally to drug substances.
        - **FDA Process Validation Guidance:** Emphasizes a lifecycle approach and deep process/product understanding. A mixture DOE provides this deep understanding for the formulation itself and can be used to justify the **Bill of Materials (BOM)** and the acceptable ranges for each component.
        """)

#====================================================================================== 14. PROCESS OPTIMIZATION: FROM DOE TO AI  =================================================================================================   
def render_process_optimization_suite():
    """Renders the comprehensive, interactive module for the full optimization workflow."""
    st.markdown("""
    #### Purpose & Application: The Complete Optimization Workflow
    **Purpose:** To demonstrate the end-to-end modern workflow for process optimization, from initial characterization with **DOE/RSM**, to deeper analysis with **Machine Learning**.
    
    **Strategic Application:** This dashboard integrates two powerful techniques to tell a complete story.
    1.  **DOE/RSM:** Provides a simple, causal, and interpretable model that serves as the basis for a regulatory Design Space filing.
    2.  **ML & PDP:** Uncovers more complex, non-linear relationships from the data, providing a more accurate "digital twin" of the process for deeper understanding and internal optimization.
    """)
    
    st.info("""
    **Interactive Demo:** Use the sidebar controls to define the "true" physics of your process. The dashboard follows a real-world workflow:
    1.  Check the **Statistical Results** to see which factors drive the process.
    2.  Compare the **Process Maps**: the simple, smooth RSM map vs. the complex, more accurate ML map.
    """)
    
    with st.sidebar:
        st.subheader("Process Simulation Controls")
        st.markdown("**True Process Effects**")
        temp_slider = st.slider("üå°Ô∏è Temperature Main Effect", -10.0, 10.0, 2.0, 1.0)
        ph_slider = st.slider("üß¨ pH Main Effect", -10.0, 10.0, 1.0, 1.0)
        interaction_slider = st.slider("üîÑ pH x Temp Interaction Effect", -10.0, 10.0, -3.0, 1.0)
        temp_quad_slider = st.slider("üå°Ô∏è Temperature Curvature", -10.0, 0.0, -5.0, 1.0)
        ph_quad_slider = st.slider("üß¨ pH Curvature", -10.0, 0.0, -8.0, 1.0)
        asymmetry_slider = st.slider("‚õ∞Ô∏è Process Asymmetry", -5.0, 5.0, -3.0, 0.5)
        noise_slider = st.slider("üé≤ Experimental Noise (SD)", 0.1, 5.0, 1.0, 0.1)
        st.markdown("---")
        st.markdown("**Quality Requirement**")
        yield_threshold_slider = st.slider("Acceptable Yield Threshold (%)", 85, 99, 95, 1)

    fig_pareto, fig_rsm_3d, fig_rsm_2d, pdp_buffer, anova_table, opt_ph, opt_temp, max_resp = plot_doe_optimization_suite(
        ph_effect=ph_slider, temp_effect=temp_slider, interaction_effect=interaction_slider,
        ph_quad_effect=ph_quad_slider, temp_quad_effect=temp_quad_slider,
        asymmetry_effect=asymmetry_slider, noise_sd=noise_slider,
        yield_threshold=yield_threshold_slider
    )

    st.header("Statistical Results & Predicted Optimum")
    col1, col2 = st.columns([0.55, 0.45])
    with col1:
        st.plotly_chart(fig_pareto, use_container_width=True)
    with col2:
        st.subheader("Predicted Optimum & KPIs (from RSM Model)")
        m1, m2, m3 = st.columns(3)
        m1.metric("Optimal pH", f"{opt_ph:.2f}")
        m2.metric("Optimal Temp (¬∞C)", f"{opt_temp:.2f}")
        m3.metric("Max Predicted Yield", f"{max_resp:.1f}%")
        st.markdown("---")
        st.markdown("##### Statistical Model Summary (ANOVA)")
        st.dataframe(anova_table.style.format({'p-value': '{:.4f}'}).applymap(
            lambda p: 'background-color: #C8E6C9' if p < 0.05 else '', subset=['p-value']),
            use_container_width=True, height=240)

    st.header("Process Visualizations: RSM vs. Machine Learning")
    col3, col4 = st.columns(2)
    with col3:
        st.plotly_chart(fig_rsm_3d, use_container_width=True)
        st.plotly_chart(fig_rsm_2d, use_container_width=True)
    with col4:
        st.image(pdp_buffer)
        st.markdown("""
        **Comparing the Maps:**
        - The **RSM plots (left)** show a smooth, symmetrical, and easily interpretable model of the process. This is ideal for regulatory submissions.
        - The **ML plot (right)** reveals the more complex "ground truth" learned from the data, including the asymmetric cliff created by the simulation. This is superior for deep process understanding and internal optimization.
        """)

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow:**
        1.  **Start with the Pareto Plot:** This is your primary diagnostic from the simple RSM model. It tells you which factors are the main causal drivers of the process.
        2.  **Build the Regulatory Map (RSM):** The 2D topographic map (bottom-left) is your official process map. The green shaded area is your validated **Design Space (PAR)**, and the white-dashed box is your **NOR**.
        3.  **Build the High-Fidelity Map (ML):** The 2D heatmap (right) is your more accurate "digital twin." Notice how it captures the sharp "cliff" on the low-pH side that the smoother RSM model can only approximate. This map is better for finding the true optimum and understanding process risks.
        
        **Core Insight:** The simple RSM model is for **validation and communication**. The complex ML model is for **accuracy and deep understanding**. A mature QbD program uses both in tandem. The workflow demonstrates a powerful modern paradigm: **Use DOE to build a simple, causal foundation. Then, use ML on larger historical datasets to refine that understanding and create a high-fidelity "digital twin" of your process that can be used for automated optimization.**
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The Two-Model Strategy for Compliance and Performance
    
        #### The Problem: The Simplicity vs. Accuracy Trade-Off
        A company is characterizing a high-value bioprocess. They face a critical dilemma:
        - **Option A (Simplicity):** Use a traditional DOE/RSM model. The model is simple, interpretable, and easy to defend to regulators, but its smooth, quadratic assumptions may not capture the true, complex "cliffs and valleys" of the biological process. This leaves potential performance gains on the table.
        - **Option B (Accuracy):** Use a powerful but "black box" AI/ML model. This model can create a highly accurate "digital twin" of the process, but its complexity makes it difficult to interpret and a "nightmare" to justify in a regulatory submission.
    
        #### The Impact: Choosing Between Profitability and Compliance
        Forcing a choice between these two options leads to sub-optimal business outcomes.
        - **Choosing Simplicity:** The company files a simple, compliant Design Space but operates a process that is less efficient than it could be, permanently sacrificing millions in potential yield and profit.
        - **Choosing Accuracy:** The company struggles to validate the complex AI model for regulatory purposes, leading to submission delays. Or, they operate based on a black box they don't fully understand, exposing them to the risk of unexpected failures.
    
        #### The Solution: A Two-Model Strategy for Two Different Jobs
        This suite demonstrates the powerful modern approach: **you don't have to choose**. You build and validate *both* models, because they have two different, complementary business purposes.
        1.  **The Regulatory Model (DOE/RSM):** This is your **official, validated Design Space**. It is a simple, conservative, and highly defensible map that you submit to the FDA. Its purpose is to guarantee compliance and product quality.
        2.  **The Operational Model (AI/ML):** This is your **internal, high-fidelity "digital twin"**. It is a more accurate and complex model used by your process engineers for deep understanding, troubleshooting, and continuous improvement. Its purpose is to maximize performance and profitability *within* the validated boundaries set by the regulatory model.
    
        #### The Consequences: Achieving Both Compliance and Competitive Advantage
        - **Without This:** The company is forced into a false choice between a simple but sub-optimal model and an accurate but risky one.
        - **With This:** The two-model strategy provides the best of both worlds. The company can **satisfy regulatory expectations** with a simple, robust RSM model, securing operational flexibility. Simultaneously, they can **drive market-leading performance** by using the more accurate AI model for internal optimization. This approach allows a company to be both fully compliant and a top performer in their industry.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Optimization Terms
        - **RSM (Response Surface Methodology):** A statistical technique for modeling and optimizing a response based on a set of input variables, typically using a quadratic model derived from a planned DOE.
        - **Gradient Boosting (e.g., XGBoost):** A powerful machine learning algorithm that builds a predictive model as an ensemble of many simple "weak" decision trees. It is known for high accuracy and the ability to capture complex, non-linear relationships.
        - **PDP (Partial Dependence Plot):** A visualization that shows the marginal effect of one or two features on the predicted outcome of a machine learning model. It helps to understand the model's behavior.
        - **Gradient Descent:** An iterative optimization algorithm used to find the minimum of a function. In this context (maximization), it follows the positive gradient ("steepest ascent") to "climb the hill" of the predicted response surface to find the optimum.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "ML is Magic" Fallacy**
An analyst takes a messy historical dataset, trains a complex ML model, and uses Gradient Descent to find a "perfect" optimum without understanding the underlying causality.
- **The Flaw:** The historical data may contain confounding. The model might learn that "high yield is correlated with Operator Bob," but this is not an actionable insight. Trying to optimize for "more Bob" is nonsensical. The model has learned a correlation, not a causal lever.""")
        st.success("""üü¢ **THE GOLDEN RULE: DOE for Causality, ML for Complexity**
A robust optimization strategy uses the best of both worlds.
1.  **Establish Causality with DOE:** First, use a planned DOE to prove which parameters are true causal levers for the process. This grounds your model in scientific reality.
2.  **Capture Complexity with ML:** Once you have a large historical dataset, use a more powerful ML model to capture the complex, non-linear interactions your simple RSM model might miss.
3.  **Optimize on Causal Levers:** Finally, use optimization algorithms like Gradient Descent on your ML model, but only allow it to optimize the parameters you have already proven to be causal. This ensures your final "optimum" is both accurate and actionable.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: A Convergence of Titans
        This dashboard represents the convergence of three separate, powerful intellectual traditions that developed over nearly a century.
        1.  **Statistical Experimentation (DOE/RSM, 1920s-1950s):** Pioneered by **Sir Ronald A. Fisher** and later perfected for optimization by **George Box**. The focus was on extracting maximum causal information from a minimum number of planned, physical experiments. This was the era of "small data" and deep statistical theory.
        2.  **Machine Learning (Gradient Boosting, 1990s-2000s):** With the rise of computing power, researchers like Jerome Friedman developed algorithms like Gradient Boosting Machines (GBM). The focus shifted from simple, interpretable models to highly accurate, complex algorithms that could learn intricate patterns from large "found" datasets. **XGBoost** (2014) is a highly optimized, modern implementation of these ideas.
        3.  **Mathematical Optimization (Gradient Descent, 1847):** The core idea of "climbing the hill" by following the steepest path was first proposed by Augustin-Louis Cauchy. For over a century, it was a mathematical curiosity. The AI revolution of the 2010s turned it into the engine of modern deep learning, as it is the algorithm used to train virtually every neural network.
        
        **The Modern Synthesis:** Today, we can combine these three titans. We use the principles of Fisher and Box to design smart experiments, the predictive power of Friedman's algorithms to build an accurate map, and the efficiency of Cauchy's optimization to find the peak.
        """)
        
    with tabs[5]:
        st.markdown("""
        This integrated workflow is a direct implementation of the most advanced principles of **Quality by Design (QbD)** and **Process Analytical Technology (PAT)**.
        - **ICH Q8(R2) - Pharmaceutical Development:** The DOE/RSM portion is the standard method for establishing a **Design Space**. The ML and Gradient Descent portions represent an advanced method for achieving a deeper **Process Understanding** and identifying an optimal **Control Strategy**.
        - **FDA Guidance for Industry - PAT:** Using an ML model as a "digital twin" for your process and running optimization algorithms on it is a core concept of PAT. It allows for process control and optimization to be done proactively and based on a deep, data-driven model.
        - **FDA AI/ML Action Plan & GMLP:** Deploying an ML model for process optimization in a GxP environment requires a robust validation lifecycle. This includes justifying the model choice, validating its predictive accuracy, and using explainability tools like PDP plots to ensure its reasoning is scientifically sound. The entire workflow shown here would be a key part of the validation package for an AI-driven control strategy.
        """)
        st.markdown("""
        This tool directly implements the core principles of the **ICH Q8(R2) Pharmaceutical Development** guideline, which is the foundational document for Quality by Design (QbD).
        
        ---
        ##### Key ICH Q8(R2) Definitions:
        - **Design Space (DSp):** "The multidimensional combination and interaction of input variables (e.g., material attributes) and process parameters that have been demonstrated to provide assurance of quality."
          - **Your Action:** The area inside the orange boundary on the 2D plot is your experimentally derived Design Space.

        - **Proven Acceptable Range (PAR):** "A characterized range of a process parameter for which operation within this range, while keeping other parameters constant, will result in producing a material meeting relevant quality criteria."
          - **Your Action:** In practice, the Design Space and PAR are often used interchangeably. The PAR represents the validated "edges of failure" for your process.
        
        - **Normal Operating Range (NOR):** "The range of a process parameter that is typically used during routine manufacturing."
          - **Your Action:** The NOR (green box) is a tighter range set well within the PAR. It provides a buffer and serves as the target for routine operations. An excursion within the PAR but outside the NOR triggers an investigation but is not necessarily a deviation.
          
        ---
        **The Regulatory Advantage:**
        The guideline explicitly states: **"Working within the design space is not considered as a change. Movement out of the design space is considered to be a change and would normally initiate a regulatory post-approval change process."** This provides enormous operational and regulatory flexibility, which is the primary business driver for adopting a QbD approach.
        """)
# SNIPPET: Replace your entire render_bayesian_optimization function with this final, correct version.

def render_bayesian_optimization():
    """Renders the comprehensive module for Bayesian Optimization."""
    st.markdown("""
    #### Purpose & Application: The Intelligent Experimenter
    **Purpose:** To provide a highly sample-efficient strategy for finding the true optimum of an expensive, "black box" process. **Bayesian Optimization** is an intelligent, sequential search algorithm that uses the results of past experiments to decide the single most informative experiment to run next.
    
    **Strategic Application:** This is the ideal tool for optimizing processes where **each experimental run is extremely expensive, slow, or resource-intensive**.
    - **Bioreactor Optimization:** Finding the optimal feeding strategy over a 21-day run.
    - **Formulation Science:** Optimizing a complex lyophilization cycle that takes 48 hours per run.
    - **Analytical Method Development:** Fine-tuning sensitive HPLC parameters where each run requires significant setup and equilibration time.
    """)
    st.info("""
    **Interactive Demo:** You are the Lead Scientist trying to find the temperature that maximizes process yield.
    1.  The true, hidden relationship is the grey dashed line. Your goal is to find its peak.
    2.  Use the **"Run Next Experiment"** button to step through the optimization process one experiment at a time.
    3.  At each step, observe how the algorithm updates its **Belief (blue curve)** and uses the **Acquisition Function (green curve)** to intelligently choose the next best point to sample.
    """)

    # --- True Function (hidden from the algorithm) - Defined locally in the render function ---
    def true_process_function(x):
        return (-(x - 15)**2 / 20) + 2 * np.sin(x) + 85

    x_range = np.linspace(0, 30, 200)

    # --- Session State to manage the optimization history ---
    if 'bo_history' not in st.session_state:
        st.session_state.bo_history = {'x': [2.0, 28.0], 'y': [true_process_function(2.0), true_process_function(28.0)]}

    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.subheader("Bayesian Optimization Workbench")
        acquisition_func_type = st.radio(
            "Select Acquisition Function:",
            ["Upper Confidence Bound (UCB)", "Expected Improvement (EI)"],
            horizontal=True
        )
        
        # --- START OF THE FIX ---
        # The call to the plotting function now correctly passes only 3 arguments.
        # The 'true_process_function' is no longer passed.
        fig, next_point = plot_bayesian_optimization_step(
            st.session_state.bo_history, 
            x_range, 
            acquisition_func_type
        )
        # --- END OF THE FIX ---
        
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.subheader("Experiment Control")
        if st.button("üî¨ Run Next Experiment", use_container_width=True):
            new_y = true_process_function(next_point)
            st.session_state.bo_history['x'].append(next_point)
            st.session_state.bo_history['y'].append(new_y)
            st.rerun()
        if st.button("üîÑ Reset Optimization", use_container_width=True):
            st.session_state.bo_history = {'x': [2.0, 28.0], 'y': [true_process_function(2.0), true_process_function(28.0)]}
            st.rerun()
        
        st.metric("Total Experiments Run", len(st.session_state.bo_history['x']))
        best_idx = np.argmax(st.session_state.bo_history['y'])
        st.metric("Best Yield Found So Far", f"{st.session_state.bo_history['y'][best_idx]:.2f}%")
        st.metric("at Parameter Value", f"{st.session_state.bo_history['x'][best_idx]:.2f}")

    st.divider()
    st.subheader("Deeper Dive into Bayesian Optimization")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Initial Samples (Step 0):** The process starts with a few initial, spaced-out experiments to get a first look at the landscape.
        2.  **Building a Belief (Top Plot):** The algorithm uses all the data gathered so far (red 'x' markers) to fit a flexible **surrogate model** (a Gaussian Process). This blue curve represents the algorithm's current "belief" about the true function, and the shaded blue area represents its uncertainty.
        3.  **Deciding What's Next (Bottom Plot):** The algorithm then computes an **Acquisition Function**. This function is high in two strategic places:
            -   **Exploitation:** It's high near the current known maximum, wanting to sample there to confirm the peak.
            -   **Exploration:** It's high in areas of high uncertainty (where the blue shaded band is wide), wanting to sample there to reduce its ignorance.
        4.  **Running the Next Experiment:** The algorithm chooses the single point that **maximizes the Acquisition Function** (the peak of the green curve) as the next experiment to run. This cycle repeats, with each new data point refining the model's belief and improving its next decision.

        **The Strategic Insight:** Bayesian Optimization is a powerful strategy that formally balances **exploiting known good regions** with **exploring unknown regions**. This intelligent, sequential search allows it to find the global optimum with significantly fewer experiments than a traditional DOE, which samples the entire space at once.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: Maximum Insight from Minimum Experiments

        #### The Problem: The "Expensive Experiment" Barrier
        A company needs to optimize a process where each experimental run is incredibly expensive or slow. Examples include:
        -   A 28-day cell culture run to optimize a feeding strategy, where each run costs over $50,000 in media and labor.
        -   A complex lyophilization cycle for a high-value drug product, where each 48-hour run consumes a full production-scale unit.
        -   Fine-tuning the parameters of a large, complex Machine Learning model, where each training run takes 12 hours on an expensive GPU cluster.
        In these scenarios, a traditional Design of Experiments (DOE) with 20-30 runs is simply not financially or logistically feasible.

        #### The Impact: Sub-Optimal Processes and Stifled Innovation
        This "expensive experiment" barrier forces companies into difficult and costly compromises.
        - **Under-characterized Processes:** The team can only afford a handful of runs. They perform a small, underpowered study that provides very little real information, leaving the process sub-optimal and poorly understood. The risk of failure at scale remains high.
        - **Stalled Innovation:** Promising but complex new process ideas are never pursued because the cost of optimizing them is deemed too high. The company is forced to stick with older, less efficient, but better-understood processes.
        - **Wasted Runs:** In a manual, trial-and-error approach, the team might waste several of their precious experimental runs on regions of the parameter space that are clearly not promising, simply because they lack a systematic strategy.

        #### The Solution: The Intelligent, Sample-Efficient Search Agent
        Bayesian Optimization is the purpose-built solution for this "expensive experiment" problem. It is a **sample-efficient** global optimization strategy. Unlike a DOE that plans all its experiments in advance, Bayesian Optimization is **sequential and adaptive**. It uses the information from every run it completes to intelligently decide on the single most valuable run to perform next. It is constantly asking: "Given what I know now, what is the one experiment I can run that will give me the most new information about where the peak is?"

        #### The Consequences: Making "Impossible" Optimizations Possible
        - **Without This:** Optimizing expensive, long-duration processes is often considered impossible, forcing companies to operate with sub-optimal, "good enough" procedures.
        - **With This:** Bayesian Optimization **makes the impossible possible**. It can often find a near-optimal solution in a fraction of the runs required by a traditional DOE (e.g., finding a good optimum in 10-15 runs instead of 30-50). This **dramatically reduces the cost and time of R&D**, enabling the optimization of complex, high-value processes that were previously untouchable. It is a key enabling technology for innovation in capital-intensive R&D environments.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **Bayesian Optimization:** A sequential, model-based optimization strategy for finding the maximum of a "black box" function that is expensive to evaluate.
        - **Black Box Function:** A function whose internal workings are unknown. We can only provide an input `x` and observe the output `y`. A bioreactor run is a perfect example.
        - **Surrogate Model:** A cheap, statistical model (typically a Gaussian Process) that approximates the true, expensive black box function. The surrogate model is updated after each new experiment.
        - **Gaussian Process (GP):** A powerful, flexible statistical model that defines a probability distribution over functions. It is the most common surrogate model for Bayesian Optimization because it provides both a mean prediction and a measure of uncertainty.
        - **Acquisition Function:** A function that uses the predictions and uncertainty from the surrogate model to calculate the "utility" or "value" of sampling at any given point. The next experiment is run at the point that maximizes this function.
        - **Exploration vs. Exploitation:** The fundamental trade-off in any search problem. **Exploitation** means sampling in areas where we already know the function is high. **Exploration** means sampling in areas of high uncertainty where the function *might* be high. The acquisition function provides a mathematical way to balance this trade-off.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Grid Search" Brute Force**
An analyst tries to optimize an expensive process by performing a "grid search"‚Äîtesting every possible combination of parameters at a coarse resolution.
- **The Flaw:** This is catastrophically inefficient. The number of required experiments explodes exponentially with the number of parameters (the "curse of dimensionality"). For a process with just 3 parameters at 5 levels each, a full grid search would require 5¬≥ = 125 runs, which is completely infeasible for a multi-week bioreactor experiment.""")
        st.success("""üü¢ **THE GOLDEN RULE: Spend Your Experimental Budget Intelligently**
Every experimental run is a precious resource. Bayesian Optimization is built on the principle of maximizing the knowledge gained from every single one.
1.  **Model Your Beliefs:** Use a flexible surrogate model (like a Gaussian Process) to represent everything you currently know *and don't know* about your process.
2.  **Quantify "What's Next?":** Use an acquisition function to translate your model's beliefs and uncertainties into a clear, quantitative decision about the single most valuable experiment to run next.
3.  **Iterate:** Update your beliefs with the new data and repeat. This intelligent, adaptive loop is the key to finding the optimum with minimal waste.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Kriging to Hyperparameter Tuning
        The intellectual roots of Bayesian Optimization are a fascinating blend of geology, statistics, and machine learning.
        - **The Geological Roots (Kriging):** In the 1950s, a South African mining engineer named **Danie G. Krige** developed a statistical method to more accurately predict ore concentrations in a mine based on a small number of core samples. His work was formalized by the French mathematician **Georges Matheron** in the 1960s and became known as **Kriging**. This is the statistical technique that we now call **Gaussian Process regression**. For decades, it remained a specialized tool in geostatistics.
        - **The "Efficient Global Optimization" Insight (1990s):** In the 1990s, researchers in the field of computer-aided design, notably **Jonas Mockus**, realized that this "Kriging" model could be used to build a surrogate for expensive engineering simulations. A key 1998 paper by Jones, Schonlau, and Welch, titled "Efficient Global Optimization of Expensive Black-Box Functions," popularized the idea of combining a Gaussian Process surrogate with an **Expected Improvement (EI)** acquisition function. This was the birth of modern Bayesian Optimization.
        
        **The Impact:** For years, it remained a niche academic tool. The machine learning boom of the 2010s is what truly made it famous. Researchers at companies like Google and Microsoft discovered that Bayesian Optimization was an incredibly powerful and sample-efficient way to solve one of their biggest problems: **hyperparameter tuning**. Finding the best architecture for a deep neural network was a classic "expensive black box" problem, and Bayesian Optimization provided the perfect solution. This has brought the technique to mainstream prominence, and its applications are now spreading back into the physical sciences and engineering.
        """)
        
    with tabs[5]:
        st.markdown("""
        Bayesian Optimization is a state-of-the-art method for executing the principles of Quality by Design (QbD) and Process Validation in the most efficient way possible, especially under resource constraints.
        - **ICH Q8(R2) - Pharmaceutical Development:** The goal of QbD is to develop deep "process understanding" and establish a "Design Space." Bayesian Optimization is an advanced and highly efficient strategy for exploring the parameter space to find the optimal operating point and the edges of the Design Space with a minimal number of expensive experimental runs.
        - **FDA Process Validation Guidance (Stage 1 - Process Design):** This guidance emphasizes a scientific, data-driven approach to designing a process that is capable and robust. For complex and expensive processes like continuous manufacturing or perfusion-based cell culture, Bayesian Optimization provides a feasible path to optimization that might be impossible with traditional DOE due to cost and time constraints.
        - **FDA Guidance for Industry - PAT:** A core principle of PAT is "Design and optimize processes through well-designed experiments." Bayesian Optimization is a cutting-edge example of a well-designed experimental strategy.
        - **Regulatory Justification:** Using Bayesian Optimization can be justified to regulators by framing it as a highly efficient "search strategy" for the Design Space. The final validation of the chosen optimum and the Design Space would still be performed using a traditional, pre-defined set of confirmation runs and protocols.
        """)
#====================================================================================== 15. SPLIT-PLOT DESIGNS =================================================================================================   
def render_split_plot():
    """Renders the module for Split-Plot Designs."""
    st.markdown("""
    #### Purpose & Application: The Efficient Experimenter
    **Purpose:** To design an efficient and statistically valid experiment when your study involves both **Hard-to-Change (HTC)** and **Easy-to-Change (ETC)** factors. This is a specialized form of Design of Experiments (DOE).
    
    **Strategic Application:** This design is a lifesaver during process characterization and tech transfer. A standard DOE might require you to change all factors randomly, which can be prohibitively expensive or time-consuming.
    - **Tech Transfer:** Validating a new `Media Lot` (HTC) is a major undertaking. However, once a run is started, testing different `Supplement Concentrations` (ETC) is easy.
    A split-plot design saves immense resources by minimizing the number of times you have to change the difficult factor.
    """)

    st.info("""
    **Interactive Demo:** Use the sliders to control the "true" underlying effects in the process.
    - **`Lot-to-Lot Variation`**: Creates a main effect for the 'Lot' factor. Watch the plots for Lot B shift down.
    - **`Interaction Effect`**: Makes the effect of the Supplement *depend* on the Lot. Watch the lines in the **Interaction Plot** become non-parallel, a classic sign of an interaction.
    """)

    with st.sidebar:
        st.subheader("Split-Plot Controls")
        variation_slider = st.slider(
            "Lot-to-Lot Variation (SD)",
            min_value=0.0, max_value=5.0, value=0.5, step=0.25,
            help="Controls the 'true' difference between the hard-to-change media lots. Higher values simulate a larger main effect."
        )
        interaction_slider = st.slider(
            "Lot x Supplement Interaction Effect",
            min_value=-2.0, max_value=2.0, value=0.0, step=0.2,
            help="Controls how much the supplement's effect changes between Lot A and Lot B. A non-zero value creates an interaction."
        )
    
    fig_main, fig_interaction, anova_table = plot_split_plot_doe(
        lot_variation_sd=variation_slider,
        interaction_effect=interaction_slider
    )
    
    # --- Redesigned Layout ---
    col1, col2 = st.columns([0.6, 0.4])
    with col1:
        st.plotly_chart(fig_main, use_container_width=True)
        st.plotly_chart(fig_interaction, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.markdown("##### ANOVA Results")
            st.dataframe(anova_table.style.format({'p-value': '{:.4f}'}).applymap(
                lambda p: 'background-color: #C8E6C9' if p < 0.05 else '', subset=['p-value']),
                use_container_width=True)
            
            st.markdown("""
            **Reading the Plots & Table:**
            - **Main Plot:** Visualizes the raw data and means for each experimental condition.
            - **Interaction Plot:** This is your primary tool for diagnosing interactions. If the lines are not parallel, an interaction is likely present.
            - **ANOVA Table:** Provides the statistical proof. Look for p-values < 0.05 to identify significant effects.
                - `C(Lot)`: Tests the main effect of the media lot.
                - `C(Supplement)`: Tests the main effect of the supplement.
                - `C(Lot):C(Supplement)`: Tests the **interaction effect**. This is often the most important result.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: Designing Experiments for Reality, Not Theory
        
            #### The Problem: The "All Factors are Equal" Fallacy
            An R&D team designs a standard DOE to characterize a new cell culture process. Two of the factors are the `Raw Material Lot` and the `Supplement Concentration`. A standard DOE requires that all factors be changed randomly in each run. This means the team must completely break down, clean, and set up a new bioreactor for a new `Raw Material Lot` for almost every single experimental run.
        
            #### The Impact: Prohibitively Expensive and Slow Experimentation
            The "theoretically correct" fully randomized experiment is operationally a nightmare.
            - **Massive Cost Overruns:** The cost of the experiment skyrockets. What could have been a 2-week study now takes 2 months, as each run requires a full day of non-productive setup and cleaning time. The cost of labor and materials becomes astronomical.
            - **Project Cancellation:** Faced with the enormous cost and timeline, management decides the experiment is too expensive to run. The process characterization is skipped, and the company moves forward with a poorly understood process, accepting a huge downstream risk of failure during tech transfer or commercial manufacturing.
            - **Cutting Corners:** Alternatively, the team might decide to run the experiment in a more convenient, non-randomized way (e.g., all of Lot A first, then all of Lot B) but analyze it with a standard DOE model. This is statistically invalid and produces dangerously misleading results.
        
            #### The Solution: Acknowledging and Designing for Constraints
            A Split-Plot design is a pragmatic and statistically rigorous solution that **acknowledges the physical and financial reality of the process**. It allows the experimenter to group the runs to minimize the number of changes for the **Hard-to-Change (HTC)** factor, like `Raw Material Lot`. Within each of those groups, the **Easy-to-Change (ETC)** factor, like `Supplement Concentration`, is then varied. This matches the reality of how the process is run.
        
            #### The Consequences: Feasible, Fast, and Valid Experiments
            - **Without This:** Teams are forced into a false choice between a theoretically perfect but impossibly expensive experiment, or a convenient but statistically invalid one.
            - **With This:** The Split-Plot design provides a **"best of both worlds" solution**. It dramatically **reduces the cost and time** required for experimentation by an order of magnitude, making critical process characterization studies feasible. Most importantly, it does so without sacrificing statistical validity, as the specialized split-plot analysis correctly accounts for the restricted randomization. It is the essential tool for running smart, fast, and cost-effective experiments in the real world.
            """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Split-Plot Terms
            - **Split-Plot Design:** A type of DOE used when one or more factors are "hard-to-change" while others are "easy-to-change."
            - **Hard-to-Change (HTC) Factor:** A factor that is difficult, expensive, or time-consuming to change between experimental runs (e.g., bioreactor setup, media lot).
            - **Easy-to-Change (ETC) Factor:** A factor that can be easily changed between runs (e.g., supplement concentration, agitation speed).
            - **Whole Plot:** The experimental unit to which the levels of the HTC factor are applied.
            - **Sub-Plot:** The smaller experimental units within a whole plot, to which the levels of the ETC factor are applied.
            - **Restricted Randomization:** The key feature of a split-plot design. The levels of the HTC factor are not fully randomized, which must be accounted for in the statistical analysis.
            """)

        with tabs[3]:
            st.error("""üî¥ **THE INCORRECT APPROACH: The "Pretend it's Standard" Fallacy**
            An analyst runs a split-plot experiment for convenience but analyzes it as if it were a standard, fully randomized DOE.
            - **The Flaw:** This is statistically invalid. A standard analysis assumes every run is independent, but in a split-plot, all the sub-plots within a whole plot are correlated. This error leads to incorrect p-values and a high risk of declaring an effect significant when it's just random noise.""")
            st.success("""üü¢ **THE GOLDEN RULE: Design Dictates Analysis**
            The way you conduct your experiment dictates the only valid way to analyze it.
            1.  **Recognize the Constraint:** Identify if you have factors that are much harder, slower, or more expensive to change than others.
            2.  **Choose the Right Design:** If you do, a Split-Plot design is likely the most efficient and practical choice.
            3.  **Use the Right Model:** Analyze the results using a statistical model that correctly accounts for the two different error structures (the "whole plot error" for the HTC factor and the "sub-plot error" for the ETC factor). This is typically done with a mixed-model ANOVA.""")

        with tabs[4]:
            st.markdown("""
            #### Historical Context: The Fertile Fields of Rothamsted
            **The Problem:** Like much of modern statistics, the Split-Plot design was born from the practical challenges of agriculture. Its inventors, **Sir Ronald A. Fisher** and **Frank Yates**, were working at the Rothamsted Experimental Station in the 1920s and 30s, the oldest agricultural research institution in the world. They faced a logistical nightmare: they wanted to test different large-scale treatments (like irrigation methods) and small-scale treatments (like crop varieties) in the same experiment.
            
            **The 'Aha!' Moment:** They couldn't fully randomize everything. Changing the irrigation method (the **Hard-to-Change** factor) required digging large trenches and re-routing water, so it could only be done on large sections of a field, which they called **"whole plots."** However, within each irrigated whole plot, it was very easy to plant multiple different crop varieties (the **Easy-to-Change** factor) in smaller **"sub-plots."** This physical constraint of not being able to irrigate a tiny plot differently from its neighbor forced a new way of thinking.
            
            **The Impact:** Fisher and Yates developed the specific mathematical framework for the Split-Plot ANOVA to correctly analyze the data from this **restricted randomization**. They recognized that there were two different levels of experimental error: a larger error for comparing whole plots and a smaller error for comparing sub-plots within a whole plot. By correctly partitioning the variance, they created one of the most practical and widely used experimental designs ever conceived, saving researchers in countless fields immense time and resources.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("The key to a split-plot analysis is recognizing it has two different error terms. The linear model for the design is often expressed as a mixed model:")
            st.latex(r"Y_{ijk} = \mu + \alpha_i + \gamma_{ik} + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}")
            st.markdown("""
            -   `Œº`: Overall mean.
            -   `Œ±·µ¢`: Fixed effect of the `i`-th level of the **whole-plot factor A**.
            -   `Œ≥·µ¢‚Çñ`: The random **whole-plot error**, ~ N(0, œÉ¬≤_Œ≥). This is the error term for testing factor A.
            -   `Œ≤‚±º`: Fixed effect of the `j`-th level of the **sub-plot factor B**.
            -   `(Œ±Œ≤)·µ¢‚±º`: The interaction effect.
            -   `Œµ·µ¢‚±º‚Çñ`: The random **sub-plot error**, ~ N(0, œÉ¬≤_Œµ). This is the error term for testing factor B and the interaction.
            Because `œÉ¬≤_Œ≥` is typically larger than `œÉ¬≤_Œµ`, the test for the hard-to-change factor (A) is less powerful than the test for the easy-to-change factor (B), which is the fundamental trade-off of this design.
            """)
        with tabs[5]:
            st.markdown("""
            As a specific type of Design of Experiments, Split-Plot designs are tools used to fulfill the broader regulatory expectations around process understanding and robustness.
            - **ICH Q8(R2) - Pharmaceutical Development:** The principles of efficient experimentation to gain process knowledge are central to QbD. A split-plot design is a practical tool for achieving this when certain factors are hard to change.
            - **FDA Guidance on Process Validation:** Encourages a scientific, risk-based approach to validation. Using an efficient design like a split-plot demonstrates statistical maturity and a commitment to resource optimization while still generating the required process knowledge.
            """)
#====================================================================================== 16. CAUSAL INFERENCE  =================================================================================================          
def render_causal_inference():
    """Renders the INTERACTIVE module for Causal Inference."""
    st.markdown("""
    #### Purpose & Application: Beyond "What" to "Why"
    **Purpose:** To move beyond mere correlation ("what") and ascend to the level of causation ("why"). While predictive models see shadows on a cave wall (associations), Causal Inference provides the tools to understand the true objects casting them (the underlying causal mechanisms).
    
    **Strategic Application:** This is the ultimate goal of root cause analysis and the foundation of intelligent intervention.
    - **üí° Effective CAPA:** A predictive model might say high sensor readings are *associated* with *low* purity. Causal Inference helps determine if the sensor readings *cause* low purity, or if both are driven by a third hidden variable (a "confounder") like calibration drift. This prevents wasting millions on fixing the wrong problem.
    - **üó∫Ô∏è Process Cartography:** It allows for the creation of a **Directed Acyclic Graph (DAG)**, which is a formal causal map of your process, documenting scientific understanding and guiding future analysis.
    """)
    
    st.info("""
    **Interactive Demo:** Use the slider to control the **Confounding Strength** of the `Calibration Age`. 
    - At low strength, the naive correlation (orange) is close to the true effect (green).
    - As you increase the strength, watch the naive correlation become not just wrong, but **completely inverted**‚Äîa classic demonstration of **Simpson's Paradox**.
    """)
    
    with st.sidebar:
        st.subheader("Causal Inference Controls")
        confounding_slider = st.sidebar.slider(
            "üö® Confounding Strength", 
            min_value=0.0, max_value=10.0, value=5.0, step=0.5,
            help="How strongly the 'Calibration Age' affects BOTH the Sensor Reading (drift) and the Purity (degradation)."
        )
    
    fig_dag, fig_scatter, naive_effect, adjusted_effect = plot_causal_inference(confounding_strength=confounding_slider)
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig_dag, use_container_width=True)
        st.plotly_chart(fig_scatter, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.metric(label="Biased Estimate (Naive Correlation)", value=f"{naive_effect:.3f}",
                      help="The misleading conclusion you would draw by just plotting Purity vs. Sensor Reading.")
            st.metric(label="Unbiased Estimate (True Causal Effect)", value=f"{adjusted_effect:.3f}",
                      help="The true effect of the Sensor Reading on Purity after adjusting for the confounder.")
        
            st.markdown("""
            **The Paradox Explained:**
            - **The DAG (Top Plot):** This map shows our scientific belief. We believe a higher `Sensor Reading` *causes* higher `Purity`. However, `Calibration Age` is a **confounder**: it independently *increases* the Sensor Reading (drift) and *decreases* the Purity.
            - **The Scatter Plot (Bottom):** The naive orange line looks at all data together and concludes that higher sensor readings are associated with *lower* purity. This is **Simpson's Paradox**. The green lines show the truth: *within each calibration group (New or Old)*, the relationship is positive. The adjusted model correctly identifies this true, positive causal effect.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: Investing in the Cause, Not the Symptom
        
            #### The Problem: The Correlation Trap
            A manufacturing site is experiencing a series of batch failures with low product purity. A data analyst is tasked with finding the root cause. They analyze historical data and find a strong negative correlation: batches with higher in-process sensor readings consistently have lower final purity. Based on this "data-driven" insight, management approves a multi-million dollar CAPA project to re-engineer the process to achieve lower sensor readings.
        
            #### The Impact: A Multi-Million Dollar Mistake
            The CAPA project is a catastrophic failure. The new, expensive process successfully achieves lower sensor readings, but the purity problem gets *even worse*.
            - **Wasted Investment:** Millions of dollars and a year of engineering time have been spent on a "solution" that did not work, directly impacting the company's profitability.
            - **Lost Credibility:** The data science team's credibility is shattered. Management loses faith in "data-driven" decision making.
            - **The Real Problem Festers:** The true root cause‚Äîan issue with instrument calibration drift that was affecting *both* the sensor and the purity‚Äîhas gone unaddressed for a year, leading to continued batch failures.
        
            #### The Solution: A Disciplined Framework for Causality
            Causal Inference provides the tools and the intellectual discipline to move beyond simple, and often misleading, correlations. It is the formal methodology for **Root Cause Analysis (RCA)**. It forces the team to:
            1.  **Map Assumptions (DAG):** First, explicitly draw a map (a Directed Acyclic Graph) of their scientific beliefs about what causes what.
            2.  **Identify Confounders:** Use the map to identify "backdoor paths"‚Äîhidden variables (confounders) that create spurious correlations.
            3.  **Isolate the True Effect:** Use the appropriate statistical technique (like multiple regression) to mathematically "block" the confounding paths and isolate the true, unbiased causal relationship.
        
            #### The Consequences: Effective CAPAs and a Learning Organization
            - **Without This:** Root Cause Analysis is a guessing game based on misleading correlations. CAPA projects are high-risk gambles that often fix the symptom while ignoring the disease.
            - **With This:** Causal Inference provides a rigorous, auditable, and data-driven framework for investigation. It ensures that multi-million dollar investments are directed at the **true, scientifically-validated root cause** of a problem. This leads to effective, permanent solutions, prevents the costly mistake of chasing spurious correlations, and builds a deep, causal understanding of the process that becomes a lasting competitive advantage.
            """)

        with tabs[2]:
            st.markdown("""
            ##### Glossary of Causal Terms
            - **Causation vs. Correlation:** Correlation indicates that two variables move together, but does not imply that one causes the other. Causation means that a change in one variable directly produces a change in another.
            - **Confounding:** A situation where the relationship between two variables is distorted by a third, unobserved variable (the confounder) that is associated with both.
            - **DAG (Directed Acyclic Graph):** A visual map of causal assumptions. Nodes represent variables, and directed arrows represent assumed causal effects.
            - **Backdoor Path:** A non-causal path between two variables in a DAG that creates a spurious correlation. To find the true causal effect, all backdoor paths must be "blocked."
            - **Simpson's Paradox:** A statistical phenomenon where a trend appears in several different groups of data but disappears or reverses when these groups are combined. It is a classic example of confounding.
            """)
        with tabs[3]:
            st.error("""üî¥ **THE INCORRECT APPROACH: The Correlation Trap**
            - An analyst observes that higher sensor readings are correlated with lower final purity. They recommend changing the process target to achieve lower sensor readings, believing this will improve purity.
            - **The Flaw:** This intervention would be a disaster. They are acting on a spurious correlation. The real cause of low purity is the old calibration. Their "fix" would actually make things worse by targeting the wrong variable.""")
            st.success("""üü¢ **THE GOLDEN RULE: Draw the Map, Block the Backdoors**
            A robust causal analysis follows a disciplined process.
            1.  **Draw the Map (Build the DAG):** Collaborate with Subject Matter Experts to encode all domain knowledge and causal beliefs into a formal DAG.
            2.  **Identify the Backdoor Paths:** Use the DAG to identify all non-causal "backdoor" paths that create confounding. In our case, the path `Sensor Reading <- Calibration Age -> Purity` is a backdoor.
            3.  **Block the Backdoors:** Use the appropriate statistical technique (like multiple regression) to "adjust for" the confounding variable (`Calibration Age`), blocking the backdoor path and isolating the true causal effect.""")

        with tabs[4]:
            st.markdown("""
            #### Historical Context: The Causal Revolution
            **The Problem:** For most of the 20th century, mainstream statistics was deeply allergic to the language of causation. The mantra, famously drilled into every student, was **"correlation is not causation."** While true, this left a massive void: if correlation isn't the answer, what is? Statisticians were excellent at describing relationships but had no formal language to discuss *why* those relationships existed, leaving a critical gap between data and real-world action.
            
            **The 'Aha!' Moment:** The revolution was sparked by the computer scientist and philosopher **Judea Pearl** in the 1980s and 90s. His key insight was that the missing ingredient was **structure**. He argued that scientists carry causal models in their heads all the time, and that these models could be formally written down as graphs. He introduced the **Directed Acyclic Graph (DAG)** as the language for this structure. The arrows in a DAG are not mere correlations; they are bold claims about the direction of causal influence.
            
            **The Impact:** This was a paradigm shift. By making causal assumptions explicit in a DAG, Pearl developed a complete mathematical framework‚Äîincluding his famous **do-calculus**‚Äîto determine if a causal question *could* be answered from observational data, and if so, how. This "Causal Revolution" provided the first-ever rigorous, mathematical language to move from seeing (`P(Y|X)`) to doing (`P(Y|do(X))`), transforming fields from epidemiology to economics. For this work, Judea Pearl was awarded the Turing Award in 2011, the highest honor in computer science.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("The core difference is between **Observation** and **Intervention**.")
            st.markdown("- **Observation (Correlation):** `P(Y | X = x)` asks, \"What is the expected Y for the subset of units that *we happened to observe* had a value of `x`?\" This is vulnerable to confounding.")
            st.markdown("- **Intervention (Causation):** `P(Y | do(X = x))` asks, \"What would Y be if we *intervened and forced every unit* to have a value of `x`?\" This is the true causal effect.")
            st.markdown("Pearl's **backdoor adjustment formula** shows how to calculate the intervention from observational data. To find the effect of `X` on `Y` with a set of confounders `Z`, we calculate:")
            st.latex(r"P(Y | do(X=x)) = \sum_z P(Y | X=x, Z=z) P(Z=z)")
            st.markdown("In simple terms, this means: for each level of the confounder `z`, find the relationship between `X` and `Y`, and then average those relationships across the distribution of `z`. This is precisely what a multiple regression model does when you include `Z` as a covariate.")
        with tabs[5]:
            st.markdown("""
            Causal inference is an advanced technique that provides a rigorous framework for Root Cause Analysis (RCA), a fundamental requirement of a compliant quality system.
            - **ICH Q10 - Pharmaceutical Quality System:** Mandates a system for Corrective and Preventive Actions (CAPA) that includes a thorough investigation to determine the root cause of deviations. Causal inference provides a formal language and toolset to move beyond simple correlation in these investigations.
            - **21 CFR 211.192 - Production Record Review:** Requires that any unexplained discrepancy or failure of a batch to meet its specifications "shall be thoroughly investigated."
            - **GAMP 5:** While focused on software, its principles of risk management and root cause analysis for deviations apply broadly.
            """)
            
# SNIPPET: Replace your entire render_causal_ml function with this final, high-performance version.

# SNIPPET 2: Replace the entire render_causal_ml function with this corrected version.

def render_causal_ml():
    """Renders the comprehensive module for Causal Machine Learning."""
    st.markdown("""
    #### Purpose & Application: The Unbiased AI Oracle
    **Purpose:** To provide an unbiased, "apples-to-apples" estimate of a treatment or intervention's true causal effect using complex, real-world observational data. **Causal ML (specifically, Double Machine Learning)** is a cutting-edge technique that uses the predictive power of machine learning to strip away confounding and isolate the true, underlying causal relationship.
    
    **Strategic Application:** This is the ultimate tool for data-driven Root Cause Analysis and process optimization from historical, "found" data. It allows you to answer the critical question: **"If I change this process parameter by 1 unit, what is the *true causal impact* on my final product quality, after accounting for all other confounding variables?"**
    """)
    
    st.info("""
    **Interactive Demo:** You are a Data Scientist investigating the effect of a process parameter on product purity.
    1.  Use the **"Confounding Strength"** slider to control how much the confounder distorts the relationship.
    2.  The plot will now update **in real-time**, showing how the Causal ML model (green line) uncovers the true effect while the standard model (red line) is misled by the confounding.
    """)

    with st.sidebar:
        st.subheader("Causal ML Controls")
        confounding_strength = st.slider("Confounding Strength", 0.0, 2.0, 1.0, 0.1, help="How strongly the confounder (W) influences the choice of the process parameter (T).")

    st.header("Causal Machine Learning Dashboard")
    
    # --- FIX: The UI is now fully interactive and includes the metric panel ---
    col_fig, col_kpi = st.columns([0.65, 0.35])
    
    with col_fig:
        try:
            with st.spinner("Training models..."):
                # The plotting function now returns the fig and two scalar values
                fig, causal_effect, naive_effect = plot_causal_ml_comparison(confounding_strength)
                st.plotly_chart(fig, use_container_width=True)
        except Exception as e:
            st.error(f"Could not run Causal ML analysis. Error: {e}")
            st.warning("This can occur if the `econml` library is not installed. Please run: `pip install econml`")
            # Set dummy values on error to prevent dashboard from crashing
            causal_effect, naive_effect = 0, 0
    
    with col_kpi:
        st.subheader("Statistical Verdict")
        st.metric("Unbiased Causal Effect (ATE)", f"{causal_effect:.3f}", help="The true, unbiased impact of the parameter, estimated by the Causal ML model.")
        st.metric("Biased Correlational Effect", f"{naive_effect:.3f}", help="The misleading effect from a simple linear model that ignores the confounder.")
        st.markdown("---")
        st.success("**True Causal Effect is ~2.0**")
        st.markdown("Notice how the Causal ML estimate remains stable and accurate, while the Biased estimate is distorted by the confounding strength.")

    st.divider()
    st.subheader("Deeper Dive into Causal Machine Learning")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **The Raw Data (Grey Dots):** The raw scatter plot shows a complex, messy, and seemingly non-linear relationship between the Process Parameter and the Output.
        2.  **The Standard Model Prediction (Red Dashed Line):** A standard linear regression model is fit to the data. This line represents the **biased correlation**. It is a terrible *explanation* of the true causal effect.
        3.  **The Causal ML Estimate (Green Line):** The Causal ML (LinearDML) model is then applied. It intelligently removes the influence of the confounder to isolate the true, underlying relationship. In this simulation, it correctly uncovers the simple, linear causal effect of ~2.0 that was hidden within the complex correlational data.

        **The Strategic Insight:** Predictive power is not the same as causal understanding. A standard model is a **correlation engine**. A Causal ML model is a sophisticated **causal inference engine**. For Root Cause Analysis and process optimization, you must use the right tool for the job.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: From "What Will Happen?" to "What Should We Do?"

        #### The Problem: The Predictive Model's Dangerous Advice
        A company has a massive historical dataset from its manufacturing process. They train a state-of-the-art AI/ML model that can predict the final batch yield with 95% accuracy. The model's analysis (using tools like SHAP) reveals that the single most important predictor of high yield is "Operator ID = Bob." The model's advice is clear: to increase yield, have Bob run more batches.

        #### The Impact: Actionable Insights vs. Nonsensical Correlations
        This is a classic failure mode of purely predictive modeling that leads to paralysis and mistrust in data science.
        - **Nonsensical Recommendations:** The model's advice is statistically correct but operationally useless and logically absurd. The business cannot "scale up Bob."
        - **Hidden Confounding:** The model has not learned that Bob *causes* high yield. It has learned that Bob is *correlated* with high yield. The true cause might be that Bob only works the night shift, which uses a more reliable piece of equipment (the confounder). The model has confused the symptom (Bob) with the cause (the equipment).
        - **Wasted Investment:** If management acts on the model's advice by giving Bob a raise, they have wasted money and done nothing to improve the underlying process. The true opportunity‚Äîupgrading the day-shift equipment‚Äîhas been completely missed.

        #### The Solution: The AI-Powered "What-If" Machine
        Causal ML (specifically, Double Machine Learning) is the solution to this problem. It is designed to move beyond prediction ("What will happen?") to causal inference ("What would happen if I intervene?"). It uses a clever, two-stage process:
        1.  It uses one ML model to predict the outcome from all the confounders.
        2.  It uses a second ML model to predict the *intervention* (the process parameter) from all the confounders.
        By analyzing the **residuals** (the parts the models couldn't predict), it effectively strips out all the confounding effects, leaving behind a clean, unbiased estimate of the true causal effect of the intervention.

        #### The Consequences: A True Digital Twin for Process Optimization
        - **Without This:** Companies invest in powerful predictive models but are unable to use them to make reliable causal claims, limiting their ROI to simple monitoring.
        - **With This:** Causal ML transforms a predictive model into a true **"what-if" engine** or **digital twin**. It allows the business to use its messy historical data to answer the most valuable question: **"If we change parameter X, what will be the *true causal impact* on our yield?"** This enables data-driven, high-confidence process optimization and ensures that multi-million dollar investment decisions are based on true causal understanding, not spurious correlations.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **Causal Inference:** The process of drawing a conclusion about a causal connection based on the conditions of the occurrence of an effect.
        - **Observational Data:** Data gathered from a system in its natural state, without a controlled experimental design (e.g., historical manufacturing data). This data is often rife with confounding.
        - **Confounder:** A variable that influences both the dependent variable (outcome) and independent variable (treatment), causing a spurious association.
        - **Double Machine Learning (DML):** A state-of-the-art Causal ML technique that uses two supervised machine learning models to partial out the effects of confounders, thereby providing an unbiased estimate of a treatment's causal effect. The `LinearDML` model is a specific implementation of this framework.
        - **Average Treatment Effect (ATE):** The average causal effect of a treatment or intervention across an entire population. `LinearDML` is designed to estimate the ATE.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Predictive Model as Oracle" Fallacy**
An analyst trains a powerful predictive model on historical data and uses its feature importance scores or PDP plots to make recommendations about how to change the process.
- **The Flaw:** They are fundamentally confusing **prediction** with **causation**. The model is an expert at finding correlations, not causes. Any recommendations for action based on a purely predictive model are statistically unjustified and likely to be wrong.""")
        st.success("""üü¢ **THE GOLDEN RULE: Use the Right AI for the Right Question**
1.  **If your question is "What will happen?", use a standard Predictive ML model (like RandomForest).** Its job is to find the best possible correlation-based forecast.
2.  **If your question is "What should I *do*?", you must use a Causal ML model (like Double ML).** Its job is to strip away the correlations to give you an unbiased estimate of the impact of your proposed intervention.
Using a predictive model to answer a causal question is the most common and dangerous mistake in applied data science.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: The Credibility Revolution Meets Machine Learning
        **The Problem:** For decades, economics and other social sciences were plagued by the "correlation is not causation" problem. They had massive amounts of observational data but struggled to make credible causal claims from it. This led to the **"credibility revolution"** in the 1990s, where economists like Joshua Angrist, Guido Imbens, and David Card pioneered methods like instrumental variables and regression discontinuity to find "natural experiments" in the data. For this work, they were awarded the 2021 Nobel Prize in Economics.

        **The New Problem:** These classical causal inference methods were powerful but often relied on simple linear models. The machine learning revolution of the 2010s introduced powerful non-linear models, but these models were purely predictive and had no concept of causality.
        
        **The 'Aha!' Moment (Double ML):** In a landmark 2018 paper, a team of economists and computer scientists including Victor Chernozhukov, Denis Chetverikov, and Esther Duflo (another Nobel laureate) introduced the **Double Machine Learning (DML)** framework. Their brilliant insight was to use the power of modern machine learning *against itself*. They used ML models not to predict the final outcome, but to predict and "partial out" the confounding effects from the data. This left behind a clean, residualized dataset where a simple final model could estimate the true causal effect without bias.
        
        **The Impact:** DML was a revolutionary synthesis. It combined the statistical rigor of the credibility revolution with the predictive power of the machine learning revolution. It provides a robust, general-purpose framework for answering causal questions with complex, high-dimensional, and non-linear observational data, making it a cornerstone of modern econometrics and data science.
        """)
        
    with tabs[5]:
        st.markdown("""
        Causal ML is a state-of-the-art method that provides the ultimate tool for fulfilling the regulatory requirement for **Root Cause Analysis (RCA)** and **Corrective and Preventive Action (CAPA)**, especially when using large historical datasets.
        - **ICH Q10 - Pharmaceutical Quality System:** Mandates a CAPA system that includes a thorough investigation to determine the root cause. When the only available data is observational (historical), Causal ML is the most powerful and statistically rigorous method for performing this investigation and ensuring that the proposed CAPA is based on a true causal factor, not a spurious correlation.
        - **FDA Process Validation Guidance (Stage 3 - CPV):** During Continued Process Verification, companies collect massive amounts of observational data. Causal ML can be used to analyze this data to gain a deep, causal understanding of process dynamics, which can be used to justify process improvements.
        - **ICH Q9 - Quality Risk Management:** Causal ML can be used to validate the causal assumptions made in a qualitative risk tool like an Ishikawa diagram. If the diagram claims that "Parameter X causes Impurity Y," Causal ML can be used to provide the quantitative, data-driven evidence to support or refute that claim.
        """)
##=========================================================================================================================================================================================================
##===============================================================================END ACT I UI Render ========================================================================================================================================
##=========================================================================================================================================================================================================

#========================================================================================= 1. SAMPLE SIZE FOR QUALIFICATION =====================================================================
def render_sample_size_calculator():
    """Renders the comprehensive, interactive module for calculating sample size for qualification."""
    st.markdown("""
    #### Purpose & Application: The Auditor's Question
    **Purpose:** To provide a statistically valid, audit-proof justification for a sampling plan. This tool answers the fundamental question asked in every process validation: **"How did you decide that testing 'n' samples was enough?"**
    
    **Strategic Application:** This is a critical step in writing any Validation Plan (VP) for a Performance Qualification (PQ) or for defining a lot acceptance sampling plan.
    - **Demonstrates Statistical Rigor:** Moves beyond arbitrary or "tribal knowledge" sample sizes (e.g., "we've always used n=30") to a defensible, risk-based approach.
    - **Optimizes Resources:** Using the correct statistical model (e.g., Hypergeometric for finite lots) can often reduce the required sample size compared to overly conservative methods, saving significant time and cost.
    """)
    
    st.info("""
    **Interactive Demo:** Use the controls in the sidebar to define your statistical requirements (Confidence & Reliability) and process type.
    - The **KPI** on the left shows the calculated sample size needed.
    - The **Plot** on the right visualizes the trade-off. Your specific requirement is marked by the green star. Notice how increasing reliability or confidence demands a larger sample size.
    """)
    
    with st.sidebar:
        st.subheader("Sample Size Controls")
        calc_method = st.radio(
            "Select the statistical model:",
            ["Binomial (Continuous Process / Large Lot)", "Hypergeometric (Finite Lot)"],
            help="Choose Binomial for ongoing processes or very large batches. Choose Hypergeometric for discrete, smaller batches to get a more accurate (and often smaller) sample size."
        )
        confidence_level = st.slider("Confidence Level (C)", 80.0, 99.9, 95.0, 0.1, format="%.1f%%")
        reliability = st.slider("Required Reliability (R)", 90.0, 99.9, 99.0, 0.1, format="%.1f%%")

        lot_size = None
        if "Hypergeometric" in calc_method:
            lot_size = st.number_input(
                "Enter the total Lot Size (M)", 
                min_value=10, max_value=100000, value=1000, step=10,
                help="The total number of units in the discrete batch you are sampling from."
            )
            
    # --- Calculation Logic (moved up for clarity) ---
    c = confidence_level / 100
    r = reliability / 100
    sample_size, model_used = "N/A", ""

    if "Binomial" in calc_method:
        model_used = "Binomial"
        sample_size = int(np.ceil(np.log(1 - c) / np.log(r))) if r < 1.0 else "Infinite"
    elif lot_size:
        model_used = "Hypergeometric"
        D = math.floor((1 - r) * lot_size)
        @st.cache_data
        def find_hypergeometric_n(M, D, C):
            if M <= D: return 1
            log_alpha = math.log(1 - C)
            for n in range(1, M - D + 2):
                if n > M - D: return M - D
                log_p0 = (math.lgamma(M-D+1) - math.lgamma(n+1) - math.lgamma(M-D-n+1)) - \
                         (math.lgamma(M+1) - math.lgamma(n+1) - math.lgamma(M-n+1))
                if log_p0 <= log_alpha: return n
            return M - D
        sample_size = find_hypergeometric_n(lot_size, D, c)

    # --- Dashboard Layout ---
    col1, col2 = st.columns([0.6, 0.4])
    with col1:
        fig = plot_sample_size_curves(confidence_level, reliability, lot_size, calc_method, sample_size)
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.metric(
                label=f"Required Sample Size (n) via {model_used}",
                value=f"{sample_size} units",
                help="Minimum units to test with zero failures to meet your claim."
            )
            st.success(f"""
            **Actionable Conclusion:**
            
            To demonstrate with **{confidence_level:.1f}% confidence** that your process is at least **{reliability:.1f}% reliable**, you must test **{sample_size} units** and find **zero failures**.
            """)
            st.markdown("""
            **Reading the Plot:**
            - The curves show the **best possible reliability** you can claim for a given sample size (with zero failures).
            - As `n` increases, your statistical power increases, allowing you to claim higher reliability.
            - Notice the **Hypergeometric curve (orange dash)** is always above the Binomial curve. This shows that for a finite lot, you need a slightly smaller sample size to make the same statistical claim, as each good part you draw slightly increases the chance the next one is good.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: The Risk vs. Cost Negotiation
        
            #### The Problem: The Arbitrary Sample Size
            A validation team is writing the protocol for a critical Process Performance Qualification (PPQ). When it comes to defining the sampling plan, they fall back on "tribal knowledge": "We've always used n=30." They have no statistical justification for this number, nor can they articulate what level of quality assurance it actually provides.
        
            #### The Impact: Indefensible Plans and Unmanaged Risk
            This arbitrary approach creates significant business and compliance risks:
            - **Audit Failure:** During a regulatory audit, the team is asked the inevitable question: "How did you determine this sample size was sufficient?" Answering "we've always done it this way" is a major red flag, signaling a lack of statistical control and a superficial approach to validation.
            - **Hidden Risk Acceptance:** The `n=30` plan might only provide 95% confidence in a 90% reliability rate. However, the business *assumed* it was getting 99% reliability. The company has unknowingly accepted a 10x higher defect rate risk than they were comfortable with, which could lead to future batch failures or customer complaints.
            - **Wasted Resources:** Conversely, the `n=30` plan might be overkill for a low-risk, non-critical parameter. The company may be spending significant time and money on excessive testing that provides no additional value, slowing down the validation process.
        
            #### The Solution: A Formal Negotiation Between Risk and Cost
            This tool transforms the sample size decision from an arbitrary guess into a formal, transparent **negotiation between business risk and operational cost**. It forces a critical, cross-functional conversation *before* the validation begins.
            1.  **Define the Risk (The Business Decision):** Stakeholders from Quality, Regulatory, and Business must first agree on the required quality claim: "For this critical process, we must be **95% confident** that the batch is **99% compliant**." This is a business decision about risk tolerance.
            2.  **Calculate the Cost (The Statistical Output):** The required sample size is the direct mathematical output of that business decision.
        
            #### The Consequences: A Defensible, Cost-Effective, and Risk-Based Strategy
            - **Without This:** The sampling plan is an indefensible, unquantified gamble.
            - **With This:** The sampling plan is a **defensible, cost-effective, and risk-based contract**. It provides a clear, auditable trail linking the business's risk tolerance directly to the number of samples tested. It ensures that the company is not over-spending on low-risk items or under-testing high-risk ones, leading to an optimized and compliant validation strategy.
            """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Sampling Terms
            - **Acceptance Sampling:** A statistical method used to determine whether to accept or reject a production lot of material based on the inspection of a sample.
            - **Confidence (C):** The probability that the conclusion you draw from your sample is correct. A 95% confidence level means you are accepting a 5% risk of being wrong (drawing a "lucky" good sample from a bad lot).
            - **Reliability (R):** The proportion of conforming items in the lot or process. A reliability of 99% means the true defect rate is no more than 1%.
            - **Binomial Distribution:** A probability model used when sampling from a very large or continuous population (sampling with replacement). The probability of success is constant for each trial.
            - **Hypergeometric Distribution:** A probability model used when sampling from a small, finite population without replacement. The probability of success changes with each draw.
            """)
        with tabs[3]:
            st.error("""üî¥ **THE INCORRECT APPROACH: The "Square Root of N Plus One" Fallacy**
An engineer is asked for a sampling plan and defaults to an arbitrary, non-statistical rule of thumb they learned years ago, like `n = sqrt(Lot Size) + 1`.
- **The Flaw:** This plan is completely disconnected from risk. It cannot answer the question: "What level of confidence and reliability does this plan provide?" It is indefensible during a regulatory audit.""")
            st.success("""üü¢ **THE GOLDEN RULE: State Your Risk, Then Calculate Your Sample**
A compliant and statistically sound sampling plan is always derived from pre-defined risk criteria.
1.  **First, Define the Claim:** Before touching a calculator, stakeholders (Quality, Regulatory, Clinical) must agree on the required claim. "We need to be **95% confident** that the batch is **99% compliant**."
2.  **Then, Justify the Model:** Choose the correct statistical model for the situation (Binomial for a continuous process, Hypergeometric for a finite lot).
3.  **Finally, Calculate `n`:** The required sample size is the direct mathematical output of the first two steps. This creates a clear, traceable, and audit-proof justification for your validation plan.""")
            
        with tabs[4]:
            st.markdown("""
            #### Historical Context: From Gosset's Brewery to Modern Industry
            **The Problem:** At the turn of the 20th century, William Sealy Gosset, a chemist at the Guinness brewery in Dublin, faced a problem central to industrial quality control: how to make reliable conclusions from very small samples. The classical statistical theory of the time required large samples, but Gosset could only afford to take a few measurements from each batch of beer. This practical constraint forced a revolution in statistics.
            
            **The 'Aha!' Moment:** Gosset, writing under the pseudonym "Student," developed what we now know as the **Student's t-distribution** (1908). This was the first "small-sample" statistical method. It mathematically demonstrated that with very few data points, our uncertainty is much higher than previously thought. The convention of using `n=3` is a direct, practical consequence of this insight: it represents the smallest possible sample size where you can calculate a meaningful standard deviation and thus begin to characterize the uncertainty Gosset was trying to tame.
            
            **The Military-Industrial Complex:** Decades later, during World War II, the need for robust sampling exploded. Statisticians like Harold Dodge, Harry Romig, and Abraham Wald developed modern **acceptance sampling** for the military. Their work, codified in standards like **MIL-STD-105**, provided the rigorous mathematical framework (using Binomial and Hypergeometric distributions) to link sample size to specific, contractual quality levels (AQL).
            
            **The Modern Synthesis:** Today's practices blend both legacies. Gosset's "small-sample" thinking justifies using triplicates for preliminary repeatability checks, while the military's acceptance sampling framework provides the high-assurance models needed for final product release and process qualification.
            """)
        
        with tabs[5]:
            st.markdown("""
            A statistically justified sampling plan is a fundamental expectation in any GxP-regulated environment. It provides the **objective evidence** required to support validation claims and batch disposition decisions.
            
            ---
            ##### FDA - Process Validation & CFR
            - **Process Validation Guidance (2011):** This calculator is most directly applicable to **Stage 2: Process Performance Qualification (PPQ)**. The guidance states that the number of samples should be "sufficient to provide statistical confidence of quality both within a batch and between batches." This tool provides the statistical rationale for "sufficient."
            - **21 CFR 211.165(d):** Requires that "acceptance criteria for the sampling and testing conducted... shall be adequate to assure that batches of drug products meet **appropriate statistical quality control criteria**." This calculator is a method for defining such criteria.
            - **21 CFR 211.110(b):** Requires written procedures for in-process controls and tests, including "Control procedures shall include... scientifically sound and appropriate sampling plans."
            
            ---
            ##### Medical Devices - ISO 13485 & 21 CFR 820
            - **ISO 13485:2016 (Section 7.5.6):** This global standard for medical device quality management requires that process validation activities ensure the process can "consistently produce product which meets specifications."
            - **21 CFR 820.250 (Statistical Techniques):** States that "Where appropriate, each manufacturer shall establish and maintain procedures for identifying valid statistical techniques required for establishing, controlling, and verifying the acceptability of process capability and product characteristics." This calculator is a prime example of such a technique.
            
            ---
            ##### ICH Guidelines - A Global Perspective
            - **ICH Q9 (Quality Risk Management):** The selection of a confidence and reliability level is a direct input from the Quality Risk Management process. A high-risk product or process parameter would demand higher confidence and reliability, leading to a larger sample size. This tool provides the direct link between the risk assessment and the validation sampling plan.
            
            **The Golden Thread:** Across all regulations, the expectation is the same. The choice of a sample size cannot be arbitrary. It must be **pre-defined, justified, and linked to the level of quality and assurance** required for the product. This calculator provides that traceable, scientific justification.
            """)

    with st.expander("üîé Special Topic: The Role of Triplicates (n=3) in Experimental Design"):
        st.markdown("""
        While the calculator above determines sample size for lot acceptance (a `go/no-go` decision), a common question in experimental design is: **"Why do we so often test in triplicates?"** While `n=3` is rarely sufficient for a full PQ, it is a common and justifiable choice for smaller-scale experiments for several reasons.
        
        #### Why triplicates are often used
        *   **Statistical estimate of variability:** With only one measurement, you have no way to know if the result is representative. With two, you can see if results differ, but you can‚Äôt estimate variance reliably. With three, you can calculate a mean, standard deviation, and %RSD, giving you a basic measure of repeatability.
        *   **Outlier detection:** If one result deviates significantly from the other two, you can identify possible anomalies due to instrument noise, handling error, or environmental changes.
        *   **Compliance with common scientific practice:** In many biological, chemical, and engineering fields, `n=3` is a de facto minimum for demonstrating reproducibility without making the experiment prohibitively costly or time-consuming.

        #### Mathematical Basis: The Power of `n-1`
        The ability to estimate variance from a sample is based on the concept of **degrees of freedom (df)**. The formula for sample standard deviation is:
        """)
        st.latex(r"s = \sqrt{\frac{\sum_{i=1}^{n}(x_i - \bar{x})^2}{n-1}}")
        st.markdown("""
        -   For `n=1`, the denominator is `1-1=0`. The standard deviation is undefined. You have zero degrees of freedom to estimate variability.
        -   For `n=2`, the denominator is `2-1=1`. You have one degree of freedom. You can calculate a standard deviation, but it's a very unstable "point estimate" of the true process variability.
        -   For `n=3`, the denominator is `3-1=2`. You have two degrees of freedom. This is the smallest sample size where the estimate of variability begins to gain some (though still limited) stability. It is the absolute minimum for a meaningful statistical characterization of repeatability.

        #### When triplicates alone are NOT sufficient
        For formal method validation (especially in regulated environments like FDA, ISO 17025, CLSI, ICH Q2), triplicates in a single run are almost never enough. Regulators expect a more comprehensive demonstration of robustness, usually including:
        *   Multiple runs across different days.
        *   Multiple analysts/operators.
        *   Multiple instruments (if applicable).
        *   Larger datasets for key performance parameters (e.g., accuracy, linearity, robustness).
        
        > ‚úÖ **Bottom line:** Triplicates are the statistical minimum for assessing within-run repeatability. For formal validation, they are just one piece of the puzzle‚Äînot the whole picture.
        """)
    
    with st.expander("View Detailed Statistical Methodology for Lot Acceptance"):
        st.markdown(f"""
        #### Mathematical Basis
        This calculation is rooted in probability theory. The choice of model depends on the nature of the population being sampled.
        ---
        #### 1. Binomial Model (Large Lot / Continuous Process)
        **Assumption:** The population is effectively **infinite**. Each sample is independent, and the act of sampling does not change the underlying defect rate of the process.
        **Formula:** We solve for `n` in the inequality `R^n <= 1 - C`, which gives:
        """)
        st.latex(r''' n \ge \frac{\ln(1 - C)}{\ln(R)} ''')
        st.markdown("---")
        st.markdown("""
        ##### 2. Hypergeometric Model (Finite Lot)
        **Assumption:** Sampling is done **without replacement** from a discrete lot of a known, finite size `M`.
        **Formula:** We iterate to find the smallest integer `n` that satisfies:
        """)
        st.latex(r''' P(X=0) = \frac{\binom{M-D}{n}}{\binom{M}{n}} \le 1 - C ''')
        st.markdown("""
        Where `M` is Lot Size, `D` is max allowable defects (`floor((1-R) * M)`), and `n` is Sample Size.
        """)
#========================================================================== 2. ADVANCED STABILITY DESIGN ==================================
def render_stability_design():
    """Renders the comprehensive, interactive module for Stability Study Design."""
    st.markdown("""
    #### Purpose & Application: Strategic Cost-Savings in Stability
    **Purpose:** To demonstrate how to apply risk-based, statistically justified strategies (**Bracketing** and **Matrixing**) to reduce the cost and complexity of large-scale stability studies, as outlined in **ICH Q1D**.
    
    **Strategic Application:** For a product with many variations (e.g., multiple strengths and container types), testing every combination at every timepoint is prohibitively expensive. This tool provides a powerful way for a validation leader to design and justify a resource-saving validation strategy to management and regulators without compromising compliance or scientific rigor.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Stability Program Manager.
    1.  Use the **Product Factors** selectors in the sidebar to define the complexity of your product.
    2.  Select a **Reduced Study Design** to see how it compares to the full study.
    3.  The **Heatmap Tables** visualize the number of stability pulls required for each design, and the KPIs quantify the cost savings.
    """)

    with st.sidebar:
        st.subheader("Stability Design Controls")
        strengths = st.multiselect("Select Product Strengths (mg)", [10, 25, 50, 100], default=[10, 25, 50, 100],
            help="The different dosage strengths of the product. The more strengths, the greater the benefit of a reduced design.")
        containers = st.multiselect("Select Container Types", ["Vial", "Pre-filled Syringe"], default=["Vial", "Pre-filled Syringe"],
            help="The different primary packaging configurations for the product.")
        design_type = st.radio("Select Reduced Study Design", ["Bracketing", "Matrixing"],
            help="**Bracketing:** Tests only the extremes (e.g., lowest and highest strengths). **Matrixing:** Tests a strategic subset of all combinations at specific timepoints.")

    if not strengths or not containers:
        st.warning("Please select at least one Strength and one Container type.")
        return

    fig_full, fig_reduced, pulls_saved, total_full, total_reduced = plot_stability_design_comparison(strengths, containers, design_type)

    st.header("Stability Study Design Comparison")
    col1, col2, col3 = st.columns(3)
    col1.metric("Total Pulls (Full Study)", f"{total_full}")
    col2.metric("Total Pulls (Reduced Study)", f"{total_reduced}")
    col3.metric("Samples Saved (%)", f"{pulls_saved} ({pulls_saved/total_full:.0%})")

    col_full, col_reduced = st.columns(2)
    with col_full:
        st.plotly_chart(fig_full, use_container_width=True)
    with col_reduced:
        st.plotly_chart(fig_reduced, use_container_width=True)
        
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])

    with tabs[0]:
        st.markdown("""
        **Interpreting the Designs:**
        - **Full Study:** This is the baseline, testing every combination of strength and container at every timepoint. It is the most comprehensive but also the most expensive.
        - **Bracketing:** This design assumes that the stability of the intermediate strengths is represented by the stability at the extremes. It is a powerful cost-saver, but requires strong scientific justification. As you can see, all intermediate strength/container combinations are removed from the study.
        - **Matrixing:** This design assumes that the stability of the product is similar across all combinations. It reduces the testing burden by, for example, testing only half the samples at each timepoint in a checkerboard pattern. It provides more information about the entire product family than bracketing.
        
        **The Strategic Insight:** The choice between these designs is a risk-based decision. **Bracketing** is ideal when the primary stability risk is related to the extremes (e.g., highest strength has a new excipient, lowest strength is more susceptible to degradation). **Matrixing** is ideal when the process and formulation are very consistent across all combinations and you want to reduce the overall testing load.
        """)

    with tabs[1]:
        st.markdown("""
        ### The Business Case: Accelerating Timelines by Eliminating Redundant Work

        #### The Problem: The Tyranny of the Full Factorial
        A company is preparing to launch a new drug product that will be available in 5 different strengths and 3 different container types (e.g., vial, pre-filled syringe, auto-injector). The traditional approach to stability testing requires a full, multi-year study on every single one of the 15 combinations.

        #### The Impact: The "Stability Budget" Black Hole
        This "test everything" approach creates a massive, unsustainable drain on the company's most critical resources.
        - **Exorbitant Cost:** The cost of a single stability study is enormous, including the cost of the drug product itself, the labor for pulling and testing samples, and the use of expensive, capacity-limited stability chambers. Multiplying this by 15 creates a multi-million dollar budget item.
        - **Paralyzed R&D Pipeline:** The company's finite stability chambers and QC lab capacity are completely consumed by this single, massive study. This creates a bottleneck that prevents the company from starting stability studies on new, innovative pipeline products, directly slowing down the company's growth engine.
        - **Delayed Time-to-Market:** The sheer logistical complexity of managing such a large study can lead to delays in the final regulatory submission, postponing the launch and sacrificing months of potential revenue.

        #### The Solution: A Risk-Based, Statistically Justified Reduction
        Advanced stability designs (Bracketing and Matrixing), as formally sanctioned by the ICH Q1D guideline, are not about "doing less testing." They are a **strategic, risk-based approach to eliminating scientifically redundant testing**. The core principle is to leverage prior knowledge about the product to design a leaner, more intelligent study.
        - **Bracketing** leverages the knowledge that stability failures are most likely to occur at the extremes of a factor like dosage strength.
        - **Matrixing** leverages the knowledge that the product is highly consistent across all variations, allowing for a statistically balanced, partial testing schedule.

        #### The Consequences: A Leaner, Faster, and Smarter Stability Program
        - **Without This:** The stability program is a costly, inefficient bottleneck that consumes an ever-increasing share of the R&D and Quality budget.
        - **With This:** The company can achieve the same level of regulatory compliance and product quality assurance with **up to 50% fewer samples**. This directly translates to a **multi-million dollar cost savings**, **accelerated timelines** for the current product, and, most importantly, it **frees up critical lab capacity and budget** to advance the next generation of innovative products in the R&D pipeline. It is a key strategy for maintaining a competitive edge.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Stability Design Terms
        - **Stability Study:** A formal study to determine the time period during which a drug product remains within its specifications under defined storage conditions.
        - **Full Design:** A study design in which samples for every combination of all design factors (e.g., strength, container size, lot) are tested at all timepoints.
        - **Reduced Design:** A study design in which samples for every factor combination are not all tested at all timepoints.
        - **Bracketing:** A reduced design in which only samples on the extremes of certain design factors (e.g., lowest and highest strengths) are tested at all timepoints.
        - **Matrixing:** A reduced design in which a selected subset of the total number of possible samples is tested at a specified timepoint. At a subsequent timepoint, a different subset of samples is tested.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: "Ad-Hoc Reduction"**
A team is running over budget and decides to arbitrarily skip testing some samples from their full stability protocol without pre-approval or a statistical justification.
- **The Flaw:** This invalidates the entire study. The missing data creates unexplainable gaps, and regulators will reject the study, forcing a costly repeat. It introduces unmanaged risk.""")
        st.success("""üü¢ **THE GOLDEN RULE: Justify, Document, and Pre-Approve**
A reduced stability design is a powerful, compliant strategy *only* if it is handled with formal discipline.
1.  **Justify:** The choice of a reduced design (and the specific design chosen) must be justified based on scientific knowledge and a formal risk assessment (FMEA).
2.  **Document:** The justification and the exact bracketing or matrixing plan must be explicitly detailed in the Stability Protocol.
3.  **Pre-Approve:** The protocol must be reviewed and approved by Quality Assurance *before* the study begins.""")
        
    with tabs[4]:
        st.markdown("""
        #### Historical Context: The Cost of Complexity
        As the pharmaceutical industry grew in the latter half of the 20th century, the complexity of product portfolios exploded. A single drug product might be marketed in five different strengths, three different container sizes, and be manufactured in two different facilities. The traditional expectation of running a full, multi-year stability study on every single combination became an enormous and often scientifically redundant financial burden.
        
        Regulators and industry, working through the **International Council for Harmonisation (ICH)**, recognized this challenge. They sought to create a scientifically sound, risk-based framework that would allow companies to reduce their testing burden without compromising patient safety. This led to the development of the **ICH Q1D guideline**, which formally defined and sanctioned the use of Bracketing and Matrixing designs for formal stability studies.
        """)
        
    with tabs[5]:
        st.markdown("""
        The design of stability studies is governed by a specific set of harmonized international guidelines.
        - **ICH Q1D - Bracketing and Matrixing Designs for Stability Testing:** This is the primary global guideline that provides the rationale, definitions, and requirements for implementing and justifying reduced stability study designs.
        - **ICH Q1A(R2) - Stability Testing of New Drug Substances and Products:** This guideline defines the overall requirements for stability programs, and Q1D serves as a specific appendix to it.
        - **FDA Guidance for Industry - Q1D Bracketing and Matrixing:** The FDA's formal adoption and implementation of the ICH guideline, making it a regulatory expectation in the United States.
        """)

#======================================================================= 3. METHOD COMPARISON ========================================================================
def render_method_comparison():
    """Renders the INTERACTIVE module for Method Comparison."""
    st.markdown("""
    #### Purpose & Application
    **Purpose:** To formally assess and quantify the degree of agreement and systemic bias between two different measurement methods intended to measure the same quantity.
    
    **Strategic Application:** This study is the "crucible" of method transfer, validation, or replacement. It answers the critical business and regulatory question: ‚ÄúDo these two methods produce the same result, for the same sample, within medically or technically acceptable limits?‚Äù
    """)
    
    st.info("""
    **Interactive Demo:** Use the sliders at the bottom of the sidebar to simulate different types of disagreement between a "Test" method and a "Reference" method. See in real-time how each diagnostic plot (Deming, Bland-Altman, %Bias) reveals a different aspect of the problem, helping you build a deep intuition for method comparison statistics.
    """)
    
    # --- Sidebar controls for this specific module ---
    st.sidebar.subheader("Method Comparison Controls")
    constant_bias_slider = st.sidebar.slider(
        "‚öñÔ∏è Constant Bias", 
        min_value=-10.0, max_value=10.0, value=2.0, step=0.5,
        help="A fixed offset where the Test method reads consistently higher (+) or lower (-) than the Reference method across the entire range."
    )
    proportional_bias_slider = st.sidebar.slider(
        "üìà Proportional Bias (%)", 
        min_value=-10.0, max_value=10.0, value=3.0, step=0.5,
        help="A concentration-dependent error. A positive value means the Test method reads progressively higher than the Reference at high concentrations."
    )
    random_error_slider = st.sidebar.slider(
        "üé≤ Random Error (SD)", 
        min_value=0.5, max_value=10.0, value=3.0, step=0.5,
        help="The imprecision or 'noise' of the methods. Higher error widens the Limits of Agreement on the Bland-Altman plot."
    )

    # Generate plots using the slider values
    fig, slope, intercept, bias, ua, la = plot_method_comparison(
        constant_bias=constant_bias_slider,
        proportional_bias=proportional_bias_slider,
        random_error_sd=random_error_slider
    )
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ Acceptance Criteria", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.metric(label="üìà Mean Bias (Bland-Altman)", value=f"{bias:.2f} units", help="The average systematic difference.")
            st.metric(label="üí° Deming Slope", value=f"{slope:.3f}", help="Ideal = 1.0. Measures proportional bias.")
            st.metric(label="üí° Deming Intercept", value=f"{intercept:.2f}", help="Ideal = 0.0. Measures constant bias.")
            
            st.info("Play with the sliders in the sidebar and observe the plots!")
            st.markdown("""
            - **Add `Constant Bias`:** The Deming line shifts up/down but stays parallel to the identity line. The Bland-Altman plot's mean bias line moves away from zero.
            - **Add `Proportional Bias`:** The Deming line *rotates* away from the identity line. The Bland-Altman and %Bias plots now show a clear trend, a major red flag.
            - **Increase `Random Error`:** The points scatter more widely. This has little effect on the average bias but dramatically **widens the Limits of Agreement**, making the methods less interchangeable.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: Ensuring a Seamless "Changing of the Guard"
        
            #### The Problem: The "Is the New Ruler the Same as the Old One?" Question
            A company needs to make a critical change to a measurement system. This could be:
            - **A Tech Transfer:** Transferring a validated assay from the R&D lab to a new manufacturing site's QC lab.
            - **A Method Upgrade:** Replacing an old, obsolete instrument with a new, faster model.
            - **A New Supplier:** Qualifying a second-source supplier for a critical diagnostic kit.
            In all cases, the business must answer one crucial question with data: "Does the new method give us the same answers as the old, trusted one?"
        
            #### The Impact: The High Cost of Unmanaged Method Changes
            Failing to rigorously quantify the agreement between the old and new methods can have severe consequences for the business and its patients.
            - **The "Step-Change" Disaster:** The new instrument is installed, and suddenly the process, which had been stable for years, appears to have shifted. This triggers a massive, costly investigation into the manufacturing process, only to discover weeks later that the "shift" was not real‚Äîit was just an undetected bias in the new measurement system.
            - **Invalidating Historical Data:** If the new method is biased, all new data is no longer comparable to years of historical data. This destroys the value of historical databases, makes long-term trend analysis impossible, and can create major issues during regulatory inspections.
            - **Patient Impact:** In a clinical setting, if a new diagnostic instrument has a proportional bias, it could systematically under-report values for high-risk patients, leading to missed diagnoses and improper treatment.
        
            #### The Solution: A Rigorous "Due Diligence" Study
            A Method Comparison study, using robust tools like **Bland-Altman analysis**, is the formal "due diligence" process for any method change. It is not enough to show the methods are "correlated." This study is designed to forensically dissect the nature of their disagreement by answering three specific questions:
            1.  Is there a **constant bias** (a fixed offset)?
            2.  Is there a **proportional bias** (a trend in the disagreement)?
            3.  What is the expected **range of disagreement** for any future sample (the Limits of Agreement)?
        
            #### The Consequences: A Confident and Seamless Transition
            - **Without This:** Any change to a measurement system is a high-risk gamble that can disrupt operations, invalidate historical data, and create significant compliance risks.
            - **With This:** The method comparison study provides **objective, quantitative evidence** that the new method is a trustworthy and interchangeable replacement for the old one. It allows the business to confidently make changes and upgrades to its analytical systems without disrupting manufacturing, compromising data integrity, or putting patients at risk. It is the essential tool for ensuring continuity and confidence in the face of analytical evolution.
            """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Comparison Terms
            - **Agreement:** The degree to which two different methods produce the same result for the same sample. This is different from correlation.
            - **Constant Bias:** A systematic error where one method consistently reads higher or lower than the other by a fixed amount across the entire measurement range.
            - **Proportional Bias:** A systematic error where the difference between the two methods is concentration-dependent, typically increasing as the concentration increases.
            - **Passing-Bablok Regression:** A robust, non-parametric regression method that is insensitive to outliers and assumes error in both methods. It is superior to OLS for method comparison.
            - **Bland-Altman Plot:** A graphical method to plot the difference between two measurements against their average. It is the gold standard for visualizing agreement and identifying bias.
            - **Limits of Agreement (LoA):** On a Bland-Altman plot, the interval `[mean_diff ¬± 1.96 * std_diff]` within which 95% of future differences are expected to fall.
            """)
        with tabs[3]:
            st.markdown("Acceptance criteria must be pre-defined and clinically/technically justified.")
            st.markdown("- **Deming Regression:** The 95% confidence interval for the **slope must contain 1.0**, and the 95% CI for the **intercept must contain 0**.")
            st.markdown(f"- **Bland-Altman:** The primary criterion is that the **95% Limits of Agreement (`{la:.2f}` to `{ua:.2f}`) must be clinically or technically acceptable**.")
            st.error("""
            **The Correlation Catastrophe:** Never use the correlation coefficient (R¬≤) to assess agreement. Two methods can be perfectly correlated (R¬≤=1.0) but have a huge bias (e.g., one method always reads twice as high).
            """)

        with tabs[4]:
            # FIX: Restored the full, detailed content for this tab
            st.markdown("""
            #### Historical Context & Origin
            For decades, scientists committed a cardinal sin: using **Ordinary Least Squares (OLS) regression** and the **correlation coefficient (r)** to compare methods. This is flawed because OLS assumes the x-axis (reference method) is measured without error, an impossibility.
            
            - **Deming's Correction:** While known to statisticians, **W. Edwards Deming** championed this type of regression in the 1940s. It correctly assumes both methods have measurement error, providing an unbiased estimate of the true relationship. **Passing-Bablok regression** is a robust non-parametric alternative.
            
            - **The Bland-Altman Revolution:** A 1986 paper in *The Lancet* by **J. Martin Bland and Douglas G. Altman** ruthlessly exposed the misuse of correlation and proposed their brilliantly simple alternative. Instead of plotting Y vs. X, they plotted the **Difference (Y-X) vs. the Average ((Y+X)/2)**. This directly visualizes the magnitude and patterns of disagreement and is now the undisputed gold standard.
            
            #### Mathematical Basis
            **Deming Regression:** OLS minimizes the sum of squared vertical distances. Deming regression minimizes the sum of squared distances from the points to the line, weighted by the ratio of the error variances of the two methods.
            
            **Bland-Altman Plot:** This is a graphical analysis. The key metrics are the **mean difference (bias)**, $\bar{d}$, and the **standard deviation of the differences**, $s_d$. The 95% Limits of Agreement (LoA) are calculated assuming the differences are approximately normally distributed:
            """)
            st.latex(r"LoA = \bar{d} \pm 1.96 \cdot s_d")
            st.markdown("This interval provides a predictive range: we can be 95% confident that the difference between the two methods for a future sample will fall within these limits.")
        with tabs[5]:
            st.markdown("""
            Method comparison studies are essential for method transfer, validation of a new method against a standard, or bridging studies.
            - **ICH Q2(R1) - Validation of Analytical Procedures:** The principles of comparing methods fall under the assessment of **Accuracy** and **Intermediate Precision**.
            - **USP General Chapter <1224> - Transfer of Analytical Procedures:** This chapter is entirely dedicated to the process of qualifying a laboratory to use an analytical test procedure. It explicitly mentions "Comparative Testing" as a transfer option, for which Bland-Altman and Deming regression are the standard analysis tools.
            - **CLIA (Clinical Laboratory Improvement Amendments):** In the US, clinical labs are required to perform method comparison studies to validate new tests.
            """)
#===============================================================  4. EQUIVALENCE TESTING (TOST) ================================================
def render_tost():
    """Renders the INTERACTIVE module for Two One-Sided Tests (TOST) for equivalence."""
    st.markdown("""
    #### Purpose & Application
    **Purpose:** To statistically prove that two methods or groups are **equivalent** within a predefined, practically insignificant margin. This flips the logic of standard hypothesis testing from trying to prove a difference to trying to prove a lack of meaningful difference.
    
    **Strategic Application:** This is the statistically rigorous way to handle comparisons where the goal is to prove similarity, not difference, such as in biosimilarity studies, analytical method transfers, or validating a new manufacturing site.
    """)
    
    st.info("""
    **Interactive Demo:** This new 3-plot dashboard tells a complete story.
    1.  See the raw sample data at the top.
    2.  Watch how that translates into the evidence about the *difference* in the middle plot.
    3.  See the final verdict at the bottom. The bar in Plot 3 is just a summary of the shaded area in Plot 2.
    """)
    
    with st.sidebar:
        st.subheader("TOST Controls")
        delta_slider = st.slider(
            "‚öñÔ∏è Equivalence Margin (Œî)", 1.0, 15.0, 5.0, 0.5,
            help="The 'goalposts'. Defines the zone where differences are considered practically meaningless."
        )
        diff_slider = st.slider(
            "üéØ True Difference", -10.0, 10.0, 1.0, 0.5,
            help="The actual underlying difference between the two groups in the simulation."
        )
        sd_slider = st.slider(
            "üå´Ô∏è Standard Deviation (Variability)", 1.0, 15.0, 5.0, 0.5,
            help="The random noise in the data. Higher variability widens the CI, making equivalence harder to prove."
        )
        n_slider = st.slider(
            "üî¨ Sample Size (n)", 10, 200, 50, 5,
            help="The number of samples per group. Higher sample size narrows the CI, increasing your power."
        )
    
    fig, p_tost, is_equivalent, ci_lower, ci_upper, mean_A, mean_B, diff_mean = plot_tost(
        delta=delta_slider,
        true_diff=diff_slider,
        std_dev=sd_slider,
        n_samples=n_slider
    )
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            status = "‚úÖ EQUIVALENT" if is_equivalent else "‚ùå NOT EQUIVALENT"
            if is_equivalent:
                st.success(f"### Result: {status}")
            else:
                st.error(f"### Result: {status}")
        
            st.metric(label="p-value (TOST)", value=f"{p_tost:.4f}", help="If p < 0.05, we conclude equivalence.")
            st.metric(label="üìä Observed 90% CI for Difference", value=f"[{ci_lower:.2f}, {ci_upper:.2f}]")
            st.metric(label="üìà Observed Difference", value=f"{diff_mean:.2f}",
                      help="The difference between the two sample means (Mean B - Mean A).")
            st.metric(label="‚öñÔ∏è Equivalence Margin", value=f"¬± {delta_slider:.1f} units")
        
            st.markdown("---")
            st.markdown("##### The 3-Plot Story: How the Plots Connect")
            st.markdown("""
            1.  **Plot 1 (The Samples):** Shows the raw data you collected. The vertical dashed lines mark the *mean* of each sample.
            2.  **Plot 2 (The Evidence):** This is the crucial link. It shows our statistical uncertainty about the true difference in means. The shaded area is the **90% Confidence Interval**.
            3.  **Plot 3 (The Verdict):** This is just a compact summary of Plot 2. The bar represents the exact same 90% Confidence Interval.
        
            **The conclusion of 'Equivalence' is reached when the entire shaded distribution in Plot 2 falls inside the light green 'Equivalence Zone'.**
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: The Power to Prove "Sameness"
        
            #### The Problem: The "Absence of Evidence" Fallacy
            A company completes a critical technology transfer to a new manufacturing site. They run a standard t-test to compare the average potency of batches from the new site versus the old site. The p-value is 0.35. The project manager triumphantly declares, "Success! The p-value is greater than 0.05, which proves the sites are statistically the same." They include this conclusion in their regulatory filing.
        
            #### The Impact: Regulatory Rejection and Unmanaged Risk
            This is one of the most common and dangerous statistical errors in the industry, and it has severe consequences:
            - **Guaranteed Regulatory Rejection:** An experienced regulatory reviewer will immediately reject this filing. The company has not *proven* sameness; they have simply **failed to prove a difference**. This is a critical distinction. The study may have been underpowered (too few samples, too much variability) to find a real, meaningful difference that actually exists.
            - **Accepting Unmanaged Risk:** The new site might, in fact, be producing a product that is consistently 2% lower in potency‚Äîa clinically significant difference. But because the study was poorly designed, the t-test lacked the power to detect it. The company is now unknowingly releasing a potentially inferior product based on a flawed statistical conclusion.
        
            #### The Solution: Flipping the Burden of Proof
            Equivalence Testing, using the Two One-Sided Tests (TOST) procedure, is the only statistically valid way to make a positive claim of similarity. It brilliantly flips the burden of proof. Instead of starting with the assumption that the sites are the same (and trying to disprove it), TOST starts with the assumption that the sites are **meaningfully different** (i.e., the difference is outside a pre-defined equivalence margin `Œî`). The burden is on the company to gather enough powerful evidence to **reject this assumption** and prove that any difference is, in fact, small enough to be considered practically irrelevant.
        
            #### The Consequences: A Defensible Claim and a Competitive Advantage
            - **Without This:** Any claim of "sameness" or "comparability" is statistically indefensible and represents a significant compliance risk.
            - **With This:** TOST provides **positive, objective, and statistically rigorous proof** of equivalence. Mastering this tool is not just a matter of compliance; it's a major competitive advantage. It is the required statistical engine for success in the generic and biosimilar markets, and it provides the unshakeable evidence needed to justify process changes, new equipment qualifications, and technology transfers to global regulators.
            """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Equivalence Terms
            - **Equivalence Testing:** A statistical procedure used to demonstrate that the difference between two groups or methods is smaller than a pre-specified, practically meaningless amount.
            - **TOST (Two One-Sided Tests):** The standard statistical method for performing an equivalence test. It involves testing two separate null hypotheses of "too different."
            - **Equivalence Margin (Œî):** A pre-defined range `[-Œî, +Œî]` within which two products or methods are considered to be practically equivalent. Setting this margin is a critical, risk-based decision.
            - **Confidence Interval Approach:** An equivalent method to TOST. If the 90% confidence interval for the difference between the two groups falls entirely within the equivalence margin, equivalence is demonstrated at the 5% significance level.
            """)
        with tabs[3]:
            st.error("""üî¥ **THE INCORRECT APPROACH: The Fallacy of the Non-Significant P-Value**
- A scientist runs a standard t-test and gets a p-value of 0.25. They exclaim, *"Great, p > 0.05, so the methods are the same!"*
- **This is wrong.** All they have shown is a *failure to find evidence of a difference*. **Absence of evidence is not evidence of absence.** Their study may have been underpowered (too much noise or too few samples).""")
            st.success("""üü¢ **THE GOLDEN RULE: Define 'Same Enough', Then Prove It**
The TOST procedure forces a more rigorous scientific approach.
1.  **First, Define the Margin:** Before collecting data, stakeholders must use scientific and clinical judgment to define the equivalence margin (`Œî`). This is the zone where a difference is considered practically meaningless.
2.  **Then, Prove You're Inside:** Conduct the experiment. The burden of proof is on you to show that your evidence (the 90% CI for the difference) is precise enough to fall entirely within that pre-defined margin.""")

        with tabs[4]:
            st.markdown("""
            #### Historical Context: The Rise of Generic Drugs
            **The Problem:** In the early 1980s, the pharmaceutical landscape was changing. The **1984 Hatch-Waxman Act** in the US created the modern pathway for generic drug approval. This created a new statistical challenge for regulators: how could a generic manufacturer *prove* that their drug was "the same as" the innovator's drug in terms of how it was absorbed by the body (bioequivalence)?

            **The 'Aha!' Moment:** A standard t-test was useless; failing to find a difference wasn't proof of no difference. The solution was championed by statisticians like **Donald J. Schuirmann** at the FDA. The **Two One-Sided Tests (TOST)** procedure, which had existed in the statistical literature, was identified as the perfect tool.
            
            **The Impact:** The TOST procedure became the statistical engine for bioequivalence studies worldwide. Instead of one null hypothesis of "no difference," it brilliantly frames the problem with two null hypotheses of "too different." To prove equivalence, you must reject both. This places the burden of proof squarely on the manufacturer to demonstrate similarity, a much higher and more appropriate standard for ensuring patient safety.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("TOST brilliantly flips the null hypothesis. Instead of one null of \"no difference\" (`H‚ÇÄ: Œº‚ÇÅ - Œº‚ÇÇ = 0`), you have two null hypotheses of \"too different\":")
            st.latex(r"H_{01}: \mu_B - \mu_A \leq -\Delta \quad (\text{The difference is too low})")
            st.latex(r"H_{02}: \mu_B - \mu_A \geq +\Delta \quad (\text{The difference is too high})")
            st.markdown("""
            You must run two separate one-sided t-tests to reject **both** of these null hypotheses. The overall p-value for the TOST procedure is the larger of the two individual p-values. If this final p-value is less than your alpha (e.g., 0.05), you have statistically demonstrated equivalence within the margin `[-Œî, +Œî]`.
            
            A mathematically equivalent shortcut is to calculate the **90% confidence interval** for the difference. If this entire interval falls within `[-Œî, +Œî]`, you can conclude equivalence at the 5% significance level.
            """)
            
        with tabs[5]:
            st.markdown("""
            TOST is the required statistical method for demonstrating similarity or equivalence in various regulated contexts.
            - **FDA Guidance on Bioequivalence Studies:** TOST is the standard method for proving that the rate and extent of absorption of a generic drug are not significantly different from the reference listed drug.
            - **USP General Chapter <1224> - Transfer of Analytical Procedures:** Suggests the use of equivalence testing to formally demonstrate that a receiving laboratory can obtain comparable results to the transferring laboratory.
            - **Biosimilars (BPCIA Act):** The principles of equivalence testing are central to the analytical and clinical studies required to demonstrate biosimilarity to a reference biologic product.
            """)

#=======================================================================================================================================================================================================
#============================================================================================SPECIAL SECTION COMPARING METHODS (ANOVA/T-TEST, WASSERSTEIN, TOST =====================================================================
#=========================================================================================================================================================================================================
#================================================================= 6. Wassrtein Distance =============================================================
def render_wasserstein_distance():
    """Renders the comprehensive, interactive module for Wasserstein Distance."""
    st.markdown("""
    #### Purpose & Application: The Process Fingerprint Comparator
    **Purpose:** To provide a single, holistic metric for comparing two entire process distributions. The **Wasserstein Distance** (or **Earth Mover's Distance**) calculates the "work" required to transform one distribution into another. It is the ultimate tool for answering the question: **"Are these two processes truly behaving the same?"**
    
    **Strategic Application:** This is a state-of-the-art method for tech transfer and comparability studies. While traditional tests only compare summary statistics (mean, variance), the Wasserstein distance compares the **entire process fingerprint**. It is highly sensitive to changes in mean, variance, **skewness**, and even **modality** (e.g., a process splitting into two sub-populations), making it far more robust than classic hypothesis tests.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Tech Transfer Lead comparing a new site (Site B) to a validated reference site (Site A).
    1. Use the **Process Scenario** radio buttons to simulate different types of changes at Site B.
    2. Observe the **Traditional Test p-values**. Notice how they **fail to detect** the Skew and Bimodal shifts.
    3. The **Wasserstein Distance KPI**, however, correctly flags all deviations, proving its superiority. The shaded area in the bottom plot is a visual representation of this distance.
    """)
    
    with st.sidebar:
        st.subheader("Wasserstein Demo Controls")
        scenario = st.radio(
            "Select Site B Process Scenario:",
            ["Identical", "Mean Shift", "Variance Increase", "Skewed Shift", "Bimodal Split"],
            captions=[
                "Perfect transfer, no change.",
                "Process is off-center.",
                "Process is less precise.",
                "Process has a heavy tail.",
                "Process has split in two."
            ],
            key="wasserstein_scenario",
            help="Simulate different, realistic failure modes for a tech transfer. Observe which ones traditional tests can and cannot detect."
        )
        n_samples = st.slider("Sample Size per Site (n)", 50, 1000, 200, 50, key="wasserstein_n")
        threshold = st.slider("Equivalence Threshold", 0.5, 5.0, 1.5, 0.1, key="wasserstein_thresh")

    # Generate data based on scenario
    np.random.seed(42)
    lsl, usl = 90, 110
    # Site A is always the same "golden standard" process
    df_a = pd.DataFrame({'value': np.random.normal(100, 3, n_samples)})
    
    if scenario == "Identical":
        df_b = pd.DataFrame({'value': np.random.normal(100, 3, n_samples)})
    elif scenario == "Mean Shift":
        df_b = pd.DataFrame({'value': np.random.normal(102.5, 3, n_samples)})
    elif scenario == "Variance Increase":
        df_b = pd.DataFrame({'value': np.random.normal(100, 5, n_samples)})
    elif scenario == "Skewed Shift":
        df_b = pd.DataFrame({'value': stats.skewnorm.rvs(a=5, loc=95, scale=4, size=n_samples)})
    elif scenario == "Bimodal Split":
        d1 = np.random.normal(97, 2, n_samples // 2)
        d2 = np.random.normal(103, 2, n_samples // 2)
        df_b = pd.DataFrame({'value': np.concatenate([d1, d2])})

    # --- THIS IS THE KEY CHANGE ---
    # The function is now called directly from the global scope.
    fig, emd, ttest_p, f_p, is_equivalent = plot_two_process_wasserstein(df_a, df_b, lsl, usl, threshold)
    # --- END OF KEY CHANGE ---
    
    st.header("Process Comparability Dashboard")
    col1, col2 = st.columns([0.65, 0.35])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        st.subheader("Analysis & Interpretation")
        if is_equivalent:
            st.success(f"### Verdict: ‚úÖ PASS - Processes are Equivalent")
        else:
            st.error(f"### Verdict: ‚ùå FAIL - Processes are NOT Equivalent")
        
        st.metric(label="Wasserstein Distance (EMD)", value=f"{emd:.3f}",
                  help="The 'cost' to transform Site A's distribution into Site B's. A value below the threshold indicates equivalence.")
        st.metric(label="t-test p-value (for Means)", value=f"{ttest_p:.3f}",
                  help="p < 0.05 indicates a significant difference in means.")
        st.metric(label="Levene's p-value (for Variances)", value=f"{f_p:.3f}",
                  help="p < 0.05 indicates a significant difference in variances.")

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown(f"""
        **The Core Insight for the '{scenario}' Scenario:**
        - **Identical, Mean Shift, Variance Increase:** All three tests work correctly.
        - **Skewed Shift:** Notice the `t-test` and `Levene's test` both have **high p-values**, incorrectly suggesting the processes are the same. Only the Wasserstein Distance is large enough to correctly flag this as a failure.
        - **Bimodal Split:** This is the most dangerous failure. The mean and variance might be nearly identical to Site A, so the `t-test` and `Levene's test` are completely blind and **incorrectly pass** the transfer. The **Wasserstein Distance is huge**, correctly identifying this critical failure of process control.
        
        **Strategic Conclusion:** Relying solely on tests of mean and variance creates a massive blind spot. The Wasserstein distance provides a far more robust and sensitive measure of true process comparability, making it a superior tool for risk-based tech transfer.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The High-Resolution "Process CT Scan"
    
        #### The Problem: The Blurry X-Ray of Traditional Statistics
        A company validates a critical tech transfer to a new site by showing that the mean of the new process is not significantly different from the original (t-test p > 0.05) and the variance is not significantly different (Levene's test p > 0.05). Management signs off on the transfer, believing the two processes are the same.
    
        #### The Impact: The Hidden Failure That Kills a Product Line
        Six months later, the new site experiences a series of catastrophic batch failures. The investigation reveals the devastating truth: a subtle equipment issue at the new site was causing the process distribution to become **bimodal** (splitting into two sub-populations). The mean and variance of this bimodal distribution were almost identical to the original site's normal distribution. The traditional statistical tests were like a **blurry X-ray**‚Äîthey could see the average density and overall size, but were completely blind to the critical underlying pathology. The company was flying blind for six months, producing at-risk material.
    
        #### The Solution: A High-Resolution "Process CT Scan"
        The Wasserstein Distance is a fundamentally more powerful tool because it doesn't just look at summary statistics; it compares the **entire, high-resolution fingerprint of the process distribution**. It's the difference between an X-ray and a full-body CT scan. It is sensitive to any change in the distribution:
        -   A shift in the **mean** (like a t-test).
        -   A change in the **variance** (like a Levene's test).
        -   A change in **skewness** (which traditional tests miss).
        -   A change to **bimodality** (which traditional tests are dangerously blind to).
        
        It quantifies the "work" required to transform one process's distribution into the other, providing a single, holistic, and incredibly sensitive metric of process difference.
    
        #### The Consequences: Seeing the Whole Picture and Preventing Disasters
        - **Without This:** Companies are making multi-million dollar decisions based on a low-resolution, incomplete picture of their processes. They are exposed to significant, unmanaged risk from complex failure modes that their statistical "safety net" cannot detect.
        - **With This:** The Wasserstein Distance provides a **complete, high-fidelity view of process comparability**. It is the ultimate safety net. By adopting this state-of-the-art metric, a company can de-risk its technology transfers, gain a much deeper understanding of its process dynamics, and detect subtle but critical deviations long before they escalate into catastrophic quality events.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Terms
        - **Wasserstein Distance / Earth Mover's Distance (EMD):** A measure of the distance between two probability distributions. It is the minimum "cost" of transforming one distribution into the other, where cost is defined as the amount of "dirt" moved times the distance it's moved.
        - **Probability Density Function (PDF):** A function that describes the relative likelihood for a random variable to take on a given value. The top plot shows the estimated PDFs (also called KDEs).
        - **Cumulative Distribution Function (CDF):** A function that describes the probability that a random variable will take on a value less than or equal to `x`. The bottom plot shows the CDFs. For 1D data, the Wasserstein distance is equal to the area between the two CDF curves.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Parameter Myopia" Fallacy**
An engineer validates a tech transfer by showing that the means are not significantly different (t-test p > 0.05) and the variances are not significantly different (F-test p > 0.05).
- **The Flaw:** They have only proven that two summary statistics are similar. They are completely blind to changes in the fundamental *shape* of the process distribution. A new process that produces a bimodal output (e.g., from two different filling heads) might have the same mean and variance as the original, but it is clearly not the same process.""")
        st.success("""üü¢ **THE GOLDEN RULE: Validate the Fingerprint, Not Just the Features**
A modern, robust approach to comparability goes beyond simple parameters.
1.  **Visualize First:** Always plot the overlaid distributions of the two processes, as shown in the top chart. Your eyes can often detect differences that simple tests will miss.
2.  **Use a Holistic Metric:** Use a metric like the Wasserstein distance that compares the entire "fingerprint" (the full distribution) of the processes.
3.  **Set a Practical Margin:** The equivalence threshold should be a risk-based decision, pre-defined in the validation plan. A common method is to set it based on a fraction of the specification range (e.g., 10% of USL - LSL).""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Cannonballs to Code
        **The Problem (1781):** The French mathematician **Gaspard Monge** posed a practical problem: given a pile of dirt (e.g., from an excavation) and a desired final shape (e.g., a fortification), what is the most efficient way to move the dirt? This "earth mover's problem" laid the mathematical groundwork for what we now call **optimal transport theory**.
        
        **The Long Winter:** For nearly two centuries, this remained a niche area of mathematics. The computations were too complex for practical application.
        
        **The 'Aha!' Moment (Computer Science):** In the 1990s, with the rise of computing power, computer scientists rediscovered these ideas for applications in image retrieval. They needed a way to compare two images that was robust to small shifts or rotations. They re-branded the concept as **Earth Mover's Distance (EMD)**.
        
        **The Modern Synthesis:** In the 2010s, statisticians and machine learning researchers connected EMD back to its mathematical roots and the **Wasserstein metric**. They developed highly efficient algorithms to compute it, making it a practical tool. Its ability to compare entire distributions in a way that is sensitive to shape has made it a state-of-the-art metric in fields ranging from generative AI (in GANs) to modern, robust process validation.
        """)
        
    with tabs[5]:
        st.markdown("""
        The use of advanced, distribution-based metrics is a state-of-the-art implementation of the principles of demonstrating process comparability and control.
        - **ICH Q5E - Comparability of Biotechnological/Biological Products:** This guideline requires a demonstration that manufacturing changes do not adversely impact the product. While it focuses on product quality attributes, using a holistic statistical tool like Wasserstein distance to compare the process parameter distributions provides powerful supporting evidence.
        - **FDA Process Validation Guidance:** For **Stage 3 (Continued Process Verification)**, this tool can be used to prove that a process remains in the same state of control after a change or over time. For tech transfers, it provides a much more robust proof of equivalence than traditional tests.
        """)
#=============================================================================================  COMPARISON OF COMPARE METHODS =================================================================================

def render_two_process_suite():
    """Renders the suite for comparing TWO processes or methods."""
    st.markdown("""
    #### Purpose & Application: The Head-to-Head Showdown
    **Purpose:** To provide a deep, multi-faceted comparison of **two processes or methods**. This suite is designed for classic V&V scenarios like tech transfer (Site A vs. Site B) or new method validation (New Method vs. Gold Standard).
    
    **Strategic Application:** This dashboard forces a clear-eyed view of comparability by answering three distinct questions: Are they different? Are they equivalent? Do they agree? The choice of method depends entirely on the question you are required to answer for your validation plan.
    """)

    with st.sidebar:
        st.subheader("Two-Process Scenario Gadgets")
        mean_shift = st.slider("Mean Shift (Bias)", -5.0, 5.0, 0.0, 0.25, help="Simulates a systematic bias or shift in the process average of Process B.")
        variance_change = st.slider("Variance Change Factor", 0.5, 2.0, 1.0, 0.05, help="Simulates a change in process precision in Process B. >1.0 is more variable.")
        correlation = st.slider("Correlation between Methods", 0.0, 0.99, 0.95, 0.05, help="For Bland-Altman, simulates how correlated paired measurements are. High correlation is expected.")
        random_error = st.slider("Random Measurement Error (SD)", 0.5, 5.0, 1.5, 0.25, help="The inherent 'noise' of the measurements for both processes.")
        
        st.subheader("Equivalence Criteria")
        tost_margin = st.slider("TOST Equivalence Margin (Œî)", 0.5, 5.0, 2.0, 0.1, help="The 'goalposts' for the TOST test for means.")
        wasserstein_threshold = st.slider("Wasserstein Equivalence Threshold", 0.5, 5.0, 1.5, 0.1, help="The allowance for the Wasserstein test for distributions.")

    # --- Data Generation & Analysis ---
    np.random.seed(42)
    n_samples = 150
    lsl, usl = 90, 110

    # Generate paired data for Bland-Altman
    true_values = np.random.normal(100, 5, n_samples)
    error_a = np.random.normal(0, random_error, n_samples)
    error_b_uncorr = np.random.normal(0, random_error, n_samples)
    error_b = correlation * error_a + np.sqrt(1 - correlation**2) * error_b_uncorr
    
    data_a_paired = true_values + error_a
    data_b_paired = true_values + mean_shift + error_b * variance_change
    paired_df = pd.DataFrame({'A': data_a_paired, 'B': data_b_paired, 'Average': (data_a_paired + data_b_paired) / 2, 'Difference': data_b_paired - data_a_paired})
    
    # Generate independent data for t-test, TOST, Wasserstein
    data_a_ind = np.random.normal(100, 3, n_samples)
    data_b_ind = np.random.normal(100 + mean_shift, 3 * variance_change, n_samples)

    # Perform all statistical tests
    ttest_p = stats.ttest_ind(data_a_ind, data_b_ind, equal_var=False).pvalue
    diff_mean = np.mean(data_b_ind) - np.mean(data_a_ind)
    std_err_diff = np.sqrt(np.var(data_a_ind, ddof=1)/n_samples + np.var(data_b_ind, ddof=1)/n_samples)
    df_welch = (std_err_diff**4) / (((np.var(data_a_ind, ddof=1)/n_samples)**2 / (n_samples-1)) + ((np.var(data_b_ind, ddof=1)/n_samples)**2 / (n_samples-1)))
    ci_margin = stats.t.ppf(0.95, df_welch) * std_err_diff
    tost_ci = (diff_mean - ci_margin, diff_mean + ci_margin)
    tost_is_equivalent = (tost_ci[0] >= -tost_margin) and (tost_ci[1] <= tost_margin)
    wasserstein_dist = stats.wasserstein_distance(data_a_ind, data_b_ind)
    wasserstein_is_equivalent = wasserstein_dist < wasserstein_threshold

    # --- Render Dashboard ---
    st.header("Two-Process Comparability Dashboard")
    col_plots, col_verdicts = st.columns([0.6, 0.4])
    with col_plots:
        st.subheader("Visual Evidence")
        fig_visuals = plot_two_process_dashboard(data_a_ind, data_b_ind, lsl, usl, wasserstein_dist, paired_df)
        st.plotly_chart(fig_visuals, use_container_width=True)
    with col_verdicts:
        st.subheader("Statistical Verdict Panel")
        with st.container(border=True):
            st.markdown("##### **Test 1:** Are the Means Different?")
            st.metric(label="t-test Result", value=f"p = {ttest_p:.3f}")
            if ttest_p < 0.05: st.error("‚ùå **Verdict:** The means are statistically different.")
            else: st.success("‚úÖ **Verdict:** No evidence of a difference in means.")
            st.caption("Tests H‚ÇÄ: Mean A = Mean B")
        with st.container(border=True):
            st.markdown("##### **Test 2:** Are the Means the Same?")
            st.metric(label="TOST Result", value="Equivalent" if tost_is_equivalent else "Not Equivalent")
            if tost_is_equivalent: st.success("‚úÖ **Verdict:** The means are statistically equivalent.")
            else: st.error("‚ùå **Verdict:** Cannot conclude equivalence.")
            st.caption(f"Tests if 90% CI [{tost_ci[0]:.2f}, {tost_ci[1]:.2f}] is inside ¬±{tost_margin}")
        with st.container(border=True):
            st.markdown("##### **Test 3:** Are the Fingerprints the Same?")
            st.metric(label="Wasserstein Result", value=f"Distance = {wasserstein_dist:.2f}", help=f"Threshold for equivalence is < {wasserstein_threshold}")
            if wasserstein_is_equivalent: st.success("‚úÖ **Verdict:** The distributions are statistically equivalent.")
            else: st.error("‚ùå **Verdict:** The distributions are significantly different.")
            st.caption("Compares entire process shape, not just the mean.")

    st.divider()
    st.subheader("Deeper Dive into Two-Process Comparison")
    
    tabs = st.tabs(["üí° Key Insights (The Courtroom Analogy)", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory, History & Math", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        ### Why "Failing to Prove Difference" is NOT "Proving Similarity"
        A user correctly asked: *"Shouldn't 'how different' something is be the opposite of 'how similar' it is?"* While true in conversation, it is dangerously false in statistics. This distinction is the most important concept in validation and tech transfer.
    
        #### The Courtroom Analogy: Innocent Until Proven Guilty
        Think of a standard hypothesis test (like a **t-test**) as a criminal trial.
        -   **The Null Hypothesis (H‚ÇÄ):** "The defendant is innocent." (The process means are the same: `Œº‚ÇÅ = Œº‚ÇÇ`).
        -   **The Goal of the Prosecutor (You):** To gather enough evidence (data) to reject H‚ÇÄ and prove guilt "beyond a reasonable doubt."
        -   **The p-value:** The strength of your evidence. A small p-value (p < 0.05) is like having overwhelming DNA evidence.
    
        Now, consider the two possible verdicts:
        1.  **Verdict: "Guilty" (p < 0.05):** You reject H‚ÇÄ. You have strong evidence to conclude the means are different. This is a definitive conclusion.
        2.  **Verdict: "Not Guilty" (p > 0.05):** You **fail to reject** H‚ÇÄ. The verdict is **NOT "Innocent."** It simply means the prosecutor did not present enough evidence. This could be because the defendant is truly innocent, OR because the experiment was **underpowered** (too much noise, too few samples). A t-test cannot tell the difference.
    
        > **This is why a non-significant p-value from a t-test is an inconclusive result and can NEVER be used to claim two processes are the same.**
    
        #### Equivalence Testing (TOST): Flipping the Burden of Proof
        **TOST** flips the courtroom on its head. It assumes the processes are **"Different Until Proven Similar."**
        -   **The Null Hypothesis (H‚ÇÄ):** "The processes are meaningfully different." (The difference is outside our acceptable margin `Œî`).
        -   **The Goal (You):** To gather enough strong evidence to reject this H‚ÇÄ and prove that any difference is, in fact, practically meaningless.
        
        > **This is why TOST is the correct, rigorous tool for validation.** It forces you to make a strong, positive claim of similarity, which is exactly what regulators and quality systems require.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The Right Question for the Right Risk
    
        #### The Problem: The One-Sided Story
        A team is validating a critical technology transfer. They perform a single statistical test (e.g., a t-test) and get a single answer (e.g., p=0.15). They are now in a state of "statistical limbo." They haven't proven the processes are different, but they certainly haven't proven they are the same. They have answered a single, simple question, but the business has a complex, multi-faceted risk that remains unquantified.
    
        #### The Impact: Incomplete Risk Assessment and Poor Decisions
        This one-dimensional analysis leads to poor decision-making because it fails to capture the full picture of the process change.
        - **Unseen Risks:** The team might conclude "no difference in the mean" while completely missing a critical increase in process variability or a change in the process shape (skewness).
        - **Flawed Justifications:** They may try to use the inconclusive t-test result to justify equivalence in a regulatory filing, which will be rejected.
        - **Inability to Quantify Agreement:** If the change involves a new analytical method, they have no data on the practical, sample-by-sample agreement, such as the worst-case difference a doctor might see between the two tests.
    
        #### The Solution: The "Interrogation Room" for Process Data
        The Comparability Suite is not just a collection of tests; it is a strategic **"interrogation room"** for your process data. It allows you to ask a series of precise, targeted questions to uncover the full story of the change.
        1.  **The t-test asks:** "Is there any evidence these processes are *different*?" (Useful for scientific discovery).
        2.  **The TOST asks:** "Is there strong evidence these processes are *the same*?" (Essential for regulatory proof of equivalence).
        3.  **The Wasserstein test asks:** "Are the entire process *fingerprints* the same?" (The ultimate check for any change in process behavior).
        4.  **The Bland-Altman plot asks:** "For any given sample, how much are these two methods likely to *disagree*?" (Critical for understanding clinical or practical interchangeability).
    
        #### The Consequences: A 360-Degree View of Risk and Performance
        - **Without This:** The understanding of a process change is a blurry, one-dimensional snapshot.
        - **With This:** The suite provides a **complete, 360-degree view of comparability**. It allows the business to confidently and defensibly answer every relevant question about a process change. It ensures that the right statistical tool is used to manage the right risk, leading to robust validation packages, successful regulatory submissions, and a deep, multi-faceted understanding of process performance.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Two-Process Comparison Terms
        - **t-test:** A statistical hypothesis test used to determine if there is a significant difference between the means of two groups. Its null hypothesis is that the means are equal.
        - **TOST (Two One-Sided Tests):** An equivalence test that uses two one-sided t-tests to formally prove that the difference between two means is within a pre-defined margin. Its null hypothesis is that the means are meaningfully different.
        - **Wasserstein Distance:** A non-parametric metric that quantifies the "distance" between two entire probability distributions, sensitive to changes in mean, variance, and shape.
        - **Bland-Altman Plot:** A graphical method for comparing two **paired** measurement methods. It plots the difference between the two methods against their average to visualize bias and limits of agreement. It is not suitable for comparing independent groups.
        - **Paired vs. Independent Data:** Paired data comes from measuring the same sample/subject twice (e.g., with Method A and Method B). Independent data comes from two separate, unrelated groups (e.g., Batch A vs. Batch B).
        """)

    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "p > 0.05" Fallacy**
An engineer runs a t-test on data from two manufacturing sites. The p-value is 0.40. They write in the validation report: "Because p > 0.05, we have shown the two sites are statistically equivalent."
- **The Flaw:** This is a critical statistical error. They have only shown an **absence of evidence for a difference**, which is not the same as **evidence of absence of a difference**. An auditor would immediately reject this conclusion as statistically invalid.""")
        st.success("""üü¢ **THE GOLDEN RULE: The Question Dictates the Tool**
A mature, data-driven culture uses a clear logic for choosing its statistical methods, and this choice is pre-specified in the validation plan.
1.  **If you need to prove *difference* (e.g., scientific discovery), use a t-test.**
2.  **If you need to prove *sameness* (e.g., validation/tech transfer), use an equivalence test (TOST, Wasserstein).**
3.  **If you need to measure *agreement* (e.g., method validation), use Bland-Altman analysis.**""")

    with tabs[4]:
        st.markdown("""
        #### Theory, History & Mathematical Context
        - **t-test (1908):** Invented by **William Sealy Gosset** ("Student") at the Guinness brewery to handle small samples. It asks, "How likely is it to see a difference this large between my samples if the true means were actually identical?"
        - **TOST (1980s):** Championed by **Donald Schuirmann** at the FDA to solve the bioequivalence problem for generic drugs. It flips the t-test's logic by asking two questions: "Is there strong evidence that the difference is *not* unacceptably low?" AND "Is there strong evidence that the difference is *not* unacceptably high?" You must prove both to conclude equivalence.
        - **Bland-Altman (1986):** A direct response by **Martin Bland and Douglas Altman** to the rampant misuse of correlation for method comparison. Their brilliantly simple plot of `Difference vs. Average` directly visualizes the key clinical and technical questions: What is the average bias, and what is the expected range of disagreement for a future sample?
        """)
        
    with tabs[5]:
        st.markdown("""
        Using the appropriate statistical method for comparability is a core regulatory expectation and a cornerstone of a robust Quality Management System.
        - **ICH Q5E - Comparability of Biotechnological/Biological Products:** This guideline is the primary driver for these studies. It requires a demonstration that manufacturing changes do not adversely impact product quality. The choice of statistical methods and acceptance criteria is a key component of the comparability protocol.
        - **FDA Process Validation Guidance:** For Stage 2 (PPQ) during a tech transfer, and especially for Stage 3 (CPV) when monitoring a process over time or after a change, statistical methods are required to demonstrate consistency.
        - **USP <1224> - Transfer of Analytical Procedures:** Explicitly mentions "Comparative Testing" as a transfer option, for which these statistical tools are the standard methods of analysis.
        - **21 CFR 820.250 (Statistical Techniques):** Explicitly requires the use of "valid statistical techniques." This suite is a guide to selecting such valid techniques.
        """)

def render_multi_process_suite():
    """Renders the suite for comparing THREE OR MORE processes or methods."""
    st.markdown("""
    #### Purpose & Application: The Fleet Analysis
    **Purpose:** To compare the performance of **three or more processes** simultaneously. This suite is designed for system-level V&V questions, such as qualifying multiple production lines, comparing different manufacturing sites, or evaluating multiple raw material suppliers.
    
    **Strategic Application:** This dashboard provides a complete workflow for multi-group comparison. The **ANOVA** acts as a global "fire alarm," telling you *if* a problem exists. If it does, the **Tukey's HSD** and **Q-Q Plots** act as the "forensic investigators," telling you exactly *which* processes are different and *how* they are different.
    """)
    st.info("""
    **Interactive Demo:** You are the Global Head of Manufacturing. Use the sidebar gadget to simulate a problem on **Line C**.
    - If Line C is just `Shifted`, both ANOVA and Anderson-Darling will likely fail.
    - If Line C is just `Noisy`, ANOVA may pass (since the mean is the same), but the more powerful Anderson-Darling test will correctly fail.
    """)
    st.divider()
    
    with st.sidebar:
        st.subheader("Multi-Process Scenario Gadget")
        multi_process_scenario = st.radio(
            "Select Multi-Process Scenario:",
            ["All Lines Equivalent", "Line C is Shifted", "Line C is Noisy"],
            help="Simulate different scenarios for the three production lines to see how ANOVA and A-D tests respond."
        )
    
    # Data Generation & Analysis for THREE Processes
    np.random.seed(42)
    n_samples = 150
    lsl, usl = 90, 110
    data_a_multi = np.random.normal(100, 3, n_samples)
    data_b_multi = np.random.normal(100, 3, n_samples)
    if multi_process_scenario == "All Lines Equivalent":
        data_c_multi = np.random.normal(100, 3, n_samples)
    elif multi_process_scenario == "Line C is Shifted":
        data_c_multi = np.random.normal(103, 3, n_samples)
    elif multi_process_scenario == "Line C is Noisy":
        data_c_multi = np.random.normal(100, 5, n_samples)

    df_all = pd.concat([
        pd.DataFrame({'value': data_a_multi, 'Line': 'A'}),
        pd.DataFrame({'value': data_b_multi, 'Line': 'B'}),
        pd.DataFrame({'value': data_c_multi, 'Line': 'C'})
    ], ignore_index=True)
    data_list = [df_all[df_all['Line'] == 'A']['value'], df_all[df_all['Line'] == 'B']['value'], df_all[df_all['Line'] == 'C']['value']]
    
    anova_result = f_oneway(*data_list)
    ad_result = stats.anderson_ksamp(data_list)
    ad_p_value = ad_result.pvalue
    tukey_results = pairwise_tukeyhsd(endog=df_all['value'], groups=df_all['Line'], alpha=0.05)

    # --- Render Dashboard ---
    st.header("Multi-Process Comparability Dashboard")
    col_fig, col_stats = st.columns([0.6, 0.4])
    with col_fig:
        st.subheader("Visual Evidence: Production Line Distributions")
        fig_multi = plot_multi_process_comparison(df_all, lsl, usl)
        st.plotly_chart(fig_multi, use_container_width=True)
    with col_stats:
        st.subheader("Statistical Verdict Panel")
        with st.container(border=True):
            st.markdown("##### **Test 1:** Is there *any* difference in means?")
            st.metric("ANOVA p-value", f"{anova_result.pvalue:.4f}")
            if anova_result.pvalue < 0.05: st.error("‚ùå **ANOVA Verdict:** At least one line has a different mean.")
            else: st.success("‚úÖ **ANOVA Verdict:** No evidence of a mean difference.")
        with st.container(border=True):
            st.markdown("##### **Test 2:** Are *any* distributions different?")
            st.metric("Anderson-Darling p-value", f"{ad_p_value:.4f}")
            if ad_p_value < 0.05: st.error("‚ùå **A-D Verdict:** At least one line has a different distribution.")
            else: st.success("‚úÖ **A-D Verdict:** No evidence of a distributional difference.")

    if anova_result.pvalue < 0.05:
        st.subheader("ANOVA Post-Hoc Analysis: Diagnosing the Difference")
        st.markdown("Since the ANOVA test was significant (p < 0.05), we must now investigate *which specific lines* are different. The **Tukey's HSD** test performs all pairwise comparisons, while the **Q-Q Plots** visualize how the distributions differ in shape.")
        
        tukey_df_simple = pd.DataFrame(data=tukey_results._results_table.data[1:], columns=tukey_results._results_table.data[0])
        tukey_df_simple = tukey_df_simple.sort_values(by='p-adj')
        tukey_p_adj_list = tukey_df_simple['p-adj'].tolist()
        tukey_pairs_list = [f"{g1}-{g2}" for g1, g2 in zip(tukey_df_simple['group1'], tukey_df_simple['group2'])]

        fig_posthoc = plot_comparability_dashboard(
            data_a=None, data_b=None, lsl=lsl, usl=usl, wasserstein_dist=None,
            is_multi_process_mode=True,
            tukey_p_adj=tukey_p_adj_list,
            tukey_group_pairs=tukey_pairs_list,
            qq_data_list=[d.tolist() for d in data_list], 
            line_names=df_all['Line'].unique().tolist()
        )
        st.plotly_chart(fig_posthoc, use_container_width=True)

    st.divider()
    st.subheader("Deeper Dive into Multi-Process Comparison")
    tabs = st.tabs(["üí° Method Selection Map", "‚úÖ The Business Case", "üìã Detailed Comparison Table", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory, History & Math", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        ### Method Selection Map: A Strategic Decision Framework
        Choosing your statistical weapon is the most critical decision in a comparability study. Use this guide to select and defend your approach based on the specific question you need to answer.
    
        | **Your Question** | **Recommended Tool** | **Why? (Pros)** | **What to Watch Out For (Cons)** |
        | :--- | :--- | :--- | :--- |
        | **"Is there *any* evidence of a difference in the average performance of my 3+ lines?"** | **ANOVA** | **The Universal Screener:** Fast, simple, and the industry standard for a first-pass check on means. It provides a single p-value to answer the global question of "any difference?" | **Doesn't tell you *which* lines differ.** It's a fire alarm, not a firefighter. It is strictly mean-centric, completely blind to changes in process variability or shape. |
        | **"Can I *prove* my new process mean is practically the same as the old one?"** | **TOST (Equivalence Testing)** | **The Regulatory Gold Standard:** The only method that correctly frames the hypothesis to prove similarity. Forces a crucial, upfront conversation about what "practically the same" means (the margin Œî). | **Mean-centric:** Can declare two processes equivalent even if their variances are wildly different. The choice of the margin Œî can be difficult to justify and is often a point of regulatory scrutiny. |
        | **"How well do my two measurement methods agree across their entire range?"** | **Bland-Altman / Deming** | **The Bias Detective:** The only method designed to quantify and diagnose the *type* of bias (constant vs. proportional). The Limits of Agreement are directly interpretable in the units of the measurement, making them clinically and technically relevant. | **Requires Paired Data:** You must have measured the exact same set of samples on both methods. It is completely inappropriate for comparing independent batches from two processes. |
        | **"Can I prove my new process behaves *identically* to the old one, considering its entire fingerprint?"** | **Wasserstein or Anderson-Darling** | **The Holistic Guardian:** The most powerful and robust approach. It compares the entire process "fingerprint" (shape, center, spread). Non-parametric, so it's immune to non-normal data. It is the only method here guaranteed to catch dangerous changes like bimodality. | **Less Familiar to Regulators:** May require more explanation in a submission. Selecting a defensible equivalence threshold for the Wasserstein distance can be more challenging than for the mean-based TOST. |
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: Managing a Global Manufacturing Network
    
        #### The Problem: The "Apples to Oranges" Comparison
        A company manufactures a critical product at three different global sites (A, B, and C). Each site monitors its own process using local control charts, and each site reports that its process is "capable." However, management has a nagging fear: is a "good" batch from Site A the same as a "good" batch from Site B? They have no statistically valid way to compare performance across the entire network to ensure a single, consistent standard of quality.
    
        #### The Impact: Hidden Risks and Inefficient Operations
        This inability to perform a network-level comparison creates significant strategic risks:
        - **Inconsistent Product Quality:** Site C might be running a process that is consistently biased high, but still within specification. This means patients in one region are receiving a systematically different product than patients in another, creating a potential clinical and regulatory risk.
        - **Inefficient "Best Practice" Sharing:** Site A may have a significantly less variable process than the others, but without a formal comparison, this excellence is never identified or shared. The opportunity to improve the entire network to the standard of the best site is lost.
        - **Flawed Supply Chain Decisions:** During a supply shortage, management needs to decide which site to ramp up. Without a clear, data-driven understanding of which site is truly the most capable and reliable, they are forced to make a critical supply chain decision based on incomplete information.
    
        #### The Solution: The "Fleet Analysis" for Manufacturing
        The Multi-Process Comparability Suite provides the tools for a formal **"fleet analysis"** of the entire manufacturing network. It allows a business to move from site-level monitoring to network-level strategic management.
        1.  **The Fire Alarm (ANOVA / Anderson-Darling):** These tests provide a single, global p-value to answer the first critical question: "Is there *any* statistically significant difference in performance across our entire network?"
        2.  **The Forensic Investigator (Tukey's HSD / Q-Q Plots):** If the alarm sounds, these post-hoc tests provide the immediate, detailed diagnosis. They pinpoint *exactly which* sites are different from each other and *how* their process distributions differ, guiding a targeted investigation.
    
        #### The Consequences: A Standardized, Optimized Global Network
        - **Without This:** A "global" manufacturing network is just a collection of disconnected local sites. The company has no way to enforce a single standard of quality or to identify and propagate best practices.
        - **With This:** The suite provides the **objective, data-driven foundation for true global process management**. It enables the business to **identify and correct** site-to-site performance gaps, **transfer best practices** from high-performing sites to others, and make **intelligent, risk-based supply chain decisions**. This ensures that the company's brand represents a single, consistent, and high standard of quality, regardless of where the product was made.
        """)

    with tabs[1]:
        st.markdown("""
        ### Detailed Comparison of Comparability Methods
        
        | Feature | **T-Test / ANOVA** | **TOST** | **Bland-Altman / Deming** | **Wasserstein / Anderson-Darling** |
        | :--- | :--- | :--- | :--- | :--- |
        | **Primary Goal** | Detect a *difference*. | Prove statistical *equivalence*. | Quantify *agreement* & diagnose bias. | Quantify *distributional difference*. |
        | **Key Output** | p-value | p-value or 90% CI vs. Margin | Limits of Agreement (LoA) | Distance Value or p-value |
        | **Null Hypothesis**| H‚ÇÄ: Means are equal | H‚ÇÄ: Means are *different* | (Graphical, no formal H‚ÇÄ) | H‚ÇÄ: Distributions are identical |
        | **What It Compares** | Only the **means**. | Only the **means**. | **Paired data points** from the same sample. | The **entire distribution** (shape, spread, center). |
        | **Assumptions** | Normality, Equal Variance | Normality | Differences are normal | None (Non-parametric) |
        | **Strengths** | Simple, fast, universally understood. | Rigorous proof of similarity. Regulatory standard for bioequivalence. | Excellent for method validation. Results are clinically interpretable. | Extremely robust. Sensitive to all types of change. Non-parametric. |
        | **Weaknesses** | Cannot prove equivalence. Blind to variance/shape changes. | Only compares means. Margin selection is critical. | Requires paired data. Not for comparing independent groups. | Can be less powerful for simple mean shifts. Threshold selection can be complex. |
        | **Best For...** | Quick, preliminary checks for differences in the average. | Formal proof of mean equivalence for regulatory submissions (e.g., bioequivalence). | Validating and comparing two measurement systems (e.g., lab instruments). | Robust tech transfer validation; comparing processes sensitive to changes in shape/variability. |
        """)

    with tabs[2]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "One-Tool-Fits-All" Fallacy**
An engineer uses a standard t-test for every comparison. They see a non-significant p-value (e.g., p=0.3) and triumphantly declare that their tech transfer was a success because the two sites are "statistically the same." They use a correlation coefficient (R¬≤) to claim two measurement methods agree.
- **The Flaw:** This is a chain of fundamental statistical errors. A non-significant p-value is an **absence of evidence**, not evidence of absence. The study may simply have been underpowered. Correlation does not imply agreement. This approach is not just wrong; it is a serious compliance risk that could hide a critical process failure.""")
        st.success("""üü¢ **THE GOLDEN RULE: The Question Dictates the Tool**
A mature, data-driven culture uses a clear logic for choosing its statistical methods, and this choice is pre-specified in the validation plan.
1.  **If proving *difference* matters, use a standard hypothesis test (t-test, ANOVA).** This is the language of scientific discovery. The default assumption is that things are the same, and you need strong evidence to reject that.
2.  **If proving *sameness* matters, use an equivalence test (TOST, Wasserstein, Anderson-Darling).** This is the language of engineering and compliance. The default assumption is that things are different, and you need strong evidence to prove they are practically the same.
3.  **If measuring *agreement* matters, use Bland-Altman analysis.** This is the language of metrology and diagnostics. The goal is not a binary pass/fail but to quantify the interchangeability of two measurement systems.
By pre-specifying the right tool for the right question in your validation plan, you demonstrate statistical rigor and a deep understanding of your validation objectives.""")

    with tabs[3]:
        st.markdown("""
        #### Theory, History & Mathematical Context
        This suite showcases a century of statistical evolution, driven by distinct industrial and scientific needs.
        - **ANOVA (1920s):** Invented by **Sir Ronald A. Fisher** for agricultural experiments at Rothamsted. It was a revolutionary way to efficiently test multiple "treatments" (e.g., fertilizers) at once. Its mathematical genius lies in **partitioning variance**: it breaks down the total variation in the data into a component *between* the groups and a component *within* the groups. The F-statistic is the ratio of these two variances. If the variation between groups is large relative to the variation within them, we conclude the means are different.
        
        - **TOST (1980s):** While the statistical theory was older, it was championed by the **FDA** and statisticians like **Donald Schuirmann** as the solution to the bioequivalence problem for generic drugs. It brilliantly flips the logic of hypothesis testing. Instead of one null hypothesis of "no difference" (`H‚ÇÄ: Œº‚ÇÅ - Œº‚ÇÇ = 0`), it tests two null hypotheses of "too different": `H‚ÇÄ‚ÇÅ: Œº‚ÇÅ - Œº‚ÇÇ ‚â§ -Œî` and `H‚ÇÄ‚ÇÇ: Œº‚ÇÅ - Œº‚ÇÇ ‚â• +Œî`. You must reject **both** to prove equivalence.

        - **Wasserstein Distance (1781, revived 1990s):** An old mathematical concept from optimal transport theory, it was made practical by computer scientists in the 1990s as "Earth Mover's Distance" and is now a state-of-the-art metric for comparing distributions in AI and statistics. For 1D data, its value is simply the **area between the two cumulative distribution functions (CDFs)**. This is why it captures differences in mean, variance, and shape simultaneously.

        - **Anderson-Darling Test (1952):** Developed by Theodore Anderson and Donald Darling as a powerful test for goodness-of-fit. Its k-sample version is a rigorous non-parametric test for distributional equality. Mathematically, it calculates a weighted squared distance between the empirical distribution functions, with the weights chosen to give more emphasis to the **tails of the distribution**. This makes it very sensitive to outliers and shape changes, which is often where process failures first become apparent.
        """)

    with tabs[4]:
        st.markdown("""
        Using the appropriate statistical method for comparability is a core regulatory expectation and a cornerstone of a robust Quality Management System.
        - **ICH Q5E - Comparability of Biotechnological/Biological Products:** This guideline is the primary driver. It requires a demonstration that manufacturing changes do not adversely impact product quality. The choice of statistical methods and acceptance criteria is a key component of the comparability protocol. Using advanced methods like Anderson-Darling provides stronger evidence of comparability than simple mean-based tests.
        - **FDA Process Validation Guidance:** For **Stage 2 (Process Qualification)** during a tech transfer, and especially for **Stage 3 (Continued Process Verification)** when monitoring a process over time or after a change, statistical methods are required to demonstrate consistency. This suite provides the tools to do so rigorously.
        - **ICH Q9 - Quality Risk Management:** The choice of statistical method is a risk-based decision. For a high-risk change, a more comprehensive test like Anderson-Darling would be expected. For a low-risk change, a simpler test of means like TOST might be justifiable. The rationale must be documented.
        - **21 CFR 820.250 (Statistical Techniques):** This regulation for medical devices explicitly requires the use of "valid statistical techniques" for verifying process capability and product characteristics. This dashboard is a guide to selecting and justifying such valid techniques. The documentation generated from this analysis would be a key part of the Design History File (DHF) or batch records.
        """)

#===============================================================  7. PROCESS STABILITY (SPC) ================================================
def render_spc_charts():
    """Renders the INTERACTIVE module for Statistical Process Control (SPC) charts."""
    st.markdown("""
    #### Purpose & Application: The Voice of the Process
    **Purpose:** To serve as an **EKG for your process**‚Äîa real-time heartbeat monitor that visualizes its stability. The goal is to distinguish between two fundamental types of variation:
    - **Common Cause Variation:** The natural, random "static" or "noise" inherent to a stable process. It's predictable.
    - **Special Cause Variation:** A signal that something has changed or gone wrong. It's unpredictable and requires investigation.
    
    **Strategic Application:** SPC is the bedrock of modern quality control. These charts provide an objective, data-driven answer to the critical question: "Is my process stable and behaving as expected?" They are used to prevent defects, reduce waste, and provide the evidence needed to justify (or reject) process changes.
    """)
    
    st.info("""
    **Interactive Demo:** Use the controls at the bottom of the sidebar to inject different types of "special cause" events into a simulated stable process. Observe how the I-MR, Xbar-R, and P-Charts each respond, helping you learn to recognize the visual signatures of common process problems.
    """)
    
    st.sidebar.subheader("SPC Scenario Controls")
    scenario = st.sidebar.radio(
        "Select a Process Scenario to Simulate:",
        ('Stable', 'Sudden Shift', 'Gradual Trend', 'Increased Variability'),
        captions=[
            "Process is behaving normally.",
            "e.g., A new raw material lot is introduced.",
            "e.g., An instrument is slowly drifting out of calibration.",
            "e.g., An operator becomes less consistent."
        ]
    )

    fig_imr, fig_xbar, fig_p = plot_spc_charts(scenario=scenario)
    
    st.subheader(f"Analysis & Interpretation: {scenario} Process")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.info("üí° Each chart type is a different 'lead' on your EKG, designed for a specific kind of data. Use the expanders below to see how to read each one.")
    
        with st.expander("Indivduals & Moving Range (I-MR) Chart", expanded=True):
            st.plotly_chart(fig_imr, use_container_width=True)
            st.markdown("- **Interpretation:** The I-chart tracks the process center, while the MR-chart tracks short-term variability. **Both** must be stable. An out-of-control MR chart is a leading indicator of future problems.")
    
        with st.expander("X-bar & Range (XÃÑ-R) Chart", expanded=True):
            st.plotly_chart(fig_xbar, use_container_width=True)
            st.markdown("- **Interpretation:** The X-bar chart tracks variation *between* subgroups and is extremely sensitive to small shifts. The R-chart tracks variation *within* subgroups, a measure of process consistency.")
        
        with st.expander("Proportion (P) Chart", expanded=True):
            st.plotly_chart(fig_p, use_container_width=True)
            st.markdown("- **Interpretation:** This chart tracks the proportion of defects. The control limits become tighter for larger batches, reflecting increased statistical certainty.")
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The Real-Time "Nervous System" for Manufacturing
    
        #### The Problem: The "Lagging Indicator" Trap
        A company's entire quality strategy is based on **final QC testing**. They produce an entire multi-million dollar batch of product over several days, then send a sample to the lab. They wait a week for the results, which show the batch failed its specification. Now they are in a reactive crisis mode.
    
        #### The Impact: The High Cost of "Driving with the Rear-View Mirror"
        This "inspect quality in at the end" approach is enormously wasteful and risky.
        - **Maximum Scrap Cost:** The company has already incurred 100% of the material and labor cost to produce an entire batch of worthless product. The financial loss is maximized.
        - **Ineffective Root Cause Analysis:** The failure could have been caused by a brief event that happened days ago. Trying to find the "needle in the haystack" root cause after the fact is a slow, difficult, and often inconclusive investigation.
        - **Supply Chain Disruption:** The unexpected loss of a batch can lead to stock-outs, forcing the company to pay huge expedited shipping fees and potentially failing to supply the market.
    
        #### The Solution: A Proactive, Real-Time "Nervous System"
        Statistical Process Control (SPC) charts are the **real-time nervous system** of a modern manufacturing process. Instead of waiting for the final result (a lagging indicator), SPC charts monitor the vital signs of the process (the Critical Process Parameters) as they are happening. The control chart is not designed to see if the product is in-spec; it is designed to detect **changes in the process's behavior** long before those changes result in out-of-spec product. It is a **leading indicator**.
    
        #### The Consequences: From "Detect and Scrap" to "Predict and Prevent"
        - **Without This:** The company is stuck in a costly "detect and scrap" cycle. They are constantly reacting to failures that have already happened.
        - **With This:** SPC transforms the quality paradigm to **"predict and prevent."** An out-of-control signal on a control chart is an **early warning** that the process has deviated from its normal, validated state. This allows operators to **stop, investigate, and correct** the problem in real-time. This prevents the creation of defective material, dramatically reduces scrap, enables effective root cause analysis, and ensures a stable, predictable, and profitable manufacturing operation.
        """)
        with tabs[1]:
            st.markdown("""
            ##### Glossary of SPC Terms
            - **SPC (Statistical Process Control):** A method of quality control which employs statistical methods to monitor and control a process.
            - **Control Chart:** A graph used to study how a process changes over time. Data are plotted in time order.
            - **Control Limits:** Horizontal lines on a control chart (typically at ¬±3œÉ) that represent the natural variation of a process. They are calculated from the process data itself.
            - **Common Cause Variation:** The natural, random variation inherent in a stable process. It is the "noise" of the system.
            - **Special Cause Variation:** Variation that is not inherent to the process and is caused by a specific, assignable event (e.g., a machine malfunction, a new operator). It is the "signal" that something has changed.
            - **In a State of Statistical Control:** A process from which all special causes of variation have been removed, leaving only common cause variation. Such a process is stable and predictable.
            """)
    with tabs[2]:
        st.error("""üî¥ **THE INCORRECT APPROACH: "Process Tampering"**
        This is the single most destructive mistake in SPC. The operator sees any random fluctuation within the control limits and reacts as if it's a real problem.
        - *"This point is a little higher than the last one, I'll tweak the temperature down a bit."*
        Reacting to "common cause" noise as if it were a "special cause" signal actually **adds more variation** to the process, making it worse. This is like trying to correct the path of a car for every tiny bump in the road‚Äîyou'll end up swerving all over the place.""")
        st.success("""üü¢ **THE GOLDEN RULE: Know When to Act (and When Not To)**
        The control chart's signal dictates one of two paths:
        1.  **Process is IN-CONTROL (only common cause variation):**
            - **Your Action:** Leave the process alone! To improve, you must work on changing the fundamental system (e.g., better equipment, new materials).
        2.  **Process is OUT-OF-CONTROL (a special cause is present):**
            - **Your Action:** Stop! Investigate immediately. Find the specific, assignable "special cause" for that signal and eliminate it.""")

    with tabs[3]:
        st.markdown("""
        #### Historical Context: The Birth of Modern Quality
        **The Problem:** In the early 1920s, manufacturing at Western Electric for the Bell Telephone system was a chaotic affair. The challenge was immense: how could you ensure consistency across millions of components when you couldn't tell the difference between normal, random variation and a real production problem? Engineers were lost in a "fog" of data, constantly "tampering" with the process based on noise, often making things worse.

        **The 'Aha!' Moment:** A brilliant physicist at Bell Labs, **Dr. Walter A. Shewhart**, had a revolutionary insight. In a famous 1924 internal memo, he was the first to formally articulate the critical distinction between what he called **"chance cause"** (common cause) and **"assignable cause"** (special cause) variation. He realized that as long as a process only exhibited chance cause variation, it was stable, predictable, and in a "state of statistical control."
        
        **The Impact:** The control chart was the simple, graphical tool he invented to detect the exact moment an assignable cause entered the system. This single idea was the birth of modern Statistical Process Control and laid the foundation for the entire 20th-century quality revolution, influencing giants like W. Edwards Deming and the rise of Japanese manufacturing excellence.
        """)
        st.markdown("#### Mathematical Basis")
        st.markdown("The control limits on a Shewhart chart are famously set at the process average plus or minus three standard deviations of the statistic being plotted.")
        st.latex(r"\text{Control Limits} = \mu \pm 3\sigma_{\text{statistic}}")
        st.markdown("""
        - **Why 3-Sigma?** Shewhart chose this value for sound economic and statistical reasons. For a normally distributed process, 99.73% of all data points will naturally fall within these limits. This means there's only a 0.27% chance of a point falling outside the limits purely by chance (a false alarm). This makes the chart robust; when you get a signal, you can be very confident it's real. It strikes an optimal balance between being sensitive to real problems and not causing "fire drills" for false alarms.
        - **Estimating Sigma:** In practice, the true `œÉ` is unknown. For an I-MR chart, it is estimated from the average moving range (`MR-bar`) using a statistical constant `d‚ÇÇ`:
        """)
        st.latex(r"\hat{\sigma} = \frac{\overline{MR}}{d_2}")
    with tabs[4]:
        st.markdown("""
        SPC is the primary tool for Stage 3 of the process validation lifecycle, known as Continued or Ongoing Process Verification (CPV/OPV).
        - **FDA Process Validation Guidance (Stage 3):** Explicitly states that "an ongoing program to collect and analyze product and process data... must be established." SPC charts are the standard method for this real-time monitoring.
        - **ICH Q7 - Good Manufacturing Practice for APIs:** Section 2.5 on Quality Risk Management discusses the importance of monitoring and reviewing process performance.
        - **21 CFR 211.110(a):** Requires the establishment of control procedures "to monitor the output and to validate the performance of those manufacturing processes that may be responsible for causing variability."
        """)
#===================================================================================== 8. PROCESS CAPABILITY (CpK  ============================================================================
def render_capability():
    """Renders the interactive module for Process Capability (Cpk)."""
    st.markdown("""
    #### Purpose & Application: Voice of the Process vs. Voice of the Customer
    **Purpose:** To quantitatively determine if a process, once proven to be in a state of statistical control, is **capable** of consistently producing output that meets pre-defined specification limits (the "Voice of the Customer").
    
    **Strategic Application:** This is the ultimate verdict on process performance, often the final gate in a process validation or technology transfer. It directly answers the critical business question: "Is our process good enough to reliably meet customer or regulatory requirements?" 
    - A high Cpk provides objective evidence that the process is robust and delivers high quality.
    - A low Cpk is a clear signal that the process requires fundamental improvement.
    """)
    
    st.info("""
    **Interactive Demo:** Use the **Process Scenario** radio buttons below to simulate common real-world process states. Notice how the **Capability Verdict** is only valid when the top control chart shows a stable process. The bottom plot shows how the process distribution (blue line) fits within the specification limits (red lines).
    """)

    scenario = st.radio(
        "Select Process Scenario:",
        ('Ideal (High Cpk)', 'Shifted (Low Cpk)', 'Variable (Low Cpk)', 'Out of Control (Shift)', 'Out of Control (Trend)', 'Out of Control (Bimodal)'),
        horizontal=True
    )
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        fig, cpk_val = plot_capability(scenario)
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ Acceptance Criteria", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.metric(label="üìà KPI: Process Capability (Cpk)",
                      value=f"{cpk_val:.2f}" if not np.isnan(cpk_val) else "INVALID",
                      help="Measures how well the process fits within the spec limits, accounting for centering. Higher is better.")
            
            st.markdown("""
            **The Mantra: Stability First, Capability Second.**
            - The control chart (top plot) is a prerequisite. The Cpk metric is **statistically invalid** if the process is unstable, as an unstable process has no single, predictable "voice" to measure.
            - Notice how the **'Out of Control'** scenarios all produce an invalid result. You must fix the stability problem *before* you can assess capability.
            
            **The Key Insight: Control ‚â† Capability.**
            - A process can be perfectly stable but still produce bad product. The **'Shifted'** and **'Variable'** scenarios are stable but have poor Cpk values for different reasons (poor accuracy vs. poor precision).
            
            **The Bimodal Case:**
            - The **'Bimodal'** scenario shows two distinct sub-processes running. This violates the normality assumption of Cpk and requires investigation to find and eliminate the source of the two populations.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: The Universal Scorecard for Process Excellence
        
            #### The Problem: The "Is It Good Enough?" Debate
            A manufacturing process is stable and in-control, but a significant portion of its output is borderline, hovering dangerously close to the specification limits. This leads to frequent, contentious debates between departments:
            - **Manufacturing:** "The process is running fine. The material is in-spec, so it's good to ship."
            - **Quality:** "We're seeing a lot of near-misses. This is too risky. We need to investigate."
            - **Business:** "How much risk are we really talking about? Can you give me a number?"
            There is no common, quantitative language to describe how "good" the process actually is.
        
            #### The Impact: Unmanaged Risk and the "Hidden Factory"
            This lack of a capability metric creates a "hidden factory"‚Äîa massive, invisible cost center dedicated to dealing with the consequences of a marginal process.
            - **Intensive Inspection and Testing:** The QC lab is forced to perform extensive testing because they can't trust the process to consistently produce good material.
            - **Chronic OOS Investigations:** The borderline material frequently tips over the specification limit, triggering costly and time-consuming Out-of-Specification (OOS) investigations.
            - **Low Yield and High Scrap:** A significant portion of the factory's capacity is wasted producing material that is either scrapped immediately or rejected later, directly impacting the bottom line.
            - **Inability to Improve:** Without a clear metric, the company can't set meaningful goals for process improvement or measure the ROI of new equipment.
        
            #### The Solution: A Universal Language for Performance
            The Process Capability Index (Cpk) is the solution. It is a single, powerful, and universally understood metric that distills complex process performance into a simple "scorecard." It provides a quantitative, data-driven answer to the question "Is it good enough?" by directly comparing the **"Voice of the Process"** (its natural variation) to the **"Voice of the Customer"** (the specification limits).
        
            #### The Consequences: A Data-Driven Culture of Excellence
            - **Without This:** The organization is trapped in a cycle of debate, rework, and waste, driven by subjective opinions about process performance.
            - **With This:** Cpk becomes the **common language of quality** across the entire organization.
                - **It sets clear, data-driven goals:** "Our goal for this process is to achieve and maintain a Cpk of 1.67."
                - **It justifies investment:** "Investing $500k in this new equipment is projected to increase our Cpk from 1.1 to 1.7, which will reduce our scrap rate by 90% and save us $2M per year."
                - **It drives a competitive advantage:** A company that systematically measures and improves the Cpk of its critical processes will have lower costs, higher yields, and a more reliable supply chain than its competitors. It is the fundamental KPI for a culture of continuous improvement and operational excellence.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: The Universal Scorecard for Process Excellence
        
            #### The Problem: The "Is It Good Enough?" Debate
            A manufacturing process is stable and in-control, but a significant portion of its output is borderline, hovering dangerously close to the specification limits. This leads to frequent, contentious debates between departments:
            - **Manufacturing:** "The process is running fine. The material is in-spec, so it's good to ship."
            - **Quality:** "We're seeing a lot of near-misses. This is too risky. We need to investigate."
            - **Business:** "How much risk are we really talking about? Can you give me a number?"
            There is no common, quantitative language to describe how "good" the process actually is.
        
            #### The Impact: Unmanaged Risk and the "Hidden Factory"
            This lack of a capability metric creates a "hidden factory"‚Äîa massive, invisible cost center dedicated to dealing with the consequences of a marginal process.
            - **Intensive Inspection and Testing:** The QC lab is forced to perform extensive testing because they can't trust the process to consistently produce good material.
            - **Chronic OOS Investigations:** The borderline material frequently tips over the specification limit, triggering costly and time-consuming Out-of-Specification (OOS) investigations.
            - **Low Yield and High Scrap:** A significant portion of the factory's capacity is wasted producing material that is either scrapped immediately or rejected later, directly impacting the bottom line.
            - **Inability to Improve:** Without a clear metric, the company can't set meaningful goals for process improvement or measure the ROI of new equipment.
        
            #### The Solution: A Universal Language for Performance
            The Process Capability Index (Cpk) is the solution. It is a single, powerful, and universally understood metric that distills complex process performance into a simple "scorecard." It provides a quantitative, data-driven answer to the question "Is it good enough?" by directly comparing the **"Voice of the Process"** (its natural variation) to the **"Voice of the Customer"** (the specification limits).
        
            #### The Consequences: A Data-Driven Culture of Excellence
            - **Without This:** The organization is trapped in a cycle of debate, rework, and waste, driven by subjective opinions about process performance.
            - **With This:** Cpk becomes the **common language of quality** across the entire organization.
                - **It sets clear, data-driven goals:** "Our goal for this process is to achieve and maintain a Cpk of 1.67."
                - **It justifies investment:** "Investing $500k in this new equipment is projected to increase our Cpk from 1.1 to 1.7, which will reduce our scrap rate by 90% and save us $2M per year."
                - **It drives a competitive advantage:** A company that systematically measures and improves the Cpk of its critical processes will have lower costs, higher yields, and a more reliable supply chain than its competitors. It is the fundamental KPI for a culture of continuous improvement and operational excellence.
            """)
        with tabs[1]:
            st.markdown("""
            ##### Glossary of Capability Terms
            - **Process Capability:** A measure of the ability of a process to produce output within specification limits.
            - **Specification Limits (LSL/USL):** The limits that define the acceptable range for a product's characteristic. They are determined by customer requirements or engineering design (the "Voice of the Customer").
            - **Control Limits:** The limits on a control chart that represent the natural variation of the process (the "Voice of the Process"). **They are completely unrelated to specification limits.**
            - **Cpk (Process Capability Index):** A statistical measure of process capability that accounts for how well the process is centered within the specification limits. It measures the distance from the process mean to the *nearest* specification limit.
            - **Cp (Process Potential Index):** A measure of process capability that does not account for centering. It only measures if the process is "narrow" enough to fit within the specifications.
            """)
        with tabs[2]:
            st.markdown("These are industry-standard benchmarks. For pharmaceuticals, a high Cpk in validation provides strong assurance of lifecycle performance.")
            st.markdown("- `Cpk < 1.00`: Process is **not capable**.")
            st.markdown("- `1.00 ‚â§ Cpk < 1.33`: Process is **marginally capable**.")
            st.markdown("- `Cpk ‚â• 1.33`: Process is considered **capable** (a '4-sigma' quality level).")
            st.markdown("- `Cpk ‚â• 1.67`: Process is considered **highly capable** (approaching 'Six Sigma').")

        with tabs[3]:
            st.markdown("""
            #### Historical Context: The Six Sigma Revolution
            **The Problem:** In the 1980s, the American electronics manufacturer Motorola was facing a quality crisis. Despite using traditional quality control methods, defect rates were too high to compete globally. They needed a new, more ambitious way to think about quality.

            **The 'Aha!' Moment:** An engineer named **Bill Smith**, with the backing of CEO Bob Galvin, championed a radical new idea. Instead of just being "in-spec," a process should be so good that the specification limits are at least **six standard deviations** away from the process mean. This "Six Sigma" concept was a quantum leap in quality thinking. The **Cpk index** became the simple, powerful metric to measure progress toward this goal. A Cpk of 2.0 was the statistical equivalent of achieving Six Sigma capability.

            **The Impact:** The Six Sigma initiative was a spectacular success, reportedly saving Motorola billions of dollars. It was later adopted and popularized by companies like General Electric under Jack Welch, becoming one of the most influential business management strategies of the late 20th century. Cpk moved from a niche statistical tool to a globally recognized KPI for process excellence.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("Capability analysis is a direct comparison between the **\"Voice of the Customer\"** (the allowable spread, USL - LSL) and the **\"Voice of the Process\"** (the actual, natural spread, conventionally 6œÉ).")
            st.markdown("- **Cp (Potential Capability):** Measures if the process is narrow enough, ignoring centering.")
            st.latex(r"C_p = \frac{\text{USL} - \text{LSL}}{6\hat{\sigma}}")
            st.markdown("- **Cpk (Actual Capability):** The more important metric, as it accounts for process centering. It measures the distance from the process mean to the *nearest* specification limit, in units of 3-sigma.")
            st.latex(r"C_{pk} = \min \left( \frac{\text{USL} - \bar{x}}{3\hat{\sigma}}, \frac{\bar{x} - \text{LSL}}{3\hat{\sigma}} \right)")
        with tabs[4]:
            st.markdown("""
            Process capability analysis (Cpk) is the key metric used during Stage 2 of the validation lifecycle, Process Performance Qualification (PPQ).
            - **FDA Process Validation Guidance (Stage 2):** The goal of PPQ is to demonstrate that the process, operating under normal conditions, is capable of consistently producing conforming product. A high Cpk is the statistical evidence that this goal has been met.
            - **Global Harmonization Task Force (GHTF):** For medical devices, guidance on process validation similarly requires demonstrating that the process output consistently meets predetermined requirements.
            """)

#===============================================================  5. STATISTICAL EQUIVALENCE FOR PROCESS TRANSFER ================================================
def render_statistical_equivalence_for_process_transfer():
    """Renders the comprehensive, interactive module for Process Transfer Equivalence."""
    
    # --- CASE STUDY INTEGRATION BLOCK ---
    case_study_params = {}
    is_case_study_mode = False
    active_case_key = st.session_state.get('case_study', {}).get('active_case')
    
    if active_case_key:
        current_step_index = st.session_state['case_study']['current_step']
        current_step = CASE_STUDIES[active_case_key]['steps'][current_step_index]
        
        if current_step['target_tool'] == "Statistical Equivalence for Process Transfer":
            is_case_study_mode = True
            case_study_params = current_step.get('params', {})
            with st.expander("üìñ **Case Study Context**", expanded=True):
                st.info(f"**{CASE_STUDIES[active_case_key]['title']} | Step {current_step_index + 1}: {current_step['title']}**")
                st.markdown(current_step['explanation'])
    # --- END OF INTEGRATION BLOCK ---

    st.markdown("""
    #### Purpose & Application: Statistical Proof of Transfer Success
    **Purpose:** To provide **objective, statistical proof** that a manufacturing process transferred to a new site, scale, or equipment set performs equivalently to the original, validated process.
        
    **Strategic Application:** This is a high-level validation activity that goes beyond simply showing the new site is "in control." It formally proves that the new process is **statistically indistinguishable** from the original, providing powerful evidence for regulatory filings and ensuring consistent product quality across a global network. It is the final exam of a technology transfer.
    """)
        
    if not is_case_study_mode:
        st.info("""
        **Interactive Demo:** You are the Head of Tech Transfer. Use the sidebar controls to simulate the performance of the new manufacturing site (Site B).
        - The dashboard tells a 3-part story: the **raw process comparison** (top), the **statistical evidence** about the difference (middle), and the **final verdict** (bottom).
        - **The Goal:** Achieve a "PASS" verdict by ensuring the entire evidence distribution in Plot 2 falls within the red equivalence margins.
        """)
        
    with st.sidebar:
        st.subheader("Process Equivalence Controls")
        # Use case study params to set widget values, and disable if in case study mode
        cpk_a_slider = st.slider("Original Site A Performance (Cpk)", 1.33, 2.5, case_study_params.get("cpk_site_a", 1.67), 0.01, help="The historical, validated process capability of the sending site. This is your benchmark.", disabled=is_case_study_mode)
        mean_shift_slider = st.slider("Mean Shift at Site B", -2.0, 2.0, case_study_params.get("mean_shift", 0.5), 0.1, help="Simulates a systematic bias or shift in the process average at the new site. A key risk in tech transfer.", disabled=is_case_study_mode)
        var_change_slider = st.slider("Variability Change Factor at Site B", 0.8, 1.5, case_study_params.get("var_change_factor", 1.1), 0.05, help="Simulates a change in process precision. >1.0 means the new site is more variable (worse); <1.0 means it is less variable (better).", disabled=is_case_study_mode)
        n_samples_slider = st.slider("Samples per Site (n)", 30, 200, case_study_params.get("n_samples", 50), 10, help="The number of samples taken during the PPQ runs at each site. More samples increase statistical power.", disabled=is_case_study_mode)
        margin_slider = st.slider("Equivalence Margin for Cpk (¬±)", 0.1, 0.5, case_study_params.get("margin", 0.2), 0.05, help="The 'goalposts'. How much can the new site's Cpk differ from the original and still be considered equivalent? This is a risk-based decision.", disabled=is_case_study_mode)
    
    fig, is_equivalent, diff_cpk, cpk_a_sample, cpk_b_sample, ci_lower, ci_upper = plot_process_equivalence(
        cpk_site_a=cpk_a_slider, mean_shift=mean_shift_slider,
        var_change_factor=var_change_slider, n_samples=n_samples_slider,
        margin=margin_slider
    )
        
    st.header("Results Dashboard")
    col1, col2 = st.columns([0.65, 0.35])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        st.subheader("Analysis & Interpretation")
        if is_equivalent:
            st.success("### Verdict: ‚úÖ PASS - Processes are Equivalent")
        else:
            st.error("### Verdict: ‚ùå FAIL - Processes are NOT Equivalent")
        
        c1, c2 = st.columns(2)
        c1.metric("Site A Sample Cpk", f"{cpk_a_sample:.2f}")
        c2.metric("Site B Sample Cpk", f"{cpk_b_sample:.2f}", delta=f"{(diff_cpk):.2f} vs Site A")
        
        st.metric("90% CI for Cpk Difference", f"[{ci_lower:.2f}, {ci_upper:.2f}]", help="The range of plausible true differences between the sites' Cpk values, based on the sample data.")
        st.metric("Equivalence Margin", f"¬± {margin_slider}", help="The pre-defined goalposts for success.")
        
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
    with tabs[0]:
        st.markdown("""
        **The 3-Plot Story: From Data to Decision**
        1.  **Plot 1 (Process Comparison):** This shows what you see in the raw data from the validation runs. The smooth curves represent the "Voice of the Process" for each site relative to the specification limits (the "Voice of the Customer").
        2.  **Plot 2 (Statistical Evidence):** This is the crucial bridge. It shows the result of a bootstrap simulation‚Äîa histogram of all the likely "true" differences in Cpk between the sites. The shaded area is the 90% confidence interval, representing our statistical evidence.
        3.  **Plot 3 (The Verdict):** This is a simple summary of Plot 2. The colored bar is the same 90% confidence interval. **The test passes only if this entire bar is inside the equivalence zone defined by the red dashed lines.**
        
        **Core Insight:** A tech transfer doesn't just need to produce good product (high Cpk); it needs to produce product that is **statistically consistent** with the original site. This analysis provides the formal proof.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: De-Risking Multi-Million Dollar Changes
    
        #### The Problem: The "Looks Good Enough" Fallacy
        A company executes a major manufacturing change. They perform validation runs and the new process meets the minimum quality requirement (e.g., Cpk > 1.33). Management declares the project a success.
    
        #### The Impact: The Slow Creep of Process Degradation
        This "good enough" mindset is a major strategic blunder. The new process, while technically "capable," might be significantly less robust than the original. Its Cpk might have dropped from a world-class 2.0 to a marginal 1.4. This means the process now has a much smaller buffer against normal operational variations, leading to a future of chronic, low-level deviations and higher scrap rates.
    
        #### The Solution: A Formal Proof of "Sameness"
        Statistical Equivalence analysis is the tool that moves beyond "good enough" to provide **positive, objective proof that the new process is statistically indistinguishable from the old one**. It forces the business to answer the right question: "Did we successfully replicate our gold-standard process, or did we unknowingly degrade it?"
    
        #### The Consequences: A Consistent Global Standard of Excellence
        Statistical Equivalence becomes the **gatekeeper for all major process changes**. It provides the unshakeable, data-driven evidence that a tech transfer, scale-up, or supplier change has been successful without compromising the process's validated state of control.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Transfer Terms
        - **Technology Transfer:** The process of transferring skills, knowledge, technologies, and methods of manufacturing among organizations.
        - **PPQ (Process Performance Qualification):** Stage 2 of the FDA Process Validation lifecycle, to demonstrate reproducible commercial manufacturing.
        - **Cpk (Process Capability Index):** A key performance indicator for a manufacturing process.
        - **Equivalence Testing:** A statistical procedure used to demonstrate that the difference in performance between two processes is smaller than a pre-specified, practically meaningless amount.
        - **Bootstrap Simulation:** A computer-intensive statistical method that uses resampling of the original data to estimate the sampling distribution and confidence intervals of a statistic.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Cpk is Cpk" Fallacy**
        A manager reviews the Site B PPQ data, sees a Cpk of 1.40 (which is > 1.33), and declares the transfer a success, even though the original site's Cpk was 1.80.
        - **The Flaw:** This significant drop in performance is ignored, introducing a new, hidden level of risk. They have proven capability, but not comparability.""")
        st.success("""üü¢ **THE GOLDEN RULE: Pre-Define Equivalence, Then Prove It**
        A robust tech transfer plan treats equivalence as a formal acceptance criterion.
        1.  **Define the Margin:** Before the transfer, stakeholders must agree on the equivalence margin for a key performance metric (like Cpk).
        2.  **Prove You're Inside:** Conduct the PPQ runs and perform the equivalence test. The burden of proof is on the receiving site to demonstrate that their process performance is statistically indistinguishable from the sending site, within the pre-defined margin.""")
        
    with tabs[4]:
        st.markdown("""
        #### Historical Context: A Modern Synthesis
        This tool represents a modern synthesis of two powerful statistical ideas from the 1980s: **Process Capability (Cpk)** from the Six Sigma movement and **Equivalence Testing (TOST)** from the FDA's generic drug approvals. By applying the rigorous logic of equivalence testing to a key performance indicator like Cpk, we create a powerful, modern tool for validating process transfers, scale-up, and other post-approval changes.
            """)
                
    with tabs[5]:
        st.markdown("""
        This analysis is a best-practice implementation for several key regulatory activities that require demonstrating comparability.
        - **FDA Process Validation Guidance:** This tool is ideal for **Stage 2 (Process Qualification)** when transferring a process.
        - **ICH Q5E - Comparability of Biotechnological/Biological Products:** This statistical approach provides a quantitative framework for demonstrating comparability after a manufacturing process change.
        - **Technology Transfer (ICH Q10):** A robust tech transfer protocol should have pre-defined acceptance criteria, and proving statistical equivalence of process capability is a state-of-the-art criterion.
        """)






def render_process_equivalence():
    """Renders the comprehensive, interactive module for Process Transfer Equivalence."""
    st.markdown("""
    #### Purpose & Application: Statistical Proof of Transfer Success
    **Purpose:** To provide **objective, statistical proof** that a manufacturing process transferred to a new site, scale, or equipment set performs equivalently to the original, validated process.
        
    **Strategic Application:** This is a high-level validation activity that goes beyond simply showing the new site is "in control." It formally proves that the new process is **statistically indistinguishable** from the original, providing powerful evidence for regulatory filings and ensuring consistent product quality across a global network. It is the final exam of a technology transfer.
    """)
        
    st.info("""
    **Interactive Demo:** You are the Head of Tech Transfer. Use the sidebar controls to simulate the performance of the new manufacturing site (Site B).
    - The dashboard tells a 3-part story: the **raw process comparison** (top), the **statistical evidence** about the difference (middle), and the **final verdict** (bottom).
    - **The Goal:** Achieve a "PASS" verdict by ensuring the entire evidence distribution in Plot 2 falls within the red equivalence margins.
    """)
        
    with st.sidebar:
        st.subheader("Process Equivalence Controls")
        st.markdown("**Baseline Process**")
        cpk_a_slider = st.slider("Original Site A Performance (Cpk)", 1.33, 2.5, 1.67, 0.01, help="The historical, validated process capability of the sending site. This is your benchmark.")
        st.markdown("**New Process Simulation**")
        mean_shift_slider = st.slider("Mean Shift at Site B", -2.0, 2.0, 0.5, 0.1, help="Simulates a systematic bias or shift in the process average at the new site. A key risk in tech transfer.")
        var_change_slider = st.slider("Variability Change Factor at Site B", 0.8, 1.5, 1.1, 0.05, help="Simulates a change in process precision. >1.0 means the new site is more variable (worse); <1.0 means it is less variable (better).")
        st.markdown("**Statistical Criteria**")
        n_samples_slider = st.slider("Samples per Site (n)", 30, 200, 50, 10, help="The number of samples taken during the PPQ runs at each site. More samples increase statistical power.")
        margin_slider = st.slider("Equivalence Margin for Cpk (¬±)", 0.1, 0.5, 0.2, 0.05, help="The 'goalposts'. How much can the new site's Cpk differ from the original and still be considered equivalent? This is a risk-based decision.")
    
    fig, is_equivalent, diff_cpk, cpk_a_sample, cpk_b_sample, ci_lower, ci_upper = plot_process_equivalence(
        cpk_site_a=cpk_a_slider, mean_shift=mean_shift_slider,
        var_change_factor=var_change_slider, n_samples=n_samples_slider,
        margin=margin_slider
    )
        
    st.header("Results Dashboard")
    col1, col2 = st.columns([0.65, 0.35])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        st.subheader("Analysis & Interpretation")
        if is_equivalent:
            st.success("### Verdict: ‚úÖ PASS - Processes are Equivalent")
        else:
            st.error("### Verdict: ‚ùå FAIL - Processes are NOT Equivalent")
        
        c1, c2 = st.columns(2)
        c1.metric("Site A Sample Cpk", f"{cpk_a_sample:.2f}")
        c2.metric("Site B Sample Cpk", f"{cpk_b_sample:.2f}", delta=f"{(diff_cpk):.2f} vs Site A")
        
        st.metric("90% CI for Cpk Difference", f"[{ci_lower:.2f}, {ci_upper:.2f}]", help="The range of plausible true differences between the sites' Cpk values, based on the sample data.")
        st.metric("Equivalence Margin", f"¬± {margin_slider}", help="The pre-defined goalposts for success.")
        
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
    with tabs[0]:
        st.markdown("""
        **The 3-Plot Story: From Data to Decision**
        1.  **Plot 1 (Process Comparison):** This shows what you see in the raw data from the validation runs. The smooth curves represent the "Voice of the Process" for each site relative to the specification limits (the "Voice of the Customer").
        2.  **Plot 2 (Statistical Evidence):** This is the crucial bridge. It shows the result of a bootstrap simulation‚Äîa histogram of all the likely "true" differences in Cpk between the sites. The shaded area is the 90% confidence interval, representing our statistical evidence.
        3.  **Plot 3 (The Verdict):** This is a simple summary of Plot 2. The colored bar is the same 90% confidence interval. **The test passes only if this entire bar is inside the equivalence zone defined by the red dashed lines.**
        
        **Core Insight:** A tech transfer doesn't just need to produce good product (high Cpk); it needs to produce product that is **statistically consistent** with the original site. This analysis provides the formal proof. Notice how a small `Mean Shift` or an increase in `Variability` at Site B can quickly lead to a failed equivalence test, even if Site B's Cpk is still above 1.33.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: De-Risking Multi-Million Dollar Changes
    
        #### The Problem: The "Looks Good Enough" Fallacy
        A company is executing a major, high-stakes manufacturing change, such as:
        - Transferring production of a blockbuster drug to a new, larger facility.
        - Scaling up a process from a 200L pilot reactor to a 2000L commercial reactor.
        - Qualifying a new, cheaper raw material supplier.
        
        They perform validation runs and the new process meets the minimum quality requirement (e.g., Cpk > 1.33). Management declares the project a success.
    
        #### The Impact: The Slow Creep of Process Degradation
        This "good enough" mindset is a major strategic blunder that introduces hidden, long-term risk.
        - **Reduced Robustness:** The new process, while technically "capable," might be significantly less robust than the original. Its Cpk might have dropped from a world-class 2.0 to a marginal 1.4. This means the process now has a much smaller buffer against normal operational variations, leading to a future of chronic, low-level deviations and higher scrap rates.
        - **Incomparable Data:** The company can no longer pool data from the old and new processes for annual product reviews or global trend analysis, because they are no longer statistically the same population.
        - **Regulatory Scrutiny:** Regulators expect a formal demonstration of **comparability**, not just capability. Simply meeting a minimum threshold is not sufficient proof that the change has not adversely impacted the process. This can lead to lengthy review cycles and requests for more data.
    
        #### The Solution: A Formal Proof of "Sameness"
        Statistical Equivalence analysis is the tool that moves beyond "good enough" to provide **positive, objective proof that the new process is statistically indistinguishable from the old one**. It is a formal "due diligence" on the performance of the process itself. By applying the rigorous logic of equivalence testing to a key performance indicator like Cpk, it forces the business to answer the right question: "Did we successfully replicate our gold-standard process, or did we unknowingly degrade it?"
    
        #### The Consequences: A Consistent Global Standard of Excellence
        - **Without This:** Every process change introduces the risk of silent, gradual degradation of the manufacturing network's performance. The company's "best" process slowly erodes over time.
        - **With This:** Statistical Equivalence becomes the **gatekeeper for all major process changes**. It provides the unshakeable, data-driven evidence that a tech transfer, scale-up, or supplier change has been successful without compromising the process's validated state of control. This ensures a consistent, high standard of performance across the global network, satisfies regulatory expectations for comparability, and protects the long-term profitability and robustness of the company's most valuable assets.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Transfer Terms
        - **Technology Transfer:** The process of transferring skills, knowledge, technologies, and methods of manufacturing among organizations to ensure that scientific and technological developments are accessible to a wider range of users.
        - **Process Performance Qualification (PPQ):** Stage 2 of the FDA Process Validation lifecycle, where the process design is evaluated to determine if it is capable of reproducible commercial manufacturing.
        - **Cpk (Process Capability Index):** A key performance indicator for a manufacturing process. A high Cpk (>1.33) indicates a capable process.
        - **Equivalence Testing:** A statistical procedure used to demonstrate that the difference in performance between two processes (e.g., the original and transferred site) is smaller than a pre-specified, practically meaningless amount.
        - **Bootstrap Simulation:** A computer-intensive statistical method that uses resampling of the original data to estimate the sampling distribution and confidence intervals of a statistic (like the difference in Cpk).
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Cpk is Cpk" Fallacy**
        A manager reviews the Site B PPQ data, sees a Cpk of 1.40 (which is > 1.33), and declares the transfer a success, even though the original site's Cpk was 1.80.
        - **The Flaw:** This significant drop in performance is ignored, introducing a new, hidden level of risk into the manufacturing network. The process is now less robust and more likely to fail in the future. They have proven capability, but not comparability.""")
        st.success("""üü¢ **THE GOLDEN RULE: Pre-Define Equivalence, Then Prove It**
        A robust tech transfer plan treats equivalence as a formal acceptance criterion.
        1.  **Define the Margin:** Before the transfer, stakeholders must agree on the equivalence margin for a key performance metric (like Cpk). This is a risk-based decision: how much of a performance drop are we willing to accept?
        2.  **Prove You're Inside:** Conduct the PPQ runs and perform the equivalence test. The burden of proof is on the receiving site to demonstrate that their process performance is statistically indistinguishable from the sending site, within the pre-defined margin.""")
        
    with tabs[4]:
        st.markdown("""
        #### Historical Context: A Modern Synthesis
        This tool represents a modern synthesis of two powerful statistical ideas that both came to prominence in the 1980s but in different industries:
        1.  **Process Capability (Cpk):** Popularized by the **Six Sigma** movement at Motorola, Cpk became the universal language for quantifying how well a process fits within its specification limits. It answered the question, "Is our process good enough?"
        2.  **Equivalence Testing (TOST):** Championed by the **FDA** for generic drug approvals, equivalence testing provided the rigorous framework for proving two things were "the same" within a practical margin. It answered the question, "Is Drug B the same as Drug A?"
                
        **The Impact:** In modern tech transfer and lifecycle management, these two ideas are fused. By applying the rigorous logic of equivalence testing to a key performance indicator like Cpk, we create a powerful, modern tool for validating process transfers, scale-up, and other post-approval changes. The use of computer-intensive **bootstrapping** to calculate the confidence interval for a complex metric like Cpk is a distinctly 21st-century statistical technique that makes this analysis possible.
            """)
                
    with tabs[5]:
        st.markdown("""
        This analysis is a best-practice implementation for several key regulatory activities that require demonstrating comparability.
        - **FDA Process Validation Guidance:** This tool is ideal for **Stage 2 (Process Qualification)** when transferring a process. It provides objective evidence that the receiving site has successfully reproduced the performance of the sending site.
        - **ICH Q5E - Comparability of Biotechnological/Biological Products:** While this guideline focuses on product quality attributes, its core principle is demonstrating comparability after a manufacturing process change. This statistical approach provides a quantitative framework for that demonstration.
        - **Technology Transfer (ICH Q10):** A robust tech transfer protocol should have pre-defined acceptance criteria. Proving statistical equivalence of process capability is a state-of-the-art criterion.
        - **SUPAC (Scale-Up and Post-Approval Changes):** When making a change to a validated process, this analysis can be used to prove that the change has not adversely impacted process performance.
        """)
# SNIPPET: Replace your entire render_ode_line_sync function with this final, high-performance version.

def render_ode_line_sync():
    """Renders the comprehensive module for Production Line Synchronization using ODEs."""
    st.markdown("""
    #### Purpose & Application: The Science of Flow
    **Purpose:** To model the dynamic behavior of a multi-stage production line using **Ordinary Differential Equations (ODEs)**. This approach treats the level of Work-in-Progress (WIP) in the buffers between stations as a variable that changes over time, governed by the production rates of the upstream and downstream steps.
    
    **Strategic Application:** This is a powerful simulation tool for process engineers and operations managers to **identify and manage bottlenecks**. It allows for "what-if" analysis to predict the impact of process changes (e.g., improving one machine's speed) on the entire system's stability and throughput.
    """)
    st.info("""
    **Interactive Demo:** You are the Process Engineer designing a new production line.
    1.  Use the **"Production Rates"** sliders in the sidebar to set the speed (units/hour) of each process step.
    2.  The dashboard will update in **real-time**, showing how inventory in the buffers evolves. Your goal is a **synchronized line** where buffer levels remain low and stable.
    """)

    with st.sidebar:
        st.subheader("Production Rates (units/hr)")
        # --- HELP TEXT ADDED TO EACH SLIDER ---
        r0 = st.slider("Step 1: Granulation", 50, 150, 100, help="Sets the production rate of the first process step. This is the rate at which inventory feeds into the first buffer.")
        r1 = st.slider("Step 2: Drying", 50, 150, 90, help="Sets the rate of the second step. This rate drains the first buffer and feeds the second. A low value here will create a bottleneck.")
        r2 = st.slider("Step 3: Compression", 50, 150, 110, help="Sets the rate of the third step. This rate drains the second buffer and feeds the final packaging step.")
        r3 = st.slider("Step 4: Packaging", 50, 150, 100, help="Sets the rate of the final step, representing the line's final output capacity. If this is the slowest step, inventory will accumulate in the second buffer.")
        # --- END OF ADDITIONS ---
        
    rates = [r0, r1, r2, r3]

    st.header("Production Line Dynamics Dashboard")

    fig, max_wip1, max_wip2 = plot_line_sync_ode(rates)
    bottleneck_rate = min(rates)

    col1, col2, col3 = st.columns(3)
    col1.metric("System Throughput", f"{bottleneck_rate} units/hr", help="The overall output of the line is always limited by its slowest step (the bottleneck).")
    col2.metric("Max WIP in Buffer 1", f"{max_wip1:.0f} units", help="The maximum inventory accumulation between Step 1 and 2.")
    col3.metric("Max WIP in Buffer 2", f"{max_wip2:.0f} units", help="The maximum inventory accumulation between Step 2 and 3.")

    st.plotly_chart(fig, use_container_width=True)

    st.divider()
    st.subheader("Deeper Dive into Line Synchronization")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Create a Bottleneck:** In the sidebar, set the rate for **Step 2: Drying** to be much lower than the others (e.g., 70 units/hr). Observe the chart:
            - **Buffer 1 (Blue Line):** The inventory level grows linearly and uncontrollably. This is because the upstream process (Step 1) is feeding it faster than the bottleneck (Step 2) can drain it. This is a recipe for disaster in a real factory.
            - **Buffer 2 (Red Line):** Remains empty. The downstream process (Step 3) is "starved" for material because it is waiting on the slow bottleneck step.
        2.  **Synchronize the Line:** Now, adjust all four sliders to be at or near the same rate (e.g., 100 units/hr). Observe the chart:
            - Both buffer levels remain at or near zero. The line is "balanced" or "synchronized." Material flows smoothly from one step to the next without accumulating.

        **The Strategic Insight:** A production line is like a chain‚Äîit is only as strong as its weakest link. Investing millions to speed up a non-bottleneck step is a complete waste of money; it will not increase the overall system throughput and will likely make the buffer problems worse. All improvement efforts must be focused on the **constraint** or **bottleneck** of the system.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The High Cost of Unbalanced Flow

        #### The Problem: The "Local Optimization" Trap
        Each department in a manufacturing plant is incentivized to maximize the efficiency of its own area. The granulation department invests in a new, high-speed machine. The compression department does the same. Both departments report excellent "local" efficiency metrics. However, the overall factory output has not increased, and a new, massive pile of Work-in-Progress (WIP) inventory has mysteriously appeared on the factory floor.

        #### The Impact: "Hurry Up and Wait"
        This focus on local optimization, without understanding the system as a whole, is a major source of hidden costs and inefficiency.
        - **Massive WIP Inventory:** The faster upstream process (Granulation) buries the slower downstream process (Drying) in a mountain of WIP. This ties up millions of dollars in capital, consumes valuable floor space, and increases the risk of material spoilage or obsolescence.
        - **No Increase in Throughput:** Despite the expensive new machines, the total number of finished goods coming out of the factory per day has not changed. The overall rate of the system is still dictated by its slowest step‚Äîthe bottleneck. The capital investment has had zero impact on revenue.
        - **Increased Lead Times:** The time it takes for a single unit to travel through the entire process has actually increased, because it now spends days sitting in the new WIP pile. This reduces the company's agility and its ability to respond to customer orders.

        #### The Solution: A "Digital Twin" to See the System
        Modeling the production line with ODEs creates a simple but powerful **"digital twin"** of the factory floor. It allows managers and engineers to see the system as an interconnected whole, not just a collection of separate parts. This simulation tool allows the team to:
        1.  **Identify the Bottleneck:** The model instantly shows which process step is the true constraint on the entire system's performance.
        2.  **Perform "What-If" Analysis:** Before spending a single dollar on new equipment, the team can simulate the impact: "What happens to the buffer levels and overall throughput if we increase the speed of the drying step by 10%?"
        3.  **Design for Flow:** The tool can be used to design a new production line with perfectly balanced and synchronized rates, ensuring smooth flow and minimal WIP from the start.

        #### The Consequences: Maximized Throughput and Optimized Capital
        - **Without This:** Capital investment is a high-risk gamble. Improvement efforts are misdirected at non-bottlenecks, resulting in wasted money and increased chaos.
        - **With This:** The ODE model provides the **strategic insight needed to optimize the entire value stream**. It ensures that all capital investment and process improvement efforts are focused exclusively on the bottleneck, where they will have the maximum impact. This leads to **increased throughput, reduced lead times, and minimized inventory**, directly improving the company's profitability and cash flow.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **ODE (Ordinary Differential Equation):** A mathematical equation that describes the rate of change of a system. In this model, we describe the rate of change of the buffer level (`d(WIP)/dt`) as a function of inflow and outflow rates.
        - **Value Stream Mapping (VSM):** A Lean tool for analyzing the flow of a process. This ODE model is a dynamic, mathematical version of a VSM.
        - **WIP (Work-in-Progress):** Inventory that is currently being processed or is waiting between steps in a production line.
        - **Bottleneck (or Constraint):** The single step in a process that has the lowest capacity and therefore limits the throughput of the entire system.
        - **Throughput:** The rate at which the system generates finished goods (units per unit of time). The throughput of the entire system is equal to the rate of the bottleneck.
        - **Synchronization (or Line Balancing):** The state where the production rates of all steps in a process are matched, resulting in smooth flow and minimal WIP accumulation.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Local Efficiency" Trap**
A manager's bonus is tied to the utilization of their machine. To maximize this metric, they run their machine as fast as possible, all the time, even if the downstream process isn't ready for the material.
- **The Flaw:** This manager is successfully hitting their personal target, but they are harming the overall system. They are creating a massive pile of WIP inventory that will cause problems for the downstream departments, increasing the total cost and lead time for the entire company.""")
        st.success("""üü¢ **THE GOLDEN RULE: Subordinate Everything to the Constraint**
A successful operation is managed as a single, integrated system, not a collection of independent silos. This is the core of the **Theory of Constraints (TOC)**.
1.  **Identify the Constraint:** Use data (like this model) to find the single slowest step in your process.
2.  **Exploit the Constraint:** Ensure the bottleneck is never starved for work and is always running at 100% of its capacity on high-quality material.
3.  **Subordinate Everything Else:** Every other step in the process must be synchronized to the speed of the bottleneck. It is okay for non-bottleneck machines to be idle; in fact, it is essential. Running them any faster than the bottleneck only creates waste.
""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Water Clocks to Goldratt's "The Goal"
        The use of differential equations to model dynamic systems is one of the oldest and most powerful ideas in science, dating back to the work of **Isaac Newton** in the 17th century. They are the mathematical language of physics, engineering, and finance.
        
        However, the application of this thinking to factory floor management was brilliantly popularized not by a mathematician, but by an Israeli business consultant named **Eliyahu M. Goldratt**.
        
        **The Problem:** In the 1980s, manufacturing was dominated by complex, opaque Material Requirements Planning (MRP) systems that often created more problems than they solved. Managers were focused on "local optima"‚Äîmaking their own department as efficient as possible‚Äîwithout understanding the impact on the whole system.
        
        **The 'Aha!' Moment:** In his 1984 blockbuster business novel, ***The Goal***, Goldratt introduced the **Theory of Constraints (TOC)** through a compelling story of a struggling factory manager. The core insight was revolutionary in its simplicity: **every complex system has at least one constraint (bottleneck), and the performance of the entire system is dictated by that single constraint.** Focusing improvement efforts anywhere else is a waste of time and money.
        
        **The Impact:** *The Goal* became one of the best-selling business books of all time. It provided managers with a powerful, intuitive mental model for understanding and optimizing their operations. The ODE model in this module is the formal, mathematical expression of the very same "flow dynamics" that Goldratt's characters discover through trial and error. It is a tool for applying the Theory of Constraints in a rigorous, quantitative, and predictive way.
        """)
        
    with tabs[5]:
        st.markdown("""
        While ODE modeling is a general engineering tool, its application in a GxP environment directly supports several key regulatory principles.
        - **ICH Q10 - Pharmaceutical Quality System:** Section 3.2.1 requires a system to "provide for control and identify areas for continual improvement." This ODE model is a powerful tool for identifying the primary area for improvement (the bottleneck) and for designing a more effective control strategy for the entire line.
        - **FDA Process Validation Guidance (Stage 1 - Process Design):** The guidance emphasizes the need for deep "process understanding." A dynamic model like this provides a much deeper understanding of process flow and interdependencies than a simple process map. It can be used to justify the design and capacity of buffer stages in a continuous manufacturing line.
        - **Process Analytical Technology (PAT):** For continuous manufacturing processes, a dynamic model of the line is essential. It can be used to predict the impact of disturbances and to design feed-forward and feedback control loops that maintain a state of control across the entire train.
        - **GAMP 5:** If this ODE model is used to make GxP decisions (e.g., setting production targets, justifying equipment purchases), the model and the software it runs on would need to be formally validated as a Computerized System.
        """)

# SNIPPET: Replace the entire render_lean_manufacturing function with this updated version.

def render_lean_manufacturing():
    """Renders the comprehensive module for Lean Manufacturing & Value Stream Mapping."""
    st.markdown("""
    #### Purpose & Application: The War on Waste
    **Purpose:** To introduce the principles of **Lean Manufacturing**, a management philosophy focused on the relentless elimination of "waste" to maximize value for the customer. This module focuses on its cornerstone tool:
    - **Value Stream Mapping (VSM):** A visual method for analyzing the flow of materials and information required to bring a product from start to finish. It is a powerful tool for making waste visible.
    
    **Strategic Application:** While statistical tools like SPC and Cpk focus on reducing *variation*, Lean focuses on improving *velocity*. It is a complementary discipline that is essential for optimizing the efficiency, speed, and cost-effectiveness of a validated process.
    """)
    st.info("""
    **Interactive Demo:** You are the Operations Manager.
    1.  Use the **"Current State"** sliders in the sidebar to input the cycle times and customer demand for a typical manufacturing process.
    2.  The **Value Stream Map** will instantly update to visualize the flow and highlight the "waste" (red wait times).
    3.  The **KPI Dashboard** will calculate the overall performance, including critical metrics like WIP and Capacity.
    """)

    with st.sidebar:
        st.subheader("Current State VSM Inputs")
        st.markdown("**Process Timing (hours)**")
        pt1 = st.slider("Process Time: Receipt", 1, 8, 2, help="The actual 'hands-on' or machine run time for Material Receipt. This is considered Value-Added Time.")
        wt1 = st.slider("Wait Time: Quarantine", 24, 168, 72, help="The idle time material spends in quarantine awaiting QC release. This is a primary form of waste (Muda) in a Lean system.")
        pt2 = st.slider("Process Time: Dispensing", 1, 8, 4, help="The actual 'hands-on' or machine run time for Dispensing. This is considered Value-Added Time.")
        wt2 = st.slider("Wait Time: Staging", 4, 24, 8, help="The idle time material spends staged before the next process step. This is a primary form of waste (Muda).")
        pt3 = st.slider("Process Time: Granulation", 8, 24, 12, help="The actual 'hands-on' or machine run time for Granulation. This is considered Value-Added Time.")
        wt3 = st.slider("Wait Time: Drying", 12, 48, 24, help="The idle time the product spends in the dryer or awaiting the next step. This is a primary form of waste (Muda).")
        pt4 = st.slider("Process Time: Compression", 4, 12, 8, help="The actual 'hands-on' or machine run time for tableting/compression. This is considered Value-Added Time.")
        wt4 = st.slider("Wait Time: Awaiting QC", 24, 96, 48, help="The idle time the batch spends waiting for QC lab results before it can move to packaging. This is a primary form of waste (Muda).")
        pt5 = st.slider("Process Time: QC Testing", 2, 8, 4, help="The actual 'hands-on' or analytical run time for QC Testing. This is considered Value-Added Time.")
        wt5 = st.slider("Wait Time: Awaiting Packaging", 8, 48, 24, help="The idle time the batch spends waiting for packaging line availability after QC release. This is a primary form of waste (Muda).")
        pt6 = st.slider("Process Time: Packaging", 4, 16, 8, help="The actual 'hands-on' or machine run time for final packaging. This is considered Value-Added Time.")

        process_times = [pt1, pt2, pt3, pt4, pt5, pt6]
        wait_times = [wt1, wt2, wt3, wt4, wt5]
        
        st.divider()
        st.markdown("**Operational Context**")
        customer_demand = st.number_input("Customer Demand (units/day)", value=2, min_value=0, help="The number of finished units the customer requires per day. Set to 0 to see how Takt Time becomes N/A.")
        operating_time = st.number_input("Operating Time (hours/day)", value=24, min_value=1, help="The total available production time per day.")

    # KPI Calculations
    wip_inventory = sum(wait_times)
    takt_time_hours = operating_time / customer_demand if customer_demand > 0 else float('inf')
    bottleneck_time = max(process_times)
    daily_capacity = operating_time / bottleneck_time if bottleneck_time > 0 else float('inf')

    st.header("Value Stream Mapping Dashboard")
    fig, total_cycle_time, pce = plot_value_stream_map(process_times, wait_times)
    
    # --- THIS IS THE FIX ---
    # The KPI dashboard now handles the edge case where demand is zero, preventing the crash.
    kpi_cols = st.columns(5)
    kpi_cols[0].metric("Total Lead Time", f"{total_cycle_time:.1f} Hrs", help="The total time from the start of the process to the end.")
    kpi_cols[1].metric("Process Cycle Efficiency", f"{pce:.1f}%", help="The percentage of the total lead time that is actual, value-added work. World class is >25%.")
    kpi_cols[2].metric("WIP Inventory", f"{wip_inventory:.1f} Hrs", help="Total hours of non-value-added waiting time, representing trapped capital and risk.")
    
    # Safely handle Takt Time display
    takt_display = f"{takt_time_hours:.1f} Hrs/Unit" if takt_time_hours != float('inf') else "N/A"
    kpi_cols[3].metric("Takt Time", takt_display, help="The 'heartbeat' of the customer. To meet demand, you must produce one unit every X hours.")
    
    # Safely handle Capacity display and delta
    capacity_display = f"{daily_capacity:.1f} Units/Day" if daily_capacity != float('inf') else "Infinite"
    delta_val = daily_capacity - customer_demand if daily_capacity != float('inf') else float('inf')
    delta_display = f"{delta_val:.1f}" if delta_val != float('inf') else "N/A"
    kpi_cols[4].metric("Line Capacity", capacity_display, delta=f"{delta_display} vs. Demand",
                      help="The maximum number of units this line can produce per day, limited by its slowest step (the bottleneck).")
    # --- END OF FIX ---

    st.plotly_chart(fig, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive into Lean Principles")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Core Lean Concepts & Tools", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Map the Current State:** The first step is always to create an honest VSM of the process *as it is today*. The diagram shows the flow of both materials (bottom) and information (top).
        2.  **Identify Waste (Muda):** The VSM instantly makes waste visible. The **red inventory triangles** represent the Waste of Waiting and Inventory. The **timeline at the bottom** provides the most shocking visual: the vast majority of the time is red (non-value-added).
        3.  **Find Opportunities for Kaizen:** The "Kaizen Bursts" (üí•) are visual cues that highlight the largest opportunities for improvement, signaling the ideal location for a focused **Kaizen event**.
        4.  **Diagnose the Need for Kanban:** The dashed arrow shows a "Kanban" signal. Implementing a "pull" system like this is a common solution to the massive WIP piles (red triangles) caused by a "push" system.
        5.  **Calculate KPIs:** The dashboard provides the key metrics. **PCE** shows the inefficiency. **WIP** quantifies the trapped inventory. **Capacity** identifies your bottleneck and compares it to your required **Takt Time**.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The High Cost of "Business as Usual"

        #### The Problem: The "Hidden Factory"
        A company's manufacturing process is validated and produces high-quality product. However, the total lead time from raw material to finished good is 30 days, while the actual "touch time" or value-added work is only 36 hours. The other 28.5 days are consumed by a "hidden factory"‚Äîa complex, invisible web of non-value-added activities like waiting, rework, excessive movement, and unnecessary inspections.

        #### The Impact: Strangled Cash Flow and Reduced Capacity
        This hidden factory is a massive drain on profitability and competitiveness, even if the product quality is perfect.
        - **Trapped Working Capital:** A 30-day lead time means the company has millions of dollars of capital tied up in raw materials and work-in-progress (WIP) inventory that is just sitting idle. This directly impacts cash flow and the company's financial health.
        - **Reduced Capacity and Throughput:** The factory's true capacity is not limited by its machines, but by the inefficient process flow. The long cycle times mean the facility can produce far fewer batches per year than it is theoretically capable of.
        - **Inability to Respond to Demand:** With a long lead time, the company cannot quickly respond to a sudden increase in customer demand, leading to lost sales and ceding market share to more agile competitors.

        #### The Solution: Making Waste Visible
        Lean Manufacturing provides the philosophy, and Value Stream Mapping provides the tool, to systematically **make the hidden factory visible**. It is a powerful diagnostic that shifts the focus from "how do we improve the work?" to "how do we eliminate the waiting between the work?" It guides targeted **Kaizen events** to attack the largest sources of waste.

        #### The Consequences: Increased Velocity, Capacity, and Cash Flow
        - **Without This:** The hidden factory continues to operate unchecked, silently consuming resources and capping the company's growth potential.
        - **With This:** A Lean initiative, guided by VSM, is a direct assault on the hidden factory. By systematically eliminating the "red bars" (waste), the company can:
            - **Slash Lead Times:** Often cutting total cycle time by 50% or more.
            - **Increase Capacity:** Dramatically increase the number of batches that can be produced with the *same* equipment and staff.
            - **Free Up Cash Flow:** Reduce the amount of capital tied up in inventory.
        Lean is not just about "tidying up"; it is a powerful business strategy for unlocking the hidden capacity and financial potential within your existing operations.
        """)

    with tabs[2]:
        st.markdown("""
        While the 8 Wastes (**Muda**) are the most visible problems, Lean masters from the Toyota Production System focus on two deeper, underlying causes: **Mura** and **Muri**. The goal is to attack the root cause, not just the symptom.
        
        ### The 3 Ms of Lean
        - **Mura (Êñë): Unevenness:** The inconsistency or irregularity in a process. This is often the deepest root cause of waste. Uneven schedules create periods of intense **overburden (Muri)** followed by periods of **waiting (Muda)**.
        - **Muri (ÁÑ°ÁêÜ): Overburden:** Pushing people or equipment beyond their natural limits. Muri leads to safety issues, burnout, and breakdowns, which create Muda.
        - **Muda (ÁÑ°ÈßÑ): Waste:** Any activity that consumes resources but creates no value. This is the visible **symptom** of Mura and Muri.
        
        ### The 8 Wastes of Muda (DOWNTIME)
        - **D - Defects:** Producing scrap or requiring rework.
        - **O - Overproduction:** Producing more than is needed by the next step.
        - **W - Waiting:** Idle time. **This is often the largest component of lead time.**
        - **N - Non-Utilized Talent:** Failing to engage the creativity of the workforce.
        - **T - Transportation:** Unnecessary movement of products and materials.
        - **I - Inventory:** Excess raw materials, WIP, or finished goods.
        - **M - Motion:** Unnecessary movement of people or equipment.
        - **E - Extra-Processing:** Work that adds no value for the customer.
        ---
        ### Key Lean Tools & Philosophies
        - **5S:** A workplace organization method that is a foundational pillar of Lean. It creates a clean, organized, and efficient environment, which is a prerequisite for more advanced Lean concepts.
            - **1. Seiri (Sort):** Remove all unnecessary items from the workplace.
            - **2. Seiton (Set in Order):** Arrange all necessary items so they can be easily selected for use. "A place for everything, and everything in its place."
            - **3. Seiso (Shine):** Clean the workspace and equipment on a regular basis.
            - **4. Seiketsu (Standardize):** Standardize the best practices in the work area.
            - **5. Shitsuke (Sustain):** Make it a habit and maintain the established procedures.
        - **Kaizen (ÊîπÂñÑ): Continuous Improvement:** A philosophy of making small, incremental, continuous improvements involving all employees. A **Kaizen Event** or "Kaizen Blitz" is a focused, short-term (1-5 day) workshop where a cross-functional team attacks a specific problem identified on the VSM.
        - **Kanban (ÁúãÊùø): Visual Signal:** A scheduling system for Just-in-Time (JIT) manufacturing that implements a **"pull" system**. A downstream process signals with a visual card when it needs more material, preventing overproduction.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Tool Time" Fallacy**
A manager learns about a Lean tool like "5S" (Sort, Set in Order, Shine, Standardize, Sustain) and mandates that every team must implement it, without a clear connection to a business problem.
- **The Flaw:** This is "tool time" or "Lean for Lean's sake." The team might create a beautifully organized workspace (5S), but it has no impact on the fact that their batches are sitting in quarantine for 10 days (the Waste of Waiting). They have applied a tool without first understanding the value stream.""")
        st.success("""üü¢ **THE GOLDEN RULE: Use VSM to Guide Kaizen and Control with Kanban**
A successful Lean transformation is a strategic, top-down process that focuses on root causes.
1.  **Map the Value Stream First:** The VSM is the mandatory starting point. It provides the strategic view of the entire process and makes Mura (unevenness) and Muda (waste) visible.
2.  **Charter a Kaizen Event:** Use the VSM to target the biggest "red bar" (the largest source of waste). Charter a focused, cross-functional Kaizen event to attack this specific problem with a targeted tool (like a **5S workshop** to reduce motion waste, or a setup-reduction event).
3.  **Implement and Sustain with Kanban:** Often, the solution to a large WIP problem is to implement a **Kanban** system. This "pull" system provides the control mechanism to limit inventory and sustain the gains achieved during the Kaizen event.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From the Weaving Loom to the Assembly Line
        The core principles of Lean Manufacturing were born at the **Toyota Motor Corporation** in post-WWII Japan. Engineers **Taiichi Ohno** and **Shigeo Shingo**, unable to compete with the mass production of American auto giants, developed the **Toyota Production System (TPS)**.
        
        TPS was a revolutionary philosophy built on two pillars: **Just-in-Time (JIT)** and **Jidoka** (automation with a human touch). The focus was on the relentless elimination of **Muda (waste)**, and its root causes, **Muri (overburden)** and **Mura (unevenness)**. Core tools of TPS included **Kanban** to implement a JIT "pull" system, a culture of **Kaizen** for continuous improvement, and **5S** to create the organized "visual workplace" necessary for all other systems to function. In the 1990s, American researchers, notably in the book *The Machine That Changed the World*, codified TPS principles, rebranding them as **"Lean Manufacturing."**
        """)
        
    with tabs[5]:
        st.markdown("""
        Lean principles are a key component of a modern, efficient Pharmaceutical Quality System and are strongly supported by regulatory bodies.
        - **ICH Q10 - Pharmaceutical Quality System:** This guideline is heavily focused on **process performance** and **continuous improvement**, which are the core goals of Lean. VSM is a primary tool for identifying areas for improvement.
        - **FDA Process Validation Guidance (Stage 3 - CPV):** The goal of Continued Process Verification is not just to maintain control, but also to identify opportunities for improvement. Lean tools provide the framework for turning the data from CPV into actionable, cost-saving projects.
        - **FDA Report: "Pharmaceutical cGMPs for the 21st Century - A Risk-Based Approach":** This landmark 2004 report explicitly praises the principles of Lean Manufacturing as a way to "enhance product quality and efficiency."
        - **21 CFR 211 (cGMP):** The principles of 5S directly support cGMP requirements for a clean, orderly, and sanitary facility (e.g., ¬ß211.56 Sanitation, ¬ß211.67 Equipment cleaning and maintenance) to prevent mix-ups and contamination.
        """)
        
def render_monte_carlo_simulation():
    """Renders the comprehensive module for Monte Carlo Simulation."""
    st.markdown("""
    #### Purpose & Application: The "Crystal Ball" for Process Risk
    **Purpose:** To create a "virtual factory" that can simulate thousands of future production runs in seconds. **Monte Carlo Simulation** is a computational tool that uses repeated random sampling to model the uncertainty and risk in a complex system, allowing us to see the full spectrum of potential future outcomes.
    
    **Strategic Application:** This is a state-of-the-art tool for proactive **Quality Risk Management (QRM)** and **Process Design**. It moves beyond simple "worst-case" analysis to answer the critical, probabilistic question: **"Given the known variability of all my inputs, what is the actual probability of producing a failed batch?"**
    """)
    st.info("""
    **Interactive Demo:** You are the Process Engineer for a formulation process.
    1.  Use the **"Input Variability"** sliders in the sidebar to define the known variability of your raw materials and process steps.
    2.  Set the **Specification Limits** for the final product.
    3.  Click **"Run Simulation"**. The histogram shows the predicted distribution of all possible future batch results, and the KPI calculates the statistical probability of failure.
    """)

    with st.sidebar:
        st.subheader("Input Variability Controls")
        st.markdown("**API Potency**")
        api_mean = st.slider("Mean (%)", 98.0, 102.0, 100.5, 0.1)
        api_sd = st.slider("Standard Deviation (%)", 0.1, 2.0, 0.5, 0.1)
        st.markdown("**Excipient Purity**")
        excipient_mean = st.slider("Mean (%)", 99.0, 100.0, 99.8, 0.1, key='ex_mean')
        excipient_sd = st.slider("Standard Deviation (%)", 0.05, 1.0, 0.2, 0.05, key='ex_sd')
        st.markdown("**Process Loss**")
        loss_min = st.slider("Minimum Loss (%)", 0.0, 5.0, 1.0, 0.5)
        loss_max = st.slider("Maximum Loss (%)", loss_min, 10.0, 3.0, 0.5)
        
        dist_params = {'api_mean': api_mean, 'api_sd': api_sd, 'excipient_mean': excipient_mean, 'excipient_sd': excipient_sd, 'loss_min': loss_min, 'loss_max': loss_max}
        
        st.divider()
        st.subheader("Simulation & Specification")
        n_trials = st.select_slider("Number of Simulated Batches", options=[1000, 10000, 50000, 100000], value=10000)
        spec_limits = st.slider("Specification Limits (%)", 90.0, 100.0, (95.0, 105.0), 0.5)
        lsl, usl = spec_limits

    if 'mc_fig' not in st.session_state:
        st.session_state.mc_fig = None
        st.session_state.mc_fail_rate = None

    if st.button("üöÄ Run Simulation", use_container_width=True):
        with st.spinner(f"Running {n_trials:,} virtual batches..."):
            fig, fail_rate = plot_monte_carlo_simulation(dist_params, n_trials, lsl, usl)
            st.session_state.mc_fig = fig
            st.session_state.mc_fail_rate = fail_rate
        st.rerun()

    st.header("Process Risk Dashboard")

    if st.session_state.mc_fig:
        st.metric("Predicted Failure Rate (Out of Specification)", f"{st.session_state.mc_fail_rate:.3%}")
        st.plotly_chart(st.session_state.mc_fig, use_container_width=True)
    else:
        st.info("Configure your process in the sidebar and click 'Run Simulation' to see the risk profile.")
    
    st.divider()
    st.subheader("Deeper Dive into Monte Carlo Simulation")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Define the System:** The process starts by creating a mathematical model of the system (e.g., `Final_Conc = API * Purity * (1-Loss)`).
        2.  **Characterize the Inputs (Sidebar):** This is the most critical step. The team must use historical data or validation studies to define the statistical distribution of each input variable‚Äîits mean, standard deviation, and shape.
        3.  **Run the Simulation:** The computer then runs thousands of "virtual batches." In each run, it randomly draws a value for each input from its defined distribution and calculates the resulting output.
        4.  **Analyze the Output Distribution (Histogram):** The resulting histogram is a **probabilistic forecast** of all possible future outcomes. It is a powerful visualization of the total system risk. The percentage of simulated batches that fall outside the red specification lines is the predicted long-term failure rate.

        **The Strategic Insight:** This tool demonstrates the concept of **variation propagation**. Small, acceptable variations in multiple *inputs* can stack up in unexpected ways to produce a significant number of failures in the final *output*. Monte Carlo simulation is the only tool that can accurately model and quantify this complex, cumulative risk.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: From "Worst-Case" Guessing to Probabilistic Forecasting

        #### The Problem: The Flaw of Averages and Unrealistic Worst Cases
        A team is designing a new process and needs to set the specification limits for their raw materials. They use two common but flawed approaches:
        1.  **The Flaw of Averages:** They design the process assuming all inputs will be at their average, target value.
        2.  **The Worst-Case Scenario:** They test a single "worst-case" run where all inputs are simultaneously set to their extreme limits.
        Neither of these approaches reflects reality.

        #### The Impact: Brittle Processes and Unnecessary Costs
        This simplistic view of the world leads to poor and costly decisions.
        - **Brittle Processes:** The process designed using averages is extremely fragile. In the real world, where inputs are always variable, the process experiences frequent, "unexpected" failures.
        - **Impractical Specifications:** The "worst-case" scenario is often statistically impossible (the odds of all 5 inputs being at their 3-sigma limit simultaneously can be less than one-in-a-trillion). Setting specifications based on this unrealistic scenario leads to **impossibly tight raw material specs**. This dramatically increases costs, limits the supplier base, and can make the process commercially non-viable.

        #### The Solution: A "Virtual Factory" for Proactive Risk Assessment
        Monte Carlo Simulation is the solution. It creates a **"virtual factory"** on a computer, allowing you to see the combined, probabilistic impact of real-world variation *before* you ever run the process. It moves beyond a single "worst case" to provide a full **probability distribution of all possible outcomes**. This enables powerful, data-driven business decisions:
        - **Risk-Based Specification Setting:** You can simulate the impact of widening a raw material specification. "If we relax the API Potency spec from ¬±1% to ¬±2%, our material cost will decrease by 15%, and the simulation shows the predicted batch failure rate only increases from 0.1% to 0.3%. This is an acceptable business risk."
        - **Targeted Process Improvement:** The simulation can identify which input variable's variability is the biggest contributor to the final product failures, allowing you to focus your validation and improvement efforts where they will have the greatest impact.

        #### The Consequences: A Robust Process Design and an Optimized Supply Chain
        - **Without This:** Process design is a gamble based on unrealistic assumptions. Specification setting is a battle between the over-cautious and the over-optimistic.
        - **With This:** Monte Carlo Simulation provides a **quantitative, probabilistic framework for designing for manufacturability**. It allows the company to build robust processes, set realistic and cost-effective specifications, and make data-driven trade-offs between cost, risk, and quality. It is a key tool for de-risking technology transfer and ensuring a smooth, predictable ramp-up to commercial manufacturing.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **Monte Carlo Simulation:** A computerized mathematical technique that allows people to account for risk in quantitative analysis and decision making. The technique is used by professionals in fields such as finance, project management, energy, manufacturing, engineering, and science.
        - **Stochastic Model:** A model that incorporates randomness. The inputs are not fixed numbers but are represented by probability distributions.
        - **Probability Distribution:** A mathematical function that describes the likelihood of obtaining the possible values that a random variable can take (e.g., a Normal distribution for potency, a Uniform distribution for process loss).
        - **Variation Propagation (or Error Propagation):** The study of how the uncertainty in the inputs to a mathematical model affects the uncertainty of the model's output.
        - **Probabilistic Risk Assessment (PRA):** A systematic and comprehensive methodology to evaluate risks associated with a complex engineered system, using probability as its metric. Monte Carlo simulation is the engine for PRA.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Worst-Case" Trap**
An engineer defines the "worst-case" as the scenario where all inputs are simultaneously at their worst possible values. They run a single experiment under these conditions.
- **The Flaw:** This is not a risk-based approach; it's a fear-based one. The probability of this "perfect storm" scenario occurring might be astronomically low. Designing the entire process and its specifications around an event that will never happen leads to massive over-engineering and unnecessary cost.""")
        st.success("""üü¢ **THE GOLDEN RULE: Model the Variation, Don't Just Guess at the Extremes**
A robust risk assessment embraces the reality of variation.
1.  **Characterize Your Inputs:** The most important step is to use real historical data to understand and define the probability distribution for each critical input parameter. Garbage in, gospel out.
2.  **Model the System:** Define the mathematical or logical relationship between the inputs and the critical quality output.
3.  **Simulate Thousands of Futures:** Run the Monte Carlo simulation to generate a high-resolution picture of all the plausible future outcomes.
4.  **Make a Risk-Based Decision:** Use the final output distribution to make a quantitative, probabilistic decision. For example, "A predicted failure rate of 0.2% is acceptable for this process, so the proposed specifications are approved."
""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From the Bomb to the Boardroom
        **The Problem:** During the Manhattan Project in the 1940s, scientists like **Stanislaw Ulam**, **John von Neumann**, and **Nicholas Metropolis** were working on neutron diffusion problems related to the atomic bomb. These problems were far too complex to be solved with traditional analytical mathematics. They involved a cascade of probabilistic events‚Äîa neutron traveling a random distance, hitting an atom, and causing a fission that releases more neutrons, each with its own random trajectory.

        **The 'Aha!' Moment:** Ulam, while playing solitaire, realized he could determine the probability of a successful outcome by simply playing hundreds of games and counting the number of wins. He and von Neumann adapted this idea to the neutron problem. They realized they could simulate the "life" of thousands of individual neutrons on the newly available electronic computers (like the ENIAC), using random numbers to decide the outcome at each step. By averaging the results of all these random trials, they could get a very accurate estimate of the overall behavior of the system. Because the project was secret, von Neumann gave this technique the codename **"Monte Carlo,"** after the famous casino in Monaco.
        
        **The Impact:** After the war, the method was declassified and its applications exploded. It became an essential tool in fields as diverse as particle physics, financial engineering (for modeling stock prices), and supply chain management. In modern manufacturing and process design, it has become the gold standard for **probabilistic risk assessment**, allowing engineers to move beyond simple, deterministic models and embrace the complexity and uncertainty of the real world.
        """)
        
    with tabs[5]:
        st.markdown("""
        Monte Carlo Simulation is a state-of-the-art tool for fulfilling the principles of modern, risk-based validation and Quality by Design (QbD).
        - **ICH Q9 - Quality Risk Management:** This tool is a direct, quantitative implementation of the QRM principles. It allows for a probabilistic assessment of the risk of failure, which is far more powerful than the qualitative rankings used in an FMEA. The results of a simulation can be a key input to an FMEA.
        - **ICH Q8(R2) - Pharmaceutical Development:** The simulation is a powerful tool for defining a robust **Design Space**. It allows a company to predict how the combined variability of its Critical Process Parameters (CPPs) will affect the probability of staying within the limits of its Critical Quality Attributes (CQAs).
        - **FDA Process Validation Guidance (Stage 1 - Process Design):** The guidance emphasizes building deep process understanding and designing a process that is robust and capable. Monte Carlo simulation is a key tool for achieving this during the design phase, allowing for the in-silico testing of process robustness before the first pilot batch is ever run.
        """)

#=============================================================================== 9. FIRST TIME YIELD & COST OF QUALITY  ============================================================================
def render_fty_coq():
    """Renders the comprehensive, interactive module for First Time Yield & Cost of Quality."""
    st.markdown("""
    #### Purpose & Application: The Business Case for Quality
    **Purpose:** To demonstrate the powerful financial and operational relationship between process performance (**First Time Yield**) and its business consequences (**Cost of Quality**). This tool moves beyond simple pass/fail metrics to provide a holistic view of process efficiency and the hidden costs of failure.
    
    **Strategic Application:** This dashboard is a critical communication tool for validation and engineering leaders to justify quality improvement projects to business leadership. It translates technical process metrics (like yield) into the language of the business (cost and risk), making the return on investment for validation and process improvement activities clear and tangible.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Operations Director.
    1.  Select the **Project Type** to see a realistic multi-step process.
    2.  Use the **"Process Improvement Effort"** slider in the sidebar to simulate investing in better process controls, training, and validation.
    3.  Observe the impact across the four-plot dashboard, from root cause to financial outcome.
    """)

    project_type = st.selectbox(
        "Select a Project Type to Simulate:",
        ["Pharma Process (MAb)", "Analytical Assay (ELISA)", "Instrument Qualification", "Software System (CSV)"]
    )

    with st.sidebar:
        st.subheader("Improvement Effort Controls")
        improvement_effort = st.slider("Process Improvement Effort", 0, 10, 0, 1,
            help="Simulates the level of investment in process understanding and control (e.g., more validation, better training, improved equipment). Higher effort increases 'Good Quality' costs but dramatically reduces 'Poor Quality' costs and improves yield.")

    fig_pareto, fig_spc, fig_sankey, fig_iceberg, rty_base, rty_improved, base_coq, improved_coq = plot_fty_coq(project_type, improvement_effort)

    st.header("Process Performance & Cost Dashboard")
    total_coq_base = sum(base_coq.values())
    total_coq_improved = sum(improved_coq.values())
    
    rty_name = "Right First Time" if "Software" in project_type else "Rolled Throughput Yield (RTY)"
    col1, col2, col3 = st.columns(3)
    col1.metric(rty_name, f"{rty_improved:.1%}", f"{rty_improved - rty_base:.1%}")
    col2.metric("Total Cost of Quality (COQ)", f"{total_coq_improved:,.0f} RCU", f"{total_coq_improved - total_coq_base:,.0f} RCU")
    col3.metric("Return on Quality Investment", f"{(total_coq_base - total_coq_improved) / (improvement_effort*3500 + 1):.1f}x", help="Ratio of cost savings to the investment in prevention/appraisal.")

    col_p1, col_p2 = st.columns(2)
    with col_p1:
        st.plotly_chart(fig_pareto, use_container_width=True)
        st.plotly_chart(fig_sankey, use_container_width=True)
    with col_p2:
        st.plotly_chart(fig_spc, use_container_width=True)
        st.plotly_chart(fig_iceberg, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **The 4-Plot Story: A Realistic Improvement Workflow**
        This dashboard tells a story from left to right, top to bottom, mirroring a real process improvement project.
        1.  **Where is the problem? (Pareto Chart):** This chart identifies the "vital few" steps causing the most scrap/rework. Your improvement efforts should always start here. Notice how the "Optimized" (green) bar is lowest for the step that was worst in the "Baseline" (grey).
        2.  **Why is it a problem? (SPC Chart):** This chart provides a statistical root cause for the failure at the worst step. A process with low yield is often unstable or off-center. As you apply improvement effort, this chart becomes more stable and centered, visually linking investment to improved process control.
        3.  **What is the overall impact? (Sankey Plot):** This shows the cumulative effect of all step yields on the final output (RTY). Improving the worst step has the biggest impact on widening the green "Final Output" flow.
        4.  **What is the financial consequence? (Iceberg Chart):** This translates the operational improvements into business terms. The investment in better process control (making the iceberg tip bigger) dramatically shrinks the hidden costs of failure (the much larger submerged part).
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The Rosetta Stone for Quality and Finance
    
        #### The Problem: Two Different Languages
        The organization is split into two worlds speaking different languages.
        - **The Operations World:** Speaks in terms of **things**. They talk about `yield`, `defect rates`, `scrap`, `rework`, and `process capability`.
        - **The Business World:** Speaks in terms of **money**. They talk about `profit margins`, `cost of goods sold (COGS)`, `return on investment (ROI)`, and `shareholder value`.
        When the Head of Quality asks for a $2 million investment in a new validation program to "improve the Rolled Throughput Yield from 85% to 95%," the CFO hears only the cost, not the value.
    
        #### The Impact: Underinvestment in Quality and Chronic Inefficiency
        This language barrier is the single biggest obstacle to achieving operational excellence.
        - **Quality is Seen as a Cost Center:** Because its value is not translated into financial terms, the Quality department is viewed as a pure cost center, a bureaucratic necessity. Its budget is the first to be cut.
        - **"Good Enough" Becomes the Standard:** The business tolerates a chronically inefficient process with a low RTY because they don't see the full financial picture. The "hidden factory"‚Äîthe massive cost of scrap, rework, and investigations‚Äîis accepted as "the cost of doing business" rather than a target for elimination.
        - **Failed Improvement Projects:** Engineers are unable to get funding for critical process improvement projects because they cannot articulate the financial return on investment to the executive team.
    
        #### The Solution: The "Rosetta Stone" of Quality
        First Time Yield (FTY), Rolled Throughput Yield (RTY), and the Cost of Quality (COQ) are the **Rosetta Stone** that translates the language of the factory floor directly into the language of the boardroom.
        1.  **FTY and RTY** quantify the physical reality of process efficiency.
        2.  **The COQ framework** takes those physical numbers and assigns a dollar value to them, showing the massive, hidden cost of poor quality (the submerged part of the iceberg).
        3.  **The "Return on Quality Investment"** metric provides the final, undeniable business case, proving that investing in prevention (like validation) is not a cost, but one of the highest-ROI activities the company can undertake.
    
        #### The Consequences: Quality as a Profit Center
        - **Without This:** Quality and Operations are stuck defending their budgets and unable to articulate their value. The company stagnates with inefficient processes.
        - **With This:** The COQ dashboard becomes a central tool for strategic planning. The Head of Quality can now walk into the boardroom and say: **"A $2 million investment in this validation project will reduce our internal failure costs by $8 million per year, generating a 4x ROI within 12 months."** This reframes Quality from a cost center into a **profit center** and a key driver of business value, unlocking the investment needed to achieve true operational excellence.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Quality Management Terms
        - **First Time Yield (FTY):** The percentage of units that pass a single process step without any defects or rework. Also known as First Pass Yield.
        - **Rolled Throughput Yield (RTY):** The probability that a unit will pass through all process steps without any defects. It is calculated by multiplying the FTY of all individual steps (`RTY = FTY‚ÇÅ √ó FTY‚ÇÇ √ó ... √ó FTY‚Çô`).
        - **Cost of Quality (COQ):** A methodology that quantifies the total cost of quality-related efforts and deficiencies.
        - **Prevention Costs:** Costs incurred to prevent defects from occurring in the first place (e.g., validation, training, FMEA).
        - **Appraisal Costs:** Costs incurred to detect defects (e.g., inspections, QC testing, audits).
        - **Internal Failure Costs:** Costs of defects found *before* the product is delivered to the customer (e.g., scrap, rework, investigation).
        - **External Failure Costs:** Costs of defects found *after* the product is delivered to the customer (e.g., recalls, warranty claims, lawsuits). These are the most damaging costs.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: "The Firefighting Mentality"**
A company under-invests in prevention and appraisal to minimize short-term costs. Their quality system is entirely reactive, consisting of a large QC department to "inspect quality in" at the end, and a large QA team to manage the constant deviations, rework, and scrap.
- **The Flaw:** They are paying the highest possible Cost of Quality. The massive, hidden costs of internal and external failures far outweigh the savings from skimping on prevention, and their low RTY creates unpredictable production schedules.""")
        st.success("""üü¢ **THE GOLDEN RULE: Invest in Prevention, Not Failure**
The goal of a mature quality system is to strategically shift spending from failure costs to prevention and appraisal costs.
1.  **Measure Your Yield:** Calculate FTY for every step and the overall RTY to understand where your process is "leaking."
2.  **Quantify the Cost of Quality:** Use the COQ framework to translate yield losses and failures into a financial number that gets management's attention.
3.  **Justify Investment:** Use the RTY and COQ data to build a powerful business case for investing in process validation, better equipment, and more robust quality systems. This tool shows that such investments have a massive positive return.""")
        
    with tabs[4]:
        st.markdown("""
        #### Historical Context: The Quality Gurus
        The concepts of FTY and COQ were developed by the pioneers of the 20th-century quality management movement.
        - **Rolled Throughput Yield:** This concept is a cornerstone of **Six Sigma**, the quality improvement methodology famously developed at **Motorola in the 1980s**. RTY was a powerful metric for quantifying the cumulative effect of defects in a complex process and for measuring the impact of improvement projects.
        - **Cost of Quality:** The COQ framework was first described by **Armand V. Feigenbaum** in a 1956 Harvard Business Review article and was later popularized in his book *Total Quality Control*. He was the first to categorize costs into the four buckets (Prevention, Appraisal, Internal Failure, External Failure). Independently, **Joseph M. Juran** discussed the economics of quality in his *Quality Control Handbook* and emphasized the distinction between the "Cost of Good Quality" and the "Cost of Poor Quality."
        
        Together, these concepts provided the financial and operational language for the quality revolution, allowing engineers and scientists to frame quality not as an expense, but as a high-return investment.
        """)
        
    with tabs[5]:
        st.markdown("""
        FTY and COQ are not explicitly named in many regulations, but they are the underlying metrics and business drivers for the entire GxP quality system.
        - **ICH Q9 - Quality Risk Management:** The COQ framework is a powerful tool for quantifying the financial impact of risks identified in an FMEA. The potential for high "External Failure Costs" is a key driver for risk mitigation activities.
        - **ICH Q10 - Pharmaceutical Quality System:** This guideline emphasizes the importance of **continuous improvement** and **process performance monitoring**. RTY is a key metric for monitoring process performance, and reducing the COQ is a primary goal of a continuous improvement program.
        - **FDA Process Validation Guidance (Stage 3 - CPV):** An effective Continued Process Verification program should monitor metrics like FTY and RTY. A negative trend in these metrics would trigger an investigation and corrective action, demonstrating that the quality system is working as intended.
        """)
#===================================================================================== 10. TOLERANCE INTERVALS  ============================================================================
def render_tolerance_intervals():
    """Renders the INTERACTIVE module for Tolerance Intervals."""
    st.markdown("""
    #### Purpose & Application: The Quality Engineer's Secret Weapon
    **Purpose:** To construct an interval that we can claim, with a specified level of confidence, contains a certain proportion of all individual values from a process.
    
    **Strategic Application:** This is often the most critical statistical interval in manufacturing. It directly answers the high-stakes question: **"Based on this sample, what is the range where we can expect almost all of our individual product units to fall?"**
    """)
    
    st.info("""
    **Interactive Demo:** Use the sliders below to explore the trade-offs in tolerance intervals. This simulation demonstrates how sample size and the desired quality guarantee (coverage) directly impact the calculated interval, which in turn affects process specifications and batch release decisions.
    """)
    
    # --- NEW: Sidebar controls for this specific module ---
    st.subheader("Tolerance Interval Controls")
    n_slider = st.slider(
        "üî¨ Sample Size (n)", 
        min_value=10, max_value=200, value=30, step=10,
        help="The number of samples collected. More samples lead to a narrower, more reliable interval."
    )
    coverage_slider = st.select_slider(
        "üéØ Desired Population Coverage",
        options=[90.0, 95.0, 99.0, 99.9],
        value=99.0,
        help="The 'quality promise'. What percentage of all future parts do you want this interval to contain? A higher promise requires a wider interval."
    )

    # Generate plots using the slider values
    fig, ci, ti = plot_tolerance_intervals(n=n_slider, coverage_pct=coverage_slider)
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.metric(label="üéØ Desired Coverage", value=f"{coverage_pct:.1f}% of Population", help="The proportion of the entire process output we want our interval to contain.")
            st.metric(label="üìè Resulting Tolerance Interval", value=f"[{ti[0]:.1f}, {ti[1]:.1f}]", help="The final calculated range. Note how much wider it is than the CI.")
            
            st.info("Play with the sliders in the sidebar and observe the results!")
            st.markdown("""
            - **Increase `Sample Size (n)`:** As you collect more data, your estimates of the mean and standard deviation become more reliable. Notice how both the **Confidence Interval (orange)** and the **Tolerance Interval (green)** become **narrower**. This shows the direct link between sampling cost and statistical precision.
            - **Increase `Desired Population Coverage`:** As you increase the strength of your quality promise from 90% to 99.9%, the **Tolerance Interval becomes dramatically wider**. To be more certain of capturing a larger percentage of parts, you must widen your interval.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: The Statistical "Quality Promise"
        
            #### The Problem: The "Average is Good Enough" Fallacy
            A team completes a Process Performance Qualification (PPQ) study for a new medical device component. The specification is 95-105 units. They calculate a 95% **confidence interval** for the process mean and find that it is tightly centered at [99.9, 100.1]. Based on this, they conclude the process is excellent and approve it for commercial manufacturing.
        
            #### The Impact: The Catastrophe of the Individual Unit
            This is a critical, and unfortunately common, statistical misunderstanding with severe consequences.
            - **Field Failures and Recalls:** The process, while having a perfect *average*, also has significant variability. A large percentage of *individual units* are actually falling outside the specification limits. Because the team used the wrong statistical interval, they were completely blind to this. This leads to product failures in the field, customer complaints, and potentially a multi-million dollar product recall.
            - **Failed Release Criteria:** The team tries to use the confidence interval to set release specifications for future batches. An auditor immediately rejects this, because a confidence interval makes a claim about the *average*, while a product specification applies to *every single unit*. The validation is deemed inadequate.
        
            #### The Solution: The Right Interval for the Right Question
            The Tolerance Interval is the only statistically valid tool for answering the question that truly matters to customers and regulators: **"Based on our sample, what is the range that we are confident contains almost all of our individual product units?"** It is fundamentally different from a confidence interval.
            -   A **Confidence Interval** is a statement about the **mean**.
            -   A **Tolerance Interval** is a statement about a **large proportion of the individuals**.
            
            It is the correct tool for making a statistically sound "quality promise" about the performance of the units you ship.
        
            #### The Consequences: A Defensible Specification and Managed Risk
            - **Without This:** The company is making unsubstantiated claims about the quality of its individual products. They are exposed to significant compliance risk and the potential for costly field failures.
            - **With This:** The Tolerance Interval provides the **objective, statistical evidence** to support a release specification. It allows the company to make a defensible statement like: *"We are 95% confident that 99.9% of all units produced by this process will fall between 94.5 and 105.5 units."* This allows the business to set realistic specifications, manage the risk of out-of-spec material, and provide robust, data-driven proof of quality to regulators.
            """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Statistical Intervals
            - **Confidence Interval (CI):** An interval estimate for a population **parameter** (like the mean). A 95% CI provides a plausible range for the *average* of the process.
            - **Tolerance Interval (TI):** An interval estimate for a specified **proportion of a population**. A 95%/99% TI provides a plausible range for where 99% of all *individual units* from the process will fall.
            - **Prediction Interval (PI):** An interval estimate for a **single future observation**. A 95% PI provides a plausible range for the *very next* data point.
            - **Coverage:** The proportion of the population that the tolerance interval is intended to contain (e.g., 99%).
            - **Confidence:** The probability that a given interval, constructed from a random sample, will actually contain the true value it is intended to estimate.
            """)
        with tabs[3]:
            st.error("""
            üî¥ **THE INCORRECT APPROACH: The Confidence Interval Fallacy**
            - A manager sees that the 95% **Confidence Interval** for the mean is [99.9, 100.1] and their product specification is [95, 105]. They declare victory, believing all their product is in spec.
            - **The Flaw:** They've proven the *average* is in spec, but have made no claim about the *individuals*. If process variation is high, many parts could still be out of spec.
            """)
            st.success("""
            üü¢ **THE GOLDEN RULE: Use the Right Interval for the Right Question**
            - **Question 1: "Where is my long-term process average located?"**
              - **Correct Tool:** ‚úÖ **Confidence Interval**.
            - **Question 2: "Will the individual units I produce meet the customer's specification?"**
              - **Correct Tool:** ‚úÖ **Tolerance Interval**.
              
            Never use a confidence interval to make a statement about where individual values are expected to fall.
            """)

        with tabs[4]:
            st.markdown("""
            #### Historical Context: The Surviving Bomber Problem
            The development of tolerance intervals is credited to the brilliant mathematician **Abraham Wald** during World War II. He is famous for the "surviving bombers" problem: when analyzing bullet holes on returning planes, the military wanted to reinforce the most-hit areas. Wald's revolutionary insight was that they should reinforce the areas with **no bullet holes**‚Äîbecause planes hit there never made it back.
            
            This ability to reason about an entire population from a limited sample is the same thinking behind the tolerance interval. Wald developed the statistical theory to allow engineers to make a reliable claim about **all** manufactured parts based on a **small sample**, a critical need for mass-producing interchangeable military hardware.
            
            #### Mathematical Basis
            """)
            st.latex(r"\text{TI} = \bar{x} \pm k \cdot s")
            st.markdown("""
            - **`k`**: The **k-factor** is the magic ingredient. It is a special value that depends on **three** inputs: the sample size (`n`), the desired population coverage (e.g., 99%), and the desired confidence level (e.g., 95%). This `k`-factor is mathematically constructed to account for the "double uncertainty" of not knowing the true mean *or* the true standard deviation.
            """)
        with tabs[5]:
            st.markdown("""
            Tolerance intervals are a statistically rigorous method for setting acceptance criteria and release specifications based on validation data.
            - **FDA Process Validation Guidance (Stage 2):** PPQ runs are used to demonstrate that the process can reliably produce product meeting its Critical Quality Attributes (CQAs). A tolerance interval calculated from PPQ data provides a high-confidence range where a large proportion of all future production will fall.
            - **USP General Chapter <1010> - Analytical Data:** Discusses various statistical intervals and their correct application, including tolerance intervals for making claims about a proportion of a population.
            """)

#========================================================================================= 11. BAYESIAN INFERENCE  ============================================================================
def render_bayesian():
    """Renders the interactive module for Bayesian Inference."""
    st.markdown("""
    #### Purpose & Application: The Science of Belief Updating
    **Purpose:** To employ Bayesian inference to formally and quantitatively synthesize existing knowledge (a **Prior** belief) with new experimental data (the **Likelihood**) to arrive at an updated, more robust conclusion (the **Posterior** belief).
    
    **Strategic Application:** This is a paradigm-shifting tool for driving efficient, knowledge-based validation and decision-making. In a traditional (Frequentist) world, every study starts from a blank slate. In the Bayesian world, we can formally leverage what we already know.
    - **Accelerating Tech Transfer:** Use data from an R&D validation study to form a **strong, informative prior**. This allows the receiving QC lab to demonstrate success with a smaller confirmation study, saving time and resources.
    - **Answering Direct Business Questions:** Provides a natural framework to answer the question: "Given all the evidence, what is the probability that our true pass rate is above 90%?"
    """)
    st.info("""
    **Interactive Demo:** Use the controls in the sidebar to define your prior belief and the new experimental data. Observe how the final **Posterior (blue curve)** is always a weighted compromise between your initial **Prior (green curve)** and the new **Data (red diamond)**.
    """)
    
    with st.sidebar:
        st.subheader("Bayesian Inference Controls")
        prior_type_bayes = st.radio(
            "Select Prior Belief:",
            ("Strong R&D Prior", "No Prior (Uninformative)", "Skeptical/Regulatory Prior"),
            index=0,
            help="Your belief about the pass rate *before* seeing the new data. A 'Strong' prior has a large influence; an 'Uninformative' prior lets the data speak for itself."
        )
        # --- NEW INTERACTIVE SLIDERS ---
        st.markdown("---")
        st.markdown("**New QC Data**")
        n_qc_slider = st.slider("Number of QC Samples (n)", 1, 100, 20, 1,
            help="The total number of new QC samples run in the experiment.")
        k_qc_slider = st.slider("Number of Passes (k)", 0, n_qc_slider, 18, 1,
            help="Of the new samples run, how many passed?")

    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        fig, prior_mean, mle, posterior_mean, credible_interval, prob_gt_spec = plot_bayesian(
            prior_type=prior_type_bayes,
            n_qc=n_qc_slider,
            k_qc=k_qc_slider,
            spec_limit=0.90
        )
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ Acceptance Criteria", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.metric(
                label="üéØ Primary KPI: Prob(Pass Rate > 90%)",
                value=f"{prob_gt_spec:.2%}",
                help="The posterior probability that the true pass rate meets the 90% specification. This is a direct risk statement."
            )
            st.metric(
                label="üìà Posterior Mean Pass Rate",
                value=f"{posterior_mean:.1%}",
                help="The final, data-informed belief; a weighted average of the prior and the data."
            )
            st.metric(
                label="üìä 95% Credible Interval",
                value=f"[{credible_interval[0]:.1%}, {credible_interval[1]:.1%}]",
                help="We can be 95% certain that the true pass rate lies within this interval."
            )
            st.markdown("---")
            st.metric(label="Prior Mean Rate", value=f"{prior_mean:.1%}", help="The initial belief *before* seeing the new QC data.")
            st.metric(label="Data-only Estimate (k/n)", value=f"{mle:.1%}", help="The evidence from the new QC data alone (the frequentist result).")
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: The High Cost of Corporate Amnesia
        
            #### The Problem: The "Groundhog Day" Validation
            A company is performing a technology transfer of a well-understood process to a new site. The process has been run hundreds of times at the original site with a consistent 99.5% pass rate. However, the validation protocol at the new site, written using a traditional (Frequentist) approach, pretends none of this prior knowledge exists. The new site is required to perform a large, expensive PPQ campaign (e.g., n=300) to prove, *from a blank slate*, that the process is reliable.
        
            #### The Impact: Wasted Resources and Slower Timelines
            This "corporate amnesia," where each study starts from scratch, is enormously inefficient and costly.
            - **Redundant Data Generation:** The company spends hundreds of thousands of dollars and months of time generating data to re-prove something it already knows with a high degree of certainty.
            - **Delayed Revenue:** The long, inefficient validation process at the new site delays the site's approval and its ability to contribute to the supply chain, directly impacting revenue.
            - **Inability to Answer Direct Questions:** Management asks a simple, critical business question: "Based on everything we know, what is the *probability* that this new site will achieve a pass rate above 99%?" Traditional statistics cannot answer this direct probabilistic question.
        
            #### The Solution: A Formal Framework for Accumulated Knowledge
            Bayesian Inference is the solution to corporate amnesia. It provides a formal, auditable, and mathematically rigorous framework to **combine prior knowledge with new data**. It is not about using subjective feelings; it is about using historical data to create a quantitative **Prior distribution**. This prior is then mathematically updated with the new, smaller dataset from the new site to produce a final, robust **Posterior** conclusion.
        
            #### The Consequences: Accelerated Decisions and Optimized Resources
            - **Without This:** The company is trapped in a slow, inefficient cycle of re-validating the same knowledge over and over again.
            - **With This:** Bayesian Inference becomes a powerful tool for **accelerating timelines and optimizing resources**. A tech transfer might be validated with a much smaller confirmation study, saving millions of dollars and months of time. It allows the business to make decisions that are informed by the **totality of its evidence**, not just the most recent experiment. Furthermore, it allows managers to answer direct, risk-based business questions, such as "What is the probability of success?", which is the language of business, not just statistics.
            """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Bayesian Terms
            - **Prior Distribution:** A probability distribution that represents one's belief about a parameter *before* seeing the new data.
            - **Likelihood:** The probability of observing the new data, given a specific value of the parameter. This is the information contained in the new experiment.
            - **Posterior Distribution:** The updated probability distribution for the parameter *after* combining the prior belief with the likelihood from the new data. `Posterior ‚àù Likelihood √ó Prior`.
            - **Credible Interval:** The Bayesian equivalent of a confidence interval. A 95% credible interval `[X, Y]` means there is a 95% probability that the true value of the parameter lies within that range.
            - **Conjugate Prior:** A prior distribution that, when combined with a given likelihood, results in a posterior distribution from the same family (e.g., Beta prior for binomial likelihood results in a Beta posterior).
            """)
        with tabs[3]:
            st.markdown("- The acceptance criterion is framed in terms of the **posterior distribution** and is probabilistic.")
            st.markdown("- **Example Criterion 1 (Probability Statement):** 'There must be at least a 95% probability that the true pass rate is greater than 90%.'")
            st.markdown("- **Example Criterion 2 (Credible Interval):** 'The lower bound of the **95% Credible Interval** must be above the target of 90%.'")
            st.warning("**The Prior is Critical:** In a regulated setting, the prior must be transparent, justified by historical data, and pre-specified in the validation protocol. An unsubstantiated, overly optimistic prior is a major red flag.")
        with tabs[4]:
            st.markdown("""
            #### Historical Context: The 200-Year Winter
            **The Problem:** The underlying theorem was conceived by the Reverend **Thomas Bayes** in the 1740s and published posthumously. However, for nearly 200 years, Bayesian inference remained a philosophical curiosity, largely overshadowed by the Frequentist school of Neyman and Fisher. There were two huge barriers:
            1.  **The Philosophical Barrier:** The "subjective" nature of the prior was anathema to frequentists who sought pure objectivity.
            2.  **The Computational Barrier:** For most real-world problems, calculating the denominator in Bayes' theorem (the "evidence") was mathematically intractable.
            
            **The 'Aha!' Moment (MCMC):** The **"Bayesian Revolution"** began in the late 20th century, driven by the explosion of computing power and the development of powerful simulation algorithms like **Markov Chain Monte Carlo (MCMC)**. MCMC provided a clever workaround: instead of trying to calculate the posterior distribution directly, you could create a smart algorithm that *draws samples from it*.
            
            **The Impact:** MCMC made the previously impossible possible. Scientists could now approximate the posterior distribution for incredibly complex models, making Bayesian methods practical for the first time. This has led to a renaissance of Bayesian thinking in fields from astrophysics to pharmaceutical development.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("Bayes' Theorem is elegantly simple:")
            st.latex(r"P(\theta|D) = \frac{P(D|\theta) \cdot P(\theta)}{P(D)}")
            st.markdown(r"In words: **Posterior = (Likelihood √ó Prior) / Evidence**")
            st.markdown(r"""
            - $P(\theta|D)$ (Posterior): The probability of our parameter $\theta$ (e.g., the true pass rate) given the new Data D.
            - $P(D|\theta)$ (Likelihood): The probability of observing our Data D, given a specific value of the parameter $\theta$.
            - $P(\theta)$ (Prior): Our initial belief about the distribution of the parameter $\theta$.
            
            For binomial data (pass/fail), the **Beta distribution** is a **conjugate prior**. This means if you start with a Beta prior and have a binomial likelihood, your posterior is also a simple, updated Beta distribution.
            """)
            st.latex(r"\text{Posterior} \sim \text{Beta}(\alpha_{prior} + k, \beta_{prior} + n - k)")
        with tabs[5]:
            st.markdown("""
            While less common than frequentist methods, Bayesian statistics are explicitly accepted and even encouraged by regulators in certain contexts, particularly where prior information is valuable.
            - **FDA Guidance on Adaptive Designs for Clinical Trials:** This guidance openly discusses and accepts the use of Bayesian methods for modifying trial designs based on accumulating data.
            - **FDA Guidance on Medical Device Decision Making:** The benefit-risk assessments for medical devices are often framed in a way that is highly compatible with Bayesian thinking, allowing for the formal incorporation of prior knowledge.
            - **ICH Q8, Q9, Q10:** The lifecycle and risk-based principles of these guidelines are well-aligned with the Bayesian paradigm of updating knowledge as more data becomes available.
            """)

##=======================================================================================================================================================================================================
##=================================================================== END ACT II UI Render ========================================================================================================================
##=======================================================================================================================================================================================================
#================================ 0 OEE ==================================================================================================================
# SNIPPET: Replace your entire render_oee function with this enhanced version.

def render_oee():
    """Renders the comprehensive module for Overall Equipment Effectiveness (OEE)."""
    st.markdown("""
    #### Purpose & Application: The "Hidden Factory" Uncovered
    **Purpose:** To provide a single, powerful KPI that measures the true productivity of a piece of manufacturing equipment. **Overall Equipment Effectiveness (OEE)** is a composite metric that distills all operational losses into one number, revealing the "hidden factory"‚Äîthe untapped capacity that already exists within your facility.
    
    **Strategic Application:** OEE is the gold standard for measuring manufacturing performance. It is a critical tool for identifying improvement opportunities, justifying investments, and tracking the success of Lean and Six Sigma initiatives.
    """)
    st.info("""
    **Interactive Demo:** You are the Manufacturing Supervisor.
    1.  Enter the operational data for a recent shift into the **"OEE Calculator"** in the sidebar.
    2.  The **KPIs** will instantly calculate the three core components and the final OEE score.
    3.  The **Waterfall Chart** will visualize exactly where the losses are occurring, showing you where to focus your improvement efforts.
    """)

    with st.sidebar:
        st.subheader("OEE Calculator")
        st.markdown("Enter data from a typical production shift.")
        shift_length = st.number_input(
            "Shift Length (minutes)", 
            value=480,
            help="The total duration of the shift from start to finish, e.g., 8 hours = 480 minutes."
        )
        breaks = st.number_input(
            "Planned Breaks (minutes)", 
            value=60,
            help="Time for scheduled breaks, lunches, and planned team meetings. This time is excluded from the OEE calculation base."
        )
        unplanned_downtime = st.number_input(
            "Unplanned Downtime (minutes)", 
            value=47, 
            help="All time the machine was scheduled to run but did not, e.g., breakdowns, material shortages, setup/changeover time. This directly impacts the 'Availability' score."
        )
        ideal_cycle_time = st.number_input(
            "Ideal Cycle Time (seconds/unit)", 
            value=60.0, 
            format="%.1f",
            help="The theoretical fastest possible time to produce one unit, as specified by the equipment manufacturer. This is the benchmark for 'Performance'."
        )
        total_units = st.number_input(
            "Total Units Produced", 
            value=300,
            help="The total count of all units produced during the shift, including both good parts and defects."
        )
        good_units = st.number_input(
            "Good Units Produced (no defects)", 
            value=285,
            help="The count of units that passed all quality checks and did not require any rework. This directly impacts the 'Quality' score."
        )

    # OEE Calculations
    planned_production_time = shift_length - breaks
    run_time = planned_production_time - unplanned_downtime
    
    availability = run_time / planned_production_time if planned_production_time > 0 else 0
    performance = (ideal_cycle_time / 60 * total_units) / run_time if run_time > 0 else 0
    quality = good_units / total_units if total_units > 0 else 0
    oee = availability * performance * quality
    
    st.header("OEE Performance Dashboard")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Availability", f"{availability:.1%}", help="Run Time / Planned Time. Measures downtime losses.")
    col2.metric("Performance", f"{performance:.1%}", help="Actual Output / Potential Output. Measures speed losses.")
    col3.metric("Quality", f"{quality:.1%}", help="Good Units / Total Units. Measures defect losses.")
    
    with col4:
        st.metric("Overall OEE", f"{oee:.1%}")
        if oee >= 0.85: st.success("World Class")
        elif oee >= 0.60: st.warning("Typical")
        else: st.error("Needs Improvement")
        
    fig = plot_oee_breakdown(availability, performance, quality, oee)
    st.plotly_chart(fig, use_container_width=True)

    st.divider()
    st.subheader("Deeper Dive into OEE")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã The Six Big Losses", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **Interpreting the Waterfall Chart:**
        The chart provides a clear, top-down view of where your factory's potential is being lost.
        1.  **Initial Capacity (100%):** This represents the total time available.
        2.  **Availability Loss:** This is the first major drop, caused by all planned and unplanned stops (downtime). This is your biggest lever for improvement.
        3.  **Performance Loss:** This second drop represents the machine not running at its ideal top speed (slow cycles) and minor stops.
        4.  **Quality Loss:** The final drop represents the time wasted producing defective parts that must be scrapped or reworked.
        5.  **Valuable Time (OEE):** The final green bar is the percentage of the total shift time that was truly productive‚Äîmaking good parts, as fast as possible, with no downtime.

        **The Strategic Insight:** OEE exposes the fallacy of "keeping the machine busy." A machine can be running 100% of the time but have a terrible OEE if it's running slowly or producing a high number of defects. OEE forces a holistic view of performance and focuses improvement efforts on the largest sources of loss.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: Unlocking the "Hidden Factory"

        #### The Problem: The "We Need More Capacity" Fallacy
        A manufacturing site is struggling to meet growing customer demand. The machines are running 24/7, and the team feels they are at maximum capacity. Management's default solution is a massive, multi-million dollar capital expenditure project to buy new equipment and expand the facility.

        #### The Impact: Wasted Capital and Masked Inefficiencies
        This "buy more" approach is often a costly mistake that masks deep, unresolved operational problems.
        - **Massive, Unnecessary Capital Expenditure:** The company spends millions on new equipment and facilities it may not have actually needed, a huge drain on capital that could have been used for R&D or other growth initiatives.
        - **Duplicating Bad Processes:** The new production line is a copy of the old, inefficient one. The company has simply scaled its existing waste, doubling down on its "hidden factory."
        - **Increased Operating Costs:** The larger facility now has higher overhead, maintenance, and labor costs, but with the same underlying low efficiency, leading to reduced profitability.

        #### The Solution: A Data-Driven Hunt for Hidden Capacity
        Overall Equipment Effectiveness (OEE) is the diagnostic tool that **uncovers the "hidden factory"**‚Äîthe massive amount of productive capacity that is already present but is being lost to downtime, slow cycles, and defects. OEE provides a single, powerful metric that quantifies this loss and a framework (the Six Big Losses) for systematically eliminating it. It shifts the conversation from "How can we buy more capacity?" to "How can we unlock the capacity we already own?"

        #### The Consequences: Increased Output Without Increased Capital
        - **Without This:** The company is trapped in a cycle of expensive capital projects and is blind to the massive potential locked within its existing assets.
        - **With This:** OEE provides the **data-driven business case** to focus on process improvement instead of capital expansion. By systematically improving OEE, a company can often **increase its true output by 20-50% using the exact same equipment**. This is a massive, high-ROI win. It avoids unnecessary capital expenditure, frees up cash flow, and forces the organization to achieve a state of true operational excellence before scaling, creating a leaner, more profitable, and more competitive business.
        """)

    with tabs[2]:
        st.markdown("""
        OEE provides the top-level metric, but the "Six Big Losses" provide the specific targets for improvement. They are the root causes of the losses seen in the waterfall chart.
        
        **1. Availability Losses**
        - **Breakdowns:** Unplanned equipment failures and downtime.
        - **Setup and Adjustments:** Time lost during changeovers from one product to another. This is often the largest single source of downtime.
        
        **2. Performance Losses**
        - **Minor Stops & Idling:** Short stops that don't require maintenance but disrupt the flow (e.g., a jammed sensor, a misfeed).
        - **Reduced Speed:** The equipment is run at a speed lower than its ideal, designed cycle time.
        
        **3. Quality Losses**
        - **Production Rejects:** Defects and scrap produced during steady-state production.
        - **Startup Rejects:** Defects and scrap produced during warm-up or after a changeover, before the process has stabilized.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Gaming the Metric" Fallacy**
A site manager, under pressure to improve their OEE score, decides to "help" the numbers. They reclassify major breakdowns as "planned maintenance" to boost their Availability score. They lower the "Ideal Cycle Time" in the system to boost their Performance score.
- **The Flaw:** This is a classic case of "you get what you measure." The manager has improved the score but has done nothing to improve the underlying process. They are lying with statistics, and the real-world problems of downtime and slow cycles remain, continuing to cost the company money.""")
        st.success("""üü¢ **THE GOLDEN RULE: Be Brutally Honest About the Losses**
The value of OEE comes from its unflinching honesty. The goal is not to achieve a "good score," but to have an **accurate score** that correctly identifies the biggest opportunities.
1.  **Define Standards Rigorously:** The definitions of "downtime," "ideal cycle time," and "defect" must be standardized and applied consistently.
2.  **Focus on the Losses, Not the Score:** The OEE number is just the headline. The real value is in the **waterfall chart and the Six Big Losses**, which provide the detailed, actionable roadmap for improvement.
3.  **Empower the Operators:** The people running the machine are the ones who know the true causes of minor stops and slow cycles. An effective OEE program engages them directly in identifying and eliminating these losses.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: The Birth of Total Productive Maintenance
        The concept of OEE was developed by **Seiichi Nakajima** in the 1960s as a central component of his revolutionary **Total Productive Maintenance (TPM)** methodology.
        
        **The Problem:** In post-war Japan, companies could not afford the massive, capital-intensive factories of their Western counterparts. They had to get the absolute maximum productivity out of every single piece of equipment they owned. The traditional separation between "Production" (who ran the machines) and "Maintenance" (who fixed them when they broke) was seen as a major source of inefficiency.
        
        **The 'Aha!' Moment:** Nakajima's insight was that maintenance should not be a reactive, separate activity. He proposed a holistic, team-based approach where operators themselves were empowered and trained to perform routine maintenance, cleaning, and inspection of their own equipment. The goal was to move from "I run, you fix" to "we are all responsible for the health of the machine."
        
        **The Impact:** To measure the success of this new TPM philosophy, Nakajima needed a single, powerful metric that captured all aspects of equipment performance. He created **OEE**. OEE was the perfect KPI because it combined the three distinct perspectives‚ÄîMaintenance (Availability), Engineering (Performance), and Quality‚Äîinto one number. It became the universal language for measuring and improving equipment productivity and is now the global standard for any serious manufacturing operation.
        """)
        
    with tabs[5]:
        st.markdown("""
        OEE is a key performance indicator that supports the goals of a modern Pharmaceutical Quality System and the principles of continuous improvement.
        - **ICH Q10 - Pharmaceutical Quality System:** Section 3.2.1, "Process Performance and Product Quality Monitoring System," requires a system to "provide for control and identify areas for continual improvement." OEE is a best-in-class metric for this purpose. A negative trend in OEE would be a clear signal to investigate.
        - **FDA Process Validation Guidance (Stage 3 - CPV):** An effective Continued Process Verification program should monitor key performance indicators. OEE provides a holistic measure of the health of the equipment, which is a critical component of the overall process.
        - **FDA Guidance: "Pharmaceutical cGMPs for the 21st Century":** This initiative explicitly encourages the adoption of modern quality management techniques from other industries. OEE, born from the Toyota Production System, is a prime example of such a technique that enhances efficiency and control.
        """)

#======================================= 1. PROCESS CONTROL PLAN BUILDER  ============================================================================
def render_control_plan_builder():
    """Renders the comprehensive, interactive module for a Process Control Plan."""
    st.markdown("""
    #### Purpose & Application: The Operational Playbook
    **Purpose:** To create the **Operational Playbook for Process Control.** A Control Plan is a formal, living document that links a process step to its critical parameters (CPPs), the measurement method, the specification, the sample size/frequency, the control chart used, and, most critically, the **Out-of-Control Action Plan (OCAP)**.
    
    **Strategic Application:** This tool bridges the gap between the statistical analysis performed by engineers and the on-the-floor reality of operators. It translates the outputs of your validation studies (like CPPs from a DOE and control limits from an SPC chart) into a single, clear, and actionable document that becomes the standard operating procedure for maintaining a process in a validated state.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Process Steward for a drug product.
    1.  Choose a **Critical Process Parameter (CPP)** from a real manufacturing process.
    2.  Use the **Control Strategy** inputs in the sidebar to define how this CPP will be monitored.
    3.  Observe the **Simulated SPC Chart** in real-time. Notice how a higher sampling frequency leads to faster detection of the process shift.
    """)

    cpp_options = {
        "Granulation Moisture (%)": {'lsl': 2.0, 'usl': 5.0, 'method': 'NIR Spectroscopy'},
        "Tablet Hardness (kp)": {'lsl': 10, 'usl': 15, 'method': 'Hardness Tester'},
        "Bioreactor DO (%)": {'lsl': 30, 'usl': 60, 'method': 'DO Probe'}
    }
    cpp_choice = st.selectbox("Select a Critical Process Parameter (CPP) to Control:", list(cpp_options.keys()))
    cpp_data = {'name': cpp_choice, **cpp_options[cpp_choice]}
    
    with st.sidebar:
        st.subheader("Control Strategy Builder")
        sample_size = st.slider("Sample Size (n)", 1, 10, 3, 1, help="How many samples are taken at each measurement point? A larger n increases confidence but also cost.")
        frequency = st.select_slider("Sampling Frequency", ["Once per batch", "Once per hour", "Every 15 minutes"], value="Once per hour",
            help="How often is the CPP measured? More frequent sampling detects shifts faster but has higher operational and testing costs.")
        spc_tool = st.selectbox("SPC Chart to Use", ["I-MR Chart", "Xbar-R Chart", "EWMA Chart"],
            help="The specific statistical control chart that will be used to monitor this CPP. Choose EWMA for higher sensitivity to small shifts.")

    fig_table, fig_spc, fig_flowchart = plot_control_plan_dashboard(cpp_data, sample_size, frequency, spc_tool)
    
    st.header("Control Strategy Dashboard")
    st.plotly_chart(fig_table, use_container_width=True)
    
    col1, col2 = st.columns(2)
    with col1:
        st.plotly_chart(fig_spc, use_container_width=True)
    with col2:
        st.plotly_chart(fig_flowchart, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **Interpreting the Dashboard:**
        - **Control Plan Table:** This is the formal, auditable document. It contains all the critical information for controlling the process parameter in a single place. In a real GMP environment, this would be a controlled document that links directly to SOPs and batch records.
        - **Simulated SPC Chart:** This chart provides a crucial "what-if" analysis of your chosen control strategy. A small process shift is introduced at batch #15. The purple line shows the predicted detection time. **Try changing the 'Sampling Frequency' in the sidebar from 'Once per batch' to 'Every 15 minutes' and watch the purple line move to the left, showing how faster sampling leads to faster detection.**
        - **OCAP Flowchart:** This is the user-friendly guide for the operator on the manufacturing floor. It provides a simple, unambiguous, visual decision tree for exactly what to do when a process alarm occurs.
        
        **The Strategic Insight:** The Control Plan is a balancing act between **risk and cost**. More frequent sampling provides better process control and faster deviation detection, which reduces the risk of producing non-conforming material. However, it also increases laboratory costs and operational complexity. This dashboard allows you to simulate and visualize this trade-off to find the optimal strategy that provides the necessary level of quality assurance without overburdening the system.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The "Rosetta Stone" for the Factory Floor
    
        #### The Problem: The Knowledge Gap
        A team of PhD scientists and engineers spends two years and millions of dollars on a comprehensive Quality by Design (QbD) program. They create a mountain of sophisticated validation reports, DOE analyses, and risk assessments that perfectly characterize a new process. This knowledge lives in a complex, 500-page regulatory submission document. Meanwhile, on the 24/7 manufacturing floor, an operator with a high school diploma is responsible for running this multi-million dollar process.
    
        #### The Impact: Process Knowledge That Never Reaches the Process
        This gap between the "knowledge workers" and the "operational workers" is a massive source of unmanaged risk and inefficiency.
        - **Inconsistent Responses:** An SPC chart alarms on the floor. The operator, unsure of the correct procedure, calls a supervisor, who calls an engineer. The response is slow, inconsistent, and depends entirely on which engineer is on call.
        - **"Tribal Knowledge":** The "real" way to run the process is passed down verbally from one operator to the next, with each person adding their own undocumented "tweaks." The process slowly drifts away from its validated state.
        - **Audit Failure:** During an audit, an inspector asks an operator what they would do if a specific alarm occurred. If the operator cannot give a clear, confident answer that matches a formal procedure, this is a major finding of inadequate training and process control.
    
        #### The Solution: A Single, Actionable Playbook
        The Control Plan is the **Rosetta Stone** that translates the complex knowledge from the validation package into a single, simple, and actionable playbook for the factory floor. It is the definitive, one-page summary that answers the five critical questions for any operator:
        1.  **What** do I need to control? (`Parameter`)
        2.  **How** do I measure it? (`Measurement System`)
        3.  **What** should the result be? (`Specification`)
        4.  **How often** do I need to check it? (`Sample Size / Freq.`)
        5.  **What do I do if it's wrong?** (`Reaction Plan / OCAP`)
    
        #### The Consequences: Empowered Operators and a State of Control
        - **Without This:** The millions invested in process understanding are wasted because that knowledge never translates into consistent daily execution. The process is vulnerable to human error and inconsistency.
        - **With This:** The Control Plan and its visual counterpart, the OCAP, **empower the operator** to become the first line of defense for quality. It ensures a **rapid, consistent, and compliant response** to any process deviation, 24/7. It is the essential tool that transforms the theoretical "validated state" into a living, breathing, and consistently executed reality on the manufacturing floor.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Control Strategy Terms
        - **Control Plan:** A written description of the systems and processes required for controlling a process or product. It is a living document that is updated as the process matures.
        - **Control Strategy (ICH Q10):** A planned set of controls, derived from product and process understanding, that assures process performance and product quality. It encompasses raw material specifications, process parameter controls, facility conditions, and final product testing.
        - **OCAP (Out-of-Control Action Plan):** A flowchart or documented procedure that prescribes the sequence of actions an operator must take in response to an out-of-control signal from an SPC chart. It is a critical tool for ensuring consistent and compliant responses to deviations.
        - **CPP (Critical Process Parameter):** A process parameter whose variability has an impact on a CQA and therefore must be monitored and controlled to ensure the process produces the desired quality.
        - **In-Process Control (IPC):** Checks performed during production in order to monitor and, if necessary, to adjust the process to ensure that the product conforms to its specification.
        - **Specification:** A list of detailed requirements with which the products or materials used or obtained during manufacture have to conform.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Trust the Engineers" Fallacy**
Engineers install a new SPC system that monitors 50 parameters in real-time. When a chart alarms, the operator is unsure which alarms are important and what to do, so they call the busy engineer, who may not be immediately available. By the time the engineer arrives, significant non-conforming material may have been produced.
- **The Flaw:** The system provides data but no **actionable information** for the operator. The lack of a clear Control Plan and OCAP creates confusion, delays, and significant compliance risk on the manufacturing floor.""")
        st.success("""üü¢ **THE GOLDEN RULE: If It Can Alarm, It Must Have a Plan**
A compliant and effective control strategy requires that every single monitoring chart has a clear, pre-defined, and operator-friendly action plan.
1.  **Link Controls to Risks:** The parameters included in the Control Plan should be directly linked to the high-risk items identified in the **FMEA**. This provides a documented, risk-based rationale for your monitoring strategy.
2.  **Define the Full Strategy:** The Control Plan must specify *all* aspects of the control: the what (CPP), how (Measurement System), how much (Sample Size), how often (Frequency), and who (Operator).
3.  **Create a Simple OCAP:** The OCAP must be a simple, unambiguous flowchart that an operator can follow under pressure without needing to consult an engineer for routine alarms. This empowers operators and ensures consistent, rapid responses to process deviations.""")
        
    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Shewhart's Chart to the Control Plan
        When **Walter Shewhart** invented the control chart in the 1920s, his primary focus was on the statistical tool for *detection*. The reaction to the signal was largely left to the judgment of the on-site engineer. This worked well in the simpler manufacturing environments of the time.
        
        As manufacturing processes became more complex and quality management systems more formalized, the need for a more structured response became clear. The **automotive industry**, through its development of the **Advanced Product Quality Planning (APQP)** framework in the late 1980s, formalized the **Control Plan** as a key deliverable for any new product. They recognized that a control chart was useless without a documented plan for how it would be used, by whom, and what actions would be taken in response to a signal.
        
        The **Out-of-Control Action Plan (OCAP)** evolved from this as a best practice, translating the formal table of the Control Plan into a simple, visual flowchart that is ideal for use by operators on the factory floor, especially in high-pressure situations. The combination of SPC, the Control Plan, and the OCAP creates a complete, closed-loop system for process control that is a hallmark of a mature quality system.
        """)
        
    with tabs[5]:
        st.markdown("""
        The Control Plan is a key document that demonstrates a state of control and is a primary focus of regulatory audits. It is the practical embodiment of your entire validation effort.
        - **ICH Q10 - Pharmaceutical Quality System:** The Control Plan is the operational embodiment of the **Control Strategy**, which is a central concept in ICH Q10. It is the documented proof that product and process understanding (from QbD and validation) has been successfully translated into effective, routine controls.
        - **FDA Process Validation Guidance (Stage 3 - CPV):** The guidance requires an "ongoing program to collect and analyze product and process data." The Control Plan is the document that formally defines this program, specifying what is monitored, how often, and what to do when a deviation is detected.
        - **21 CFR 211.110 (Sampling and testing of in-process materials and drug products):** This regulation requires written procedures for in-process controls, including "Control procedures shall be established to monitor the output and to validate the performance of those manufacturing processes that may be responsible for causing variability." The Control Plan is this established procedure.
        - **EU Annex 15: Qualification and Validation:** Emphasizes that processes should be monitored during routine production to assure that they remain in a state of control. The Control Plan is the primary document defining this monitoring.
        """)
#======================================= 2. RUN VALIDATION WESTGARD  ============================================================================
def render_multi_rule():
    """Renders the comprehensive, interactive module for Multi-Rule SPC (Westgard Rules)."""
    st.markdown("""
    #### Purpose & Application: The Statistical Detective
    **Purpose:** To serve as a high-sensitivity "security system" for your assay. Instead of one simple alarm, this system uses a combination of rules to detect specific types of problems, catching subtle shifts and drifts long before a catastrophic failure occurs. It dramatically increases the probability of detecting true errors while minimizing false alarms.
    
    **Strategic Application:** This is the global standard for run validation in regulated QC and clinical laboratories. While a basic control chart just looks for "big" errors, the multi-rule system acts as a **statistical detective**, using a toolkit of rules to diagnose different failure modes.
    """)
    
    st.info("""
    **Interactive Demo:** Use the **Process Scenario** radio buttons to simulate common assay failures. The chart will automatically detect and highlight any rule violations. Hover over a red diamond for a detailed diagnosis. Compare the chart to the **Power Functions** plot to understand *why* certain rules are better at catching different types of errors.
    """)
    
    st.sidebar.subheader("Westgard Scenario Controls")
    scenario = st.sidebar.radio(
        "Select a Process Scenario to Simulate:",
        options=('Stable', 'Large Random Error', 'Systematic Shift', 'Increased Imprecision'),
        captions=[
            "A normal, in-control run for reference.",
            "e.g., A single major blunder like a transcription error.",
            "e.g., A new reagent lot causes a persistent bias.",
            "e.g., A faulty pipette causes inconsistency."
        ]
    )
    fig, fig_power, violations = plot_westgard_scenario(scenario=scenario)
    
    # --- Redesigned Layout ---
    col1, col2 = st.columns([0.6, 0.4])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("##### Detected Violations")
        if not violations:
            st.success("No violations detected. Process appears to be in control.")
        else:
            violation_summary = {}
            for point, ruleset in sorted(violations.items()):
                for rule in ruleset.strip().split():
                    if rule not in violation_summary:
                        violation_summary[rule] = []
                    if point + 1 not in violation_summary[rule]:
                        violation_summary[rule].append(point + 1)
            for rule, points in violation_summary.items():
                st.warning(f"**{rule}:** Violated at points {', '.join(map(str, points))}")
        st.markdown("---")
        st.markdown("##### Rule Power Functions")
        st.plotly_chart(fig_power, use_container_width=True)
        st.markdown("""
        **Reading the Power Plot:** This chart shows which rules are best for different problems.
        - The `1-3s` rule is powerful for **large shifts** (>3œÉ) but blind to small ones.
        - Rules like `4-1s` and `10-x` are weak for large shifts but much more powerful for detecting **small, persistent shifts** (<2œÉ).
        This is why a multi-rule system is essential.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The High-Sensitivity "EKG" for Your Laboratory
    
        #### The Problem: The "Silent Failure" in the QC Lab
        A high-throughput QC or clinical laboratory relies on an automated analytical instrument to release product or report patient results. The instrument develops a subtle, systematic bias‚Äîa 1.5-sigma shift caused by a slowly degrading calibrator. A standard Shewhart control chart, designed to only catch large >3-sigma errors, is completely blind to this small shift. The lab continues to operate, unaware that every result it is reporting is now incorrect.
    
        #### The Impact: The High Cost of Erroneous Data
        This "silent failure" of an analytical method has severe consequences that ripple throughout the organization.
        - **Incorrect Batch Disposition:** In a pharmaceutical QC lab, this small bias could be the difference between a batch passing or failing. The company may be **incorrectly releasing out-of-spec product** or **incorrectly scrapping good product**, leading to direct financial loss and significant patient safety risk.
        - **Erroneous Clinical Diagnoses:** In a clinical lab, a 1.5-sigma shift can be the difference between a "normal" and "abnormal" patient result. The lab could be reporting **thousands of incorrect patient results**, leading to misdiagnoses, improper treatment, and severe clinical consequences.
        - **Loss of Confidence and Costly Investigations:** When the problem is eventually discovered, it triggers a massive, painful investigation. The lab must determine how long the problem has existed and which results must be retracted and re-tested, a catastrophic loss of credibility and a huge operational cost.
    
        #### The Solution: A Diagnostic, Multi-Rule "EKG"
        The Westgard Rules are not just another control chart; they are a **high-sensitivity diagnostic system** for your analytical methods. They act like a multi-lead EKG for your assay's heartbeat. Instead of one simple alarm for "catastrophic failure," this system uses a combination of rules to detect specific types of problems long before they become critical:
        - Rules like **2-2s** and **4-1s** are specifically designed to detect **small, systematic shifts** (like a degrading calibrator).
        - Rules like **R-4s** are designed to detect **increased imprecision** (like a faulty pipette).
    
        #### The Consequences: Proactive Error Detection and Risk Mitigation
        - **Without This:** The QC lab is vulnerable to subtle, "silent failures" that can have a massive downstream impact on the business and patients.
        - **With This:** The Westgard Rules provide an **essential layer of proactive risk mitigation**. They are the early warning system that allows the lab to **detect and correct** small analytical problems before they lead to the release of a single incorrect result. This protects patient safety, ensures the integrity of batch release decisions, and upholds the credibility and compliance of the entire laboratory operation.
        """)
        with tabs[2]:
                st.markdown("""
                ##### Glossary of Westgard Rules
                - **Systematic Error:** A consistent bias in the measurement process (e.g., a miscalibrated instrument). Detected by rules like `2-2s`, `4-1s`, `10-x`.
                - **Random Error:** Unpredictable, random fluctuations in the measurement process (e.g., pipetting variability). Detected by rules like `1-3s` and `R-4s`.
                - **1-3s Rule:** One control measurement exceeds the mean ¬± 3 standard deviations. Rejection rule, sensitive to large errors.
                - **2-2s Rule:** Two consecutive control measurements exceed the same mean ¬± 2 standard deviations. Rejection rule, sensitive to systematic error.
                - **R-4s Rule:** The range between two consecutive control measurements exceeds 4 standard deviations. Rejection rule, sensitive to random error.
                - **4-1s Rule:** Four consecutive control measurements exceed the same mean ¬± 1 standard deviation. Rejection rule, sensitive to small systematic shifts.
                - **10-x Rule:** Ten consecutive control measurements fall on the same side of the mean. Rejection rule, very sensitive to small systematic shifts.
                """)
        with tabs[3]:
            st.error("""üî¥ **THE INCORRECT APPROACH: The "Re-run & Pray" Mentality**
This operator sees any alarm, immediately discards the run, and starts over without thinking.
- They don't use the specific rule (`2-2s` vs `R-4s`) to guide their troubleshooting.
- They might engage in "testing into compliance" by re-running a control until it passes, a serious compliance violation.""")
            st.success("""üü¢ **THE GOLDEN RULE: The Rule is the First Clue**
The goal is to treat the specific rule violation as the starting point of a targeted investigation.
- **Think like a detective:** "The chart shows a `2-2s` violation. This suggests a systematic shift. I should check my calibrators and reagents first, not my pipetting technique."
- **Document Everything:** The investigation, the root cause, and the corrective action for each rule violation must be documented.""")

        with tabs[4]:
            st.markdown("""
            #### Historical Context: From the Factory Floor to the Hospital Bed
            **The Problem:** In the 1970s, clinical laboratories were becoming highly automated, but their quality control methods hadn't kept up. They were using Shewhart's simple `1-3s` rule, designed for manufacturing. However, in a clinical setting, the cost of a missed error (a misdiagnosis) is infinitely higher than the cost of a false alarm (re-running a control). The `1-3s` rule was not sensitive enough to catch the small but medically significant drifts that could occur with automated analyzers.

            **The 'Aha!' Moment:** **Dr. James O. Westgard**, a professor of clinical chemistry, recognized this dangerous gap. He realized that a single rule was a blunt instrument. Instead, he proposed using a *combination* of rules, like a series of increasingly fine filters. A "warning" rule (like `1-2s`) could trigger a check of more stringent rejection rules. 
            
            **The Impact:** In his 1981 paper, Westgard introduced his multi-rule system. It was a paradigm shift for clinical QC. It gave laboratorians a logical, flowchart-based system that dramatically increased the probability of detecting true errors while keeping the false alarm rate manageable. The "Westgard Rules" became the de facto global standard for run validation in medical labs, directly improving the quality of diagnostic data and patient safety worldwide.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("The logic is built on the properties of the normal distribution and the probability of rare events. For a stable process:")
            st.markdown("- A point outside **¬±3œÉ** is a rare event. The probability of a single point falling outside these limits by chance is very low (p ‚âà 0.0027). This makes the **1-3s** rule a high-confidence signal of a major error.")
            st.markdown("- A point outside **¬±2œÉ** is more common (p ‚âà 0.0455). Seeing one is not a strong signal. However, the probability of seeing *two consecutive points* on the same side of the mean purely by chance is much, much lower:")
            st.latex(r"P(\text{2-2s}) \approx \left( \frac{0.0455}{2} \right)^2 \approx 0.0005")
            st.markdown("This makes the **2-2s** rule a powerful and specific detector of systematic shifts with a very low false alarm rate, even though the individual points themselves are not extreme.")
        with tabs[5]:
            st.markdown("""
            Westgard Rules are the de facto standard for routine QC run validation in clinical and diagnostic laboratories, and their principles are widely adopted in pharmaceutical QC.
            - **CLIA (Clinical Laboratory Improvement Amendments):** US federal regulations that require clinical laboratories to monitor the accuracy and precision of their testing. Westgard Rules provide a compliant framework for this.
            - **ISO 15189:** The international quality standard for medical laboratories, which requires robust internal quality control procedures.
            - **USP General Chapter <1010> - Analytical Data:** Discusses the treatment of analytical data and the principles of statistical control, for which multi-rule systems are a best practice.
            """)

#======================================= 3. SMALL DRIFT DETECTION  ============================================================================
def render_ewma_cusum():
    """Renders the comprehensive, interactive module for Small Shift Detection (EWMA/CUSUM)."""
    st.markdown("""
    #### Purpose & Application: The Process Sentinel
    **Purpose:** To deploy a high-sensitivity monitoring system designed to detect small, sustained shifts in a process mean that would be invisible to a standard Shewhart control chart (like an I-MR or X-bar chart). These charts have "memory," accumulating evidence from past data to find subtle signals.

    **Strategic Application:** This is an essential "second layer" of process monitoring for mature, stable processes where large, sudden failures are rare, but slow, gradual drifts are a significant risk.
    - **üî¨ EWMA (The Sentinel):** A robust, general-purpose tool that smoothly weights past observations, excellent for detecting the onset of a gradual drift.
    - **üêï CUSUM (The Bloodhound):** A specialized, high-power tool that is the fastest possible detector for a shift of a specific, pre-defined magnitude.
    """)
    
    st.info("""
    **Interactive Demo:** Select a **Failure Scenario** and a **Shift Size**. Observe how quickly each chart (marked with a red 'X') detects the problem after the "Process Change Begins" line. For small shifts and gradual drifts, notice how the I-Chart often fails to detect the issue at all.
    """)
    
    with st.sidebar:
        st.sidebar.subheader("Small Shift Detection Controls")
        scenario_slider = st.sidebar.radio(
            "Select Failure Scenario:",
            ('Sudden Shift', 'Gradual Drift'),
            captions=["An abrupt change in the process mean.", "A slow, creeping change over time."]
        )
        shift_size_slider = st.sidebar.slider(
            "Select Process Shift Size (in multiples of œÉ):",
            min_value=0.25, max_value=3.5, value=0.75, step=0.25,
            help="Controls the magnitude of the process shift. Small shifts are much harder for standard charts to detect."
        )

    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        fig, i_time, ewma_time, cusum_time = plot_ewma_cusum_comparison(
            shift_size=shift_size_slider,
            scenario=scenario_slider
        )
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.markdown(f"##### Detection Performance for a **{shift_size_slider}œÉ** {scenario_slider}")
            st.metric(
                label="I-Chart: Time to Detect",
                value=f"{int(i_time)} points" if not np.isnan(i_time) else "Not Detected",
                help="Number of points after the shift began before the I-Chart alarmed."
            )
            st.metric(
                label="EWMA: Time to Detect",
                value=f"{int(ewma_time)} points" if not np.isnan(ewma_time) else "Not Detected",
                help="Number of points after the shift began before the EWMA chart alarmed."
            )
            st.metric(
                label="CUSUM: Time to Detect",
                value=f"{int(cusum_time)} points" if not np.isnan(cusum_time) else "Not Detected",
                help="Number of points after the shift began before the CUSUM chart alarmed."
            )
        
            st.markdown("""
            **The Core Insight:**
            Try simulating a small (`< 1.5œÉ`) **Gradual Drift**. The I-Chart is completely blind, giving a false sense of security. The EWMA and CUSUM charts, because they have memory, accumulate the small signals over time and reliably sound the alarm. This demonstrates why relying only on Shewhart charts creates a significant blind spot for modern, high-precision processes.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: The High Cost of "Drifting into Failure"
        
            #### The Problem: The "Boiling Frog" Syndrome
            A high-volume, continuous manufacturing process is monitored using traditional Shewhart SPC charts. A critical component in the process begins to wear, causing a very small, gradual drift in a key quality attribute‚Äîa shift of only 0.75 sigma. This shift is so small that it is completely hidden within the normal "common cause" variation. The Shewhart chart, which is memoryless and designed to ignore small fluctuations, sees nothing wrong.
        
            #### The Impact: The "Death by a Thousand Cuts"
            The process is like a frog in a slowly heating pot of water; it is drifting towards a catastrophic failure, but the alarm system is silent.
            - **Massive Scrap Events:** For weeks, the process produces material that is technically "in-spec" but is trending ever closer to the specification limit. Suddenly, a normal random fluctuation is enough to push a large volume of product out of spec. This results in a massive, multi-million dollar scrap event that appears to have happened "all at once," but was actually the culmination of a long, undetected drift.
            - **Reduced Process Capability:** The undetected drift erodes the process's Cpk, reducing its robustness and making it more susceptible to failure from normal input variations.
            - **Reactive Firefighting:** When the failure finally occurs, the investigation is a fire drill. Because there were no early warnings, the team has no leading indicators to guide their root cause analysis, leading to extended downtime and lost production.
        
            #### The Solution: A Monitoring System with "Memory"
            EWMA and CUSUM charts are the solution to the "boiling frog" problem. Unlike Shewhart charts, they are designed with **statistical memory**. They are intentionally designed to detect the *signal* of a small, persistent drift that is buried in the *noise* of common cause variation. They accumulate the evidence of these small deviations over time, like a detective building a case, until the evidence is strong enough to sound a definitive, early alarm.
        
            #### The Consequences: Proactive Control and Maximized Uptime
            - **Without This:** The company is vulnerable to large, unexpected scrap events and is stuck in a reactive mode, only fixing problems after they have become catastrophic.
            - **With This:** EWMA and CUSUM charts provide an **essential early warning system** for modern, high-precision processes. They allow engineers to **detect and correct** small drifts long before they result in out-of-spec product. This **prevents major scrap events**, maintains a high level of process capability, maximizes equipment uptime, and shifts the operational culture from reactive firefighting to proactive, data-driven control.
            """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Small Shift Terms
            - **Shewhart Chart (e.g., I-Chart):** A "memoryless" control chart that evaluates each data point independently. It is excellent for detecting large shifts but insensitive to small, gradual drifts.
            - **EWMA (Exponentially Weighted Moving Average):** A "memory-based" chart that computes a weighted average of all past and current observations. The weights decay exponentially over time.
            - **CUSUM (Cumulative Sum):** A "memory-based" chart that plots the cumulative sum of deviations from a target. It is the fastest possible chart for detecting a shift of a specific, pre-defined magnitude.
            - **Lambda (Œª):** The weighting parameter for an EWMA chart (0 < Œª ‚â§ 1). A small Œª gives the chart a long memory, making it sensitive to tiny shifts.
            - **ARL (Average Run Length):** The average number of points that will be plotted on a control chart before an out-of-control signal occurs.
            """)
        with tabs[3]:
            st.error("""üî¥ **THE INCORRECT APPROACH: The "One-Chart-Fits-All Fallacy"**
A manager insists on using only I-MR charts for everything because they are easy to understand.
- They miss a slow 1-sigma drift for weeks, producing tons of near-spec material.
- When a batch finally fails, they are shocked and have no leading indicators to explain why. They have been flying blind.""")
            st.success("""üü¢ **THE GOLDEN RULE: Layer Your Statistical Defenses**
The goal is to use a combination of charts to create a comprehensive security system.
- **Use Shewhart Charts (I-MR, X-bar) as your front-line "Beat Cops":** They are unmatched for detecting large, sudden special causes.
- **Use EWMA or CUSUM as your "Sentinels":** Deploy them alongside Shewhart charts to stand guard against the silent, creeping threats that the beat cops will miss.
This layered approach provides a complete picture of process stability.""")

        with tabs[4]:
            st.markdown(r"""
            #### Historical Context: The Second Generation of SPC
            **The Problem:** Dr. Walter Shewhart's control charts of the 1920s were a monumental success. However, they were designed like a **smoke detector**‚Äîbrilliantly effective at detecting large, sudden events ("fires"), but intentionally insensitive to small, slow changes to avoid overreaction to random noise. By the 1950s, industries like chemistry and electronics required higher precision. The critical challenge was no longer just preventing large breakdowns, but detecting subtle, gradual drifts that could slowly degrade quality. A new kind of sensor was needed.

            **The 'Aha!' Moment (CUSUM - 1954):** The first breakthrough came from British statistician **E. S. Page**. Inspired by **sequential analysis** from WWII munitions testing, he realized that instead of looking at each data point in isolation, he could **accumulate the evidence** of small deviations over time. The Cumulative Sum (CUSUM) chart was born. It acts like a **bloodhound on a trail**, ignoring random noise by using a "slack" parameter `k`, but rapidly accumulating the signal once it detects a persistent scent in one direction.

            **The 'Aha!' Moment (EWMA - 1959):** Five years later, **S. W. Roberts** of Bell Labs proposed a more flexible alternative, inspired by **time series forecasting**. The Exponentially Weighted Moving Average (EWMA) chart acts like a **sentinel with a memory**. It gives the most weight to the most recent data point, a little less to the one before, and so on, with the influence of old data decaying exponentially. This creates a smooth, sensitive trend line that effectively filters out noise while quickly reacting to the beginning of a real drift.

            **The Impact:** These two inventions were not replacements for Shewhart's charts but essential complements. They gave engineers the sensitive, memory-based tools they needed to manage the increasingly precise and complex manufacturing processes of the late 20th century.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("The elegance of these charts lies in their simple, recursive formulas.")
            st.markdown("- **EWMA (Exponentially Weighted Moving Average):**")
            st.latex(r"EWMA_t = \lambda \cdot Y_t + (1-\lambda) \cdot EWMA_{t-1}")
            st.markdown(r"""
            - **`Œª` (lambda):** This is the **memory parameter** (0 < Œª ‚â§ 1). A small `Œª` (e.g., 0.1) creates a chart with a long memory, making it very sensitive to tiny, persistent shifts. A large `Œª` (e.g., 0.4) creates a chart with a short memory, behaving more like a Shewhart chart.
            """)
            st.markdown("- **CUSUM (Cumulative Sum):**")
            st.latex(r"SH_t = \max(0, SH_{t-1} + (Y_t - T) - k)")
            st.markdown(r"""
            - This formula tracks upward shifts (`SH`).
            - **`T`**: The process target or historical mean.
            - **`k`**: The **"slack" or "allowance" parameter**, typically set to half the size of the shift you want to detect quickly (e.g., `k = 0.5œÉ`). This makes the CUSUM chart a highly targeted detector.
            """)
        with tabs[5]:
            st.markdown("""
            These advanced analytical methods are key enablers for modern, data-driven approaches to process monitoring and control, as encouraged by global regulators.
            - **FDA Guidance for Industry - PAT ‚Äî A Framework for Innovative Pharmaceutical Development, Manufacturing, and Quality Assurance:** This tool directly supports the PAT initiative's goal of understanding and controlling manufacturing processes through timely measurements to ensure final product quality.
            - **FDA Process Validation Guidance (Stage 3 - Continued Process Verification):** These advanced methods provide a more powerful way to meet the CPV requirement of continuously monitoring the process to ensure it remains in a state of control.
            - **ICH Q8(R2), Q9, Q10 (QbD Trilogy):** The use of sophisticated models for deep process understanding, real-time monitoring, and risk management is the practical implementation of the principles outlined in these guidelines.
            - **21 CFR Part 11 / GAMP 5:** If the model is used to make GxP decisions (e.g., real-time release), the underlying software and model must be fully validated as a computerized system.
            """)
#======================================= 4. MULTIVARIATE SPC  ============================================================================
def render_multivariate_spc():
    """Renders the comprehensive, interactive module for Multivariate SPC."""
    st.markdown("""
    #### Purpose & Application: The Process Doctor
    **Purpose:** To monitor the **holistic state of statistical control** for a process with multiple, correlated parameters. Instead of using an array of univariate charts (like individual nurses reading single vital signs), Multivariate SPC (MSPC) acts as the **head physician**, integrating all information into a single, powerful diagnosis.
    
    **Strategic Application:** This is an essential methodology for modern **Process Analytical Technology (PAT)** and real-time process monitoring. In complex systems like bioreactors or chromatography, parameters are interdependent. A small, coordinated deviation‚Äîa "stealth shift"‚Äîcan be invisible to individual charts but represents a significant excursion from the normal operating state. MSPC is designed to detect exactly these events.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Process Engineer. Use the **Process Scenario** radio buttons in the sidebar to simulate different types of multivariate process failures. First, observe the **Process State Space** plot to see how the failure looks visually, then see which **Control Chart (T¬≤ or SPE)** detects the problem, and finally, check the **Contribution Plot** in the 'Key Insights' tab to diagnose the root cause.
    """)

    with st.sidebar:
        st.subheader("Multivariate SPC Controls")
        scenario = st.sidebar.radio(
            "Select a Process Scenario to Simulate:",
            ('Stable', 'Shift in Y Only', 'Correlation Break'),
            captions=["A normal, in-control process.", "A 'stealth shift' in one variable.", "An unprecedented event breaks the model."],
            help="Choose a scenario to see how T¬≤ and SPE charts are sensitive to different types of failures."
        )

    fig_scatter, fig_charts, fig_contrib, t2_ooc, spe_ooc, error_type_str = plot_multivariate_spc(scenario=scenario)
    
    st.plotly_chart(fig_scatter, use_container_width=True)
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig_charts, use_container_width=True)
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üî¨ SME Analysis", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.markdown("""
            **Understanding the Process State Space Plot:**
            The top plot is your map of the process's **Normal Operating Region (NOR)**, defined by the two ellipses.
            -   **Green Ellipse (95% Confidence):** Represents the most common, expected region of operation.
            -   **Red Ellipse (99% Confidence):** This is the multivariate **control limit**. Any point falling outside this ellipse is a statistical signal of a special cause.
            
            The ellipses are stretched diagonally because the process parameters are **correlated**. The shape is determined by the **Mahalanobis distance**, which accounts for this correlation.
        
            ---
            **Analysis of the '{scenario}' Scenario:**
            """.format(scenario=scenario))
        
            if scenario == 'Stable':
                st.success("The process is stable. The monitoring points (black stars) fall within the confidence ellipses, and both the T¬≤ and SPE charts show only normal variation.")
            elif scenario == 'Shift in Y Only':
                st.warning("**Diagnosis: A 'Stealth Shift' has occurred.** The monitoring points have shifted upwards, falling outside the red ellipse. This is detected by the **T¬≤ chart**, which is sensitive to deviations from the process center.")
            elif scenario == 'Correlation Break':
                st.error("**Diagnosis: An Unprecedented Event.** The monitoring points have fallen off the established correlation line. This is detected by the **SPE chart**, which is sensitive to deviations *from* the process model itself.")
            
            if fig_contrib is not None:
                st.markdown("--- \n ##### Root Cause Diagnosis")
                st.plotly_chart(fig_contrib, use_container_width=True)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: The Head Physician for Your Process
        
            #### The Problem: The "Army of Univariate Charts" Fallacy
            A company monitors a complex bioreactor by plastering the control room wall with dozens of individual SPC charts‚Äîone for Temperature, one for pH, one for Dissolved Oxygen, etc. One day, a multi-million dollar batch fails. In the post-mortem investigation, the team is horrified to discover that **every single one of the individual control charts was perfectly in-control** for the entire duration of the run.
        
            #### The Impact: The Catastrophic "Stealth Failure"
            The company has fallen into a massive statistical blind spot.
            - **Failure to See the Big Picture:** The individual charts are like nurses, each monitoring a single vital sign. A patient can have a normal temperature, a normal heart rate, and normal blood pressure, but the *combination* of these values might be highly abnormal and signal an impending heart attack. The "army of charts" has no "head physician" to integrate all the information and see the holistic state of the patient.
            - **Alarm Fatigue:** Conversely, with dozens of charts, false alarms are a constant nuisance. Operators become desensitized and start to ignore the charts altogether, defeating the purpose of SPC.
            - **Unmanaged Risk:** The company has a false sense of security. They believe they have a controlled process, but they are completely blind to multivariate "stealth failures," which are often the most common and dangerous failure modes in complex, correlated systems.
        
            #### The Solution: A Single, Holistic Health Score
            Multivariate SPC (MSPC) is the **head physician for your process**. It uses techniques like Principal Component Analysis (PCA) to learn the normal, correlated "fingerprint" of a healthy process. It then distills dozens or even hundreds of process parameters into just two powerful vital signs:
            1.  **Hotelling's T¬≤:** A single number that measures the distance of the process from its normal operating center, accounting for all correlations.
            2.  **SPE (Squared Prediction Error):** A single number that measures how much the process has deviated from its normal correlation structure, flagging unprecedented events.
        
            #### The Consequences: Early Detection of Complex Failures
            - **Without This:** The company is flying blind, vulnerable to complex failures that their monitoring system is structurally incapable of detecting.
            - **With This:** MSPC provides a **holistic, real-time view of process health**. It is the essential tool for managing modern, complex, and automated processes. It **eliminates alarm fatigue** by replacing dozens of charts with just two. Most critically, it provides the **early warning system needed to detect subtle, coordinated drifts and "stealth failures,"** allowing engineers to intervene and prevent catastrophic batch failures long before they occur.
            """)
        
        with tabs[2]:
            st.markdown("""
            ##### Glossary of MSPC Terms
            - **MSPC (Multivariate SPC):** A method for monitoring the stability of a process with multiple, correlated variables simultaneously.
            - **Mahalanobis Distance:** A measure of the distance between a point and a distribution. Unlike Euclidean distance, it accounts for the correlation between variables, effectively measuring distance in "standard deviations" in a multidimensional space.
            - **Hotelling's T¬≤:** A multivariate statistic that is the square of the Mahalanobis distance. It measures the distance of a point from the center of a data cloud, adjusted for correlation. It is sensitive to shifts *along* the correlation structure of the data.
            - **SPE (Squared Prediction Error):** Also known as DModX. A statistic that measures the distance of a point *from* the PCA model of the process. It is sensitive to new events or a breakdown in the correlation structure.
            - **PCA (Principal Component Analysis):** An unsupervised machine learning technique used to reduce the dimensionality of a dataset while preserving as much variance as possible. It is the engine for building the MSPC model.
            - **Contribution Plot:** A diagnostic plot used to identify which of the original process variables are responsible for a T¬≤ or SPE alarm.
            """)
            
        with tabs[3]:
            st.error("""üî¥ **THE INCORRECT APPROACH: The "Army of Univariate Charts" Fallacy**
Using dozens of individual charts is doomed to fail due to alarm fatigue and its blindness to "stealth shifts." It's like having nurses for each vital sign, but no doctor to interpret the full patient picture. A process can be "out of control" even when every individual parameter is "in control" if their combination is abnormal.""")
            st.success("""üü¢ **THE GOLDEN RULE: Detect with T¬≤/SPE, Diagnose with Contributions**
A robust MSPC program is a two-stage process.
1.  **Stage 1: Detect.** Use the **T¬≤ and SPE charts** as your primary, holistic health monitors to answer the simple question: "Is something wrong with my process?"
2.  **Stage 2: Diagnose.** If either chart alarms, *then* you use **contribution plots** to drill down and identify which of the original process variables are responsible for the signal. This provides a clear, data-driven starting point for the root cause investigation.""")

        with tabs[3]:
            st.markdown("""
            #### Historical Context: The Crisis of Dimensionality
            In the 1930s, statistics was largely a univariate world. Tools like Student's t-test and Shewhart's control charts were brilliant for analyzing one variable at a time. But scientists and economists were facing increasingly complex problems with dozens of correlated measurements. 
            
            **The 'Aha!' Moment (Hotelling):** The creator of this powerful technique was **Harold Hotelling**, one of the giants of 20th-century mathematical statistics. His genius was in generalization. He recognized that the squared t-statistic, $t^2 = (\\bar{x} - \\mu)^2 / (s^2/n)$, was a measure of squared distance, normalized by variance. In a 1931 paper, he introduced the **Hotelling's T-squared statistic**, which replaced the univariate terms with their vector and matrix equivalents. It provided a single number that represented the "distance" of a point from the center of a multivariate distribution, elegantly solving the problem of testing multiple means at once while accounting for all their correlations. The T¬≤ statistic is fundamentally a measure of the **Mahalanobis distance**, which was earlier formulated by P. C. Mahalanobis in 1936.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("- **T¬≤ (Hotelling's T-Squared):** A measure of the **Mahalanobis distance**. It calculates the squared distance of a point `x` from the center of the data `xÃÑ`, but it first 'warps' the space by the inverse of the covariance matrix `S‚Åª¬π` to account for correlations.")
            st.latex(r"T^2 = (\mathbf{x} - \mathbf{\bar{x}})' \mathbf{S}^{-1} (\mathbf{x} - \mathbf{\bar{x}})")
            st.markdown("- **SPE (Squared Prediction Error):** The sum of squared residuals after projecting a data point onto the principal component model of the process. For a new point **x**, it is the squared distance to the PCA model plane.")
            st.latex(r"SPE = || \mathbf{x} - \mathbf{P}\mathbf{P}'\mathbf{x} ||^2")

        with tabs[4]:
            st.markdown("""
            #### SME Analysis: From Raw Data to Actionable Intelligence
            As a Subject Matter Expert (SME), this tool isn't just a data science curiosity; it's a powerful diagnostic and risk-management engine.

            ##### How is this data gathered and what are the parameters?
            The data used by this model is a simplified version of what we collect during **late-stage development and tech transfer validation runs**. Key parameters like `Temperature` and `Pressure` are logged in a LIMS or ELN from instruments.

            ##### How do we interpret the plots and gain insights?
            The true power here is moving from "what happened" to "why it happened."
            -   **Process State Space Plot:** This is our primary visualization of the process's "healthy" state, defined by the ellipses. It immediately shows if a deviation is a simple shift or a more complex breakdown of the process model itself.
            -   **Contribution Plots:** When an alarm sounds, this is our **automated root cause investigation tool**. For a T¬≤ alarm, it points to the variable that has shifted. For an SPE alarm, it points to the variables whose relationship has broken down.

            ##### How would we implement this?
            1.  **Phase 1 (Model Building):** Use data from 20-30 successful, validated runs to build the PCA model and establish the control limits (T¬≤ and SPE).
            2.  **Phase 2 (Monitoring):** Deploy the charts for real-time monitoring as part of Continued Process Verification (CPV).
            3.  **Phase 3 (Automated Triage):** When an alarm triggers, automatically generate the contribution plot and attach it to an electronic alert sent to the process engineer. This dramatically accelerates the investigation.
            """)
            
        with tabs[5]:
            st.markdown("""
            These advanced analytical methods are key enablers for modern, data-driven approaches to process monitoring and control, as encouraged by global regulators.
            - **FDA Guidance for Industry - PAT ‚Äî A Framework for Innovative Pharmaceutical Development, Manufacturing, and Quality Assurance:** This tool directly supports the PAT initiative's goal of understanding and controlling manufacturing processes through timely measurements to ensure final product quality.
            - **FDA Process Validation Guidance (Stage 3 - Continued Process Verification):** These advanced methods provide a more powerful way to meet the CPV requirement of continuously monitoring the process to ensure it remains in a state of control.
            - **ICH Q8(R2), Q9, Q10 (QbD Trilogy):** The use of sophisticated models for deep process understanding, real-time monitoring, and risk management is the practical implementation of the principles outlined in these guidelines.
            - **21 CFR Part 11 / GAMP 5:** If the model is used to make GxP decisions (e.g., real-time release), the underlying software and model must be fully validated as a Computerized System.
            """)
            
#====================================================================== 5. STABILITY ANALYSIS (SHELF LIFE) ACT III ============================================================================
def render_stability_analysis():
    """Renders the module for pharmaceutical stability analysis."""
    st.markdown("""
    #### Purpose & Application: The Expiration Date Contract
    **Purpose:** To fulfill a statistical contract with patients and regulators. This analysis determines the shelf-life for a drug product by proving, with high confidence, that a Critical Quality Attribute (CQA) like potency will remain within specification over time.
    
    **Strategic Application:** This is a mandatory, high-stakes analysis for any commercial pharmaceutical product, as required by the **ICH Q1E guideline**. It is the data-driven foundation of the expiration date printed on every vial and box. An incorrectly calculated shelf-life can lead to ineffective medicine, patient harm, and massive product recalls.
    """)
    
    st.info("""
    **Interactive Demo:** Use the sliders to simulate different stability profiles.
    - **`Batch-to-Batch Variation`**: The most critical parameter. At low values, batches are consistent and can be "pooled". At high values, the slopes differ, the **Poolability test fails (p < 0.25)**, and the pooled model is technically invalid.
    - **`Degradation Rate` & `Assay Variability`**: These control the overall stability and measurement noise, directly impacting the final shelf-life.
    """)
    
    with st.sidebar:
        st.sidebar.subheader("Stability Analysis Controls")
        degradation_slider = st.sidebar.slider("üìâ Mean Degradation Rate (%/month)", -1.0, -0.1, -0.4, 0.05)
        noise_slider = st.sidebar.slider("üé≤ Assay Variability (SD)", 0.2, 2.0, 0.5, 0.1)
        batch_var_slider = st.sidebar.slider("üè≠ Batch-to-Batch Variation (SD)", 0.0, 0.5, 0.1, 0.05,
            help="Controls the variability in starting potency and degradation rate between batches. High values will cause the poolability test to fail.")

    fig, shelf_life, fitted_slope, poolability_p = plot_stability_analysis(
        degradation_rate=degradation_slider,
        noise_sd=noise_slider,
        batch_to_batch_sd=batch_var_slider
    )
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.metric(label="üìà Approved Shelf-Life (from Pooled Model)", value=f"{shelf_life}")
        st.metric(label="üìä Poolability Test (p-value for slopes)", value=f"{poolability_p:.3f}",
                  help="ICH Q1E suggests p > 0.25 to justify pooling batches. A low p-value indicates slopes are significantly different.")
    
        if poolability_p > 0.25:
            st.success("‚úÖ p > 0.25: Batches are statistically similar. Pooling data is justified.")
        else:
            st.error("‚ùå p < 0.25: Batches show different degradation rates. A pooled shelf-life is not appropriate; the worst batch should be considered.")
    
        st.markdown("""
        **Reading the Plot:**
        - **Dashed Lines:** The individual trend line for each batch. If these lines are not parallel, the batches are degrading at different rates.
        - **Black Line:** The average trend across all batches (the pooled model).
        - **Red Dotted Line:** The conservative 95% lower confidence bound on the pooled mean. The shelf-life is where this line crosses the red specification limit.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The Multi-Million Dollar Expiration Date
    
        #### The Problem: The "Best Guess" Expiration Date
        A company needs to set the expiration date for a new, life-saving drug. Without a formal statistical framework, the decision is based on heuristics or a simple linear regression on averaged data. They fail to account for the two most critical sources of uncertainty: the batch-to-batch variability of the manufacturing process and the statistical uncertainty of the degradation trend itself.
    
        #### The Impact: A High-Stakes Gamble with Patient Safety and Profit
        An incorrectly calculated shelf-life is not a minor error; it's a major business and compliance failure.
        - **Patient Safety Risk (Overestimated Shelf-Life):** If the shelf-life is too long, patients could receive a sub-potent or degraded drug, leading to treatment failure and direct harm. This can trigger a catastrophic, multi-million dollar product recall, severe FDA action, and irreparable damage to the company's reputation.
        - **Financial Loss (Underestimated Shelf-Life):** If the shelf-life is too short, the company is forced to discard perfectly good, valuable inventory. For a high-value biologic, shortening the shelf-life by just a few months can result in **tens of millions of dollars in annual write-offs**, directly impacting the company's profitability. It also creates supply chain instability.
    
        #### The Solution: A Statistically Defensible "Contract"
        The ICH Q1E methodology is the solution. It is a formal, statistically rigorous framework for analyzing stability data that is accepted by regulators worldwide. It is not just about fitting a line; it is a disciplined process designed to manage risk:
        1.  **Assess Consistency (Poolability):** First, it forces you to statistically prove that your manufacturing process is consistent by testing if different batches degrade at the same rate.
        2.  **Quantify Uncertainty:** It uses a conservative **95% lower confidence interval** on the degradation trend. This means the calculated shelf-life is not a "best guess," but a high-confidence lower bound that explicitly accounts for statistical uncertainty.
    
        #### The Consequences: A Reliable Product and an Optimized Supply Chain
        - **Without This:** The expiration date is an unsubstantiated guess, exposing the company to significant patient safety and financial risk.
        - **With This:** The stability analysis provides the **objective, statistical evidence** to support a safe, reliable, and compliant expiration date. It is a **binding contract with patients and regulators** that the product will be safe and effective throughout its entire lifecycle. A well-executed study that justifies the longest possible shelf-life provides a major competitive advantage, maximizing revenue, minimizing waste, and ensuring a stable and predictable supply chain.
        """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Stability Terms
            - **Stability Study:** A formal study to determine the time period during which a drug product remains within its established specifications under defined storage conditions.
            - **Shelf-Life (Expiration Date):** The time period during which a drug product is expected to remain within the approved specification for use, if stored under defined conditions.
            - **ANCOVA (Analysis of Covariance):** A statistical test used to compare the slopes of regression lines between different groups (e.g., batches). It is the required test for determining if stability data can be pooled.
            - **Pooling:** The practice of combining data from multiple batches into a single dataset to estimate a common shelf-life. This is only permissible if the batches are statistically shown to be degrading at the same rate.
            - **Confidence Interval on the Mean:** The statistical basis for shelf-life. The shelf-life is the time point where the lower 95% confidence bound for the mean degradation trend line intersects the specification limit.
            """)
        with tabs[3]:
            st.error("""üî¥ **THE INCORRECT APPROACH: The "Blind Pooling" Fallacy**
An analyst takes all stability data, throws it into one regression model, and calculates a shelf-life, without checking if the batches are behaving similarly.
- **The Flaw:** If one batch is a "fast degrader," its behavior will be masked by the better-performing batches. The pooled model will overestimate the shelf-life, creating a significant risk that some batches of product will fail specification long before their printed expiration date.""")
            st.success("""üü¢ **THE GOLDEN RULE: Earn the Right to Pool**
The ICH Q1E guideline is built on a principle of statistical conservatism to protect patients.
1.  **First, Prove Poolability:** Perform a statistical test (ANCOVA) to check for a significant difference between batch slopes. The standard criterion is a **p-value > 0.25** for the interaction term.
2.  **If Poolable:** Combine the data and determine the shelf-life from the pooled model's confidence interval.
3.  **If NOT Poolable:** Analyze batches separately. The overall shelf-life must be based on the shortest shelf-life determined among all the batches.""")

        with tabs[4]:
            st.markdown(r"""
            #### Historical Context: The ICH Revolution
            **The Problem:** Prior to the 1990s, the requirements for stability testing could differ significantly between major markets like the USA, Europe, and Japan. This forced pharmaceutical companies to run slightly different, redundant, and costly stability programs for each region to gain global approval. The lack of a harmonized statistical approach meant that data might be interpreted differently by different agencies, creating regulatory uncertainty.
            
            **The 'Aha!' Moment:** The **International Council for Harmonisation (ICH)** was formed to end this inefficiency. A key working group was tasked with creating a single, scientifically sound standard for stability testing. This resulted in a series of guidelines, with **ICH Q1A** defining the required study conditions and **ICH Q1E ("Evaluation of Stability Data")** providing the definitive statistical methodology.
            
            **The Impact:** ICH Q1E, adopted in 2003, was a landmark guideline. It codified the use of regression analysis, formal statistical tests for pooling data across batches (ANCOVA), and the critical principle of using confidence intervals on the mean trend to determine shelf-life. It created a level playing field and a global gold standard, ensuring that the expiration date on a medicine means the same thing in New York, London, and Tokyo, and that it is backed by rigorous statistical evidence.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("The core of the analysis is the **ANCOVA (Analysis of Covariance)** model, which tests if the slopes differ between batches:")
            st.latex(r"Y_{ij} = \beta_{0} + \alpha_{i} + \beta_{1}X_{ij} + (\alpha\beta)_{i}X_{ij} + \epsilon_{ij}")
            st.markdown("""
            -   `Y·µ¢‚±º`: The CQA for batch `i` at time `j`.
            -   `Œ±·µ¢`: The effect of batch `i` on the intercept.
            -   `Œ≤‚ÇÅ`: The common slope for all batches.
            -   `(Œ±Œ≤)·µ¢`: The **interaction term**, representing the *additional* slope for batch `i`.
            The poolability test is a hypothesis test on the interaction term:
            -   `H‚ÇÄ`: All `(Œ±Œ≤)·µ¢` are zero (all slopes are equal).
            -   `H‚ÇÅ`: At least one `(Œ±Œ≤)·µ¢` is not zero (at least one slope is different).
            If the p-value for this test is > 0.25, we fail to reject H‚ÇÄ and proceed with a simpler, pooled model: `Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œµ`.
            """)
        with tabs[5]:
            st.markdown("""
            Stability analysis and shelf-life determination are governed by a specific set of harmonized international guidelines.
            - **ICH Q1E - Evaluation of Stability Data:** This is the primary global guideline that dictates the statistical methodology for analyzing stability data, including the use of regression analysis, confidence intervals, and the rules for pooling data from different batches.
            - **ICH Q1A(R2) - Stability Testing of New Drug Substances and Products:** Defines the study design, storage conditions, and testing frequency for stability studies.
            - **FDA Guidance for Industry - Q1E Evaluation of Stability Data:** The FDA's adoption and implementation of the ICH guideline.
            """)
#=================================================================== 6. RELIABILITY / SURVIVAL ANALYSIS ACT III ============================================================================
def render_survival_analysis():
    """Renders the module for Survival Analysis."""
    st.markdown("""
    #### Purpose & Application: The Statistician's Crystal Ball
    **Purpose:** To model "time-to-event" data and forecast the probability of survival over time. Its superpower is its unique ability to handle **censored data**-observations where the study ends before the event (e.g., failure or death) occurs. It allows us to use every last drop of information, even from the subjects who "survived" the study.
    
    **Strategic Application:** This is the core methodology for reliability engineering and is essential for predictive maintenance, risk analysis, and clinical research.
    - **Predictive Maintenance:** Instead of replacing parts on a fixed schedule, you can model their failure probability over time, moving from guesswork to a data-driven strategy.
    - **Clinical Trials:** The gold standard for analyzing endpoints like "time to disease progression" or "overall survival."
    """)

    st.info("""
    **Interactive Demo:** Use the sliders in the sidebar to simulate different reliability scenarios.
    - **`Group B Reliability`**: A higher value simulates a more reliable new component. Watch the red curve flatten and separate from the blue curve.
    - **`Censoring Rate`**: A higher rate simulates a shorter study. Notice the uncertainty (shaded confidence intervals) grows wider, and the "At Risk" numbers drop faster.
    """)

    with st.sidebar:
        st.sidebar.subheader("Survival Analysis Controls")
        lifetime_slider = st.sidebar.slider(
            "Group B Reliability (Lifetime Scale)",
            min_value=15, max_value=45, value=30, step=1,
            help="Controls the characteristic lifetime of the 'New Component' (Group B). A higher value means it's more reliable."
        )
        censor_slider = st.sidebar.slider(
            "Censoring Rate (%)",
            min_value=0, max_value=80, value=20, step=5,
            help="The percentage of items that are still 'surviving' when the study ends. Simulates shorter vs. longer studies."
        )
    
    fig, median_a, median_b, p_value = plot_survival_analysis(
        group_b_lifetime=lifetime_slider, 
        censor_rate=censor_slider/100.0
    )
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            st.metric(
                label="Log-Rank Test p-value", 
                value=f"{p_value:.3f}", 
                help="A p-value < 0.05 indicates a statistically significant difference between the survival curves."
            )
            st.metric(
                label="Median Survival (Group A)", 
                value=f"{median_a:.1f} Months" if not np.isnan(median_a) else "Not Reached",
                help="Time at which 50% of Group A have experienced the event."
            )
            st.metric(
                label="Median Survival (Group B)", 
                value=f"{median_b:.1f} Months" if not np.isnan(median_b) else "Not Reached",
                help="Time at which 50% of Group B have experienced the event."
            )
        
            st.markdown("""
            **Reading the Dashboard:**
            - **The Stepped Lines:** The Kaplan-Meier curves show the estimated probability of survival over time.
            - **Shaded Areas:** The 95% confidence intervals. They widen as fewer subjects are "at risk."
            - **Vertical Ticks:** Censored items (e.g., study ended).
            - **"At Risk" Table:** Shows how many subjects are still being followed at each time point. This provides crucial context for the reliability of the curve estimates.
            """)
        
        with tabs[1]:
            st.markdown("""
            ### The Business Case: From Reactive Repairs to Predictive Maintenance
        
            #### The Problem: The "Run-to-Fail" Maintenance Strategy
            A manufacturing facility operates on a reactive maintenance schedule. A critical pump on a multi-million dollar purification skid is run until it breaks. When it fails catastrophically during a production run, the entire line goes down.
        
            #### The Impact: The High Cost of Unplanned Downtime
            This "run-to-fail" strategy is enormously expensive and disruptive.
            - **Catastrophic Production Loss:** The unplanned downtime can last for days while a new part is sourced and installed, resulting in hundreds of thousands of dollars in lost production and potentially causing a critical drug shortage.
            - **Excessive Inventory Costs:** To mitigate this risk, the company is forced to carry a huge, expensive inventory of spare parts for every critical component, tying up millions of dollars in capital.
            - **Inefficient Preventative Maintenance:** The alternative is often a hyper-conservative preventative maintenance (PM) schedule, where parts are replaced on a fixed, frequent schedule "just in case." This is also wasteful, as it involves throwing away perfectly good components that had many months of useful life remaining.
        
            #### The Solution: A Data-Driven Reliability "Crystal Ball"
            Survival Analysis is the statistical engine for **Reliability Engineering and Predictive Maintenance**. It provides a "crystal ball" to forecast the failure probability of a component over time. Its unique ability to handle **censored data** is the key‚Äîit can learn from the components that *haven't* failed yet, which is crucial for predicting the lifetime of reliable parts. This allows the business to answer critical questions:
            1.  **"What is the probability this pump will fail in the next 3 months?"** (Risk Assessment)
            2.  **"Is the new supplier's component (Group B) truly more reliable than the old one (Group A)?"** (Supplier Qualification)
            3.  **"What is the Median Time to Failure for this part?"** (Informing PM schedules and spare part strategy)
        
            #### The Consequences: Maximized Uptime and Optimized Capital
            - **Without This:** Maintenance is a high-stakes guessing game, lurching between the expensive extremes of reactive failure and wasteful over-maintenance.
            - **With This:** Survival Analysis provides the **data-driven foundation for a modern, predictive maintenance program**. It allows the company to move beyond a "one-size-fits-all" PM schedule to a **Condition-Based Maintenance (CBM)** strategy. Critical components are replaced just before their failure probability becomes unacceptably high, **maximizing uptime, minimizing the risk of catastrophic failure, and optimizing capital expenditure** on spare parts inventory. It is a core tool for any world-class manufacturing operation.
            """)
        with tabs[1]:
            st.markdown("""
            ##### Glossary of Survival Terms
            - **Survival Analysis:** A branch of statistics for analyzing the expected duration of time until one or more events happen (e.g., failure of a component, death of a patient).
            - **Time-to-Event Data:** Data that consists of a time measurement until an event of interest occurs.
            - **Censoring:** A key feature of survival data where the event of interest has not occurred for some subjects by the end of the study. Censored data provides valuable information (e.g., "the lifetime is *at least* 24 months").
            - **Kaplan-Meier Estimator:** A non-parametric statistic used to estimate the survival function from lifetime data. It is the standard method for creating survival curves.
            - **Log-Rank Test:** A statistical test used to compare the survival distributions of two or more groups.
            - **Median Survival Time:** The time point at which 50% of the subjects in a group have experienced the event.
            """)
        with tabs[2]:
            st.error("""üî¥ **THE INCORRECT APPROACH: The "Pessimist's Fallacy"**
This is a catastrophic but common error that leads to dangerously biased results.
- An analyst wants to know the average lifetime of a component. They take data from a one-year study, **throw away all the censored data** (the units that were still working at one year), and calculate the average time-to-failure for only the units that broke.
- **The Flaw:** This is a massive pessimistic bias. You have selected **only the weakest items** that failed early and completely ignored the strong, reliable items that were still going strong.""")
            st.success("""üü¢ **THE GOLDEN RULE: Respect the Censored Data**
The core principle of survival analysis is that censored data is not missing data; it is valuable information.
- A tick on the curve at 24 months is not an unknown. It is a powerful piece of information: **The lifetime of this unit is at least 24 months.**
- The correct approach is to **always use a method specifically designed to handle censoring**, like the Kaplan-Meier estimator. This method correctly incorporates the information from both the "failures" and the "survivors" to produce an unbiased estimate of the true survival function.""")

        with tabs[3]:
            st.markdown(r"""
            #### Historical Context: The 1958 Revolution
            **The Problem:** In the mid-20th century, clinical research was booming, but a major statistical hurdle remained. How could you fairly compare two cancer treatments in a trial where, at the end of the study, many patients in both groups were still alive? Or some had moved away and were "lost to follow-up"? Simply comparing the percentage of deaths at the end was inefficient and biased. Researchers needed a way to use the information from every single patient, for the entire duration they were observed.

            **The 'Aha!' Moment:** This all changed in 1958 with a landmark paper in the *Journal of the American Statistical Association* by **Edward L. Kaplan** and **Paul Meier**. Their paper, "Nonparametric Estimation from Incomplete Observations," introduced the world to what we now universally call the **Kaplan-Meier estimator**.
            
            **The Impact:** It was a revolutionary breakthrough. They provided a simple, elegant, and statistically robust non-parametric method to estimate the true survival function, even with heavily censored data. This single technique unlocked a new era of research in medicine, enabling the rigorous analysis of clinical trials that is now standard practice. It also became a cornerstone of industrial reliability engineering, allowing for accurate lifetime predictions of components from studies that end before all components have failed.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("The Kaplan-Meier estimate of the survival function `S(t)` is a product of conditional probabilities, calculated at each distinct event time `t·µ¢`:")
            st.latex(r"S(t_i) = S(t_{i-1}) \times \left( 1 - \frac{d_i}{n_i} \right)")
            st.markdown(r"""
            - **`S(t·µ¢)`** is the probability of surviving past time `t·µ¢`.
            - **`n·µ¢`** is the number of subjects "at risk" (i.e., still surviving and not yet censored) just before time `t·µ¢`.
            - **`d·µ¢`** is the number of events (e.g., failures) that occurred at time `t·µ¢`.
            """)
            
            # --- THIS IS THE CORRECTED BLOCK ---
            st.markdown("The confidence interval for the survival probability is often calculated using **Greenwood's formula**, which estimates the variance of `S(t)`:")
            st.latex(r"\hat{Var}(S(t)) \approx S(t)^2 \sum_{t_i \leq t} \frac{d_i}{n_i(n_i - d_i)}")
        with tabs[4]:
            st.markdown("""
            Survival analysis is the standard methodology for time-to-event data in clinical trials and is also used for reliability engineering in medical devices.
            - **ICH E9 - Statistical Principles for Clinical Trials:** Discusses the appropriate analysis of time-to-event data, including the handling of censored data, for which Kaplan-Meier is the standard non-parametric method.
            - **FDA 21 CFR 820.30 (Design Controls):** For medical devices, design validation requires demonstrating reliability. Survival analysis is used to analyze data from reliability testing to predict the probability of failure over time.
            - **ICH Q1E:** The principles can also be applied to stability data to model the "time to Out-of-Specification (OOS)" event.
            """)
#======================================================================== 7. TIME SERIES ANALYSIS ACT III ============================================================================
# ==============================================================================
# UI RENDERING FUNCTION (Time Series Forecasting Suite) - COMPLETE SME VERSION
# ==============================================================================
def render_time_series_suite():
    """Renders the comprehensive, interactive module for the Time Series Forecasting Suite."""
    st.markdown("""
    #### Purpose & Application: The Modern Forecaster's Toolkit
    **Purpose:** To compare and contrast a suite of powerful time series forecasting models, from classical statistical methods to modern automated libraries. This tool demonstrates that there is no single "best" model; the optimal choice depends entirely on the underlying structure of your data.
    
    **Strategic Application:** This dashboard serves as a decision-making and training tool for anyone involved in demand planning, resource forecasting, or process monitoring. It allows you to simulate different real-world data scenarios (e.g., a sudden trend change, multiple seasonalities) and instantly see which forecasting model performs best, providing a clear rationale for your choice of tool.
    """)
    st.info("""
    **Interactive Demo:** This streamlined suite is now fast and stable.
    1.  Use the **sidebar controls** to configure your desired data scenario.
    2.  Select the specific **Models to Run** to see how they compare.
    3.  The results will now appear almost instantly.
    """)
    
    with st.sidebar:
        st.subheader("Time Series Controls")
        
        models_to_run = st.multiselect(
            "Select Models to Run:",
            options=['Holt-Winters', 'SARIMA', 'ETS'],
            default=['Holt-Winters', 'SARIMA', 'ETS'],
            help="Select which models to fit. These pure-Python models are fast and stable."
        )
        
        trend_type = st.radio("Trend Type", ['Additive', 'Multiplicative'], help="Additive: linear growth. Multiplicative: exponential growth.")
        seasonality_type = st.radio("Seasonality Type", ['None', 'Single (Yearly)']) # Removed Multiple for stability
        noise_level = st.slider("Noise Level (SD)", 1.0, 20.0, 5.0, 1.0)
        changepoint_strength = st.slider("Trend Changepoint Strength", -5.0, 5.0, 0.0, 0.5, help="Simulates an abrupt change in the trend's slope.")

    if not models_to_run:
        st.warning("Please select at least one model to run from the sidebar.")
        return

    # No "Run" button or caching needed, as these models are fast enough.
    fig, mae_scores = plot_forecasting_suite(models_to_run, trend_type, seasonality_type, noise_level, changepoint_strength)

    st.header("Forecasting Suite Dashboard")
    col1, col2 = st.columns([0.65, 0.35])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        st.subheader("Model Performance (MAE)")
        st.markdown("Lower Mean Absolute Error (MAE) is better.")
        
        if mae_scores:
            best_model = min(mae_scores, key=mae_scores.get)
            for name, score in sorted(mae_scores.items(), key=lambda item: item[1]):
                st.markdown(f"**{name}:** `{score:.2f}` {'ü•á' if name == best_model else ''}")
        else:
            st.warning("No models could be successfully fitted to the data.")
            
    st.divider()
    st.subheader("Deeper Dive: Model Selection & Comparison")
    
    # The detailed informational tabs remain unchanged and will display correctly.
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üí° Method Selection Map", "üìä Scoring Table", "üìã Glossary", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    with tabs[0]:
        st.markdown("""
        ### Method Selection Map: A Strategic Decision Framework
        This streamlined suite compares the three dominant philosophies of classical time series forecasting.

        | **Your Primary Goal** | **Recommended Tool** | **Why? (Pros)** | **What to Watch Out For (Cons)** |
        | :--- | :--- | :--- | :--- |
        | **Simplicity and Interpretability:** "I need a fast, clear model of level, trend, and season." | **Holt-Winters** | **The Craftsman:** Directly models the three components you can see, making it the easiest to explain to stakeholders. | **Less Flexible:** It's a specialist tool that assumes a simple, repeating seasonal pattern and can be less accurate than other methods. |
        | **Statistical Rigor & Defensibility:** "I need to model the process's internal 'memory' and prove it in a submission." | **SARIMA** | **The Watchmaker:** The gold standard for statistical formality. Excellent for short-term forecasts on stable processes with strong autocorrelation. | **Requires Expertise & Stationarity:** Difficult to tune the parameters correctly. The need for differencing can make it harder to interpret. |
        | **Automation & Flexibility:** "I need a robust model that can automatically handle different types of trend and seasonality." | **ETS** | **The Automation Specialist:** It automatically selects the best combination of error, trend, and seasonal components (additive or multiplicative), providing a highly flexible and robust forecast. | **More of a 'Black Box':** While the final components are interpretable, the underlying state-space model is more complex than Holt-Winters. |
        """)
        st.warning("**Note:** The `Prophet` and `TBATS` models were removed from this interactive demo due to their high computational and memory requirements, which cause instability in web app environments. The core educational comparison remains.")
    with tabs[1]:
        st.subheader("From Reactive Firefighting to Proactive Control")
        st.markdown("""
        This section outlines the strategic business case for implementing a robust forecasting program within a manufacturing or quality control environment. It moves beyond the technical details to answer the critical question: "Why should we invest in this capability?"
        
        #### The Problem: Operating in the Dark
        Without a formal forecasting capability, an organization is fundamentally reactive. It is "flying blind" and can only respond to events *after* they have occurred. Key decisions about production planning, inventory levels, and resource allocation are based on gut feelings, historical averages, or simple linear extrapolations, which are often wrong.
        
        #### The Impact of the Problem: A Cascade of Hidden Costs
        This reactive state creates a cascade of tangible and intangible costs that are often misdiagnosed as "the cost of doing business":
        - **Operational Inefficiency:** Overproduction of reagents leads to waste; underproduction leads to stock-outs and production delays. Staffing is misaligned with true workload, leading to periods of costly overtime followed by periods of idle time.
        - **Supply Chain Instability:** Inability to accurately forecast demand for a product or consumption of raw materials leads to the "bullwhip effect," causing amplified supply disruptions and expedited shipping costs.
        - **Quality & Compliance Risk:** A gradual, negative trend in a process parameter (like yield or purity) may go unnoticed for months. By the time it's detected, a lengthy, painful investigation is required, and significant amounts of at-risk product may have been produced, leading to deviations and potential regulatory scrutiny.
        
        #### The Solution: A Validated Forecasting Program
        The solution is to implement a structured, multi-stage forecasting program that transforms data from a historical record into a forward-looking strategic asset.
        1.  **Descriptive Phase (Understand the Past):** Use EDA and decomposition to understand the historical patterns, trends, and seasonalities in your key processes.
        2.  **Predictive Phase (Forecast the Future):** Use a competitive suite of models (as shown in this dashboard) to create a statistically validated forecast for key parameters. This includes generating **prediction intervals**‚Äîa high-confidence "cone of uncertainty" where future values are expected to fall.
        3.  **Prescriptive Phase (Drive Better Decisions):** The forecast becomes the basis for action. The prediction intervals can be used as **dynamic control limits** in a Continued Process Verification (CPV) program. A forecast of high future demand can trigger early raw material orders, preventing a future bottleneck.
        
        #### The Consequences: A Tale of Two Futures
        The decision to implement or neglect a forecasting program has profound consequences for the business.

        | Consequence | **‚ùå Without Forecasting (The Reactive World)** | **‚úÖ With Forecasting (The Proactive World)** |
        | :--- | :--- | :--- |
        | **Culture** | Firefighting and crisis management. Decisions are based on anecdote and reaction. | Data-driven and strategic. Decisions are based on forward-looking evidence. |
        | **Costs** | High hidden costs from scrap, rework, expedited freight, and unplanned downtime. | Optimized inventory and resource allocation. Minimized waste and operational delays. |
        | **Compliance** | Frequent deviations from unexplained process drift. Lengthy, reactive root cause investigations. | Early warning of process drifts triggers proactive investigation, preventing deviations. Forecasts provide objective evidence for planning. |
        | **Performance** | Unpredictable production schedules and chronic instability. | Stable, predictable process performance. Improved on-time delivery and supply chain reliability. |
        """)
    with tabs[2]:
        st.markdown("""
        ### Method Selection Map: A Strategic Decision Framework
        Choosing your statistical weapon is the most critical decision in a forecasting study. This is not just a technical choice; it's a strategic one that impacts the reliability, interpretability, and scalability of your results. Use this guide to select and defend your approach based on the specific question you need to answer for your process or business.

        | **Your Data's Primary Feature** | **Recommended Tool** | **Why? (Pros)** | **What to Watch Out For (Cons)** |
        | :--- | :--- | :--- | :--- |
        | **Clean, simple trend and single, regular seasonality.** | **Holt-Winters / ETS** | **The Craftsman:** Highly interpretable, fast, and robust for classic time series data. It directly models the components you can see, making it easy to explain. | **Single Seasonality Only:** Cannot handle multiple overlapping cycles (e.g., weekly and yearly). It's a specialist tool for a specific type of data. |
        | **Strong autocorrelation; need for statistical rigor and defensibility.** | **SARIMA** | **The Watchmaker:** The gold standard for statistical formality. Excellent for short-term forecasts on stable processes where the "memory" of the process is important. Unbeatable for regulatory submissions that require deep statistical justification. | **Requires Expertise:** Difficult to tune the 7+ parameters correctly. The mandatory "stationarity" requirement means you're modeling changes, not absolute values, which can complicate interpretation for business stakeholders. |
        | **Multiple seasonalities, holidays, and trend changes.** | **Prophet** | **The Smartwatch:** Highly automated, robust to messy data, and excels at fitting multiple seasonalities. Its intuitive parameters make it easy to incorporate domain knowledge (e.g., a planned shutdown). | **Less Statistically Formal:** It's a pragmatic engineering tool, not a rigorous statistical model. It can be a "black box" and may not capture complex autocorrelation structures as well as SARIMA. |
        | **Multiple, complex, and non-integer seasonalities (e.g., 5.5-day cycles).** | **TBATS** | **The Music Producer:** The specialist for very complex seasonality. It can decompose signals like a sound engineer, isolating multiple overlapping frequencies. Highly automated. | **Computationally Slow:** Can be the slowest model to fit. The complex combination of components (Box-Cox, Fourier terms, ARMA errors) can be very difficult to interpret and explain. |
        | **Complex, non-linear patterns without clear seasonality; multivariate inputs.** | **Deep Learning (LSTM/TCN)** | **The AI Pattern Recognizer:** Can learn any pattern from sufficient data. Excellent for multivariate forecasting (e.g., predicting yield from temperature, pH, and feed rate simultaneously). | **Requires Huge Data:** Needs much more data than statistical models. It's a "black box" with low interpretability and is computationally expensive to train and validate. |
        """)
    with tabs[3]:
        st.markdown("""
        ### Scoring Table: Model Capabilities at a Glance
        (Scored ‚≠ê to ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê, where ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê is Excellent/Natively Supported)

        | Feature | Holt-Winters | ARIMA | SARIMA | Prophet | TBATS | ETS (MFLES) |
        | :--- | :---: | :---: | :---: | :---: | :---: | :---: |
        | **Interpretability** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
        | <small>*SME Commentary*</small>| <small>Components are intuitive.</small> | <small>Coefficients are clear.</small>| <small>Seasonal terms add complexity.</small>| <small>Decomposition is clear.</small>| <small>Very complex internal state.</small>| <small>Components are intuitive.</small>|
        | **Automation** | ‚≠ê‚≠ê‚≠ê | ‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
        | <small>*SME Commentary*</small>| <small>Smoothing params can be fit.</small> | <small>Requires manual ACF/PACF.</small>| <small>Manual tuning is hard.</small>| <small>Excellent "out of the box."</small>| <small>Fully automated selection.</small>| <small>Automated model selection.</small>|
        | **Speed** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
        | <small>*SME Commentary*</small>| <small>Very fast recursive equations.</small> | <small>Fast for simple models.</small>| <small>Seasonal fitting is slower.</small>| <small>Fast (uses Stan for fitting).</small>| <small>Can be very slow to optimize.</small>| <small>Fast state-space calculation.</small>|
        | **Trend Handling** | Linear | Requires Differencing | Requires Differencing | Piecewise Linear | Flexible | Additive/Multiplicative |
        | <small>*SME Commentary*</small>| <small>Simple linear/damped.</small> | <small>Removes trend to model changes.</small>| <small>Removes trend to model changes.</small>| <small>Auto-detects changepoints.</small>| <small>Can model complex trends.</small>| <small>Can model exponential growth.</small>|
        | **Single Seasonality**| ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | N/A | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
        | <small>*SME Commentary*</small>| <small>Its primary use case.</small> | <small>N/A</small>| <small>Its primary use case.</small>| <small>Handles it easily.</small>| <small>Handles it easily.</small>| <small>Its primary use case.</small>|
        | **Multiple Seasons**| ‚≠ê | ‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê |
        | <small>*SME Commentary*</small>| <small>Not supported.</small> | <small>Not supported.</small>| <small>Not supported.</small>| <small>**Best in class.**</small>| <small>**Best in class.**</small>| <small>Not supported.</small>|
        | **Robustness to Gaps** | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
        | <small>*SME Commentary*</small>| <small>Requires imputation.</small> | <small>Requires imputation.</small>| <small>Requires imputation.</small>| <small>Handles missing data natively.</small>| <small>Can handle some missing data.</small>| <small>Can handle some missing data.</small>|
        | **Exogenous Variables** | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
        | <small>*SME Commentary*</small>| <small>Not standard.</small> | <small>ARIMAX is a standard extension.</small>| <small>SARIMAX is standard.</small>| <small>Easy to add 'regressors'.</small>| <small>Can be added.</small>| <small>Can be added.</small>|
        """)
    with tabs[4]:
        st.markdown("""
        ##### Glossary of Forecasting Models
        - **Holt-Winters:** A triple exponential smoothing model that explicitly models level, trend, and a single seasonal component by giving exponentially decaying weight to past observations.
          - *SME Insight:* Think of it as a manager who keeps three numbers on a whiteboard: the current average (level), the weekly growth (trend), and the fact that sales always dip in July (seasonality). They update these three numbers after each new data point comes in.
        
        - **ARIMA (AutoRegressive Integrated Moving Average):** A class of models that explains a series based on its own past values (AR), the degree of differencing needed to make it stationary (I), and its past forecast errors (MA).
          - *SME Insight:* This is the classical watchmaker's approach. It assumes the underlying process is stable and can be described by a few precise relationships between its own past "ticks" and "tocks." It's not interested in the absolute value, but in the rhythm and memory of the process.

        - **SARIMA (Seasonal ARIMA):** An extension of ARIMA that adds seasonal components (`P, D, Q, m`) to model a single, fixed seasonal pattern. It is the classical gold standard for seasonal data.
          - *SME Insight:* This is the watchmaker who also has a calendar. It understands both the short-term rhythm (ARIMA) and the long-term annual rhythm (SARIMA).

        - **Prophet:** An automated, decomposable model from Meta. It treats forecasting as a curve-fitting problem, fitting trend, multiple seasonalities, and holiday effects as separate components.
          - *SME Insight:* Think of it as a modern sound engineer mixing a track. It has separate faders for the bassline (trend), the weekly drumbeat (weekly seasonality), the yearly synth melody (yearly seasonality), and special effects for holidays. It finds the best mix of all these components to match the song of your data.

        - **ETS (Error, Trend, Seasonality / ExponenTial Smoothing):** A powerful state-space framework for all exponential smoothing models. It can flexibly handle additive or multiplicative components for error, trend, and seasonality.
          - *SME Insight:* This is the growth specialist. The key feature is its ability to handle **multiplicative** trend (exponential growth) and seasonality (seasonal swings that get bigger as the level increases), which is very common in business data.

        - **TBATS (Trigonometric, Box-Cox, ARMA, Trend, Seasonal):** A highly complex, automated state-space model that can handle multiple and complex seasonalities using trigonometric Fourier terms.
          - *SME Insight:* This is the "kitchen sink" model. It throws every known statistical trick at the problem: transformations for non-linearity (Box-Cox), Fourier terms for complex seasonality (Trigonometric), and ARMA for the errors. It's powerful but can be a black box.
        """)
    with tabs[5]:
        st.markdown("""
        #### Theory, History & Mathematical Context
        The models in this suite represent a fascinating evolution of thought, driven by changing industrial needs and computational power.

        - **The Pragmatic Era - Exponential Smoothing (1950s):** The core ideas behind **Holt-Winters** and **ETS** were developed by Robert Goodell Brown, Charles Holt, and Peter Winters. In the post-war industrial boom, they needed simple, efficient methods for inventory forecasting that could be computed by hand or with mechanical calculators. Their solution‚Äîexponentially weighted averages‚Äîwas intuitive, fast, and good enough for the job.

        - **The Rigor Revolution - Box-Jenkins (1970):** George Box and Gwilym Jenkins published their seminal work, *Time Series Analysis: Forecasting and Control*. This was a monumental shift. They introduced the **ARIMA** methodology, bringing a new level of statistical rigor to the field. They championed a disciplined cycle of model **identification** (using ACF/PACF plots), **estimation**, and **diagnostic checking**. **SARIMA** was the natural extension for seasonal data, becoming the academic and statistical gold standard for decades.

        - **The Unification Era - State-Space Models (1990s-2000s):** For years, exponential smoothing and ARIMA were seen as separate, competing philosophies. Researchers like Rob Hyndman and his colleagues showed they were two sides of the same coin. They developed the **ETS** and **TBATS** models, which placed all of these methods into a single, unified state-space framework. This allowed for a more systematic and automated approach to model selection and, crucially, the calculation of robust prediction intervals.

        - **The Scale Era - Engineering Meets Statistics (2017):** Facebook's Core Data Science team faced a problem of scale: thousands of non-expert analysts needed to generate high-quality forecasts for business metrics. The manual, expert-driven Box-Jenkins method was too slow. They released **Prophet**, a model designed not for statistical purity, but for robust, scalable, and intuitive forecasting of business time series. Its focus on automation, intuitive parameters, and handling messy data represented a major shift towards a more engineering-driven approach to forecasting.
        """)
    with tabs[6]:
        st.markdown("""
        ### Regulatory & Compliance Context
        While "forecasting" is not a specific GxP activity, the use of time series models to monitor, control, and make decisions about a validated process is a core component of a modern, data-driven Pharmaceutical Quality System.

        #### The Golden Thread: From Monitoring to Proactive Control
        1.  **Stage 1: Basic Monitoring (ICH Q7, 21 CFR 211):** At a minimum, manufacturers are required to monitor critical process parameters. This is often done retrospectively.
        2.  **Stage 2: Continued Process Verification (CPV) (FDA Process Validation Guidance, Stage 3):** This is where forecasting becomes essential. A validated time series model can establish a **dynamic control band** (prediction intervals) for a process parameter. An observation falling outside this band can serve as an early warning of a potential drift, allowing for proactive investigation long before a standard SPC chart would alarm.
        3.  **Stage 3: Process Analytical Technology (PAT) (FDA PAT Guidance):** This is the most advanced application. A robust, validated forecasting model is a key element of a **"digital twin"** of the process. It can be used for:
            -   **Feed-forward Control:** If the model forecasts a future deviation, it can trigger an automated adjustment to another parameter to counteract the problem before it occurs.
            -   **Real-Time Release Testing (RTRT):** A highly accurate model that forecasts a Critical Quality Attribute (CQA) can be used as part of the evidence package to release a batch without waiting for the final, slow laboratory test.

        #### Validation Requirements (GAMP 5 & 21 CFR Part 11)
        If a forecasting model is used to make any GxP decision (e.g., triggering an alarm in a CPV program, justifying a batch release under RTRT), the model and the software it runs on are considered a **Computerized System** and must be fully validated. The validation package would need to include:
        -   **URS/FS:** A formal document defining the model's intended use, the data inputs, the required prediction accuracy, and the outputs.
        -   **Model Validation:** Objective evidence (e.g., performance on a held-out test set, backtesting on historical data) demonstrating the model is accurate and reliable for its intended purpose.
        -   **Change Control:** A procedure for managing the model lifecycle, including how and when the model will be retrained, re-validated, and deployed.
        
        > **Bottom Line:** In a regulated environment, a forecasting model is not just a statistical tool; it is a validated piece of software whose performance and reliability must be documented and controlled.
        """)
#=============================================================================================== Prophet  ============================================================================================================================
def render_prophet_forecasting():
    """Renders the dedicated, stable module for Prophet forecasting."""
    st.markdown("""
    #### Purpose & Application: The Automated Forecasting Engine
    **Purpose:** To provide a dedicated environment for exploring the powerful **Prophet** forecasting library from Meta. This module is designed to showcase Prophet's ability to handle complex scenarios with multiple seasonalities and trend changes that challenge classical models.
    """)
    st.warning("""
    **Performance Notice:** The Prophet model is computationally intensive. After clicking "Run Analysis," please allow **5-15 seconds** for the computation to complete.
    """)
    
    if 'prophet_cache' not in st.session_state:
        st.session_state.prophet_cache = {}
        st.session_state.prophet_fig = None
        st.session_state.prophet_mae = None

    with st.sidebar:
        st.subheader("Prophet Forecasting Controls")
        
        trend_type_p = st.radio("Trend Type", ['Additive', 'Multiplicative'], key='p_trend')
        seasonality_type_p = st.radio("Seasonality Type", ['Single (Yearly)', 'Multiple (Yearly + Quarterly)'], key='p_season')
        noise_level_p = st.slider("Noise Level (SD)", 1.0, 20.0, 5.0, 1.0, key='p_noise')
        changepoint_strength_p = st.slider("Trend Changepoint Strength", -5.0, 5.0, 0.0, 0.5, help="Simulates an abrupt change in the trend's slope.", key='p_cp')

        if st.button("üöÄ Run Prophet Analysis", use_container_width=True):
            cache_key = (trend_type_p, seasonality_type_p, noise_level_p, changepoint_strength_p)
            if cache_key in st.session_state.prophet_cache:
                st.toast("Loading results from cache!", icon="‚ö°")
                fig, mae_score = st.session_state.prophet_cache[cache_key]
                st.session_state.prophet_fig = fig
                st.session_state.prophet_mae = mae_score
            else:
                with st.spinner("Fitting Prophet model... This may take a moment."):
                    fig, mae_score = plot_prophet_only(trend_type_p, seasonality_type_p, noise_level_p, changepoint_strength_p)
                    st.session_state.prophet_fig = fig
                    st.session_state.prophet_mae = mae_score
                    st.session_state.prophet_cache[cache_key] = (fig, mae_score)
            st.rerun()

    st.header("Prophet Forecasting Dashboard")
    
    if st.session_state.prophet_fig is None:
        st.info("Configure your scenario in the sidebar and click 'Run Prophet Analysis' to see the results.")
    else:
        col1, col2 = st.columns([0.65, 0.35])
        with col1:
            st.plotly_chart(st.session_state.prophet_fig, use_container_width=True)
        with col2:
            st.subheader("Model Performance")
            if st.session_state.prophet_mae is not None:
                st.metric("Prophet MAE", f"{st.session_state.prophet_mae:.2f}")
            else:
                st.warning("The model failed to produce a valid score.")
            
    st.divider()
    st.subheader("Deeper Dive into Prophet")
    
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business case", "üí° Prophet's Place in the Toolkit", "üìã Glossary", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.subheader("How to Interpret the Dashboard: A Guided Tour")
        st.markdown("""
        This interactive dashboard is a virtual laboratory for exploring the Prophet forecasting model. By manipulating the "ground truth" of the data in the sidebar, you can discover Prophet's unique strengths.
    
        ##### The Main Plot: The Prophet in Action
        The primary chart shows the Prophet model's forecast (dotted colored line) against the true data (black line). The light shaded area represents the model's **prediction interval**, its "cone of uncertainty." The **Mean Absolute Error (MAE)** on the right is the scorecard of its performance.
    
        ---
        ##### Challenge 1: The Multi-Seasonality Problem
        1.  In the sidebar, set **Seasonality Type** to `Multiple (Yearly + Quarterly)`.
        2.  Click **"Run Prophet Analysis"**.
        3.  **Observe:** Notice how Prophet's forecast correctly captures the complex, bumpy pattern created by two overlapping seasonal cycles. Classical models like Holt-Winters would fail at this task. This is one of Prophet's primary strengths.
    
        ---
        ##### Challenge 2: The Trend Changepoint Problem
        1.  Set **Seasonality Type** back to `Single (Yearly)`.
        2.  Increase the **Trend Changepoint Strength** to a significant positive or negative value.
        3.  Click **"Run Prophet Analysis"**.
        4.  **Observe:** Prophet automatically detects the abrupt change in the trend's slope and adjusts its forecast accordingly. A standard linear trend model would continue along the old trajectory, leading to large errors.
    
        **The Core Strategic Insight:** Prophet's power comes from its **decomposability**. It doesn't see the time series as a single, complex signal. Instead, it models it as a sum of simpler pieces: a trend, a yearly pattern, holidays, etc. By fitting these components separately, it can robustly handle the messy, multi-layered data common in real-world business and manufacturing processes.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: Democratizing and Scaling Forecasting
    
        #### The Problem: The Forecasting Bottleneck
        In most organizations, high-quality forecasting is a niche skill, practiced by a small team of statisticians or data scientists. Business analysts, process engineers, and lab managers‚Äîthe people with the deep domain knowledge‚Äîcannot easily create their own forecasts. They are forced to rely on simple, often inaccurate, Excel-based extrapolations or wait in a long queue for the central analytics team.
    
        #### The Impact: Lost Opportunities and Slow Decisions
        This bottleneck prevents the organization from becoming truly data-driven.
        - **Slow, Infrequent Forecasts:** Forecasting is a high-effort, special project, not a routine operational activity. Decisions are made on stale, outdated information.
        - **Lack of Domain Knowledge:** A central analyst might build a statistically perfect model but be unaware of an upcoming planned maintenance shutdown or a marketing promotion that will dramatically impact the data, rendering the forecast useless.
        - **Limited Adoption:** Business users don't trust the "black box" models handed to them by the central team, so they revert to using their own spreadsheets and gut feelings.
    
        #### The Solution: A Tool for Domain Experts, Not Just Statisticians
        Prophet was designed from the ground up to solve this exact problem. Its value proposition is not just accuracy, but **accessibility and scalability**. It is a tool that **democratizes forecasting**.
        1.  **Intuitive Tuning:** The parameters are designed to be understood by domain experts. An engineer can easily add a `holiday` for a planned shutdown or adjust the `changepoint_prior_scale` to make the trend more or less flexible, without needing a PhD in statistics.
        2.  **Automation & Scale:** It is designed to be run programmatically, allowing a company to automatically generate thousands of reliable forecasts every day for every product, reagent, or quality metric.
        3.  **Robustness by Default:** It is built to handle the messy data (missing values, outliers, changepoints) that is common in the real world, producing a reasonable forecast "out of the box" with minimal effort.
    
        #### The Consequences: A Proactive, Data-Driven Culture
        - **Without This:** Forecasting remains a specialized, bottlenecked activity, and the organization remains reactive.
        - **With This:** Prophet empowers domain experts across the organization to create their own high-quality forecasts. This **fuses statistical power with local expertise**, leading to more accurate, trusted, and actionable insights. It enables a culture where every team can make proactive, forward-looking decisions, transforming the entire business from reactive to predictive.
        """)

    with tabs[2]:
        st.subheader("Prophet's Place in the Forecasting Toolkit")
        st.markdown("""
        This is a deep-dive module focused on a single, powerful tool. To see how Prophet compares to the broader family of forecasting models, please visit the main **"Time Series Forecasting Suite"**. However, the table below provides a quick summary of the key trade-offs between Prophet and its main conceptual rivals.

        | Feature | **Prophet (The Smartwatch)** | **SARIMA (The Watchmaker)** | **Exponential Smoothing (The Craftsman)** |
        | :--- | :--- | :--- | :--- |
        | **Philosophy** | Pragmatic Curve-Fitting | Rigorous Statistical Model | Intuitive Smoothing |
        | **Strengths** | Automation, messy data, multiple seasons, changepoints, holidays. | Statistical defensibility, captures complex autocorrelation. | Speed, interpretability, handles multiplicative trends. |
        | **Weaknesses**| Can be a "black box," less formal. | Requires expertise, manual tuning, and stationary data. | Only handles a single seasonality, struggles with messy data. |
        | **Best For...** | Business forecasting at scale. | Stable processes requiring high statistical rigor. | Clean, simple data with a single seasonal pattern. |
        """)

    with tabs[3]:
        st.markdown("""
        ##### Glossary of Prophet Terms
        - **Prophet:** An open-source forecasting library from Meta (formerly Facebook) based on a decomposable additive model.
        - **Decomposable Model:** A model that represents the time series as a sum of several independent components, typically `y(t) = trend + seasonality + holidays + error`.
        - **Trend Changepoints:** Points in time where the growth rate of the trend is allowed to change. Prophet can detect these automatically or allow the user to specify them.
        - **Fourier Series:** The mathematical technique Prophet uses to model seasonality. It approximates any periodic signal by summing up a series of simple sine and cosine waves. This is what allows it to model multiple seasonalities simultaneously.
        - **Stan:** The high-performance Bayesian statistical modeling language that Prophet uses under the hood to perform the model fitting. The `cmdstanpy` logs you see are from this engine.
        - **Regressors:** Additional external variables that can be added to the model to improve the forecast (e.g., adding temperature as a regressor to forecast energy consumption).
        """)
        
    with tabs[4]:
        st.markdown("""
        #### Theory, History & Mathematical Context
        **The Problem (The Facebook Scale):** In the mid-2010s, Facebook faced a unique business challenge. They had thousands of internal analysts who needed to produce high-quality forecasts for a vast number of business metrics (e.g., user growth, event attendance, ad revenue). The classical forecasting tools, particularly the dominant **Box-Jenkins methodology (ARIMA)**, were too slow, too manual, and required too much statistical expertise to be deployed at this scale.

        **The 'Aha!' Moment (Reframing the Problem):** The Facebook Core Data Science team, led by Sean J. Taylor and Ben Letham, had a key insight. They decided to reframe forecasting not as a complex time series problem, but as a more familiar **curve-fitting problem**. They drew inspiration from a class of statistical models known as **Generalized Additive Models (GAMs)**.
        
        **The Impact (Prophet, 2017):** They built Prophet from the ground up to be a pragmatic, engineering-focused solution.
        - The **trend `g(t)`** was modeled with simple piecewise linear segments, with automatic detection of "changepoints" where the slope could change.
        - **Seasonality `s(t)`** was modeled using a mathematical tool called a **Fourier series**, which can approximate any periodic signal. This was the key to easily modeling multiple seasonalities (e.g., weekly and yearly) at once.
        - **Holidays `h(t)`** were treated as simple special events.

        By fitting these components together using the powerful Bayesian engine **Stan**, they created a tool that was highly automated, robust to the messy data common in business, and incredibly easy for non-experts to tune. Prophet represents a major philosophical shift from the statistical purity of ARIMA to a more scalable, accessible, and engineering-driven approach to forecasting.
        
        ---
        #### Mathematical Basis
        Prophet is a decomposable additive model with three main components:
        """)
        st.latex(r"y(t) = g(t) + s(t) + h(t) + \epsilon_t")
        st.markdown(r"""
        -   **`g(t)` (Trend):** A piecewise linear or logistic growth model. The model automatically selects changepoints where the growth rate can change.
        -   **`s(t)` (Seasonality):** A Fourier series to model periodic changes. For a yearly seasonality with period `P=365.25`, the series is:
        """)
        st.latex(r"s(t) = \sum_{n=1}^{N} \left( a_n \cos\left(\frac{2\pi nt}{P}\right) + b_n \sin\left(\frac{2\pi nt}{P}\right) \right)")
        st.markdown(r"""
        -   **`h(t)` (Holidays):** A simple indicator for special events or holidays that affect the time series.
        -   **`Œµt` (Error):** The random error term, assumed to be normally distributed.
        """)
        
    with tabs[5]:
        st.markdown("""
        ### Regulatory & Compliance Context
        While "forecasting" is not a specific GxP activity, the use of time series models to monitor, control, and make decisions about a validated process is a core component of a modern, data-driven Pharmaceutical Quality System. Prophet, despite its "black box" nature, can be a validated and compliant tool if the proper framework is used.

        #### The Golden Thread: From Monitoring to Proactive Control
        1.  **Stage 2: Continued Process Verification (CPV) (FDA Process Validation Guidance, Stage 3):** This is the most common application. A validated Prophet model can establish a **dynamic control band** (its prediction intervals) for a process parameter. An observation falling outside this band serves as an early warning of a potential drift, allowing proactive investigation long before a standard SPC chart would alarm.
        2.  **Stage 3: Process Analytical Technology (PAT) (FDA PAT Guidance):** A robust, validated Prophet model is a key element of a **"digital twin"** of a process. It can be used to forecast CQAs and support Real-Time Release Testing (RTRT) strategies.

        #### Validation Requirements for Prophet (GAMP 5 & 21 CFR Part 11)
        If a Prophet model is used to make GxP decisions, it is considered a **Computerized System** and must be fully validated. The validation package is crucial for defending its use to an auditor.
        -   **URS/FS:** A formal document defining the model's intended use (e.g., "to forecast monthly reagent consumption with an MAE < 10%"), the data inputs, and the required prediction accuracy.
        -   **Model Validation Protocol & Report:** This is the core of the evidence. It must demonstrate the model's performance on a held-out, independent test set. **Backtesting**‚Äîsimulating how the model would have performed on historical data‚Äîis a key part of this. The final MAE on the test set is the objective evidence that the model is fit for purpose.
        -   **Change Control:** A procedure for managing the model lifecycle. It must define how and when the model will be retrained, how the performance of the new model will be verified against the old one, and the approval process for deploying an updated model.
        
        > **Bottom Line:** An auditor does not need to understand the complex math inside Prophet. They need to see a robust, pre-defined, and documented validation process that proves the model is accurate, reliable, and under control for its specific, intended GxP task.
        """)

# SNIPPET: Add this complete, missing function to the "ALL UI RENDERING FUNCTIONS" section.

def render_mva_pls():
    """Renders the comprehensive, interactive module for Multivariate Analysis (MVA/PLS)."""
    st.markdown("""
    #### Purpose & Application: The "Digital Chemist"
    **Purpose:** To build a **"Digital Chemist"**‚Äîa predictive model that can infer a difficult-to-measure property (like concentration) from an easy-to-measure, high-dimensional signal (like an NIR spectrum). **Partial Least Squares (PLS)** is the workhorse algorithm for this task, as it excels at building robust models from data that has many, highly correlated variables.
    
    **Strategic Application:** This is a cornerstone of **Process Analytical Technology (PAT)**. It is used to move quality control from slow, offline lab tests to fast, in-line, real-time measurements. This is critical for process understanding, real-time release, and advanced process control.
    """)
    st.info("""
    **Interactive Demo:** You are the PAT Scientist.
    1.  Use the sliders in the sidebar to control the properties of the simulated spectral data.
    2.  The dashboard will automatically build and validate a PLS model.
    3.  **Key Plots:** The **Cross-Validation plot** is used to select the optimal model complexity. The **VIP Scores plot** is the primary diagnostic tool for understanding which parts of the spectrum (which wavelengths) the model is using to make its predictions.
    """)

    with st.sidebar:
        st.subheader("MVA/PLS Simulation Controls")
        signal_strength = st.slider("Signal Strength", 1.0, 5.0, 2.0, 0.5, help="Controls how strongly the spectral peaks are correlated with the Y-variable (concentration). Higher strength makes for an easier modeling problem.")
        noise_sd = st.slider("Measurement Noise (SD)", 0.1, 1.0, 0.2, 0.1, help="Controls the amount of random noise in the spectral measurements. Higher noise makes it harder to build an accurate model.")

    st.header("Multivariate Analysis (PLS) Dashboard")
    
    fig, r2, q2, n_comp, rmsecv = plot_mva_pls(signal_strength=signal_strength, noise_sd=noise_sd)
    
    kpi_cols = st.columns(4)
    kpi_cols[0].metric("Model Fit (R¬≤)", f"{r2:.3f}", help="How well the model fits the training data. A high R¬≤ can be misleading if the model is overfit.")
    kpi_cols[1].metric("Predictive Power (Q¬≤)", f"{q2:.3f}", help="The cross-validated R¬≤. This is a much more honest measure of the model's true predictive ability on new data.")
    kpi_cols[2].metric("Optimal Complexity", f"{n_comp} LVs", help="The number of latent variables (LVs) selected by cross-validation to maximize Q¬≤ without overfitting.")
    kpi_cols[3].metric("Prediction Error (RMSECV)", f"{rmsecv:.2f}", help="The Root Mean Squared Error of Cross-Validation. The typical prediction error in the original units of Y.")

    st.plotly_chart(fig, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive into Multivariate Analysis")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Visualize the Data (Plot 1):** The raw data shows a complex, high-dimensional signal where the subtle differences are hard to see.
        2.  **Determine Model Complexity (Plot 2):** This is the most critical step. We want to choose the number of latent variables (LVs) that maximizes the **predictive power (Q¬≤, green line)**, not just the **fit (R¬≤, blue line)**. The point where the green line peaks or plateaus is the optimal complexity. Adding more LVs beyond this point is "overfitting"‚Äîthe model starts memorizing noise instead of learning the true signal.
        3.  **Evaluate Performance (Plot 3):** The Predicted vs. Actual plot is the final report card for the chosen model. For a good model, the points should fall tightly along the 45-degree line.
        4.  **Interpret the Model (Plot 4):** The VIP Scores plot is our XAI tool. It tells us *which* original variables (wavelengths) are most important to the model's prediction. The peaks in the VIP plot should correspond to the real chemical signals in the spectral data (highlighted in green), which gives us confidence that the model is scientifically valid.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The "Digital Chemist" for Real-Time Control

        #### The Problem: The Slow, Expensive Lab Test
        A critical quality attribute (CQA) for a multi-million dollar batch, such as protein concentration or blend uniformity, can only be measured by a slow, destructive, and expensive laboratory test (e.g., HPLC, wet chemistry). The manufacturing team must take a sample, send it to the QC lab, and wait for hours or even days for the result.

        #### The Impact: The High Cost of Flying Blind
        This time lag creates massive inefficiency and risk.
        - **No Real-Time Control:** The process is essentially "flying blind." By the time the lab result comes back, the batch is already finished. If there was a problem, it's too late to fix it, and the entire batch may have to be scrapped.
        - **Bottlenecks and Long Cycle Times:** The slow lab test is a major bottleneck in the production process, extending cycle times and tying up capital in work-in-progress inventory.
        - **Limited Process Understanding:** Because testing is slow and expensive, only a few samples are taken per batch. This provides a very limited, low-resolution picture of the process, making it difficult to troubleshoot or optimize.

        #### The Solution: A "Digital Chemist" on the Production Line
        Multivariate Analysis (MVA), using tools like Partial Least Squares (PLS), is the engine behind **Process Analytical Technology (PAT)**. It creates a **"Digital Chemist"**‚Äîa validated mathematical model that can infer the slow, expensive CQA from a fast, easy, and non-destructive measurement (like a Near-Infrared (NIR) spectrum). Instead of waiting days for an HPLC result, you can get a highly accurate prediction in seconds, directly on the production line.

        #### The Consequences: A Faster, Smarter, and More Controlled Process
        - **Without This:** The company is stuck in a slow, reactive "make and test" paradigm.
        - **With This:** MVA and PAT enable a revolutionary shift to a "measure and control" paradigm.
            - **Real-Time Process Control:** With a prediction every second, the process can be controlled in real-time, preventing deviations before they happen.
            - **Real-Time Release Testing (RTRT):** A highly accurate MVA model can be used to release the batch directly, eliminating the QC lab bottleneck and slashing cycle times from weeks to days.
            - **100% Inspection:** Instead of a few grab samples, the company can effectively achieve 100% inspection of the process, providing an unprecedented level of quality assurance and process understanding.
        This is a transformative technology that is a cornerstone of the modern "Pharma 4.0" factory.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of MVA Terms
        - **MVA (Multivariate Analysis):** A set of statistical techniques used to analyze data that contains more than one variable.
        - **PLS (Partial Least Squares Regression):** A powerful multivariate regression technique that is a "gold standard" for chemometrics. It is highly effective for datasets with many, highly correlated predictor variables (like a spectrum) and a relatively small number of samples.
        - **Latent Variable (LV):** An underlying, unobserved variable that is constructed from the original, measured variables. PLS works by finding the LVs in the predictor data (X) that are most relevant to predicting the outcome data (Y).
        - **R¬≤ (R-squared):** A measure of how well the model **fits** the data it was trained on.
        - **Q¬≤ (Q-squared):** The cross-validated R¬≤. It is a measure of how well the model **predicts** new, unseen data. It is the most important metric for evaluating a PLS model.
        - **RMSECV (Root Mean Squared Error of Cross-Validation):** The typical prediction error of the model, in the original units of the Y-variable.
        - **VIP (Variable Importance in Projection):** A score that summarizes the importance of each individual variable (e.g., wavelength) to the overall PLS model. Variables with a VIP score > 1 are generally considered important.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Overfitting" Trap**
An analyst keeps adding more latent variables (LVs) to their PLS model because it makes the R¬≤ value (the blue line in Plot 2) get closer and closer to 1.0.
- **The Flaw:** They are overfitting the model. The R¬≤ is a measure of how well the model "memorized" the training data. The Q¬≤ (the green line) shows the true story: after a certain point, adding more LVs actually makes the model *worse* at predicting new data, because it has started to model the random noise.""")
        st.success("""üü¢ **THE GOLDEN RULE: Maximize Prediction (Q¬≤), Not Just Fit (R¬≤)**
A robust and defensible MVA model is built for prediction, not just for fitting.
1.  **Use Cross-Validation:** Always use a rigorous cross-validation technique to estimate the model's true predictive power (Q¬≤).
2.  **Choose Complexity Wisely:** Select the number of latent variables that gives the highest Q¬≤, or where Q¬≤ begins to plateau. This is the "sweet spot" that balances model power with robustness.
3.  **Validate with VIP:** Use the VIP scores to ensure that the model is scientifically sound. The most important variables identified by the model should make chemical or physical sense to a subject matter expert.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: The Chemist's Data Dilemma
        **The Problem:** In the 1960s and 70s, new analytical instruments like Near-Infrared (NIR) spectrometers were becoming available. These instruments were amazing‚Äîthey could generate a spectrum with hundreds of data points in seconds. But this created a massive statistical problem: **"p >> n"** (many more variables `p` than samples `n`). A standard multiple linear regression model completely fails in this situation. How could you build a predictive model from this firehose of correlated data?

        **The 'Aha!' Moment:** The solution was developed by the Swedish statistician **Herman Wold** in the 1970s. He developed the algorithm for **Partial Least Squares (PLS)**. The genius of PLS is that it doesn't try to use all the original variables at once. Instead, it first squashes the hundreds of X-variables (the spectrum) down into a small number of new, artificial variables called **latent variables**. The key insight is that these latent variables are constructed to be maximally correlated with the Y-variable (e.g., concentration).
        
        **The Impact:** PLS was a revolutionary breakthrough for the field of **chemometrics** (the application of statistics to chemical data). It provided a robust and powerful tool to turn complex, high-dimensional instrument data into actionable, predictive models. It became the foundational algorithm for **Process Analytical Technology (PAT)**, enabling the shift from slow, offline testing to fast, real-time process monitoring and control.
        """)
        
    with tabs[5]:
        st.markdown("""
        MVA and PLS are the core statistical engines for **Process Analytical Technology (PAT)**, which is a major regulatory initiative.
        - **FDA Guidance for Industry - PAT ‚Äî A Framework for Innovative Pharmaceutical Development, Manufacturing, and Quality Assurance:** This is the primary guidance document. It encourages the use of online or in-line measurements (like NIR spectroscopy) combined with multivariate models (like PLS) to achieve real-time process understanding and control.
        - **ICH Q8(R2) - Pharmaceutical Development:** A validated MVA model is a key component of a robust **Control Strategy** and a powerful tool for building the deep **Process Understanding** required for a QbD submission.
        - **GAMP 5 & 21 CFR Part 11:** An MVA model used for real-time release or in-process control is considered a high-risk (GAMP Category 5) **Computerized System**. The model itself‚Äîits development, validation, and lifecycle management‚Äîmust be rigorously documented and controlled to ensure data integrity and reliability. The VIP plot is a key piece of evidence in the validation package, demonstrating that the model's logic is scientifically sound.
        """)

#========================================================================== 9. PREDICTIVE QC (CLASSIFICATION) ACT III============================================================================
def render_predictive_modeling_suite():
    """Renders the comprehensive, interactive module for Predictive Modeling."""
    st.markdown("""
    #### Purpose & Application: The AI Gatekeeper
    **Purpose:** To build an **AI Gatekeeper** that can inspect in-process data and predict, with high accuracy, whether a batch will ultimately pass or fail its final QC specifications. This suite compares three levels of model complexity: simple linear, powerful ensemble, and a highly flexible neural network.
    
    **Strategic Application:** This is the foundation of real-time release and proactive quality control. By predicting outcomes early, we can prevent failures, optimize resources, and accelerate batch release with robust statistical evidence.
    """)
    st.info("""
    **Interactive Demo:** You are the Lead Data Scientist.
    1.  Use the **Scenario Controls** to define the complexity of the pass/fail relationship.
    2.  Use the **MLP Hyperparameter Tuner** to configure your neural network.
    3.  Click **"Run Model Comparison"** to train all three models and see their performance on the ROC curve.
    """)

    if 'pred_fig' not in st.session_state:
        st.session_state.pred_fig = None
        st.session_state.pred_aucs = None

    with st.sidebar:
        st.subheader("Scenario Controls")
        boundary_slider = st.slider(
            "Boundary Complexity",
            min_value=4, max_value=25, value=12, step=1,
            help="Controls how non-linear the true pass/fail boundary is. Lower values create a more complex 'island' that is harder for linear models to solve."
        )
        st.divider()
        st.subheader("MLP Hyperparameter Tuner")
        
        layer_options = {
            "1 Layer (32)": (32,),
            "2 Layers (64, 32)": (64, 32),
            "3 Layers (100, 50, 25)": (100, 50, 25)
        }
        layer_choice = st.selectbox(
            "Hidden Layers & Neurons", 
            options=list(layer_options.keys()), 
            index=1,
            help="Controls the architecture (depth and width) of the neural network. More layers and neurons can learn more complex patterns but increase the risk of overfitting."
        )
        
        activation_choice = st.selectbox(
            "Activation Function", 
            options=['relu', 'tanh'], 
            index=0,
            help="The non-linear function neurons use to process information. 'ReLU' is the modern default, known for efficiency. 'Tanh' is a classic alternative."
        )
        
        learning_rate_choice = st.select_slider(
            "Learning Rate", 
            options=[0.0001, 0.001, 0.01, 0.1], 
            value=0.001,
            help="Controls how quickly the model learns. A smaller rate is slower but more stable. A large rate can cause the model to learn incorrectly and 'overshoot' the optimal solution."
        )

        mlp_params = {
            'layers': layer_options[layer_choice],
            'activation': activation_choice,
            'learning_rate': learning_rate_choice
        }
        
        if st.button("üöÄ Run Model Comparison", use_container_width=True):
            with st.spinner("Training and evaluating models..."):
                fig, auc_lr, auc_rf, auc_mlp = plot_predictive_modeling_suite(boundary_slider, mlp_params)
                st.session_state.pred_fig = fig
                st.session_state.pred_aucs = {'lr': auc_lr, 'rf': auc_rf, 'mlp': auc_mlp}
            st.rerun()

    st.header("Predictive Modeling Dashboard")

    if st.session_state.pred_fig:
        st.plotly_chart(st.session_state.pred_fig, use_container_width=True)
        st.subheader("Model Performance Scorecard (AUC)")
        col1, col2, col3 = st.columns(3)
        aucs = st.session_state.pred_aucs
        col1.metric("Logistic Regression AUC", f"{aucs['lr']:.3f}")
        col2.metric("Random Forest AUC", f"{aucs['rf']:.3f}")
        col3.metric("MLP Neural Network AUC", f"{aucs['mlp']:.3f}")
    else:
        st.info("Configure your scenario and tune the MLP in the sidebar, then click 'Run Model Comparison' to see the results.")

    st.divider()
    st.subheader("Deeper Dive into Predictive Modeling")
    
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üí° Model Selection Guide", "üìã Glossary", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.subheader("How to Interpret the Dashboard: A Guided Tour")
        st.markdown("""
        This interactive dashboard is a virtual laboratory for comparing different types of predictive models. By controlling the "problem" and the "solution," you can build a deep intuition for their behavior.
    
        ##### The Plots: The Arena of Competition
        - **Decision Boundaries (Plots 1-3):** These maps show how each model sees the world. The color shows the predicted probability of failure, and the dark line is the 50% "decision boundary." A good model's boundary should closely match the true pattern of red (Fail) and blue (Pass) dots.
        - **ROC Curves (Plot 4):** This is the objective scorecard. A curve that is closer to the top-left corner is better. The **Area Under the Curve (AUC)** is the final performance metric. An AUC of 1.0 is a perfect classifier.
    
        ---
        ##### Challenge 1: The Linear vs. Non-Linear Problem
        1. In the sidebar, set **Boundary Complexity** to a high value (e.g., `25`). This creates a simple, almost linear separation. Click **Run**.
        2. **Observe:** All three models perform very well, with similar, high AUCs. The simple Logistic Regression is just as good as the complex MLP.
        3. Now, set **Boundary Complexity** to a low value (e.g., `8`). This creates a difficult, non-linear "island" of failures. Click **Run**.
        4. **Observe:** The Logistic Regression boundary is a straight line that completely fails to capture the pattern, and its AUC plummets. The Random Forest and MLP, which can learn non-linear shapes, perform much better.
    
        ---
        ##### Challenge 2: The Hyperparameter Tuning Problem
        1. Keep **Boundary Complexity** low (`8`).
        2. In the MLP Tuner, try a "bad" configuration: `1 Layer (32)`, `learning_rate=0.1`. Click **Run**.
        3. **Observe:** The MLP may perform poorly, with a messy boundary and a lower AUC than the Random Forest.
        4. Now, try a "good" configuration: `2 Layers (64, 32)`, `learning_rate=0.001`. Click **Run**.
        5. **Observe:** The well-tuned MLP now likely has the best performance of all three models.
    
        **The Core Strategic Insight:** The best model is a function of both the **complexity of the problem** and the **quality of the tuning**. For simple problems, simple models are best. For complex problems, powerful models like neural networks are necessary, but they require careful hyperparameter tuning to unlock their full potential.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: From Reactive Post-Mortem to Proactive Gatekeeper
    
        #### The Problem: Quality Control is a Lagging Indicator
        Traditional Quality Control is a **reactive, end-of-line inspection**. A company invests hundreds of thousands of dollars to manufacture an entire batch of product over several days. Only *after* all this value has been added, a sample is sent to the lab. If it fails, the entire batch is scrap. The QC test is a post-mortem, not a control.
    
        #### The Impact: The High Cost of Waiting and Wasting
        This reactive model is a massive source of financial waste and operational inefficiency.
        - **Maximized Scrap Cost:** The company has already incurred 100% of the material and labor cost to produce an entire batch of worthless product. The financial loss is maximized.
        - **Long Cycle Times & Tied-Up Capital:** Batches sit in quarantine for days or weeks awaiting QC results. This ties up millions of dollars in inventory and delays revenue recognition, directly impacting the company's cash flow.
        - **Limited Process Understanding:** A simple pass/fail result provides very little information about *why* the batch was good or bad, hindering any real attempt at continuous improvement.
    
        #### The Solution: The "AI Gatekeeper" for In-Process Control
        A validated predictive classification model acts as an **in-process "AI Gatekeeper."** By analyzing real-time data from the manufacturing process (e.g., sensor readings from a bioreactor), the model can predict the final quality outcome long before the batch is finished. This transforms the quality paradigm from reactive to proactive.
        1.  **Proactive Intervention:** If a model predicts a high probability of failure for a batch currently in progress, engineers can be alerted to **intervene in real-time**, potentially saving the batch and preventing a multi-million dollar loss.
        2.  **Risk-Based QC & Real-Time Release:** Batches with a very high predicted probability of passing, based on a perfect in-process profile, could be eligible for reduced QC testing or even **Real-Time Release Testing (RTRT)**. This dramatically accelerates release timelines from weeks to days, a massive competitive advantage.
        3.  **Deep Process Understanding:** The model itself becomes a "digital twin" of the process. Tools like **Explainable AI (XAI)** can be used to understand exactly which in-process parameters are the key drivers of success or failure, providing invaluable insights for process optimization.
    
        #### The Consequences: A Faster, Cheaper, and Smarter Operation
        - **Without This:** The company is stuck in a slow, wasteful cycle of "make, wait, and scrap."
        - **With This:** The Predictive Modeling Suite provides the engine for a **faster, cheaper, and smarter manufacturing operation**. It directly **reduces scrap**, **accelerates revenue**, and provides the deep process knowledge needed to achieve a true state of control and a sustainable competitive advantage.
        """)

    with tabs[2]:
        st.subheader("Model Selection Guide: Crawl, Walk, Run")
        st.markdown("""
        This suite demonstrates a "crawl, walk, run" approach to predictive modeling. The choice of model is a strategic decision based on your team's skill, the complexity of your problem, and your need for interpretability.

        | **Phase** | **Model** | **Analogy** | **When to Use It** |
        | :--- | :--- | :--- | :--- |
        | **Crawl** | **Logistic Regression** | **The Glass Box:** | **Always start here.** It's your baseline. If this simple, highly interpretable model performs well enough, your job might be done. It's fast, robust, and easy to explain to auditors. |
        | **Walk** | **Random Forest** | **The Reliable Workhorse:** | Use this when your linear model is not performing well. It's excellent at automatically capturing non-linearities and interactions without requiring extensive tuning. It's often the "good enough" champion. |
        | **Run** | **MLP Neural Network** | **The High-Performance Engine:** | Use this when you have a large dataset and believe there are very complex, abstract patterns that other models are missing. It requires careful tuning and validation but offers the highest potential performance. |
        """)

    with tabs[3]:
        st.markdown("""
        ##### Glossary of Predictive Modeling Terms
        - **Classification:** A supervised machine learning task where the goal is to predict a categorical class label (e.g., "Pass" or "Fail").
        - **Logistic Regression:** A linear model used for binary classification. It models the probability of a class by fitting a linear equation to the log-odds of the event.
        - **Random Forest:** A powerful **ensemble** learning method that operates by constructing a multitude of **decision trees** at training time. The final prediction is the "majority vote" of all the individual trees.
        - **MLP (Multilayer Perceptron):** A classic type of feedforward artificial neural network. It consists of an input layer, one or more **hidden layers** of computational nodes (**neurons**), and an output layer.
        - **Activation Function:** A non-linear function (like `ReLU` or `tanh`) applied by neurons in an MLP. This is what allows the network to learn complex, non-linear decision boundaries.
        - **Hyperparameters:** The "settings" of a model that are not learned from the data but are set by the user before training (e.g., the number of hidden layers in an MLP, the learning rate).
        - **Decision Boundary:** The line or surface that separates the different classes in the feature space.
        - **ROC Curve & AUC:** The primary method for evaluating a classifier's performance. The **ROC Curve** plots the True Positive Rate vs. the False Positive Rate, and the **Area Under the Curve (AUC)** summarizes this plot into a single number from 0.5 (random) to 1.0 (perfect).
        """)
        
    with tabs[4]:
        st.markdown("""
        #### Theory, History & The Two Cultures
        This dashboard perfectly illustrates what the famous statistician Leo Breiman called the "Two Cultures" of statistical modeling.

        - **Culture 1: The Data Modeling Culture (The Glass Box):** The primary goal is to create a simple, interpretable statistical model that can explain the relationship between the input variables and the output. **Logistic Regression (David Cox, 1958)** is a masterpiece of this culture. It's a direct, interpretable generalization of linear regression for binary outcomes. The focus is on inference and understanding.

        - **Culture 2: The Algorithmic Modeling Culture (The Black Box):** The primary goal is predictive accuracy. The internal mechanism of the model is secondary to its ability to make correct predictions on new data. **Random Forest (Leo Breiman, 2001)** and **MLP Neural Networks (Rosenblatt, 1958; refined through decades)** are quintessential examples. They achieve high performance through complexity that is not easily interpretable.

        - **The Modern Synthesis (Explainable AI):** Today, the field of **Explainable AI (XAI)**, as shown in the next module, is dedicated to bridging this gap. It provides tools like SHAP to interrogate "black box" models, allowing us to get the high performance of the algorithmic culture while still gaining the understanding prized by the data modeling culture.
        
        ---
        #### Mathematical Basis
        - **Logistic Regression:** Models the log-odds of the outcome as a linear function and uses the logistic (sigmoid) function to map this to a probability.
          `p = 1 / (1 + e^-(Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + ...))`
        - **Random Forest:** An ensemble of `N` decision trees. The final prediction is the majority vote of all trees.
          `Prediction(x) = mode{ Tree‚ÇÅ(x), Tree‚ÇÇ(x), ..., Tree_N(x) }`
        - **MLP Neural Network:** A composition of nested functions. Each layer applies a linear transformation followed by a non-linear activation function.
          `Output = Activation(W‚ÇÇ * Activation(W‚ÇÅ * Input + b‚ÇÅ) + b‚ÇÇ)`
        """)
        
    with tabs[5]:
        st.markdown("""
        ### Regulatory & Compliance Context
        The use of AI/ML models for GxP decisions like batch release is a cutting-edge topic with evolving regulatory expectations. A robust validation package is non-negotiable.

        #### Key Regulatory Frameworks
        - **FDA AI/ML Action Plan & GMLP:** The FDA is actively developing its framework for regulating AI/ML-based software. Key principles include **Good Machine Learning Practice (GMLP)**, which emphasizes data quality, model transparency, and robust performance testing on independent data. This module directly addresses GMLP by comparing models and evaluating them on a test set.
        - **FDA Guidance on Process Validation (Stage 3 - CPV):** A validated predictive model is a powerful tool for **Continued Process Verification**. It can provide real-time assurance that a process is remaining in its validated state.
        - **PAT & Real-Time Release Testing (RTRT):** This is the ultimate application. A highly accurate and fully validated model can be a key component of a **Process Analytical Technology (PAT)** strategy, providing the scientific evidence to support **Real-Time Release** of a product, a major goal of modern pharmaceutical manufacturing.
        
        #### The Validation Package for a Predictive Model
        An auditor will expect to see a comprehensive validation package for any model used in a GxP context. This would include:
        1.  **Model Requirements Specification (URS/FS):** What is the model's intended use? What is the required level of accuracy (e.g., AUC > 0.95)?
        2.  **Data Management Plan:** How was the training and testing data collected, managed, and versioned to ensure integrity?
        3.  **Model Design & Training Specification (DS):** A detailed description of the final model, including the chosen algorithm, all hyperparameters (as tuned in this module), and the feature set.
        4.  **Model Validation Report:** The objective evidence of the model's performance on an independent, held-out test set. This report would include the ROC curves and AUC metrics shown in this dashboard.
        5.  **Change Control Procedure:** A formal procedure that defines how the model will be monitored over time and the criteria for when it must be retrained and re-validated.
        """)
        
def render_xai_shap():
    """Renders the module for Explainable AI (XAI) using SHAP."""
    st.markdown("""
    #### Purpose & Application: The AI Root Cause Investigator
    **Purpose:** To deploy an **AI Investigator** that forces a complex "black box" model to confess exactly *why* it predicted a specific assay run would fail. **Explainable AI (XAI)** cracks open the black box to reveal the model's reasoning.
    
    **Strategic Application:** This is a crucial tool for validating and deploying predictive models in a regulated GxP environment. Instead of just getting a pass/fail prediction, you get a full root cause analysis for every run.
    - **Model Validation:** Confirm that the model is flagging runs for scientifically valid reasons (e.g., a low calibrator slope) and not due to spurious correlations.
    - **Proactive Troubleshooting:** If the model predicts a high risk of failure, SHAP immediately points to the most likely reasons, allowing for proactive intervention.
    """)

    st.info("""
    **Interactive Demo:** This dashboard shows a full XAI workflow. **The tool is computationally intensive, and depending on latency, it may take a minute to load.**
    1.  **Global Explanations:** See the model's overall strategy in the Beeswarm plot.
    2.  **Local Explanations:** Select a specific case to investigate and see its root cause analysis in the Waterfall plot.
    3.  **Feature Deep Dive:** Use the PDP/ICE Plot to explore how the model uses a single feature across all samples.
    """)

    with st.sidebar:
        st.sidebar.subheader("XAI Investigation Controls")
        case_choice = st.sidebar.radio(
            "Select Case for Local Explanation:",
            options=['highest_risk', 'lowest_risk', 'most_ambiguous'],
            format_func=lambda key: {
                'highest_risk': "Highest Failure Risk",
                'lowest_risk': "Lowest Failure Risk",
                'most_ambiguous': "Most Ambiguous (50/50)"
            }[key]
        )
        
        feature_names = ['Operator Experience (Months)', 'Reagent Age (Days)', 'Calibrator Slope', 'QC Level 1 Value', 'Instrument ID']
        dependence_feature_choice = st.sidebar.selectbox(
            "Select Feature for Deep Dive Plot:",
            options=feature_names
        )
    
    summary_buf, waterfall_buf, pdp_buf, instance_df, outcome, idx = plot_xai_shap(
        case_to_explain=case_choice,
        dependence_feature=dependence_feature_choice
    )
    
    # --- THIS IS THE CORRECTED TAB CREATION ---
    tabs = st.tabs(["Global Explanations", "Local Explanation", "Feature Deep Dive", "‚úÖ The Business Case", "üî¨ SME Analysis", "üìã Glossary", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.subheader("Global Feature Importance (The Model's General Strategy)")
        st.markdown("This **Beeswarm plot** shows the impact of every feature for every sample. Each dot is a single run. Red dots are high feature values, blue are low. A positive SHAP value increases the prediction of failure.")
        st.image(summary_buf)
    
    with tabs[1]:
        st.subheader(f"Local Root Cause Analysis for Run #{idx} ({case_choice.replace('_', ' ').title()})")
        st.markdown(f"**This run was selected for analysis. Actual Outcome: `{outcome}`**")
        st.dataframe(instance_df)
        st.markdown("The **Waterfall plot** below shows exactly how each feature contributed to this specific prediction, starting from the base rate and building to the final risk score.")
        st.image(waterfall_buf)
        
    with tabs[2]:
        st.subheader(f"Feature Deep Dive: '{dependence_feature_choice}'")
        st.markdown("""
        The **Partial Dependence Plot (PDP)** with **Individual Conditional Expectation (ICE)** lines shows how a feature affects predictions.
        - **The Red Dashed Line (PDP):** Shows the *average* effect on the predicted probability of failure as the feature value changes.
        - **The Light Blue Lines (ICE):** Each line represents a *single run*. This shows if the feature's effect is consistent across the dataset or if there are interactions.
        """)
        st.image(pdp_buf)
        
    with tabs[3]:
        st.markdown("""
        ### The Business Case: The "Flight Recorder" for Your AI
    
        #### The Problem: The "Black Box" Dilemma
        A data science team has built a powerful "black box" AI model (like a deep neural network or a large XGBoost model) that can predict batch failures with 98% accuracy. They present this to the Quality and Regulatory leadership. The leaders are impressed by the accuracy but ask two simple, devastating questions:
        1.  **"How can we be sure it's not just latching onto a spurious correlation? How do we know it's making decisions for the right *scientific* reasons?"**
        2.  **"When it flags a batch as high-risk, what specific corrective action are we supposed to take? The model just gives us a number, not a reason."**
    
        #### The Impact: Untrusted, Unusable, and Un-validatable AI
        Without answers to these questions, the high-accuracy model is commercially and regulatorily useless.
        - **Lack of Trust and Adoption:** Engineers and operators will not trust or use a tool that cannot explain its reasoning. They will revert to their traditional methods, and the multi-million dollar AI investment will sit on a shelf, providing no value.
        - **Inactionable Outputs:** A prediction of "98% chance of failure" is not an action. Without knowing *why* the model is concerned, the operations team has no starting point for a corrective action, defeating the purpose of an early warning system.
        - **Guaranteed Regulatory Failure:** No regulatory body (like the FDA) will approve a "black box" model for use in a GxP decision-making process. The inability to explain a model's logic makes it impossible to validate.
    
        #### The Solution: The AI Translator and Flight Recorder
        Explainable AI (XAI) tools like SHAP are the solution to the "black box" problem. They act as a powerful **translator and flight recorder** for your AI models.
        1.  **The Translator (Local Explanations):** For any single prediction, the SHAP waterfall plot translates the complex model's output into a simple, human-readable "root cause analysis," showing exactly which factors pushed the prediction up or down.
        2.  **The Flight Recorder (Global Explanations):** The SHAP summary plot provides a global overview of the model's behavior across thousands of predictions. This allows you to validate that the model's overall "strategy" is scientifically sound and not based on artifacts in the data.
    
        #### The Consequences: A Trustworthy, Actionable, and Compliant AI
        - **Without This:** Powerful AI models remain "science projects"‚Äîtechnically impressive but unusable in the real world of regulated manufacturing.
        - **With This:** XAI is the **essential enabling technology** that makes it possible to deploy complex AI in a GxP environment. It **builds trust** with users, provides **actionable insights** for operators, and generates the **objective evidence** needed to validate the model and satisfy regulatory requirements for transparency and control. It is the key that unlocks the business value of your AI investments.
        """)

    with tabs[4]:
        st.markdown("""
        #### SME Analysis: From Raw Data to Actionable Intelligence
        As a Subject Matter Expert (SME) in process validation and tech transfer, this tool isn't just a data science curiosity; it's a powerful diagnostic and risk-management engine. Here‚Äôs how we would use this in a real-world GxP environment.

        ---
        
        ##### How is this data gathered and what are the parameters?
        The data used by this model is a simplified version of what we collect during **late-stage development, process characterization, and tech transfer validation runs**.
        -   **Data Gathering:** Every time an assay run is performed, we log key parameters in a Laboratory Information Management System (LIMS) or an Electronic Lab Notebook (ELN).
        -   **Parameters Considered:** `Operator Experience`, `Reagent Age`, `Calibrator Slope`, `QC Value`, and `Instrument ID` are all critical process parameters and material attributes that influence assay performance.

        ---

        ##### How do we interpret the plots and gain insights?
        The true power here is moving from "what happened" to "why it happened."
        -   **Global Plot (Beeswarm):** This is our first validation checkpoint for the model itself. If I saw that a scientifically irrelevant factor was the most important, I would reject the model. The fact that `Operator Experience` and `Calibrator Slope` are top drivers gives me confidence that the AI's "thinking" aligns with scientific reality.
        -   **Local Plot (Waterfall):** This is our **automated root cause investigation tool**. For the "Highest Risk" case, the plot instantly shows the root cause narrative: e.g., an inexperienced operator combined with old reagents created a high-risk situation.
        -   **Feature Deep Dive (PDP/ICE):** This helps us define our **Design Space or Normal Operating Range**. For example, by plotting `Reagent Age`, we can see the exact point where the red line (average risk) starts to sharply increase, allowing us to set an internal expiry date (e.g., "do not use reagents older than 60 days") based on data, not just a guess.
        
        ---

        ##### How would we implement this?
        1.  **Phase 1 (Silent Monitoring):** The model runs in the background, helping us spot trends in failure causes during process monitoring meetings.
        2.  **Phase 2 (Advisory Mode):** The system is integrated with the LIMS. It can generate advisories like: **"Warning: Reagent Lot XYZ is 85 days old. This significantly increases risk. Consider using a newer lot."**
        3.  **Phase 3 (Proactive Control / Real-Time Release):** A fully validated model's predictions can become part of the batch record. A run with a very low predicted risk and a favorable SHAP explanation could be eligible for **Real-Time Release Testing (RTRT)**, accelerating production timelines.
        """)
    with tabs[5]:
        st.markdown("""
        ##### Glossary of XAI Terms
        - **XAI (Explainable AI):** A field of AI dedicated to creating techniques that produce machine learning models that are understandable and trustworthy to human users.
        - **SHAP (SHapley Additive exPlanations):** A game theory-based approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory.
        - **SHAP Value:** The average marginal contribution of a feature value to the prediction across all possible coalitions of features. It represents the impact of that feature on pushing the model's prediction away from the baseline.
        - **Global Explanation:** An explanation of the model's overall behavior (e.g., the Beeswarm plot, which shows which features are most important on average).
        - **Local Explanation:** An explanation for a single, specific prediction (e.g., the Waterfall plot, which shows how each feature contributed to the risk score for one specific batch).
        - **PDP (Partial Dependence Plot):** A plot that shows the marginal effect of a feature on the predicted outcome of a model.
        """)
    with tabs[6]:
        st.markdown("""
        Explainable AI (XAI) is a critical emerging field for the validation of AI/ML models in a regulated environment. It addresses the "black box" problem.
        - **FDA AI/ML Action Plan:** The FDA is actively developing its framework for regulating AI/ML-based software. A key principle is transparency, and XAI methods like SHAP provide the evidence that a model's reasoning is scientifically sound.
        - **GAMP 5 - A Risk-Based Approach to Compliant GxP Computerized Systems:** The principles of system validation require a thorough understanding and verification of the system's logic. For an AI model, XAI is essential for fulfilling the spirit of User and Functional Requirement Specifications (URS/FS).
        - **Good Machine Learning Practice (GMLP):** While not yet a formal regulation, this set of principles is emerging as a standard for developing robust and trustworthy ML models in healthcare, and explainability is a core pillar.
        """)

def render_clustering():
    """Renders the module for unsupervised clustering."""
    st.markdown("""
    #### Purpose & Application: The Data Archeologist
    **Purpose:** To act as a **data archeologist**, sifting through your process data to discover natural, hidden groupings or "regimes." Without any prior knowledge, it can uncover distinct "civilizations" within your data, answering the question: "Are all of my 'good' batches truly the same, or are there different ways to be good?"
    
    **Strategic Application:** This is a powerful exploratory tool for deep process understanding. It moves you from assumption to discovery.
    - **Process Regime Identification:** Can reveal that a process is secretly operating in different states (e.g., due to raw material lots, seasons, or operator techniques), even when all batches are passing specification.
    - **Root Cause Analysis:** If a failure occurs, clustering can help determine which "family" of normal operation the failed batch was most similar to, providing critical clues for the investigation.
    """)
    
    st.info("""
    **Interactive Demo:** Use the sliders to control the underlying data structure.
    - **`True Number of Clusters`**: Changes the "correct" answer for `k`. See if the diagnostic plots (Elbow and Silhouette) can find it.
    - **`Cluster Separation` & `Spread`**: Control how easy or hard the clustering task is. Well-separated, tight clusters are easy to find and will result in a high Silhouette Score.
    """)

    with st.sidebar:
        st.sidebar.subheader("Clustering Controls")
        n_clusters_slider = st.sidebar.slider("True Number of Clusters", 2, 8, 3, 1,
            help="Controls the actual number of distinct groups generated in the data.")
        separation_slider = st.sidebar.slider("Cluster Separation", 5, 25, 15, 1,
            help="Controls how far apart the centers of the data clusters are.")
        spread_slider = st.sidebar.slider("Cluster Spread (Noise)", 1.0, 10.0, 2.5, 0.5,
            help="Controls the standard deviation (spread) within each cluster. Higher spread means more overlap.")
    
    fig, silhouette_val, optimal_k = plot_clustering(
        separation=separation_slider,
        spread=spread_slider,
        n_true_clusters=n_clusters_slider
    )
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.metric(label="üìà Optimal 'k' Found by Model", value=f"{optimal_k}",
                  help="The number of clusters recommended by the Silhouette analysis.")
        st.metric(label="üó∫Ô∏è Cluster Quality (Silhouette Score)", value=f"{silhouette_val:.3f}",
                  help="A measure of how distinct the final clusters are. Higher is better (max 1.0).")
    
        st.markdown("""
        **Reading the Dashboard:**
        - **1. Discovered Regimes:** The main plot shows how the K-Means algorithm (using the optimal `k`) has partitioned the data.
        - **2. Elbow Method:** A heuristic for finding `k`. The "elbow" (often near the true `k`) is the point of diminishing returns where adding more clusters doesn't significantly reduce the total within-cluster variance (Inertia).
        - **3. Silhouette Analysis:** A more robust method. The peak of this curve indicates the value of `k` that results in the most dense and well-separated clusters. This is often a more reliable guide than the Elbow Method.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The "Data Archaeologist" for Hidden Opportunities
    
        #### The Problem: The "All Good Batches are the Same" Assumption
        A company manufactures a product using a process that is considered stable and capable. All batches pass their final QC specifications. Management assumes that the process is operating in a single, consistent state. They believe that all successful batches are essentially interchangeable.
    
        #### The Impact: Missed Opportunities and Hidden Risks
        This assumption of uniformity is often a costly illusion that masks significant underlying patterns.
        - **Missed Optimization Opportunities:** The process might actually be operating in three distinct "regimes." Batches in "Regime A" (perhaps associated with a specific raw material supplier) might have a consistently higher yield or greater robustness, but this excellence is never identified because its signal is averaged out with the other regimes. The opportunity to make Regime A the new standard is completely missed.
        - **Hidden Risks:** "Regime C" might be producing passing material, but it could be operating dangerously close to a "cliff edge" of failure. Without identifying this sub-population of batches, the company is blind to the inherent fragility of this operating state, leaving them vulnerable to a sudden, "unexpected" wave of batch failures if a minor process shift occurs.
        - **Ineffective Troubleshooting:** When a rare failure does occur, the investigation team is flying blind. They have no idea which of the "normal" operating states the failed batch was most similar to, making root cause analysis a slow, frustrating process of guesswork.
    
        #### The Solution: A "Data Archaeologist" to Uncover Hidden Civilizations
        Unsupervised Clustering is a **"data archaeologist."** It sifts through years of historical process data without any preconceived notions and asks a simple, powerful question: "Are there any natural, hidden groupings or 'civilizations' in this data?" It is a tool of pure **discovery**. It can identify distinct process regimes, hidden customer segments, or different types of equipment behavior that were previously unknown.
    
        #### The Consequences: From One-Size-Fits-All to Targeted Excellence
        - **Without This:** The company operates on a flawed, overly simplistic model of its own processes, leaving significant value and risk on the table.
        - **With This:** The discovery of clusters is the **starting point for major strategic improvements**.
            - **It enables targeted optimization:** "Let's investigate why Cluster A is so much more efficient and make that our new global standard."
            - **It enables proactive risk reduction:** "Let's investigate why Cluster C is so much more variable and either eliminate that operating mode or implement tighter controls."
            - **It accelerates troubleshooting:** "The failed batch belongs to Cluster B. Let's focus our investigation on the raw material lots and equipment settings unique to that cluster."
        Clustering transforms a mountain of historical data from a dead archive into a living source of actionable business intelligence.
        """)
        with tabs[2]:
            st.markdown("""
            ##### Glossary of Clustering Terms
            - **Unsupervised Learning:** A type of machine learning where the algorithm learns patterns from untagged data, without a pre-defined outcome to predict.
            - **Clustering:** The task of grouping a set of objects in such a way that objects in the same group (a cluster) are more similar to each other than to those in other groups.
            - **K-Means:** A popular clustering algorithm that aims to partition `n` observations into `k` clusters in which each observation belongs to the cluster with the nearest mean (cluster centroid).
            - **Inertia (WCSS):** The sum of squared distances of samples to their closest cluster center. The Elbow Method looks for the "elbow" in the plot of Inertia vs. `k`.
            - **Silhouette Score:** A metric used to evaluate the quality of clusters. It measures how similar an object is to its own cluster compared to other clusters. Scores range from -1 to +1, with a high value indicating dense and well-separated clusters.
            """)
        with tabs[3]:
            st.error("""üî¥ **THE INCORRECT APPROACH: The "If It Ain't Broke..." Fallacy**
- An analyst discovers three distinct clusters of successful batches. A manager responds, *"Interesting, but all of those batches passed QC, so who cares? Let's move on."*
- **The Flaw:** This treats a treasure map as a doodle. The discovery is important *because* all batches passed! It means there are different-and potentially more or less robust-paths to success. One "regime" might be operating dangerously close to a failure cliff.""")
            st.success("""üü¢ **THE GOLDEN RULE: A Cluster is a Clue, Not a Conclusion**
The discovery of clusters is the **start** of the investigation, not the end.
1.  **Find the Clusters:** Use an algorithm like K-Means and diagnostics like the Silhouette plot to find a statistically sound grouping.
2.  **Profile the Clusters (This is the most important step!):** Treat each cluster as a separate group. Ask:
    - Do batches in Cluster 1 use a different raw material lot than Cluster 2?
    - Were the batches in Cluster 3 all run by the night shift?
This profiling step is what turns a statistical finding into actionable process knowledge.""")

        with tabs[4]:
            st.markdown("""
            #### Historical Context: The Dawn of Unsupervised Learning
            **The Problem:** In the 1950s, the field of statistics was almost entirely focused on "supervised" problems-testing pre-defined hypotheses or building models to predict a known outcome. But what if you didn't have a hypothesis? What if you just had a mountain of data and wanted to know if there were any natural structures hidden within it?

            **The 'Aha!' Moment:** This was a challenge faced at Bell Labs, a hotbed of innovation. **Stuart Lloyd**, in a 1957 internal memo, proposed a simple, iterative algorithm to solve this problem for signal processing. His goal was to find a small set of "codebook vectors" (cluster centroids) that could efficiently represent a much larger set of signals, a form of data compression. Independently, others like E. W. Forgy and James MacQueen developed similar ideas. MacQueen was the first to coin the term "k-means" in a 1967 paper.
            
            **The Impact:** The development of K-Means was a direct consequence of the dawn of the digital computing age, as the iterative process was finally feasible. It became a canonical example of an **unsupervised learning** algorithm, a paradigm where the goal is not to predict a known label but to discover the inherent structure in the data itself. Along with Principal Component Analysis (PCA), K-Means helped lay the groundwork for the modern field of data mining and data science.
            """)
            st.markdown("#### Mathematical Basis")
            st.markdown("K-Means is an optimization algorithm. Its objective is to find the set of `k` cluster centroids `C` that minimizes the **Within-Cluster Sum of Squares (WCSS)**, also known as inertia.")
            st.latex(r"\text{WCSS} = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2")
            st.markdown("""
            -   `k`: The number of clusters.
            -   `C·µ¢`: The set of all points belonging to cluster `i`.
            -   `Œº·µ¢`: The centroid (mean) of cluster `i`.
            The algorithm iteratively performs two steps until the cluster assignments no longer change:
            1.  **Assignment Step:** Assign each data point `x` to the cluster `C·µ¢` with the nearest centroid `Œº·µ¢`.
            2.  **Update Step:** Recalculate the centroid `Œº·µ¢` for each cluster by taking the mean of all points assigned to it.
            """)
        with tabs[5]:
            st.markdown("""
            These advanced analytical methods are key enablers for modern, data-driven approaches to process monitoring and control, as encouraged by global regulators.
            - **FDA Guidance for Industry - PAT ‚Äî A Framework for Innovative Pharmaceutical Development, Manufacturing, and Quality Assurance:** This tool directly supports the PAT initiative's goal of understanding and controlling manufacturing processes through timely measurements to ensure final product quality.
            - **FDA Process Validation Guidance (Stage 3 - Continued Process Verification):** These advanced methods provide a more powerful way to meet the CPV requirement of continuously monitoring the process to ensure it remains in a state of control.
            - **ICH Q8(R2), Q9, Q10 (QbD Trilogy):** The use of sophisticated models for deep process understanding, real-time monitoring, and risk management is the practical implementation of the principles outlined in these guidelines.
            - **21 CFR Part 11 / GAMP 5:** If the model is used to make GxP decisions (e.g., real-time release), the underlying software and model must be fully validated as a computerized system.
            """)

def render_anomaly_detection():
    """Renders the module for unsupervised anomaly detection using a stable, static tree."""
    st.markdown("""
    #### Purpose & Application: The AI Bouncer
    **Purpose:** To deploy an **AI Bouncer** for your data-a smart system that identifies rare, unexpected observations (anomalies) without any prior knowledge of what "bad" looks like. It doesn't need a list of troublemakers; it learns the "normal vibe" of the crowd and flags anything that stands out.
    
    **Strategic Application:** This is a game-changer for monitoring complex processes where simple rule-based alarms are blind to new problems.
    - **Novel Fault Detection:** It can flag a completely new type of process failure the first time it occurs, because it looks for "weirdness," not pre-defined failures.
    - **"Golden Batch" Investigation:** Can find which batches, even if they passed all specifications, were statistically unusual. These "weird-but-good" batches often hold the secrets to improving process robustness.
    """)

    st.info("""
    **Interactive Demo:** Use the **Expected Contamination** slider to control the model's sensitivity.
    1.  Observe the final classification in the **main results plot**.
    2.  The **digital tree plot** on the right shows how an anomaly (the red star) is rapidly "boxed in" by random splits.
    3.  The **score distribution plot** shows how this translates to a clean separation of anomaly scores. The slider moves the black decision threshold.
    """)

    with st.sidebar:
        st.sidebar.subheader("Anomaly Detection Controls")
        contamination_slider = st.sidebar.slider(
            "Expected Contamination (%)",
            min_value=1, max_value=25, value=10, step=1,
            help="Your assumption about the percentage of anomalies in the data. This tunes the model's sensitivity by moving the decision threshold."
        )

    # The new plotting function returns three separate, stable figures
    fig_scatter, fig_hist, fig_tree_viz, num_anomalies = plot_isolation_forest_static_tree(contamination_rate=contamination_slider/100.0)

    # --- NEW STABLE LAYOUT ---
    st.header("Anomaly Detection Dashboard")
    col1, col2 = st.columns([0.6, 0.4])

    with col1:
        st.plotly_chart(fig_scatter, use_container_width=True)

    with col2:
        st.plotly_chart(fig_tree_viz, use_container_width=True)
        st.plotly_chart(fig_hist, use_container_width=True)

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.metric("Total Data Points Scanned", value="215")
        st.metric("Anomalies Flagged by Model", value=f"{num_anomalies}")
    
        st.markdown("""
        **Reading the Dashboard:**
        - **1. Detection Results:** The model successfully identifies the scattered outliers (red crosses) while classifying the main data cloud as normal (blue circles).
        - **2. How an Isolation Tree Works:** This plot visualizes the core concept. The algorithm rapidly "boxes in" an anomalous point (the red star) with a few random, axis-aligned splits. Normal points in the dense cloud would require many more splits to be isolated.
        - **3. Score Distribution:** This plot shows the result of the isolation process. The true outliers (red histogram) have higher anomaly scores because they are easy to isolate. The black line is the decision threshold controlled by the `Contamination` slider.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The "AI Immune System" for Novel Threats
    
        #### The Problem: You Can't Write a Rule for a Failure You've Never Seen
        A company has a sophisticated process monitoring system with dozens of rule-based alarms (e.g., "Alert if Temperature > 38¬∞C"). This system is excellent at detecting known, historical failure modes. One day, a complex, novel failure occurs‚Äîa subtle change in the *correlation* between two parameters that have never failed before. All the individual parameters remain within their normal limits. The rule-based alarm system is completely silent.
    
        #### The Impact: The "Zero-Day" Failure
        This is the equivalent of a "zero-day" exploit in cybersecurity. The company is completely blind to a new and potentially catastrophic failure mode.
        - **Catastrophic Batch Loss:** The novel fault leads to a multi-million dollar batch failure that occurs without any warning from the existing control systems.
        - **Lengthy, Costly Investigation:** The subsequent root cause investigation is a nightmare. Since none of the standard alarms were triggered, the team has no starting point. They spend weeks or months sifting through terabytes of data, trying to find the "needle in a haystack" that explains the failure.
        - **Loss of Confidence:** Management loses confidence in the very expensive and complex monitoring systems that failed to prevent the disaster.
    
        #### The Solution: An "AI Immune System" That Detects "Weirdness"
        Unsupervised Anomaly Detection, using an algorithm like Isolation Forest, acts as an **AI Immune System** for your process. Unlike a rule-based system that looks for specific, known "pathogens," this system learns the signature of "healthy" data and flags anything that is statistically "non-self" or "weird." It is not programmed to find specific failures; it is trained to find **any deviation from normalcy**. Its key advantage is its ability to detect failure modes that have **never been seen before**.
    
        #### The Consequences: Resilience Against the Unknown
        - **Without This:** The company is only protected against the failures of the past. They are completely vulnerable to new and emerging failure modes, a significant risk in a world of complex, evolving processes.
        - **With This:** Anomaly Detection provides a **critical, second layer of defense**. It is the proactive surveillance system that is constantly scanning for the "unknown unknowns." When it flags an anomaly, it is providing the invaluable first signal that a new failure mode is emerging. This allows engineers to **investigate and correct novel problems** long before they lead to a catastrophic failure, making the entire manufacturing process more resilient, robust, and secure.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Anomaly Terms
        - **Anomaly Detection:** The identification of rare items, events, or observations which raise suspicions by differing significantly from the majority of the data.
        - **Unsupervised Learning:** This approach is unsupervised because it does not require pre-labeled examples of "anomalies" to learn. It learns the structure of "normal" and flags anything that deviates.
        - **Isolation Forest:** An unsupervised anomaly detection algorithm based on the principle that anomalies are "few and different," making them easier to isolate than normal points.
        - **Isolation Tree:** A random binary tree used to partition the data. The path length from the root to a leaf node represents how easy it was to isolate a point.
        - **Anomaly Score:** A score derived from the average path length across all trees in the forest. Anomalies will have a short average path length and thus a high anomaly score.
        - **Contamination:** A user-defined parameter that sets the expected proportion of anomalies in the dataset. It is used to set the decision threshold on the anomaly scores.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Glitch Hunter"**
When an anomaly is detected, the immediate reaction is to dismiss it as a data error.
- *"Oh, that's just a sensor glitch. Delete the point and move on."*
- *"Let's increase the contamination parameter until the alarms go away."*
This approach treats valuable signals as noise. It's like the bouncer seeing a problem, shrugging, and looking the other way. You are deliberately blinding yourself to potentially critical process information.""")
        st.success("""üü¢ **THE GOLDEN RULE: An Anomaly is a Question, Not an Answer**
The goal is to treat every flagged anomaly as the start of a forensic investigation.
- **The anomaly is the breadcrumb:** When the bouncer flags someone, you ask questions. "What happened in the process at that exact time? Was it a specific operator? A new raw material lot?"
- **Investigate the weird-but-good:** If a batch that passed all specifications is flagged as an anomaly, it's a golden opportunity. What made it different? Understanding these "good" anomalies is a key to process optimization.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: Flipping the Problem on its Head
        **The Problem:** For decades, "outlier detection" was a purely statistical affair, often done one variable at a time. This falls apart in high-dimensional data where an event might be anomalous not because of one value, but because of a strange *combination* of many values. Most methods focused on building a complex model of what "normal" data looks like and then flagging anything that didn't fit. This was often slow and brittle.

        **The 'Aha!' Moment:** In a 2008 paper, Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou introduced the **Isolation Forest** with a brilliantly counter-intuitive insight. Instead of trying to define "normal," they decided to just try to **isolate** every data point. They reasoned that anomalous points are, by definition, "few and different." This makes them much easier to separate from the rest of the data. Like finding a single red marble in a jar of blue ones, it's easy to "isolate" because it doesn't blend in.
        
        **The Impact:** This simple but powerful idea had huge consequences. The algorithm was extremely fast because it didn't need to model the whole dataset; it could often identify an anomaly in just a few steps. It worked well in high dimensions and didn't rely on any assumptions about the data's distribution. The Isolation Forest became a go-to method for unsupervised anomaly detection.
        """)
        st.markdown("#### Mathematical Basis")
        st.markdown("The algorithm is built on an ensemble of `iTrees` (Isolation Trees). Each `iTree` is a random binary tree built by recursively making random splits on random features.")
        st.markdown("The **path length** `h(x)` for a point `x` is the number of splits required to isolate it. Anomalies, being different, will have a much shorter average path length across all trees in the forest. The final anomaly score `s(x, n)` for a point is calculated based on its average path length `E(h(x))`:")
        st.latex(r"s(x, n) = 2^{-\frac{E(h(x))}{c(n)}}")
        st.markdown("Where `c(n)` is a normalization factor based on the sample size `n`. Scores close to 1 are highly anomalous, while scores much smaller than 0.5 are normal.")
    with tabs[5]:
        st.markdown("""
        These advanced analytical methods are key enablers for modern, data-driven approaches to process monitoring and control, as encouraged by global regulators.
        - **FDA Guidance for Industry - PAT ‚Äî A Framework for Innovative Pharmaceutical Development, Manufacturing, and Quality Assurance:** This tool directly supports the PAT initiative's goal of understanding and controlling manufacturing processes through timely measurements to ensure final product quality.
        - **FDA Process Validation Guidance (Stage 3 - Continued Process Verification):** These advanced methods provide a more powerful way to meet the CPV requirement of continuously monitoring the process to ensure it remains in a state of control.
        - **ICH Q8(R2), Q9, Q10 (QbD Trilogy):** The use of sophisticated models for deep process understanding, real-time monitoring, and risk management is the practical implementation of the principles outlined in these guidelines.
        - **21 CFR Part 11 / GAMP 5:** If the model is used to make GxP decisions (e.g., real-time release), the underlying software and model must be fully validated as a computerized system.
        """)
            
def render_advanced_ai_concepts():
    """Renders the interactive dashboard for advanced AI concepts in a V&V context."""
    st.markdown("""
    #### Purpose & Application: A Glimpse into the AI Frontier for V&V
    **Purpose:** To provide a high-level, interactive overview of how cutting-edge AI architectures can solve some of the most difficult challenges in bioprocess V&V and technology transfer.
    """)
    st.info("""
    Select an AI concept below. The diagram will become an interactive simulation of a real-world biotech problem, and a dedicated set of controls will appear in the sidebar to let you explore the solution.
    """)

    concept_key = st.radio(
        "Select an Advanced AI Concept to Explore:", 
        ["Transformers", "Graph Neural Networks (GNNs)", "Reinforcement Learning (RL)", "Generative AI"],
        horizontal=True
    )
    
    p1, p2, p3 = 0, 0, 0
    with st.sidebar:
        st.subheader(f"{concept_key} Controls")
        if concept_key == "Transformers":
            p1 = st.select_slider("Focus Attention on Day:", options=list(range(14)), value=10,
                help="Select a day in the batch. The red bars show how much the model 'attends to' other days when making a prediction based on this day's data.")
        elif concept_key == "Graph Neural Networks (GNNs)":
            p1 = st.slider("Strength of Evidence", 0.0, 2.0, 1.0, 0.1,
                           help="How strongly to propagate the 'guilt' signal backwards from the failed QC test. High strength quickly pinpoints the likely source lots.")
        elif concept_key == "Reinforcement Learning (RL)":
            p1 = st.slider("Cost of Feed Waste ($/L)", 1.0, 10.0, 3.0, 0.5,
                           help="How 'expensive' wasted feed media is. Higher cost forces the AI agent to learn a more conservative, efficient feeding strategy.")
            p2 = st.slider("Process Variability", 0.1, 1.0, 0.3, 0.1,
                           help="The amount of random, unpredictable noise in the cell culture. High variability makes aggressive control strategies risky.")
        elif concept_key == "Generative AI":
            p1 = st.slider("Model 'Creativity' (Variance)", 0.1, 1.0, 0.3, 0.1,
                           help="Controls the diversity of the generated failure data. Low values are safe but less informative. High values are diverse but may be unrealistic.")
    
    fig = plot_advanced_ai_concepts(concept_key, p1, p2, p3)
    
    col1, col2 = st.columns([0.65, 0.35])
    with col1:
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
        
        with tabs[0]:
            if concept_key == "Transformers":
                st.metric(label="üß† Core Concept", value="Self-Attention")
                st.markdown("**The AI Historian for Your Batch Record.** It learns long-range dependencies, such as how an early event in a batch influences the final outcome weeks later.")
            elif concept_key == "Graph Neural Networks (GNNs)":
                st.metric(label="üß† Core Concept", value="Message Passing")
                st.markdown("**The System-Wide Process Cartographer.** It models your entire facility as a network to trace contamination or failures back to their root cause.")
            elif concept_key == "Reinforcement Learning (RL)":
                st.metric(label="üß† Core Concept", value="Reward Maximization")
                st.markdown("**The AI Process Optimization Pilot.** It learns the optimal control strategy for a process by running millions of experiments in a safe, digital twin environment.")
            elif concept_key == "Generative AI":
                st.metric(label="üß† Core Concept", value="Distribution Learning")
                st.markdown("**The Synthetic Data Factory.** It learns from a few examples of rare events and generates new, realistic synthetic examples to train more robust predictive models.")
        with tabs[1]:
            st.markdown("""
            ### The Business Case: Solving "Impossible" Problems at Scale
    
            #### The Problem: The Limits of Traditional Analytics
            Modern biopharmaceutical development and manufacturing present challenges of immense scale and complexity that traditional statistical tools were not designed to handle. For example:
            - How do you trace a single contamination event back through a complex supply chain of hundreds of raw material lots and dozens of shared equipment pieces?
            - How do you find the truly optimal, multi-day feeding strategy for a bioreactor with dozens of interacting parameters?
            - How do you train a robust predictive model when catastrophic failures are so rare that you only have a handful of examples to learn from?
    
            #### The Impact: Unsolved Problems and Capped Potential
            These challenges represent the final frontier of process optimization. Without new tools, the business is forced to accept certain limitations as "the cost of doing business."
            - **Inconclusive Investigations:** Root cause analysis for contamination often ends in "inconclusive," leading to costly, broad-based CAPAs (e.g., "retrain all operators") that don't fix the underlying issue.
            - **Sub-Optimal "Cookbook" Processes:** Bioreactor recipes are based on a few successful DOEs but are likely sub-optimal, leaving significant yield and profit on the table.
            - **Fragile Predictive Models:** A model trained on only a few failure examples is brittle and unreliable, unable to provide the high-confidence predictions needed for real-time release.
    
            #### The Solution: A Purpose-Built AI for Every Complex Problem
            Advanced AI architectures are not just "better" machine learning; they are **purpose-built solutions for specific, complex data structures and problems**.
            1.  **For Networks (GNNs):** A Graph Neural Network is designed to understand the **relational structure** of your supply chain, making it the perfect tool for automated, high-speed root cause analysis in lot genealogy.
            2.  **For Dynamic Control (RL):** Reinforcement Learning is designed to solve **sequential decision-making problems**. It can explore millions of potential strategies in a digital twin to discover a novel, high-performance feeding strategy that a human never would.
            3.  **For Data Scarcity (Generative AI):** A Generative AI model can learn the "essence" of a rare failure from just a few examples and then **generate hundreds of new, realistic synthetic examples**, providing the rich dataset needed to train a robust and reliable predictive model.
    
            #### The Consequences: Unlocking the Next Level of Performance
            - **Without This:** The company's performance hits a plateau, limited by the complexity that traditional tools can handle.
            - **With This:** These advanced AI tools are the key to unlocking the next level of operational excellence. They enable **automated root cause analysis**, discover **super-human process control strategies**, and **de-risk operations by augmenting rare data**. This represents a significant step-change in capability, providing a massive competitive advantage in process robustness, efficiency, and speed.
            """)
        with tabs[2]:
            # --- THIS IS THE DYNAMIC GLOSSARY ---
            st.markdown("##### Glossary of Key Terms")
            if concept_key == "Transformers":
                st.markdown("- **Self-Attention:** A mechanism that allows a model to weigh the importance of different parts of the input sequence when processing a specific part. This is the core innovation of the Transformer.")
                st.markdown("- **Tokenization:** The process of converting a sequence of data (like a time series) into a series of discrete units or 'tokens' that the model can understand.")
            elif concept_key == "Graph Neural Networks (GNNs)":
                st.markdown("- **Graph:** A data structure consisting of 'nodes' (entities, e.g., raw material lots) and 'edges' (the relationships between them).")
                st.markdown("- **Message Passing:** The fundamental operation of a GNN, where each node aggregates information from its neighbors to update its own state or representation.")
            elif concept_key == "Reinforcement Learning (RL)":
                st.markdown("- **Agent:** The AI model that learns to make decisions (e.g., the feeding policy).")
                st.markdown("- **Environment:** The world the agent interacts with. In this case, a high-fidelity simulation of a bioreactor (a 'digital twin').")
                st.markdown("- **Reward:** A numerical signal that tells the agent how good or bad its last action was. The agent's goal is to maximize the cumulative reward over time.")
            elif concept_key == "Generative AI":
                st.markdown("- **Generative Model:** An AI model that learns the underlying probability distribution of a dataset and can generate new, synthetic samples from that distribution.")
                st.markdown("- **Discriminative Model:** The more common type of AI model, which learns to distinguish between different categories of data (e.g., a classifier that predicts Pass/Fail).")
            # --- END OF DYNAMIC GLOSSARY ---

        with tabs[3]:
            st.success("""
            üü¢ **THE GOLDEN RULE: Match the Architecture to the Problem Structure**
            The core insight of modern AI is that **data has a shape**.
            - If your problem has the shape of a **sequence** (like a batch record), use an architecture designed for sequences (like a Transformer).
            - If your problem has the shape of a **network** (like a supply chain), use an architecture designed for networks (like a GNN).
            - If your problem has the shape of a **control loop** (like optimizing a process), use an architecture designed for control (like RL).
            - If your problem is a **lack of data**, use an architecture designed to **create** data (like Generative AI).
            Using the right architecture is the key to unlocking state-of-the-art performance.
            """)
            if concept_key == "Transformers":
                st.success("üü¢ **THE GOLDEN RULE:** Tokenize Your Process Narrative. Convert continuous data into a discrete sequence of meaningful events (e.g., `[Feed_Event, pH_Excursion, Operator_Shift]`).")
            elif concept_key == "Graph Neural Networks (GNNs)":
                st.success("üü¢ **THE GOLDEN RULE:** Your Graph IS Your Model. The most important work is defining the nodes (e.g., equipment, lots) and edges (e.g., 'used-in' relationships).")
            elif concept_key == "Reinforcement Learning (RL)":
                st.success("üü¢ **THE GOLDEN RULE:** The Digital Twin is the Dojo. An RL agent must be trained in a high-fidelity simulation to learn optimal control strategies with zero real-world risk.")
            elif concept_key == "Generative AI":
                st.success("üü¢ **THE GOLDEN RULE:** Validate the Forgeries. The generated data is only useful if it is proven to be statistically indistinguishable from real data.")

        
        with tabs[4]:
            if concept_key == "Transformers":
                st.markdown("The 2017 Google Brain paper, **'Attention Is All You Need,'** introduced the Transformer. It discarded recurrence and relied solely on **self-attention**, allowing every point in a sequence to directly 'look at' every other point. This is the foundation of all modern Large Language Models.")
            elif concept_key == "Graph Neural Networks (GNNs)":
                st.markdown("Pioneered around 2017, GNNs generalize deep learning to irregular graph data. The core insight is **message passing**, where each node updates its state by aggregating information from its neighbors, allowing complex relationships to be modeled.")
            elif concept_key == "Reinforcement Learning (RL)":
                st.markdown("With roots in control theory, the modern era of RL was sparked by **DeepMind's AlphaGo** (2016). By combining traditional RL with deep neural networks, they demonstrated that an AI agent could learn superhuman strategies in complex environments through self-play.")
            elif concept_key == "Generative AI":
                st.markdown("Revolutionized in 2014 by Ian Goodfellow's invention of **Generative Adversarial Networks (GANs)**, which pit a 'Generator' and a 'Discriminator' against each other. This adversarial game forces the generator to create incredibly realistic synthetic data.")
        
        with tabs[5]:
            st.markdown("""
            These advanced AI/ML methods are key enablers for modern, data-driven approaches to process monitoring and control, as encouraged by global regulators.
            - **FDA AI/ML Action Plan & GMLP:** These tools are part of the emerging field of AI/ML in regulated industries. They must align with principles of transparency, risk management, and model lifecycle management as defined in developing guidance like the FDA's Action Plan and Good Machine Learning Practice (GMLP).
            - **FDA Guidance for Industry - PAT ‚Äî A Framework for Innovative Pharmaceutical Development, Manufacturing, and Quality Assurance:** This tool directly supports the PAT initiative's goal of understanding and controlling manufacturing processes through timely measurements to ensure final product quality.
            - **ICH Q8(R2), Q9, Q10 (QbD Trilogy):** The use of sophisticated models for deep process understanding, real-time monitoring, and risk management is the practical implementation of the principles outlined in these guidelines.
            - **21 CFR Part 11 / GAMP 5:** If the model is used to make GxP decisions (e.g., real-time release), the underlying software and model must be fully validated as a computerized system.
            """)
#==============================================================================================================================================================================================
#======================================================================NEW METHODS UI RENDERING ==============================================================================================
#=============================================================================================================================================================================================
# ==============================================================================
# UI RENDERING FUNCTION (Method 1)
# ==============================================================================
def render_mewma_xgboost():
    """Renders the MEWMA + XGBoost Diagnostics module."""
    st.markdown("""
    #### Purpose & Application: The AI First Responder
    **Purpose:** To create a two-stage "Detect and Diagnose" system. A **Multivariate EWMA (MEWMA)** chart acts as a highly sensitive alarm for small, coordinated drifts in a process. When it alarms, a pre-trained **XGBoost + SHAP model** instantly performs an automated root cause analysis.
    
    **Strategic Application:** This represents the state-of-the-art in intelligent process monitoring.
    - **Detect:** The MEWMA chart excels at finding subtle "stealth shifts" that individual charts would miss because it understands the process's normal correlation structure.
    - **Diagnose:** Instead of technicians guessing the cause of an alarm, the SHAP plot provides an immediate, data-driven "Top Suspects" list, dramatically accelerating troubleshooting.
    """)
    st.info("""**Interactive Demo:** Use the sliders to control the simulated process. **The tool is computationally intensive, and depending on latency, it may take a minute to load.**
    - **`Gradual Drift Magnitude`**: Controls how quickly the Temp and Pressure variables drift away from baseline.
    - **`MEWMA Lambda (Œª)`**: Controls the "memory" of the chart. A smaller lambda makes it more sensitive to tiny, persistent drifts.
    """)
    with st.sidebar:
        st.sidebar.subheader("MEWMA + XGBoost Controls")
        drift_slider = st.sidebar.slider("Gradual Drift Magnitude", 0.0, 0.1, 0.03, 0.01, help="Controls the rate of the slow, creeping drift introduced into Temp & Pressure.")
        lambda_slider = st.sidebar.slider("MEWMA Lambda (Œª)", 0.05, 0.5, 0.2, 0.05, help="Controls the 'memory' of the MEWMA chart. Smaller values give longer memory.")
    fig_dashboard, buf_waterfall, alarm_time = plot_mewma_xgboost(drift_magnitude=drift_slider, lambda_mewma=lambda_slider)
    st.plotly_chart(fig_dashboard, use_container_width=True)
    st.subheader("Diagnostic Analysis")
    if alarm_time:
        col1, col2 = st.columns([0.4, 0.6])
        with col1:
            st.metric("Alarm Status", "üö® OUT-OF-CONTROL")
            st.metric("First Detection Time", f"Observation #{alarm_time}")
            st.markdown("""**Interpretation:**
            - **Top Plot:** The gradual drift is almost invisible to the naked eye.
            - **Bottom Plot:** The MEWMA chart successfully accumulates evidence of this drift until it crosses the control limit.""")
        with col2:
            st.markdown("##### Automated Root Cause Diagnosis (SHAP Waterfall)")
            st.image(buf_waterfall)
            st.markdown("The waterfall plot explains the alarm. Red bars (`Temp`, `Pressure`) pushed the risk of failure higher.")
    else:
        st.success("‚úÖ IN-CONTROL: No alarm detected in the monitoring phase.")
    st.markdown("---")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Phase 1 (Training):** A multivariate model of the normal process is built using data from validated, successful runs. A classifier model (XGBoost) is also trained to distinguish between "in-control" and "out-of-control" states based on historical data.
        2.  **Phase 2 (Monitoring):** The **MEWMA chart** monitors the process in real-time. It is the high-sensitivity "smoke detector."
        3.  **Phase 3 (Diagnosis):** When the MEWMA alarms, the data from that time point is fed to the **XGBoost + SHAP** model. The resulting waterfall plot provides an immediate, rank-ordered list of the variables that contributed most to the alarm, guiding the investigation.
        
        **The Strategic Insight:** This hybrid approach solves the biggest weakness of traditional multivariate SPC. While charts like Hotelling's T¬≤ or MEWMA are good at *detecting* a problem, they are poor at *diagnosing* it. By fusing a sensitive statistical detector with a powerful, explainable AI diagnostician, you get the best of both worlds: robust detection and rapid, data-driven root cause analysis.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The Automated Forensic Investigator
    
        #### The Problem: The "Smoke Alarm with No Location"
        A complex bioprocess with over 50 correlated parameters is being monitored by a standard multivariate control chart (like Hotelling's T¬≤). At 3 AM, the chart alarms, signaling a deviation from the normal operating state. The on-call engineer is paged. They know *that* there is a problem, but they have no idea *which* of the 50 parameters (or combination of them) is the cause.
    
        #### The Impact: The Diagnostic Bottleneck and Extended Downtime
        This "diagnostic bottleneck" is a major source of inefficiency and risk in modern manufacturing.
        - **Slow, Manual Investigation:** The engineer begins a slow, manual "whack-a-mole" investigation, individually checking the trends of dozens of parameters, trying to find a signal. This can take hours, or even days.
        - **Extended Downtime & Scrap Production:** While this investigation is ongoing, the process is either shut down (costing millions in lost production time) or, worse, allowed to continue running, potentially producing tens of millions of dollars of non-conforming, scrap material.
        - **Inconclusive Findings:** The root cause of a subtle, coordinated drift can be almost impossible to find with the naked eye, leading to an inconclusive investigation and the risk that the problem will recur.
    
        #### The Solution: A Two-Stage "Detect and Diagnose" System
        This hybrid model provides an elegant, two-stage solution that mimics an expert's workflow, but at machine speed.
        1.  **The Detector (MEWMA):** A highly sensitive Multivariate EWMA chart acts as the "smoke detector." Its sole job is to provide a single, reliable signal that *something* in the process has changed.
        2.  **The Diagnostician (XGBoost + SHAP):** The moment the MEWMA alarms, it automatically triggers the AI diagnostician. An XGBoost model confirms the anomalous state, and a SHAP analysis instantly provides a **ranked "Top Suspects" list**‚Äîa waterfall plot showing exactly which parameters contributed most to the alarm.
    
        #### The Consequences: From Days of Guesswork to Minutes of Confirmation
        - **Without This:** A multivariate alarm is the start of a long, stressful, and expensive manual investigation.
        - **With This:** The hybrid system transforms the workflow. The alarm is no longer just a problem; it's a problem delivered with its own **automated root cause analysis**. The engineer's job shifts from slow, open-ended discovery to rapid, targeted **confirmation**. This **accelerates troubleshooting from days to minutes**, minimizes downtime, prevents scrap, and ensures that corrective actions are precise, effective, and data-driven.
        """)
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **MEWMA (Multivariate EWMA):** A multivariate SPC chart that applies the "memory" of an EWMA to a vector of correlated process variables. It is highly sensitive to small, persistent drifts in the process mean.
        - **XGBoost (Extreme Gradient Boosting):** A powerful and efficient machine learning algorithm that builds a predictive model as an ensemble of many simple decision trees.
        - **SHAP (SHapley Additive exPlanations):** A game theory-based method for explaining the output of any machine learning model. It calculates the contribution of each feature to a specific prediction.
        - **Waterfall Plot:** A SHAP visualization that shows how each feature's SHAP value contributes to pushing the model's prediction from a baseline value to the final output.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The 'Whack-a-Mole' Investigation**
A multivariate alarm sounds. Engineers frantically check dozens of individual parameter charts, trying to find a clear signal. They might chase a noisy pH sensor, ignoring the subtle, combined drift in Temp and Pressure that is the real root cause, wasting valuable time while the process may be producing non-conforming material.""")
        st.success("""üü¢ **THE GOLDEN RULE: Detect Multivariately, Diagnose with Explainability**
1.  **Trust the multivariate alarm.** It sees the process holistically and can detect problems that are invisible to univariate charts.
2.  **Use the explainable AI diagnostic (SHAP) as your first investigative tool.** It instantly narrows the search space from all possible causes to the most probable ones based on historical patterns.
3.  **Confirm with SME knowledge.** The SHAP output is a powerful clue, not a final verdict. Use this data-driven starting point to guide the subject matter expert's investigation. This turns a slow, manual investigation into a rapid, data-driven confirmation.""")
    with tabs[4]:
        st.markdown("""
        #### Historical Context: The Diagnostic Bottleneck
        **The Problem:** By the 1980s, engineers had powerful multivariate detection tools like Hotelling's T¬≤ chart. However, these charts were slow to detect small, persistent drifts. The invention of the univariate EWMA chart was a major step forward, but the multivariate world was still waiting for its "high-sensitivity" detector.

        **The First Solution (MEWMA):** In 1992, Lowry et al. published their paper on the Multivariate EWMA (MEWMA) chart. The insight was a direct generalization: apply the "memory" and "weighting" concepts of EWMA to the vector of process variables. This created a chart that was exceptionally good at detecting small, coordinated shifts that T¬≤ would miss.

        **The New Problem (The Diagnostic Bottleneck):** But MEWMA created a new challenge. An alarm from a MEWMA chart is just a single number crossing a line. It tells you *that* the system has drifted, but gives no information about *which* variables are the cause. This has been the primary weakness of MSPC for decades.

        **The Modern Fusion:** This is where the AI revolution provided the missing piece. **XGBoost** (2014) offered a way to build highly accurate models to predict an alarm state, and **SHAP** (2017) provided the key to unlock that model's "black box." By fusing the robust statistical detection of MEWMA with the powerful, explainable diagnostics of XGBoost and SHAP, we finally solved the diagnostic bottleneck, creating a true "detect and diagnose" system.
        """)
    with tabs[5]:
        st.markdown("""
        This advanced hybrid system is a state-of-the-art implementation of the principles of modern process monitoring and control.
        - **FDA Process Validation Guidance (Stage 3 - Continued Process Verification):** This tool provides a highly effective method for meeting the CPV requirement of continuously monitoring the process to ensure it remains in a state of control, and for investigating any departures.
        - **FDA Guidance for Industry - PAT:** This "Detect and Diagnose" system is a direct implementation of the PAT goal of understanding and controlling manufacturing through timely measurements and feedback loops.
        - **ICH Q9 (Quality Risk Management):** By providing rapid root cause analysis, this system significantly reduces the risk associated with process deviations, minimizing their duration and impact.
        - **GAMP 5 & 21 CFR Part 11:** As this system uses an AI/ML model to provide diagnostic information for a GxP process, the model and the software it runs on would need to be validated as a Computerized System.
        """)
# ==============================================================================
# UI RENDERING FUNCTION (Method 2)
# ==============================================================================
def render_bocpd_ml_features():
    """Renders the Bayesian Online Change Point Detection module."""
    st.markdown("""
    #### Purpose & Application: The AI Seismologist
    **Purpose:** To provide a real-time, probabilistic assessment of process stability. Unlike traditional charts that give a binary "in/out" signal, **Bayesian Online Change Point Detection (BOCPD)** calculates the full probability distribution of the "current run length" (time since the last change). It acts like a seismologist, constantly looking for the tremors that signal a process earthquake.
    
    **Strategic Application:** This is a sophisticated method for monitoring high-value processes where understanding uncertainty is critical.
    - **Monitoring Model Performance:** Instead of monitoring raw data, this tool monitors the forecast errors (residuals) from a predictive ML model. BOCPD can detect when the model's performance suddenly degrades, signaling that the underlying process has changed in a way the model doesn't understand.
    - **Adaptive Alarming:** Instead of a fixed control limit, you can set alarms based on probability (e.g., "alarm if the probability of a recent changepoint exceeds 90%").
    """)

    st.info("""
    **Interactive Demo:** Use the sliders to control the nature of the process change at observation #100.
    - The top two plots show the raw data and the model residuals. Notice how the change is much more apparent in the residuals.
    - The bottom two plots show the BOCPD output. A high-confidence detection is marked by the **"Most Likely Run Length"** dropping to zero and the **"Probability of a Changepoint"** spiking towards 100%.
    """)

    with st.sidebar:
        st.sidebar.subheader("BOCPD Controls")
        autocorr_slider = st.sidebar.slider("Change in Autocorrelation", 0.0, 0.8, 0.4, 0.1,
            help="How much the process's 'memory' or smoothness changes at the changepoint. A large change is easier to detect.")
        noise_inc_slider = st.sidebar.slider("Increase in Noise (Factor)", 1.0, 5.0, 2.0, 0.5,
            help="The factor by which the process standard deviation increases after the change point. A value of 2 means the noise doubles.")

    fig, change_prob = plot_bocpd_ml_features(autocorr_shift=autocorr_slider, noise_increase=noise_inc_slider)
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.subheader("Analysis & Interpretation")
        st.metric("True Changepoint Location", "Obs #100")
        st.metric("Detection Certainty at Changepoint", f"{change_prob:.1%}",
                  help="The posterior probability that a changepoint occurred at exactly observation #100.")
        
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Raw Data (Plot 1):** Shows the simulated process. The change in dynamics at the red line (e.g., becoming less smooth and more noisy) is subtle and very difficult to spot by eye.
        2.  **Model Residuals (Plot 2):** We monitor the errors from a simple predictive model that was trained on the "normal" process. Because the process dynamics change at the changepoint, the model starts making bigger, more volatile errors, making the change much more visible.
        3.  **Most Likely Run Length (Plot 3):** The algorithm's "best guess" of how long the process has been stable. Note the sharp drop to zero at the changepoint, signaling a reset.
        4.  **Probability of a Changepoint (Plot 4):** This is the clearest signal. The algorithm becomes highly confident that a changepoint has just occurred right at observation #100.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The "Earthquake Detector" for Your Process
    
        #### The Problem: The "Silent Shift" in Process Behavior
        A critical manufacturing process appears to be stable. The mean is on target, the variance is within limits, and the traditional SPC charts are all in-control. However, a subtle, fundamental change has occurred: a filter is beginning to clog, or a catalyst is slowly losing its effectiveness. This doesn't change the *average* value of the process, but it changes its **dynamic behavior**‚Äîthe data becomes less smooth (lower autocorrelation) and more erratic (higher noise).
    
        #### The Impact: The Unforeseen Catastrophe
        Traditional monitoring systems are completely blind to this kind of change.
        - **False Sense of Security:** The company believes it is operating in a state of control, while in reality, the underlying process is fundamentally different and less stable than the validated process. The process has become a ticking time bomb.
        - **Catastrophic Failure:** The degraded state makes the process highly vulnerable to a small input variation that it would have easily handled before. This triggers a sudden, "unexpected" and catastrophic batch failure.
        - **Inconclusive Investigation:** The root cause analysis is a nightmare. All the standard charts look normal right up until the point of failure, providing no leading indicators or clues as to why the process suddenly failed.
    
        #### The Solution: Monitoring the "Behavior," Not Just the "Value"
        This hybrid approach is a sophisticated **"earthquake detector"** for your process. It is not designed to detect a simple shift in the average; it is designed to detect a fundamental change in the **rules that govern the process**.
        1.  **The ML Model (The Seismometer):** A simple time series model (like an AR model) is trained on the "normal" process data. This model learns the normal "rhythm" or dynamic fingerprint of a healthy process.
        2.  **The Residuals (The Seismic Signal):** The model's one-step-ahead forecast errors (the residuals) are monitored. As long as the process is stable, these errors will be small and random. When the process dynamics change, the model's predictions will start to fail, and the residuals will become large and volatile. This is the seismic signal.
        3.  **BOCPD (The Analyst):** The Bayesian Online Change Point Detection algorithm analyzes this seismic signal in real-time, providing a direct, high-confidence probability that the "ground has shifted" and the process has entered a new, unknown state.
    
        #### The Consequences: From "What is the Value?" to "Is the Process Behaving Normally?"
        - **Without This:** The company is only monitoring the surface-level values of its process, blind to deeper changes in its fundamental health and stability.
        - **With This:** This system provides a **deeper layer of process understanding and control**. It allows the company to detect subtle but critical changes in process dynamics long before they can escalate into a major failure. It is the essential tool for managing the health of complex, dynamic systems where the *behavior* of the process is just as important as its average output.
        """)
        
    with tabs[1]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **BOCPD (Bayesian Online Change Point Detection):** A real-time algorithm that calculates the full probability distribution of the "run length" (the time since the last statistically significant change in a process).
        - **Changepoint:** A point in time where the statistical properties of a time series (e.g., its mean, variance, or autocorrelation) change.
        - **Run Length:** The amount of time that has passed since the last changepoint. BOCPD's output is a probability distribution over this value.
        - **Residuals:** The difference between an observed value and a predicted value from a model. Monitoring residuals is a powerful way to detect when a process deviates from its expected behavior.
        - **Hazard Rate:** A parameter in the BOCPD algorithm that represents the prior probability of a changepoint occurring at any given step.
        """)
        
    with tabs[2]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The 'Delayed Reaction'**
Waiting for a traditional SPC chart to alarm on a complex signal (like a change in autocorrelation or variance) can take a very long time, if it alarms at all. The underlying assumptions of the chart are violated, but it may not produce a clear signal until significant out-of-spec material has been produced.""")
        st.success("""üü¢ **THE GOLDEN RULE: Monitor Model Residuals, Not Just Raw Data**
For complex, dynamic processes, the most sensitive way to detect a change is to:
1.  **Build a model of the process's **normal** behavior.** This model (which could be a simple AR(1) model or a complex Neural Network) learns the "fingerprint" of a stable process.
2.  **Monitor the **residuals** (forecast errors) of that model.**
3.  When the process changes, the model's assumptions will be violated, and the residuals will change their behavior (e.g., become larger, more volatile, or biased). This provides a powerful and early signal that something is fundamentally wrong. BOCPD is an excellent algorithm for detecting this change in the residuals.""")

    with tabs[3]:
        st.markdown("""
        #### Historical Context: From Offline to Online
        **The Problem:** For decades, changepoint detection was primarily an *offline*, retrospective analysis. An engineer would collect an entire dataset, run a complex algorithm, and get a result like: "A significant change was detected at observation #152." While useful for forensic investigations after a failure, this was useless for preventing the failure in the first place.

        **The 'Aha!' Moment:** In their 2007 paper, "Bayesian Online Changepoint Detection," **Ryan P. Adams and David J.C. MacKay** presented a brilliantly elegant solution. Their key insight was to reframe the problem from finding a single "best" changepoint to calculating the full, evolving probability distribution of the "run length" (the time since the last changepoint). 
        
        **The Impact:** This probabilistic approach was a game-changer. It provided a much richer output than a simple binary alarm, and its recursive, online nature was computationally efficient enough to run in real-time on streaming data. It effectively transformed changepoint detection from a historical analysis tool into a modern, real-time process monitoring system, especially powerful when combined with feature extraction from modern ML models.
        """)
        
    with tabs[4]:
        st.markdown("""
        This advanced hybrid system is a state-of-the-art implementation of the principles of modern process monitoring and control.
        - **FDA Process Validation Guidance (Stage 3 - Continued Process Verification):** This tool provides a highly effective method for meeting the CPV requirement of continuously monitoring the process to ensure it remains in a state of control, and for investigating any departures.
        - **FDA Guidance for Industry - PAT:** This "Model and Monitor Residuals" approach is a direct implementation of the PAT goal of understanding and controlling manufacturing through timely measurements and feedback loops.
        - **ICH Q9 (Quality Risk Management):** By providing rapid and probabilistic detection of process changes, this system can significantly reduce the risk of producing non-conforming material.
        - **GAMP 5 & 21 CFR Part 11:** As this system uses an ML model to provide diagnostic information for a GxP process, the model and the software it runs on would need to be validated as a Computerized System.
        """)
# ==============================================================================
# UI RENDERING FUNCTION (Method 3)
# ==============================================================================
def render_kalman_nn_residual():
    """Renders the Kalman Filter + Residual Chart module."""
    st.markdown("""
    #### Purpose & Application: The AI Navigator
    **Purpose:** To track and predict the state of a dynamic process in real-time, even with noisy sensor data. The **Kalman Filter** acts as an optimal "navigator," constantly predicting the process's next move and then correcting its course based on the latest measurement. The key output is the **residual**‚Äîthe degree of "surprise" at each measurement.
    
    **Strategic Application:** This is fundamental for state estimation in any time-varying system.
    - **Intelligent Filtering:** Provides a smooth, real-time estimate of a process's true state, filtering out sensor noise.
    - **Early Fault Detection:** By placing a control chart on the residuals, we create a highly sensitive alarm system. If the process behaves in a way the Kalman Filter didn't predict, the residuals will jump out of their normal range, signaling a fault long before the raw data looks abnormal.
    """)
    st.info("""
    **Interactive Demo:** Use the sliders to tune the Kalman Filter. At time #70, a sudden shock is introduced.
    - **`Process Noise (Q)`**: This is a critical tuning parameter. A **low Q** means you trust your model more (smoother estimate). A **high Q** means you trust your measurements more (estimate tracks noisy data).
    - **`Measurement Noise (R)`**: The known noise of your sensor.
    - **`Shock Magnitude`**: The size of the process fault.
    """)
    with st.sidebar:
        st.sidebar.subheader("Kalman Filter Controls")
        q_slider = st.sidebar.slider("Process Noise (Q)", 0.0, 0.5, 0.01, 0.005, format="%.4f",
            help="Model Uncertainty. How much you expect the true state to randomly change at each step. Higher Q makes the filter more responsive to measurements.")
        noise_slider = st.sidebar.slider("Measurement Noise (R)", 0.5, 5.0, 1.0, 0.5,
            help="Sensor Uncertainty. The known standard deviation of the measurement sensor.")
        shock_slider = st.sidebar.slider("Process Shock Magnitude", 1.0, 20.0, 10.0, 1.0,
            help="The magnitude of the sudden, unexpected event that occurs at time #70.")

    fig, alarm_time = plot_kalman_nn_residual(
        measurement_noise=noise_slider,
        shock_magnitude=shock_slider,
        process_noise_q=q_slider
    )
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.subheader("Analysis & Interpretation")
        st.metric("Process Shock Event", "Time #70")
        st.metric("Alarm on Residuals", f"Time #{alarm_time}" if alarm_time else "Not Detected", help="The time the residual chart first detected the shock.")

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **State Estimation (Plot 1):** The red line is the Kalman Filter's "best guess" of the true state (black dashed line), created by optimally blending its internal model with the noisy measurements (grey dots). The shaded red area is the filter's own uncertainty about its estimate.
        2.  **Tuning the Filter (Sidebar):**
            - **Increase `Process Noise (Q)`:** Watch the red estimate line become "jittery" and follow the noisy measurements more closely. The uncertainty band also widens. This tells the filter "don't trust your model, trust the data."
            - **Decrease `Process Noise (Q)`:** The red line becomes much smoother, ignoring the noisy data. This tells the filter "my model is good, trust the prediction."
        3.  **Fault Detection (Plot 2):** This chart shows the "surprise" at each step. Notice the huge spike at time #70 when the process shock occurs‚Äîthe measurement was far from what the filter predicted. This provides an unambiguous alarm.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The AI Navigator for Seeing Through the Fog
    
        #### The Problem: The "Noisy Sensor" Dilemma
        A critical process is controlled based on a sensor that is inherently "noisy"‚Äîits readings fluctuate randomly around the true process value. This noise is so large that it completely masks the small, subtle process drifts that are the leading indicators of a future failure.
    
        #### The Impact: Blindness to Emerging Failures
        This "fog" of sensor noise creates a dangerous operational blind spot.
        - **Delayed Detection:** A standard SPC chart placed on the raw, noisy sensor data will have extremely wide control limits. It will be completely insensitive to small but meaningful process shifts. A critical deviation might go undetected for hours or even days.
        - **Inability to Control:** It's impossible to implement advanced process control strategies. Any attempt to automatically adjust the process based on the noisy sensor readings would result in "process tampering"‚Äîthe control system would be constantly over-reacting to random noise, actually increasing variability and making the process worse.
        - **False Sense of Security:** The team may believe the process is stable, when in reality, it is slowly drifting out of its validated state, hidden behind the wall of noise.
    
        #### The Solution: An "AI Navigator" to Find the True Signal
        The Kalman Filter is an optimal "AI Navigator" for your process. It is a powerful algorithm that can see through the fog of sensor noise to provide a crystal-clear, real-time estimate of the process's **true, hidden state**. It does this by intelligently blending two pieces of information at every time step:
        1.  **Its internal model's prediction** of where the process *should* be.
        2.  **The new, noisy measurement** from the sensor.
        
        The key output for fault detection is the **residual**‚Äîthe difference between the prediction and the measurement. This is a measure of "surprise." By placing a control chart on these residuals, we create a hyper-sensitive alarm system.
    
        #### The Consequences: Seeing Clearly and Acting Early
        - **Without This:** The company is flying blind, making critical decisions based on noisy, unreliable data. They are forced into a reactive mode, only able to detect large, catastrophic failures.
        - **With This:** The Kalman Filter provides a **smooth, reliable, real-time estimate of the true process state**, enabling tighter, more effective control. More importantly, the residual chart acts as a powerful **early warning system**. It can detect a small process fault the moment it occurs, because even a small deviation creates a "surprise" that stands out from the now-filtered noise. This allows engineers to **detect and correct faults at their inception**, long before they can impact product quality.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **Kalman Filter:** An optimal algorithm for estimating the hidden state of a dynamic system from a series of noisy measurements. It operates in a two-step "predict-update" cycle.
        - **State Estimation:** The process of estimating the true, unobservable internal state of a system based on external, noisy measurements.
        - **Residual (or Innovation):** The difference between an actual measurement and the Kalman Filter's prediction of that measurement. A large residual indicates a "surprise" and signals that the process is not behaving as the model expected.
        - **Process Noise (Q):** A tuning parameter representing the uncertainty in the process model itself. It's the amount you expect the true state to randomly change between time steps.
        - **Measurement Noise (R):** A parameter representing the known uncertainty or variance of the measurement sensor.
        - **Kalman Gain:** A value calculated at each step that determines how much weight to give to the new measurement versus the model's prediction.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: Monitoring Raw, Noisy Data**
Charting the raw measurements (grey dots) directly would lead to a wide, insensitive control chart. The process shock might not even trigger an alarm if it's small relative to the measurement noise. You are blind to subtle deviations from the expected *behavior*.""")
        st.success("""üü¢ **THE GOLDEN RULE: Model the Expected, Monitor the Unexpected**
1.  Use a dynamic model (like a Kalman Filter or a Neural Network) to capture the known, predictable behavior of your process.
2.  This model separates the signal into two streams: the predictable part (the state estimate) and the unpredictable part (the residuals).
3.  Place your high-sensitivity control chart on the **residuals**. This is monitoring the "unexplained" portion of the data, which is where novel faults will always appear first.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Space Race to Bioreactor
        **The Problem:** During the height of the Cold War and the Space Race, a fundamental challenge was navigation. How could you guide a missile, or more inspiringly, a spacecraft to the Moon, using only a stream of noisy, imperfect sensor readings? You needed a way to fuse the predictions from a physical model (orbital mechanics) with the incoming data to get the best possible estimate of your true position and velocity.

        **The 'Aha!' Moment:** In 1960, **Rudolf E. K√°lm√°n** published his landmark paper describing a recursive algorithm that provided the optimal solution to this problem. The **Kalman Filter** was born. Its elegant two-step "predict-update" cycle was computationally efficient enough to run on the primitive computers of the era.
        
        **The Impact:** The filter was almost immediately adopted by the aerospace industry and was a critical, mission-enabling component of the **NASA Apollo program**. Without the Kalman Filter to provide reliable real-time state estimation, the lunar landings would not have been possible. Its applications have since exploded into countless fields.
        
        **The Neural Network Connection:** The classic Kalman Filter assumes you have a good *linear* model of your system. But what about a complex, non-linear bioprocess? The modern approach is to replace the linear model with a **Recurrent Neural Network (RNN)**. The RNN *learns* the complex non-linear dynamics from data, and the Kalman Filter framework provides the mathematically optimal way to blend the RNN's predictions with new sensor measurements.
        """)
        
    with tabs[5]:
        st.markdown("""
        This advanced hybrid system is a state-of-the-art implementation of the principles of modern process monitoring and control.
        - **FDA Process Validation Guidance (Stage 3 - Continued Process Verification):** This tool provides a highly effective method for meeting the CPV requirement of continuously monitoring the process to ensure it remains in a state of control, and for investigating any departures.
        - **FDA Guidance for Industry - PAT:** This "Model and Monitor Residuals" approach is a direct implementation of the PAT goal of understanding and controlling manufacturing through timely measurements and feedback loops.
        - **ICH Q9 (Quality Risk Management):** By providing rapid detection of deviations from the expected process trajectory, this system can significantly reduce risk.
        - **GAMP 5 & 21 CFR Part 11:** As this system uses an AI/ML model to provide diagnostic information for a GxP process, the model and the software it runs on would need to be validated as a Computerized System.
        """)
# ==============================================================================
# UI RENDERING FUNCTION (Method 4)
# ==============================================================================
def render_rl_tuning():
    """Renders the Reinforcement Learning for Chart Tuning module."""
    st.markdown("""
    #### Purpose & Application: The AI Economist
    **Purpose:** To use **Reinforcement Learning (RL)** to automatically tune the parameters of a control chart (e.g., EWMA's `Œª` and `L`) to achieve the best possible **economic performance**. It finds the optimal balance in the fundamental trade-off between reacting too quickly (costly false alarms) and reacting too slowly (costly missed signals).
    
    **Strategic Application:** This moves SPC from a purely statistical exercise to a business optimization problem.
    - **Customized Monitoring:** The RL agent designs a chart specifically tuned to your process's unique failure modes and your business's specific cost structure.
    - **Risk-Based Control:** For a high-value product, the cost of a missed signal is enormous, so the agent will design a highly sensitive chart. For a low-cost intermediate, it may design a less sensitive chart to avoid nuisance alarms.
    """)
    st.info("""
    **Interactive Demo:** You are the business manager. Use the sliders to define the economic reality and target failure mode for your process. The dashboard shows the full optimization landscape the RL agent explores to find the best solution.
    """)
    with st.sidebar:
        st.sidebar.subheader("RL Economic & Process Controls")
        cost_fa_slider = st.sidebar.slider("Cost of a False Alarm ($)", 1, 20, 1, 1,
            help="The economic cost ($) of a single false alarm (stopping the process, investigating a non-existent problem, scrapping material).")
        cost_delay_slider = st.sidebar.slider("Cost of Detection Delay ($/unit time)", 1, 20, 5, 1,
            help="The economic cost ($) incurred for *each time unit* that a real process shift goes undetected (e.g., cost of producing scrap).")
        shift_size_slider = st.sidebar.slider("Target Shift Size to Detect (œÉ)", 0.5, 3.0, 1.0, 0.25,
            help="The magnitude of the critical process shift you want to detect as quickly as possible.")

    fig_3d, fig_2d, opt_lambda, opt_L, min_cost = plot_rl_tuning(
        cost_false_alarm=cost_fa_slider,
        cost_delay_unit=cost_delay_slider,
        shift_size=shift_size_slider
    )
    
    st.header("Economic Optimization Landscape")
    col1, col2 = st.columns([0.5, 0.5])
    with col1:
        st.plotly_chart(fig_3d, use_container_width=True)
    with col2:
        st.subheader("Analysis & Interpretation")
        st.metric("Optimal Œª Found by RL", f"{opt_lambda:.3f}", help="The optimal EWMA memory parameter.")
        st.metric("Optimal Limit Width (L) Found by RL", f"{opt_L:.2f}œÉ", help="The optimal control limit width in multiples of sigma.")
        st.metric("Minimum Achievable Cost", f"${min_cost:.3f}", help="The best possible economic performance for this chart.")
        st.markdown("""
        **The RL Agent's Solution:**
        The agent balances two competing goals: maximizing the time between false alarms (ARL‚ÇÄ) and minimizing the time to detect a real shift (ARL‚ÇÅ). The **3D Cost Surface** shows the combined economic result of this trade-off. The agent finds the `(Œª, L)` combination at the bottom of this "cost valley" to design the most profitable control chart.
        """)

    st.header("Diagnostic Plots & Final Chart")
    st.plotly_chart(fig_2d, use_container_width=True)
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Define the Costs:** The most critical step is for the business to provide realistic cost estimates for a false alarm and a detection delay.
        2.  **Explore the Landscape (Plot 1):** The 3D surface shows that poor parameter choices can be extremely costly. The agent's job is to find the lowest point.
        3.  **Understand the Trade-off (Plots 2 & 3):** The contour plots show the underlying statistical reality. Designs in the top-right have a very long time to a false alarm (high ARL‚ÇÄ) but are slow to detect real shifts (high ARL‚ÇÅ). Designs in the bottom-left are fast to detect shifts but have frequent false alarms.
        4.  **Deploy the Optimal Chart (Plot 4):** The final chart is the EWMA chart built with the economically optimal `Œª` and `L` parameters.
        
        **The Strategic Insight:** Try increasing the **Cost of Detection Delay**. The RL agent will find a new optimum with a smaller `Œª` and/or `L`, creating a more "nervous" and sensitive chart, because the business has decided that missing a shift is more costly than having a few extra false alarms.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The AI Economist for Your Control Strategy
    
        #### The Problem: The "One-Size-Fits-All" Control Chart
        A company uses a standard "cookbook" approach to Statistical Process Control. Every control chart in the facility is set up with the same default parameters (e.g., 3-sigma limits for a Shewhart chart, or `Œª=0.2, L=3` for an EWMA chart). This approach completely ignores the unique economic and risk profile of each individual process.
    
        #### The Impact: A Sub-Optimal and Inefficient Control Strategy
        This "one-size-fits-all" mentality is a classic example of "false efficiency" that creates significant hidden costs.
        - **Nuisance Alarms on Low-Risk Processes:** A non-critical buffer prep process is monitored with a hyper-sensitive chart. It generates frequent false alarms, causing operators to waste hours on unnecessary investigations, leading to "alarm fatigue" where all alarms are eventually ignored.
        - **Slow Detection on High-Risk Processes:** A final, high-value drug substance purification step is monitored with a standard, insensitive chart. A small but critical process drift goes undetected for an entire shift, resulting in a **multi-million dollar batch loss** that a more sensitive chart would have caught hours earlier.
        - **Indefensible Parameters:** During an audit, an inspector asks: "Why did you choose a lambda of 0.2 for this critical process?" The team has no answer beyond "it's the textbook default," demonstrating a superficial, non-risk-based approach to their control strategy.
    
        #### The Solution: A Custom-Tuned, Economically Optimal "AI Economist"
        Reinforcement Learning (RL) provides a revolutionary solution. It acts as an **AI Economist** that can automatically design a control chart with the **perfect, custom-tuned parameters for a specific process's economic and risk profile**. The process is:
        1.  **Define the Economics:** The business provides two key inputs: the cost of a false alarm and the cost per hour of a missed detection.
        2.  **Simulate and Learn:** The RL agent runs millions of simulated experiments in a "digital twin" of the process. It tries thousands of different combinations of chart parameters (`Œª` and `L`).
        3.  **Maximize "Profit":** The agent learns which combination of parameters results in the **lowest total cost of quality** (the sum of false alarm costs and missed signal costs) over the long run.
    
        #### The Consequences: A Risk-Based, High-ROI Control Strategy
        - **Without This:** The control strategy is a collection of generic, sub-optimal rules that do not reflect the true business risks.
        - **With This:** Every single control chart in the facility is **provably optimized to maximize economic return**. High-risk processes are monitored with high-sensitivity charts, while low-risk processes are monitored with less sensitive charts that reduce nuisance alarms. This creates a **lean, risk-based, and highly defensible control strategy** that minimizes the total cost of quality and focuses resources where they matter most.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **Reinforcement Learning (RL):** A field of AI where an "agent" learns to make optimal decisions by interacting with an environment (or a simulation) to maximize a cumulative reward.
        - **Economic Design of Control Charts:** A framework for selecting control chart parameters (like `Œª` and `L`) to minimize a total cost function, rather than relying on purely statistical rules.
        - **Loss Function:** A mathematical function that quantifies the economic cost associated with a control chart's performance, typically including the cost of false alarms and the cost of detection delays.
        - **ARL‚ÇÄ (Average Run Length - In Control):** The average number of samples taken before a false alarm occurs. A higher ARL‚ÇÄ is better.
        - **ARL‚ÇÅ (Average Run Length - Out of Control):** The average number of samples taken to detect a true process shift of a given magnitude. A lower ARL‚ÇÅ is better.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The 'Cookbook' Method**
A scientist reads a textbook that says 'use Œª=0.2 and L=3 for EWMA charts.' They apply these default values to every process, regardless of the process stability, the value of the product, or the economic consequences of an error.""")
        st.success("""üü¢ **THE GOLDEN RULE: Design the Chart to Match the Risk and the Business**
The control chart is not just a statistical tool; it's an economic asset. The tuning parameters should be deliberately chosen to minimize the total expected cost of quality for a specific process.
1.  **Quantify the Risk:** Work with stakeholders to define the costs of both Type I errors (false alarms) and Type II errors (missed signals).
2.  **Define the Target:** Identify the critical process shift that you must detect quickly.
3.  **Optimize:** Use a framework like this to find the chart parameters that provide the most cost-effective monitoring solution. This creates a highly defensible, data-driven control strategy.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: The Unfulfilled Promise
        **The Problem:** The idea of designing control charts based on economics is surprisingly old, dating back to the work of **Acheson Duncan in the 1950s**. He recognized that the choice of chart parameters was an economic trade-off between the cost of looking for trouble (sampling and investigation) and the cost of not finding it in time (producing defective product). However, the mathematics required to find the optimal solution were incredibly complex and relied on many assumptions that were difficult to verify in practice. For decades, "Economic Design of Control Charts" remained an academically interesting but practically ignored field.

        **The 'Aha!' Moment (Simulation):** The modern solution came not from better math, but from more computing power. **Reinforcement Learning (RL)**, a field that exploded in the 2010s with successes like AlphaGo, provided a new paradigm. Instead of solving complex equations, an RL agent could learn the optimal strategy through millions of trial-and-error experiments in a fast, simulated "digital twin" of the manufacturing process. The agent's "reward" is simply the inverse of the economic loss function.
        
        **The Impact:** The rise of RL and high-fidelity process simulation has finally made the promise of economic design a practical reality. It allows engineers to move beyond statistical "rules of thumb" and design monitoring strategies that are provably optimized for their specific business and risk environment.
        """)
        
    with tabs[5]:
        st.markdown("""
        This advanced design methodology is a state-of-the-art implementation of the principles of modern process monitoring and control.
        - **ICH Q9 (Quality Risk Management):** This tool provides a direct, quantitative link between the business and patient risks (captured in the cost function) and the technical design of the control strategy. It is a perfect example of a risk-based approach to process monitoring.
        - **FDA Process Validation Guidance (Stage 3 - Continued Process Verification):** An economically designed chart provides a highly defensible rationale for the chosen monitoring strategy during CPV.
        - **FDA Guidance for Industry - PAT:** This tool supports the PAT goal of designing and controlling manufacturing based on a deep understanding of the process and the risks involved.
        - **GAMP 5 & 21 CFR Part 11:** If the RL agent and its simulation environment are used to formally set and justify control limits for a GxP process, the software and models would require validation as a Computerized System.
        """)
        
# ==============================================================================
# UI RENDERING FUNCTION (Method 5)
# ==============================================================================
def render_tcn_cusum():
    """Renders the TCN + CUSUM module."""
    st.markdown("""
    #### Purpose & Application: The AI Signal Processor
    **Purpose:** To create a powerful, hybrid system for detecting tiny, gradual drifts hidden within complex, non-linear, and seasonal time series data. A **Temporal Convolutional Network (TCN)** first learns and "subtracts" the complex but predictable patterns. Then, a **CUSUM chart** is applied to the remaining signal (the residuals) to detect any subtle, underlying drift.
    
    **Strategic Application:** This is for monitoring complex, dynamic processes like bioreactors or utility systems, where normal behavior is non-linear and cyclical.
    - **Bioreactor Monitoring:** A TCN can learn the complex S-shaped growth curve and daily cycles. The CUSUM on the residuals can then detect if the underlying cell growth rate is slowly starting to decline, signaling a problem with the media or culture health long before the raw data looks abnormal.
    """)
    st.info("""
    **Interactive Demo:** Use the sliders to control the simulated bioprocess.
    - **`Gradual Drift`**: Controls how quickly the hidden drift pulls the process away from its normal baseline.
    - **`Daily Cycle Strength`**: Controls the amplitude of the predictable, daily fluctuations. Notice that even with very strong cycles, the CUSUM chart on the TCN's residuals effectively detects the hidden drift.
    """)
    with st.sidebar:
        st.sidebar.subheader("TCN-CUSUM Controls")
        drift_slider = st.sidebar.slider("Gradual Drift Magnitude", 0.0, 0.2, 0.05, 0.01,
            help="The slope of the hidden linear trend added to the data. This is the subtle signal the CUSUM chart must find.")
        seasonality_slider = st.sidebar.slider("Daily Cycle Strength", 0.0, 5.0, 1.0, 0.5,
            help="Controls the amplitude of the cyclical patterns in the data. The TCN's job is to learn and remove this 'noise'.")

    fig, alarm_time = plot_tcn_cusum(drift_magnitude=drift_slider, daily_cycle_strength=seasonality_slider)
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.subheader("Analysis & Interpretation")
        st.metric("Drift Detection Time", f"Hour #{alarm_time}" if alarm_time else "Not Detected",
                  help="The time the CUSUM chart first signaled a significant deviation in the residuals.")

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Model the Predictable (Plot 1):** A TCN is trained on historical data from successful batches. It learns the complex but normal patterns, such as the S-shaped growth curve and daily cycles. The red dashed line is the TCN's forecast of what *should* be happening.
        2.  **Isolate the Unpredictable (Plot 3):** The model's forecast errors (the residuals) are calculated. This isolates the part of the signal that the model *cannot* explain. For a healthy process, these residuals should be random noise.
        3.  **Monitor for Drift (Plot 2):** The CUSUM chart is applied to these residuals. It is designed to ignore random noise but will accumulate the small, persistent signal caused by the hidden drift, eventually crossing the red control limit and firing a clear alarm (red 'X').
    
        **The Strategic Insight:** This hybrid approach allows you to apply a highly sensitive statistical tool (CUSUM) to a process that would normally violate all of its assumptions. The TCN acts as an intelligent "pre-processor," removing the complex, non-stationary patterns and allowing the simple, powerful CUSUM to do its job effectively.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: Ultra-Sensitive Monitoring for "Un-Chartable" Processes
    
        #### The Problem: The "Too Complex to Control" Process
        A company's most valuable asset is a complex, dynamic bioprocess. The process data is highly non-linear (following an S-shaped growth curve) and has strong cyclical patterns (due to daily feed cycles). Traditional SPC charts, which assume a stable, flat mean, are completely useless for monitoring this process. They would be in a constant state of false alarm.
    
        #### The Impact: Flying Blind on Your Most Valuable Asset
        This inability to apply rigorous, real-time statistical monitoring to the company's most critical process creates a massive business risk.
        - **Delayed Deviation Detection:** A subtle but critical problem occurs‚Äîa slow contamination or a degradation in the cell line's viability. This causes a tiny, gradual drift away from the "golden batch" trajectory. Without a sensitive monitoring system, this drift goes completely undetected.
        - **Catastrophic Failure at Harvest:** The team only discovers the problem at the very end of the multi-week process, when the final yield is 30% below target. This results in a **multi-million dollar batch loss**.
        - **Inability to Scale or Transfer:** The process is treated as a form of "black magic" that only works under perfect conditions. Any attempt to scale up or transfer the process to a new site is a high-risk gamble, because the organization lacks the deep, quantitative understanding to ensure its success.
    
        #### The Solution: The AI-Powered "Signal Processor"
        The TCN-CUSUM system is a sophisticated, two-stage "signal processor" that makes the un-chartable chartable.
        1.  **The TCN (The Noise-Canceling Headphone):** A Temporal Convolutional Network (TCN) is a powerful deep learning model. It is trained on data from dozens of successful "golden batches." Its job is to learn and then **mathematically subtract** all the complex but *predictable* patterns‚Äîthe S-shaped growth, the daily cycles, etc. This is like putting on a pair of noise-canceling headphones to filter out the overwhelming, predictable noise.
        2.  **The CUSUM (The Sensitive Microphone):** What remains after the TCN's filtering is the **residual signal**‚Äîthe part of the process behavior the AI couldn't predict. The CUSUM chart, a hyper-sensitive statistical "microphone," is then applied to this clean signal. It can now easily detect the tiny, persistent whisper of a true process drift that was previously drowned out by the noise.
    
        #### The Consequences: Achieving a State of Control on Complex Systems
        - **Without This:** The company is forced to manage its most valuable and complex processes with intuition and lagging indicators, exposing them to massive financial risk.
        - **With This:** The TCN-CUSUM hybrid provides the **first-ever ability to apply rigorous, ultra-sensitive statistical process control to highly complex, dynamic, and non-linear systems**. It enables the **early detection of subtle drifts** in real-time, preventing catastrophic batch failures, maximizing yield, and providing the deep process understanding required to confidently scale up and transfer these high-value processes.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **TCN (Temporal Convolutional Network):** A type of deep learning architecture that applies convolutional layers to time series data. It is known for its ability to capture long-range patterns efficiently.
        - **CUSUM (Cumulative Sum):** A "memory-based" control chart that plots the cumulative sum of deviations from a target. It is the fastest possible chart for detecting a shift of a specific, pre-defined magnitude.
        - **Residuals:** The difference between the actual data and the TCN's forecast. By applying CUSUM to the residuals, we monitor for changes in the part of the signal the TCN *could not* predict.
        - **Receptive Field:** The span of historical data that a TCN uses to make a single prediction. TCNs use dilated convolutions to achieve very large receptive fields efficiently.
        - **Non-Stationary Data:** A time series whose statistical properties such as mean and variance change over time. Bioprocess data is typically non-stationary.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: Charting the Raw Data**
Applying a CUSUM chart directly to the raw bioprocess data would be a disaster. The massive swings from the S-shaped growth curve and the daily cycles would cause constant false alarms, making the chart completely useless. The true, tiny drift signal would be completely buried in the predictable patterns.""")
        st.success("""üü¢ **THE GOLDEN RULE: Separate the Predictable from the Unpredictable**
This is a fundamental principle of modern process monitoring for complex, dynamic systems.
1.  **Model the Known:** Use a sophisticated forecasting model (like a TCN or LSTM) to learn and mathematically remove the complex, **known patterns** (like growth curves and seasonality) from your data.
2.  **Monitor the Unknown:** Apply a sensitive change detection algorithm (like CUSUM or EWMA) to the model's **residuals** (the forecast errors). This focuses your monitoring on the part of the signal that is truly changing and unpredictable, where novel faults will always appear first.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: The Evolution of Sequence Modeling
        **The Problem:** For years, Recurrent Neural Networks (RNNs) and their advanced variant, LSTMs, were the undisputed kings of sequence modeling. However, their inherently sequential nature-having to process time step `t` before moving to `t+1`-made them slow to train on very long sequences and difficult to parallelize on modern GPUs.

        **The 'Aha!' Moment:** In 2018, a paper by **Bai, Kolter, and Koltun**, "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling," showed that a different architecture could outperform LSTMs on many standard sequence tasks while being much faster. They systematized the **Temporal Convolutional Network (TCN)**. The key insight was to adapt techniques from computer vision (Convolutional Neural Networks) for time-series data. By using **causal convolutions** (to prevent seeing the future) and **dilated convolutions** (which exponentially increase the field of view), TCNs could learn very long-range patterns in parallel.

        **The Impact:** TCNs provided a powerful, fast, and often simpler alternative to LSTMs, becoming a go-to architecture for many time-series applications. Fusing this modern deep learning model with a classic, high-sensitivity statistical chart like **CUSUM (Page, 1954)** creates a hybrid system that leverages the best of both worlds: the pattern-recognition power of deep learning and the statistical rigor of classic SPC.
        """)
        
    with tabs[5]:
        st.markdown("""
        This advanced hybrid system is a state-of-the-art implementation of the principles of modern process monitoring and control.
        - **FDA Process Validation Guidance (Stage 3 - Continued Process Verification):** This tool provides a highly effective method for meeting the CPV requirement of continuously monitoring complex, non-stationary processes.
        - **FDA Guidance for Industry - PAT:** This "Model and Monitor Residuals" approach is a direct implementation of the PAT goal of understanding and controlling manufacturing through timely measurements and feedback loops.
        - **ICH Q9 (Quality Risk Management):** By providing early detection of subtle drifts, this system can significantly reduce the risk of producing non-conforming material.
        - **GAMP 5 & 21 CFR Part 11:** As this system uses an AI/ML model to provide diagnostic information for a GxP process, the model and the software it runs on would need to be validated as a Computerized System.
        """)
# ==============================================================================
# UI RENDERING FUNCTION (Method 6)
# ==============================================================================
def render_lstm_autoencoder_monitoring():
    """Renders the LSTM Autoencoder + Hybrid Monitoring module."""
    st.markdown("""
    #### Purpose & Application: The AI Immune System
    **Purpose:** To create a sophisticated, self-learning "immune system" for your process. An **LSTM Autoencoder** learns the normal, dynamic "fingerprint" of a healthy multivariate process. It then generates a single health score: the **reconstruction error**. We then deploy a **hybrid monitoring system** on this health score to detect different types of diseases (anomalies).
    
    **Strategic Application:** This is a state-of-the-art approach for unsupervised anomaly detection in multivariate time-series data, like that from a complex bioprocess.
    - **Learns Normal Dynamics:** The LSTM Autoencoder learns the complex, time-dependent correlations between many process parameters.
    - **One Health Score:** It distills hundreds of parameters into a single, chartable health score.
    - **Hybrid Detection:** An **EWMA chart** detects slow-onset diseases (gradual degradation), while a **BOCPD algorithm** detects acute events (sudden shocks).
    """)
    st.info("""
    **Interactive Demo:** Use the sliders to introduce two different types of anomalies into the multivariate process.
    - **`Gradual Drift Rate`**: Controls a slow, creeping deviation in both Temp and pH. Watch the **EWMA chart (orange)** in the bottom plot slowly rise to catch this.
    - **`Spike Magnitude`**: Controls a sudden shock at time #200. Watch the **BOCPD probability (purple)** in the bottom plot instantly react to this.
    """)
    with st.sidebar:
        st.sidebar.subheader("LSTM Anomaly Controls")
        drift_slider = st.sidebar.slider("Gradual Drift Rate", 0.0, 0.05, 0.02, 0.005,
            help="Controls how quickly the process drifts away from its normal behavior after time #100. Simulates gradual equipment degradation.")
        spike_slider = st.sidebar.slider("Spike Magnitude", 1.0, 5.0, 2.0, 0.5,
            help="Controls the size of the sudden shock at time #200. Simulates a sudden process fault or sensor failure.")

    fig, ewma_time, bocpd_time = plot_lstm_autoencoder_monitoring(drift_rate=drift_slider, spike_magnitude=spike_slider)
    
    col1, col2 = st.columns([0.7, 0.3])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
        
    with col2:
        st.subheader("Analysis & Interpretation")
        st.metric("EWMA Drift Detection Time", f"Hour #{ewma_time}" if ewma_time else "Not Detected",
                  help="Time the EWMA chart alarmed on the slow drift.")
        st.metric("BOCPD Spike Detection Time", f"Hour #{bocpd_time}" if bocpd_time else "Not Detected",
                  help="Time the BOCPD algorithm alarmed on the sudden spike.")

    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **Learn the "Fingerprint" (Plot 1):** The LSTM Autoencoder is trained only on data from healthy, successful batches. It learns to reconstruct the normal, correlated dance between Temperature and pH. The dashed lines show the AI's reconstruction.
        2.  **Generate a Health Score (Plot 2):** When the live process data deviates from normal (due to the drift or spike), the AI struggles to reconstruct it. This failure is quantified as the **Reconstruction Error**, which serves as a single, powerful "health score" for the entire multivariate system.
        3.  **Deploy a Layered Defense (Plot 3):** Two specialized detectors monitor the health score:
            - The **EWMA chart (orange)** is insensitive to the sudden spike but accumulates the small, persistent signal from the gradual drift, eventually sounding an alarm.
            - The **BOCPD algorithm (purple)** ignores the slow drift but instantly detects the abrupt change caused by the spike, signaling an acute event.
        
        **The Strategic Insight:** This architecture creates a comprehensive "immune system" for your process. The LSTM Autoencoder is the T-cell that learns "self," and the hybrid monitoring charts are the antibodies and macrophages that detect and classify different types of "non-self" threats.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The "AI Immune System" for Your Process
    
        #### The Problem: The One-Dimensional Alarm System
        A company monitors its critical, multi-million dollar process with a series of simple, one-dimensional alarms (e.g., "Alert if Temperature > 38¬∞C"). This system is good at catching simple, single-variable failures. However, it is completely blind to more complex problems, such as a slow, gradual degradation of the entire process or a sudden shock that affects multiple parameters at once.
    
        #### The Impact: Late Detection and Generic, Unactionable Alarms
        This simplistic monitoring strategy leaves the company vulnerable and inefficient.
        - **Delayed Detection of Chronic Issues:** A slow, creeping degradation (like a filter slowly clogging) affects multiple parameters in a subtle, coordinated way. The one-dimensional alarms won't trigger until the problem is severe, resulting in significant process inefficiency or a batch failure that could have been predicted weeks earlier.
        - **Inability to Differentiate Threats:** When a major event does occur, multiple alarms might trigger at once. The system screams "Problem!" but gives the operator no information about the *type* of problem. Is this a sudden, acute shock (like an equipment failure) or the culmination of a long, chronic drift? This ambiguity slows down the root cause investigation.
    
        #### The Solution: A Multi-Layered "AI Immune System"
        This hybrid system is a state-of-the-art solution that mimics the sophistication of the human immune system.
        1.  **The T-Cell (LSTM Autoencoder):** The LSTM Autoencoder is the "T-cell." It is trained exclusively on data from healthy, "self" processes. It learns the complex, dynamic "fingerprint" of normal operation. Its output is a single, real-time **"health score"** (the reconstruction error).
        2.  **The Antibodies (EWMA):** The EWMA chart acts like the body's "antibodies." It is specifically designed to detect **chronic diseases** by looking for a slow, persistent increase in the health score, signaling a gradual process drift or degradation.
        3.  **The Macrophages (BOCPD):** The BOCPD algorithm acts like the "macrophages." It is designed to detect **acute trauma** by looking for a sudden, sharp spike in the health score, signaling an immediate shock or fault in the system.
    
        #### The Consequences: From Generic Alarms to an Intelligent Diagnosis
        - **Without This:** The monitoring system is a collection of dumb, disconnected alarms that are slow to react and provide no context.
        - **With This:** The company has a **sophisticated, intelligent immune system** for its most critical process. It can not only detect deviations far earlier than traditional methods, but it can also **automatically classify the type of threat**. An "EWMA alarm" immediately tells the engineer to look for a slow, chronic issue (like sensor drift). A "BOCPD alarm" tells them to look for a sudden, acute event (like a pump failure). This transforms monitoring from a simple alarm system into an **automated, real-time diagnostic engine**, dramatically accelerating troubleshooting and risk mitigation.
        """)
        
    with tabs[1]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **LSTM (Long Short-Term Memory):** A type of recurrent neural network (RNN) that is excellent at learning long-range, time-dependent patterns in sequential data.
        - **Autoencoder:** An unsupervised neural network trained to reconstruct its input. It consists of an **Encoder** that compresses the data into a low-dimensional "fingerprint" and a **Decoder** that attempts to recreate the original data from that fingerprint.
        - **Reconstruction Error:** The difference between the original input data and the autoencoder's reconstructed output. It is a powerful anomaly score; if the model cannot reconstruct the data well, the error will be high, indicating an anomaly.
        - **Hybrid Monitoring:** The practice of applying multiple, specialized control charts (like EWMA for drifts and BOCPD for spikes) to a single "health score" like the reconstruction error to create a comprehensive detection system.
        """)
        
    with tabs[2]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "One-Tool" Mindset**
An engineer tries to use a single Shewhart chart on the reconstruction error. It misses the slow drift entirely, and while it might catch the big spike, it gives no probabilistic context and provides no diagnostic information about the *type* of failure (chronic vs. acute).""")
        st.success("""üü¢ **THE GOLDEN RULE: Use a Layered Defense for Anomaly Detection**
Different types of process failures leave different signatures in the data. A robust monitoring system must use a combination of tools, each specialized for a different type of signature. By running EWMA (for drifts) and BOCPD (for shocks) in parallel on the same AI-driven health score, you create a comprehensive immune system that can effectively detect and begin to classify both chronic and acute process diseases.""")

    with tabs[3]:
        st.markdown("""
        #### Historical Context: A Powerful Synthesis
        **The Problem:** Monitoring high-dimensional time-series data (like a bioreactor with hundreds of sensors) for anomalies is extremely difficult. A fault might not be a single sensor going haywire, but a subtle change in the *temporal correlation* between many sensors. How can you detect a deviation from a complex, dynamic "normal" state without having any examples of what "abnormal" looks like?

        **The 'Aha!' Moment (Synthesis):** This architecture became a popular and powerful technique in the late 2010s by intelligently combining three distinct ideas to solve the problem piece by piece:
        1.  **The Autoencoder:** A classic neural network design for unsupervised learning. It learns to compress data down to its essential features and then decompress it back to the original. Its ability to reconstruct the input serves as a measure of normalcy.
        2.  **The LSTM:** The Long Short-Term Memory network (**Hochreiter & Schmidhuber, 1997**) was the perfect choice to build the encoder and decoder, as it is specifically designed to learn the "grammar" and patterns of sequential data. Fusing these created the **LSTM Autoencoder**.
        3.  **Hybrid Monitoring:** The final piece was realizing that the autoencoder's output‚Äîthe reconstruction error‚Äîis a single, powerful time series representing the health of the process. This allowed engineers to apply the best-in-class univariate monitoring tools, like **EWMA** and **BOCPD**, to this signal, creating a specialized, layered defense system.
        """)
        
    with tabs[4]:
        st.markdown("""
        This advanced hybrid system is a state-of-the-art implementation of the principles of modern process monitoring and control.
        - **FDA Process Validation Guidance (Stage 3 - Continued Process Verification):** This tool provides a highly effective method for meeting the CPV requirement of continuously monitoring complex, multivariate processes.
        - **FDA Guidance for Industry - PAT:** This "learn the fingerprint" approach is a direct implementation of the PAT goal of understanding and controlling manufacturing through timely measurements and feedback loops.
        - **ICH Q9 (Quality Risk Management):** By providing early detection of both gradual and sudden deviations, this system can significantly reduce the risk of producing non-conforming material.
        - **GAMP 5 & 21 CFR Part 11:** As this system uses an AI/ML model to provide diagnostic information for a GxP process, the model and the software it runs on would need to be validated as a Computerized System.
        """)

# ==============================================================================
# UI RENDERING FUNCTION (PSO + Autoencoder)
# ==============================================================================
def render_pso_autoencoder():
    """Renders the PSO + Autoencoder module with a stable static plot and intuitive controls."""
    st.markdown("""
    #### Purpose & Application: The AI-Powered "Red Team"
    **Purpose:** To deploy an **AI-powered "Red Team"** that relentlessly searches for the hidden weaknesses in your process. This hybrid model uses a **Particle Swarm Optimization (PSO)** algorithm to find the specific combination of process parameters that cause the most "surprise" or deviation from normal, as measured by the reconstruction error of a pre-trained **LSTM Autoencoder**.
    
    **Strategic Application:** This is a state-of-the-art method for **AI-driven robustness testing and Design Space definition**. Instead of randomly picking points for worst-case analysis, you are using an intelligent swarm to find the true "edges of failure" for a high-value process.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Process Scientist. Use the sidebar sliders to change the **Process Anomaly Landscape** itself.
    - **`Landscape Complexity`**: Adds more "hills and valleys," making it harder for the AI to find the true global peak.
    - **`Noise Level`**: Adds random noise, making the signal harder to distinguish.
    - Observe how the PSO algorithm performs on the challenge you've created.
    """)

    project_context_name = st.selectbox(
        "Select a Project Context to Simulate:",
        ['Pharma Process', 'Assay', 'Instrument', 'Software', 'IVD'],
        help="The anomaly landscape and parameter names will change to match a realistic scenario for the selected context."
    )

    # --- NEW, MORE INTUITIVE SLIDERS ---
    with st.sidebar:
        st.subheader("Process Landscape Controls")
        complexity_slider = st.slider("Landscape Complexity", 0.0, 5.0, 1.0, 0.5, 
            help="Controls how 'rugged' the anomaly surface is. High values create many local peaks, making the search harder.")
        noise_slider = st.slider("Noise Level", 0.0, 5.0, 0.5, 0.5, 
            help="Controls the amount of random noise on the surface, which can obscure the true peak.")

    # 1. Run the simulation with the new landscape controls.
    # We use fixed, sensible values for the PSO algorithm itself now.
    zz, x_range, y_range, history_start, history_end, gbest_position, best_score = run_pso_simulation(
        n_particles=50, n_iterations=75, 
        project_context=project_context_name,
        landscape_complexity=complexity_slider,
        noise_level=noise_slider
    )
    
    # 2. Get the labels from the global dictionary for plotting.
    context_info = PSO_CONTEXTS[project_context_name]
    x_label = context_info['x_label']
    y_label = context_info['y_label']
    
    # 3. Create the static figure. The function call is simplified.
    fig = create_pso_static_figure(
        zz, tuple(x_range), tuple(y_range), history_start, history_end, tuple(gbest_position),
        project_context_name, x_label, y_label
    )
    
    col1, col2 = st.columns([0.65, 0.35])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        st.subheader("Analysis & Interpretation")
        st.metric(f"Worst-Case {x_label}", f"{gbest_position[0]:.2f}")
        st.metric(f"Worst-Case {y_label}", f"{gbest_position[1]:.2f}")
        st.metric("Maximum Anomaly Score Found", f"{best_score:.3f}")
        st.success("""
        **Actionable Insight:** The simulation has identified the process conditions most likely to cause an anomalous run. The next step is to design a lab experiment that deliberately targets these conditions to confirm the model's prediction and define the true edge of the process's Design Space.
        """)

    # --- The "Deeper Dive" tabs remain unchanged ---
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown(f"""
        **Interpreting the Simulation for a {project_context_name}:**
        - **The Landscape:** The heatmap represents the "process health" as understood by an Autoencoder. The peaks (hot colors) are regions where the process behaves in unexpected ways.
        - **The Swarm's Journey:** The plot shows the final state of the PSO search. The particles started randomly (**white 'x' markers**) but communicated and learned, converging on the area of highest anomaly score (**cyan circles**). The **green star** marks the single highest-risk condition found.
        - **The Strategic Insight:** This approach automates and supercharges worst-case analysis. Instead of relying on engineers to guess at high-risk conditions, you deploy an AI agent to find them for you.
        """)
    
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The AI-Powered "Red Team" for Process Robustness
    
        #### The Problem: The "Edge of Failure" is Unknown
        A company has a "golden batch" process that works perfectly when run in the center of its defined range. However, they need to perform robustness studies or define a formal Design Space, which requires understanding how the process behaves at its extremes‚Äîthe "edge of failure." The traditional approach is to use SME intuition to guess at "worst-case" conditions, a method that is slow, subjective, and often misses the true weaknesses.
    
        #### The Impact: Fragile Processes and Incomplete Validation
        This inability to systematically find a process's weaknesses has significant business consequences:
        - **"Validation by Luck":** The team runs a few robustness studies based on their best guesses. The process passes, and they declare it robust. In reality, they simply didn't test the one specific *combination* of parameters that would have caused a catastrophic failure. The process is fragile, but this weakness remains hidden.
        - **Inefficient Tech Transfer:** During a tech transfer, the new site experiences a series of strange, intermittent failures that the original site never saw. The root cause is a subtle interaction between two parameters that was never explored, leading to months of costly troubleshooting and delays.
        - **Ultra-Conservative Design Spaces:** Lacking confidence in where the true "cliffs" are, the team is forced to define a very small, ultra-conservative Normal Operating Range. This limits manufacturing flexibility and can reduce the overall process yield and profitability.
    
        #### The Solution: An AI "Red Team" to Find Your Weaknesses
        This hybrid model is a state-of-the-art solution that automates and supercharges robustness testing. It functions like an AI-powered "Red Team" for your process:
        1.  **The Autoencoder (The Defender):** First, an Autoencoder is trained on data from dozens of successful "golden batches." It learns the complex, multivariate "fingerprint" of a healthy process. Its job is to defend this definition of normalcy.
        2.  **The Particle Swarm (The Attacker):** The PSO algorithm is then deployed as the attacker. Its mission is to **find the specific combination of process inputs that the Autoencoder is least able to reconstruct**‚Äîin other words, the input conditions that are most "surprising" or "anomalous" to a model trained on perfection. It relentlessly searches for the highest peak on the reconstruction error landscape.
    
        #### The Consequences: A Truly Robust Process and a Defensible Design Space
        - **Without This:** Robustness testing is an inefficient guessing game that often provides a false sense of security.
        - **With This:** The company has a **systematic, data-driven, and automated method for discovering its process's hidden weaknesses**. The output of the PSO search is not a guess; it is a rank-ordered list of the highest-risk conditions to test in the lab. This allows the team to **efficiently and confidently** define the true "edges of failure," leading to the creation of a **truly robust process** and a **maximally large, scientifically-defensible Design Space**.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **Autoencoder (AE):** An unsupervised neural network that learns a compressed representation of data. Its `reconstruction error`‚Äîhow well it can recreate the original input‚Äîis a powerful anomaly score. In this case, it learns the "fingerprint" of a golden batch.
        - **Particle Swarm Optimization (PSO):** A population-based optimization algorithm inspired by the social behavior of bird flocking or fish schooling.
        - **Particle:** An individual agent in the swarm, representing a potential solution (a set of process parameters like `Metabolic Shift Day` and `Peak VCD`).
        - **pbest (Personal Best):** The best position (highest anomaly score) a single particle has discovered so far.
        - **gbest (Global Best):** The best position discovered by *any* particle in the entire swarm so far.
        - **Adversarial Testing:** A technique for testing a system by actively trying to find inputs that cause it to fail. Here, PSO acts as the adversary against the process model.
        """)
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: One-Factor-at-a-Time (OFAT) Robustness**
An engineer tests robustness by running one batch at the low end of the pH range, another at the high end, and repeats for temperature.
- **The Flaw:** This method is guaranteed to miss failures caused by *interactions*. The true "cliff of failure" might be at a specific *combination* of normal pH and high temperature that OFAT would never test.""")
        st.success("""üü¢ **THE GOLDEN RULE: Use Intelligent Search to Find Your Weaknesses**
A modern, AI-driven approach to robustness testing treats the problem as a formal optimization search.
1.  **Model Normalcy:** First, build a model (like an Autoencoder) that learns the "fingerprint" of your ideal, "golden" process from historical data.
2.  **Define the Objective:** Frame the goal not as finding the *best* outcome, but as finding the inputs that produce the *worst* outcome (i.e., maximize the anomaly score from your model).
3.  **Deploy an Optimizer:** Use a powerful search algorithm like PSO to intelligently and efficiently explore the parameter space and find this worst-case condition. This provides a data-driven, highly defensible candidate for your robustness studies.""")
    with tabs[4]:
        st.markdown("""
        #### Historical Context: A Powerful Synthesis of AI
        This module represents a cutting-edge fusion of two powerful AI concepts from different eras.
        - **The Autoencoder:** This concept has roots in the early days of neural networks and was popularized by figures like Geoffrey Hinton in the 1980s as a method for unsupervised feature learning. The idea of using its reconstruction error for anomaly detection became prominent in the 2010s with the rise of deep learning.
        - **Particle Swarm Optimization (PSO):** In 1995, social psychologist James Kennedy and electrical engineer Russell Eberhart had a brilliant insight. Inspired by simulations of bird flocking, they realized they could create a simple yet remarkably effective optimization algorithm. The core idea was that individual "particles" could find optimal solutions by balancing their own experience (`pbest`) with the collective wisdom of the group (`gbest`). It was a foundational algorithm in the field of **swarm intelligence**.
        
        **The Modern Fusion:** This tool demonstrates a modern synthesis. We use a sophisticated deep learning model (the Autoencoder) to create a complex, data-driven "landscape" of our process health. Then, we deploy a classic swarm intelligence algorithm (PSO) to efficiently search that landscape for its highest peaks. This combination allows us to solve complex robustness and optimization problems that would be intractable with either method alone.
        """)
    with tabs[5]:
        st.markdown("""
        This advanced hybrid system is a state-of-the-art implementation of the principles of modern process monitoring and control.
        - **ICH Q8(R2) - Pharmaceutical Development:** This tool provides a powerful, data-driven method for exploring the **Design Space** and identifying the "edges of failure," which is a core activity in process characterization. The worst-case conditions it identifies are prime candidates for robustness studies.
        - **FDA Process Validation Guidance (Stage 1 - Process Design):** This approach provides a deep level of "process understanding" that the guidance emphasizes.
        - **ICH Q9 (Quality Risk Management):** This is a form of proactive risk discovery. Instead of just assessing known risks, this system actively searches for new, unknown risk scenarios (combinations of parameters that lead to anomalous states).
        - **GAMP 5 & 21 CFR Part 11:** As this system uses an AI/ML models to inform critical decisions about the process operating range, the models themselves would require a robust validation lifecycle to be used in a GxP environment.
        """)

#======================================================================== PSO and AUTOENCODER FINAL UI =============================================================
def render_pso_autoencoder():
    """Renders the PSO + Autoencoder module with a stable static plot and intuitive controls."""
    st.markdown("""
    #### Purpose & Application: The AI-Powered "Red Team"
    **Purpose:** To deploy an **AI-powered "Red Team"** that relentlessly searches for the hidden weaknesses in your process. This hybrid model uses a **Particle Swarm Optimization (PSO)** algorithm to find the specific combination of process parameters that cause the most "surprise" or deviation from normal, as measured by the reconstruction error of a pre-trained **LSTM Autoencoder**.
    
    **Strategic Application:** This is a state-of-the-art method for **AI-driven robustness testing and Design Space definition**. Instead of randomly picking points for worst-case analysis, you are using an intelligent swarm to find the true "edges of failure" for a high-value process.
    """)
    
    st.info("""
    **Interactive Demo:** You are the Process Scientist. Use the sidebar sliders to change the **Process Anomaly Landscape** itself.
    - **`Landscape Complexity`**: Adds more "hills and valleys," making it harder for the AI to find the true global peak.
    - **`Noise Level`**: Adds random noise, making the signal harder to distinguish.
    - Observe how the PSO algorithm performs on the challenge you've created. The KPIs and plot will now update correctly.
    """)

    project_context_name = st.selectbox(
        "Select a Project Context to Simulate:",
        ['Pharma Process', 'Assay', 'Instrument', 'Software', 'IVD'],
        help="The anomaly landscape and parameter names will change to match a realistic scenario for the selected context."
    )

    # --- NEW, MORE INTUITIVE SLIDERS ---
    with st.sidebar:
        st.subheader("Process Landscape Controls")
        complexity_slider = st.slider("Landscape Complexity", 0.0, 5.0, 1.0, 0.5, 
            help="Controls how 'rugged' the anomaly surface is. High values create many local peaks, making the search harder.")
        noise_slider = st.slider("Noise Level", 0.0, 5.0, 0.5, 0.5, 
            help="Controls the amount of random noise on the surface, which can obscure the true peak.")

    # 1. Run the simulation with the new landscape controls and a fixed set of PSO parameters.
    # The function call now correctly matches the function definition.
    zz, x_range, y_range, history_start, history_end, gbest_position, best_score = run_pso_simulation(
        n_particles=50, n_iterations=75, 
        project_context=project_context_name,
        landscape_complexity=complexity_slider,
        noise_level=noise_slider
    )
    
    # 2. Get the labels from the global dictionary for plotting.
    context_info = PSO_CONTEXTS[project_context_name]
    x_label = context_info['x_label']
    y_label = context_info['y_label']
    
    # 3. Create the static figure using only simple, hashable arguments.
    fig = create_pso_static_figure(
        zz, tuple(x_range), tuple(y_range), history_start, history_end, tuple(gbest_position),
        project_context_name, x_label, y_label
    )
    
    col1, col2 = st.columns([0.65, 0.35])
    with col1:
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        st.subheader("Analysis & Interpretation")
        # --- BUG FIX #2: CORRECT VARIABLE NAME ---
        # Changed `best_params` to the correct variable `gbest_position`.
        st.metric(f"Worst-Case {x_label}", f"{gbest_position[0]:.2f}")
        st.metric(f"Worst-Case {y_label}", f"{gbest_position[1]:.2f}")
        # --- END BUG FIX #2 ---
        st.metric("Maximum Anomaly Score Found", f"{best_score:.3f}")
        st.success("""
        **Actionable Insight:** The simulation has identified the process conditions most likely to cause an anomalous run. The next step is to design a lab experiment that deliberately targets these conditions to confirm the model's prediction and define the true edge of the process's Design Space.
        """)

    # The "Deeper Dive" tabs can remain as they are.
    st.divider()
    st.subheader("Deeper Dive")
    tabs = st.tabs(["üí° Key Insights", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    with tabs[0]:
        st.markdown(f"""
        **Interpreting the Simulation for a {project_context_name}:**
        - **The Landscape:** The heatmap represents the "process health" as understood by an Autoencoder. The peaks (hot colors) are regions where the process behaves in unexpected ways.
        - **The Swarm's Journey:** The plot shows the final state of the PSO search. The particles started randomly (**white 'x' markers**) and converged on the area of highest anomaly score (**cyan circles**). The **green star** marks the single highest-risk condition found.
        - **The Strategic Insight:** This approach automates and supercharges worst-case analysis. Instead of relying on engineers to guess at high-risk conditions, you deploy an AI agent to find them for you.
        """)
    # ... (rest of tabs are unchanged and correct) ...
    with tabs[1]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **Autoencoder (AE):** An unsupervised neural network that learns a compressed representation of data. Its `reconstruction error`‚Äîhow well it can recreate the original input‚Äîis a powerful anomaly score. In this case, it learns the "fingerprint" of a golden batch.
        - **Particle Swarm Optimization (PSO):** A population-based optimization algorithm inspired by the social behavior of bird flocking or fish schooling.
        - **Particle:** An individual agent in the swarm, representing a potential solution (a set of process parameters like `Metabolic Shift Day` and `Peak VCD`).
        - **pbest (Personal Best):** The best position (highest anomaly score) a single particle has discovered so far.
        - **gbest (Global Best):** The best position discovered by *any* particle in the entire swarm so far.
        - **Adversarial Testing:** A technique for testing a system by actively trying to find inputs that cause it to fail. Here, PSO acts as the adversary against the process model.
        """)
    with tabs[2]:
        st.error("""üî¥ **THE INCORRECT APPROACH: One-Factor-at-a-Time (OFAT) Robustness**
An engineer tests robustness by running one batch at the low end of the pH range, another at the high end, and repeats for temperature.
- **The Flaw:** This method is guaranteed to miss failures caused by *interactions*. The true "cliff of failure" might be at a specific *combination* of normal pH and high temperature that OFAT would never test.""")
        st.success("""üü¢ **THE GOLDEN RULE: Use Intelligent Search to Find Your Weaknesses**
A modern, AI-driven approach to robustness testing treats the problem as a formal optimization search.
1.  **Model Normalcy:** First, build a model (like an Autoencoder) that learns the "fingerprint" of your ideal, "golden" process from historical data.
2.  **Define the Objective:** Frame the goal not as finding the *best* outcome, but as finding the inputs that produce the *worst* outcome (i.e., maximize the anomaly score from your model).
3.  **Deploy an Optimizer:** Use a powerful search algorithm like PSO to intelligently and efficiently explore the parameter space and find this worst-case condition. This provides a data-driven, highly defensible candidate for your robustness studies.""")
    with tabs[3]:
        st.markdown("""
        #### Historical Context: A Powerful Synthesis of AI
        This module represents a cutting-edge fusion of two powerful AI concepts from different eras.
        - **The Autoencoder:** This concept has roots in the early days of neural networks and was popularized by figures like Geoffrey Hinton in the 1980s as a method for unsupervised feature learning. The idea of using its reconstruction error for anomaly detection became prominent in the 2010s with the rise of deep learning.
        - **Particle Swarm Optimization (PSO):** In 1995, social psychologist James Kennedy and electrical engineer Russell Eberhart had a brilliant insight. Inspired by simulations of bird flocking, they realized they could create a simple yet remarkably effective optimization algorithm. The core idea was that individual "particles" could find optimal solutions by balancing their own experience (`pbest`) with the collective wisdom of the group (`gbest`). It was a foundational algorithm in the field of **swarm intelligence**.
        
        **The Modern Fusion:** This tool demonstrates a modern synthesis. We use a sophisticated deep learning model (the Autoencoder) to create a complex, data-driven "landscape" of our process health. Then, we deploy a classic swarm intelligence algorithm (PSO) to efficiently search that landscape for its highest peaks. This combination allows us to solve complex robustness and optimization problems that would be intractable with either method alone.
        """)
    with tabs[4]:
        st.markdown("""
        This advanced hybrid system is a state-of-the-art implementation of the principles of modern process monitoring and control.
        - **ICH Q8(R2) - Pharmaceutical Development:** This tool provides a powerful, data-driven method for exploring the **Design Space** and identifying the "edges of failure," which is a core activity in process characterization. The worst-case conditions it identifies are prime candidates for robustness studies.
        - **FDA Process Validation Guidance (Stage 1 - Process Design):** This approach provides a deep level of "process understanding" that the guidance emphasizes.
        - **ICH Q9 (Quality Risk Management):** This is a form of proactive risk discovery. Instead of just assessing known risks, this system actively searches for new, unknown risk scenarios (combinations of parameters that lead to anomalous states).
        - **GAMP 5 & 21 CFR Part 11:** As this system uses an AI/ML models to inform critical decisions about the process operating range, the models themselves would require a robust validation lifecycle to be used in a GxP environment.
        """)

# SNIPPET: Replace your entire render_digital_twin function with this enhanced version.

def render_digital_twin():
    """Renders the comprehensive module for Digital Twin & Real-Time Simulation."""
    st.markdown("""
    #### Purpose & Application: The "Flight Simulator" for Your Process
    **Purpose:** To create a **Digital Twin**‚Äîa high-fidelity, real-time, virtual replica of a physical process. This dynamic simulation is not a static model; it lives and breathes alongside the real process, enabling advanced monitoring, prediction, and "what-if" analysis.
    
    **Strategic Application:** This is the pinnacle of the **Process Analytical Technology (PAT)** and **Pharma 4.0** initiatives. A validated digital twin is a transformative business asset that enables:
    - **Intelligent Monitoring:** Detect deviations from the "golden batch" trajectory in real-time.
    - **Proactive Control:** Forecast future states and enable feed-forward control to prevent deviations before they occur.
    - **Virtual Experimentation:** Test process changes or train operators in a safe, simulated "flight simulator" environment with zero risk to actual product.
    """)
    st.info("""
    **Interactive Demo:** You are the Lead Process Engineer monitoring a live batch.
    1.  The dashboard shows the **Real Process** (blue line) running alongside the **Digital Twin's Forecast** (grey dashed line).
    2.  Use the **"Inject Fault"** controls in the sidebar to introduce an unexpected event into the real process.
    3.  Observe the results. The two lines will diverge, and the **Process Health Score** in the bottom plot will spike, triggering an alert long before a traditional SPC chart would.
    """)

    with st.sidebar:
        st.subheader("Process Fault Simulation")
        fault_type = st.radio(
            "Select Fault Type", 
            ["None", "Drift", "Shift"],
            help="Choose the type of unexpected event to inject into the live process. 'Drift' is a slow, gradual deviation. 'Shift' is a sudden, step-change."
        )
        fault_time = st.slider(
            "Fault Injection Time (Minutes)", 
            0, 100, 50,
            help="The exact time point at which the simulated fault begins to occur."
        )
        fault_magnitude = st.slider(
            "Fault Magnitude", 
            0.0, 10.0, 5.0, 0.5,
            help="Controls the severity of the fault. A larger magnitude will create a larger deviation and a bigger spike in the Health Score."
        )

    st.header("Digital Twin Monitoring Dashboard")
    fig = plot_digital_twin_dashboard(fault_type, fault_magnitude, fault_time)
    st.plotly_chart(fig, use_container_width=True)

    st.divider()
    st.subheader("Deeper Dive into Digital Twins")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **A Realistic Workflow & Interpretation:**
        1.  **The Forecast (Top Plot):** The digital twin (often a powerful time series model like an LSTM or Prophet) continuously generates a forecast of the process's "golden trajectory"‚Äîthe ideal path it should be following.
        2.  **The Comparison:** This forecast is compared in real-time to the actual data coming from the live process sensors.
        3.  **The Health Score (Bottom Plot):** The difference between the forecast and the actual data (the residual) is calculated. This residual is a powerful, unified **"health score."** As long as the real process is behaving as expected, this score will be near zero.
        4.  **The Alarm:** When a fault occurs, the real process deviates from the twin's forecast. The health score spikes, crossing a pre-defined alert threshold and providing an immediate, highly sensitive signal that the process has entered an unexpected state.

        **The Strategic Insight:** A digital twin transforms process monitoring from simple "in-or-out" SPC charting to a much more sophisticated **conformance monitoring**. The question is no longer "Is the value within its limits?" but rather, "Is the process behaving in the way we expect it to?" This allows for the detection of subtle, complex deviations that would be invisible to traditional methods.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The "Flight Simulator" for High-Value Manufacturing

        #### The Problem: The High Cost of Physical Experimentation
        A company needs to optimize a complex, multi-million dollar bioprocess, train new operators, or investigate potential failure modes. The only way to do this is through physical experimentation on the production equipment. This is incredibly slow, expensive, and carries the risk of causing a catastrophic failure that could damage the equipment or lose a batch.

        #### The Impact: Risk Aversion and Stifled Improvement
        This high cost and risk associated with physical experimentation creates a culture of extreme conservatism and stifles innovation.
        - **"Don't Touch It" Mentality:** The process, once validated, is "locked down." Potentially valuable process improvements are never explored because the cost and risk of experimentation are deemed too high. The process stagnates.
        - **Sub-Optimal Operator Training:** New operators are trained primarily through classroom learning and by shadowing experienced staff. They never get to experience or learn how to handle rare, high-stakes process excursions in a hands-on way until a real crisis is already underway.
        - **Reactive Troubleshooting:** When a deviation occurs, the only way to test a hypothesis about the root cause is to try it on the next multi-million dollar production batch, a high-risk and inefficient way to solve problems.

        #### The Solution: A Safe, Virtual "Flight Simulator"
        A Digital Twin is the solution. It is a **high-fidelity, real-time "flight simulator" for your process**. By creating a validated, virtual replica of the physical asset, it unlocks a revolutionary new set of capabilities that are impossible to achieve in the physical world.
        1.  **Risk-Free Optimization:** Engineers can run thousands of "virtual experiments" on the digital twin to discover novel, high-performance operating conditions without consuming any raw materials or putting any real batches at risk.
        2.  **Immersive Operator Training:** New operators can be trained in a virtual environment. They can experience and learn to handle simulated crisis scenarios (e.g., a pump failure, a contamination event), building critical skills in a completely safe setting.
        3.  **Proactive "What-If" Analysis:** Before making a change, managers can ask the digital twin critical questions: "What is the predicted impact on our final CQA if we switch to this new raw material lot?"

        #### The Consequences: Accelerated Innovation and a "Self-Driving" Factory
        - **Without This:** Process improvement is slow, expensive, and high-risk. The company is always constrained by the limits of physical experimentation.
        - **With This:** The Digital Twin becomes the **central hub for process knowledge and innovation**. It dramatically **accelerates process optimization**, **reduces the risk of change**, and provides a **powerful platform for operator training**. It is the foundational technology for the next generation of manufacturing, enabling the shift from manual control to the proactive, data-driven, and eventually autonomous "lights-out" factory of the future.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **Digital Twin:** A virtual representation that serves as the real-time digital counterpart of a physical object or process. It is a dynamic model that is continuously updated with data from its physical counterpart.
        - **Simulation:** A model that imitates the operation of a real-world process or system over time. A digital twin is a specific, highly advanced type of simulation.
        - **State Estimation:** The process of using a mathematical model and sensor data to estimate the true, unobservable internal state of a system (e.g., using a Kalman Filter).
        - **Conformance Monitoring:** The practice of comparing the real-time behavior of a process to the expected behavior predicted by a "golden batch" model or a digital twin.
        - **PAT (Process Analytical Technology):** An FDA initiative that encourages the use of timely measurements, process understanding, and feedback controls to ensure final product quality is built-in. Digital twins are a key enabling technology for PAT.
        - **Pharma 4.0:** The application of Industry 4.0 principles (such as IoT, AI, and digital twins) to the pharmaceutical manufacturing industry.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Static Model" Fallacy**
A company builds a complex model of their process once, during initial development. They use it to set their specifications and then file it away. They never update it with new production data.
- **The Flaw:** This is a static snapshot, not a living twin. Over time, the physical process will inevitably change due to subtle equipment wear, raw material shifts, and environmental factors. The static model becomes an increasingly inaccurate and misleading representation of reality.""")
        st.success("""üü¢ **THE GOLDEN RULE: The Twin Must Evolve with the Process**
A true digital twin is not a one-time project; it is a living asset that must be continuously managed and updated throughout the product lifecycle.
1.  **Build from First Principles & Data:** The best twins combine a physics-based or mechanistic understanding of the process with a data-driven model (e.g., an LSTM or TCN) that learns the complex nuances from real-world data.
2.  **Validate Rigorously:** The digital twin must be formally validated to prove that its predictions accurately reflect the behavior of the physical process.
3.  **Continuously Monitor & Retrain:** There must be a formal program (as part of Stage 3 CPV) to monitor the twin's performance over time. When the twin's predictions start to diverge from reality, it's a signal that either the physical process has changed or the model needs to be retrained on new data.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From NASA's "Mirror" to the Smart Factory
        **The Problem:** The concept of a digital twin was born out of the most extreme engineering challenge imaginable: the **NASA Apollo program**. During the perilous Apollo 13 mission in 1970, an oxygen tank exploded, crippling the spacecraft. On the ground at Mission Control in Houston, engineers had a problem: how could they test potential solutions without endangering the astronauts?

        **The 'Aha!' Moment:** They had the answer: a series of full-scale, high-fidelity simulators on the ground that **"mirrored"** the exact configuration of the spacecraft in the sky. These were the world's first true digital twins. They allowed the engineers to test and validate their rescue plan‚Äîlike the famous "square peg in a round hole" CO‚ÇÇ filter solution‚Äîin a safe, virtual environment before transmitting the instructions to the crew.
        
        **The Impact:** This powerful concept of a mirrored, real-time simulation was later formalized by Dr. Michael Grieves in 2002 for manufacturing. The rise of the **Internet of Things (IoT)**, cloud computing, and advanced AI in the 2010s is what made this vision a practical reality for mainstream industry. The ability to collect massive amounts of real-time sensor data and feed it into a powerful cloud-based AI model has made the digital twin the central pillar of the **Industry 4.0** (or **Pharma 4.0**) revolution. It has moved from a crisis management tool for spacecraft to a core strategic asset for the modern smart factory.
        """)
        
    with tabs[5]:
        st.markdown("""
        The Digital Twin is the ultimate expression of the principles of modern, data-driven validation and process control and is strongly aligned with the future direction of regulatory oversight.
        - **FDA Guidance for Industry - PAT ‚Äî A Framework for Innovative Pharmaceutical Development, Manufacturing, and Quality Assurance:** The digital twin is the core enabling technology for PAT. It provides the deep "process understanding" and the predictive capability required for advanced control strategies and Real-Time Release Testing (RTRT).
        - **FDA Process Validation Guidance (Stage 3 - Continued Process Verification):** The digital twin is the most advanced form of a CPV program. Its health score provides a continuous, holistic measure of whether the process remains in its validated state.
        - **ICH Q12 - Technical and Regulatory Considerations for Pharmaceutical Product Lifecycle Management:** This guideline encourages a more robust, post-approval Change Management Protocol. A validated digital twin can provide powerful in-silico evidence to support the justification for certain process changes, potentially reducing the regulatory burden.
        - **GAMP 5 & 21 CFR Part 11:** A digital twin used for any GxP purpose (especially for RTRT) is a highly complex Computerized System that requires a rigorous validation package. The validation must prove not only that the software is reliable, but that the underlying mathematical model is accurate and fit for its intended use.
        """)
#==================================================================================================================================================================================================================
#========================================================================================== LAST TOOLS ============================================================================================================
# SNIPPET: Add these six new rendering functions to the "ALL UI RENDERING FUNCTIONS" section of your app.py file.

def render_mpc():
    """Renders the comprehensive module for Model Predictive Control."""
    st.markdown("""
    #### Purpose & Application: The AI Process Pilot
    **Purpose:** To deploy an intelligent **AI Process Pilot** that can anticipate future disturbances and act *proactively* to keep a process on target. **Model Predictive Control (MPC)** uses a dynamic model of the process (a digital twin) to look ahead, optimizing its control moves over a future time horizon.
    
    **Strategic Application:** This is a state-of-the-art control strategy that moves beyond simple reactive control (like a PID loop) to proactive, intelligent control. It is essential for processes with long delays, complex dynamics, or where minimizing overshoot and settling time is critical for product quality and throughput.
    """)
    st.info("""
    **Interactive Demo:** You are a Control Engineer. A known disturbance (e.g., a raw material change) will occur at Time=40.
    - **`Control Aggressiveness`**: Controls how strongly each controller reacts to an error.
    - **Observe:** The **Reactive Controller (orange)** only acts *after* the disturbance hits, leading to a large overshoot. The **MPC Controller (green)** sees the disturbance coming and begins to adjust *before* it happens, resulting in a much smoother, more controlled response.
    """)

    with st.sidebar:
        st.subheader("MPC Simulation Controls")
        disturbance_size = st.slider("Disturbance Magnitude", 1.0, 10.0, 5.0, 0.5, help="The size of the unexpected process shock at Time=40.")
        control_aggressiveness = st.slider("Control Aggressiveness (Kp)", 0.1, 1.0, 0.5, 0.1, help="How strongly the controllers react to errors. High values can cause instability.")
        
    st.header("Control Strategy Performance Dashboard")
    fig, overshoot_mpc, overshoot_reactive = plot_mpc_simulation(disturbance_size, control_aggressiveness)
    
    col1, col2 = st.columns(2)
    col1.metric("MPC Overshoot", f"{overshoot_mpc:.2f} units", help="Peak deviation from setpoint for the MPC controller.")
    col2.metric("Reactive Overshoot", f"{overshoot_reactive:.2f} units", help="Peak deviation from setpoint for the reactive controller.")
    st.plotly_chart(fig, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive into Model Predictive Control")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **Interpreting the Simulation:**
        - **Reactive Controller (Orange):** This controller is "blind" to the future. It only knows the current error (the difference between its current state and the setpoint). When the disturbance hits at Time=40, it is caught by surprise and overreacts, causing a large overshoot and a long settling time.
        - **MPC Controller (Green):** This controller is "prescient." It uses its internal process model to simulate the future. Because it "knows" a disturbance is coming, it begins making small, proactive adjustments *before* the event. When the disturbance hits, the MPC is already prepared, resulting in a much smaller deviation and a faster return to the setpoint.

        **The Strategic Insight:** Traditional control is about correcting past errors. Model Predictive Control is about **optimizing future performance**. By using a predictive model, MPC can achieve a level of performance and stability that is fundamentally impossible for a purely reactive controller, especially in complex processes with long delays.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: From "Driving by the Rear-View Mirror" to "Autopilot"

        #### The Problem: The Lagging Control System
        A complex, high-value process (like a large-scale bioreactor or a continuous manufacturing line) is controlled by a traditional PID (Proportional-Integral-Derivative) control system. This system is a **reactive workhorse**. It measures the current error and adjusts accordingly. However, the process has long delays‚Äîit can take hours for a change in a feed rate to have a measurable impact on the output.

        #### The Impact: Chronic Oscillation and Sub-Optimal Performance
        This reactive, "rear-view mirror" approach in a slow process leads to chronic inefficiency and risk.
        - **Process Oscillations:** The PID controller makes an adjustment, but due to the time lag, it doesn't see the effect immediately. It overreacts, pushing the process too far in the other direction. This creates a constant, wasteful oscillation around the setpoint, never truly settling. This variability reduces yield and can compromise product quality.
        - **Inability to Handle Constraints:** The PID controller doesn't understand physical limits. It might try to command a valve to open to 110%, or a pump to exceed its maximum flow rate, leading to equipment stress and inefficient control.
        - **Sub-Optimal Throughput:** Because the system is unstable and prone to overshooting, engineers are forced to run the process with very conservative, sub-optimal setpoints to provide a large safety buffer, sacrificing throughput and profitability.

        #### The Solution: The "Autopilot" for Your Process
        Model Predictive Control (MPC) is the **"autopilot" for your chemical plant or bioreactor**. It is a fundamentally more intelligent control strategy that uses a **digital twin** of the process to look into the future. At every control step, it runs thousands of simulations to answer the question: "Given my current state and my model of the process, what is the optimal sequence of control moves over the next several hours to keep me on target while respecting all equipment constraints?" It then implements only the first step of that optimal plan and repeats the entire calculation at the next time step.

        #### The Consequences: Increased Throughput, Stability, and Profitability
        - **Without This:** The process is chronically unstable, inefficient, and must be run conservatively, leaving significant value on the table.
        - **With This:** MPC provides a **step-change in control performance**. By proactively managing disturbances and optimizing the control path, it **dampens oscillations, reduces variability, and allows the process to be run much closer to its optimal, high-throughput setpoints**. In the chemical and refining industries, where MPC is standard, it is credited with unlocking billions of dollars in value through increased efficiency and throughput. It is a key enabling technology for the high-performance, autonomous operations of Pharma 4.0.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **MPC (Model Predictive Control):** An advanced method of process control that uses a dynamic model of the process to predict its future behavior and make optimal control decisions.
        - **Digital Twin:** The real-time, dynamic model of the process that MPC uses as its "crystal ball" to simulate future states.
        - **Control Horizon:** The number of future time steps over which the MPC calculates its optimal sequence of control moves.
        - **Prediction Horizon:** The total time horizon into the future that the MPC simulates to evaluate the impact of its control moves.
        - **Setpoint:** The target value for a critical process parameter that the control system is trying to maintain.
        - **PID Controller (Proportional-Integral-Derivative):** The most common form of reactive feedback controller in industry. It calculates a control action based on the present error (P), past errors (I), and predicted future errors (D).
        """)

    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Aggressive PID" Fallacy**
An engineer tries to fix a sluggish, oscillating process by aggressively tuning the PID controller (e.g., cranking up the Proportional gain).
- **The Flaw:** This makes the problem worse. The controller overreacts even more violently to any small error, leading to wilder oscillations and instability. The fundamental problem is the time delay, which a reactive controller cannot solve.""")
        st.success("""üü¢ **THE GOLDEN RULE: Control the Future, Not the Past**
A robust, modern control strategy is predictive, not just reactive.
1.  **Invest in the Model:** The performance of an MPC system is entirely dependent on the quality of its underlying dynamic model (the digital twin). The first and most critical step is to develop a high-fidelity model based on a combination of first principles (physics, chemistry) and real-world process data.
2.  **Define the Objective:** The MPC's goal is not just to stay at a setpoint, but to do so while minimizing an **objective function**. This function can be tuned to prioritize different business goals, such as minimizing energy consumption, minimizing settling time, or maximizing yield.
3.  **Deploy and Monitor:** Once deployed, the performance of the MPC controller itself must be monitored to ensure the digital twin remains an accurate representation of the real process.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Shell Oil to SpaceX
        **The Problem:** In the 1970s, the oil and chemical industries were facing a new challenge. Their processes were slow, complex, and had many interacting variables and constraints. Traditional PID controllers, which are brilliant for simple, fast-acting systems, were struggling to provide stable control, leading to inefficiency and waste.
        
        **The 'Aha!' Moment:** Engineers and mathematicians, particularly Charles Cutler at Shell Oil, pioneered a new approach in the late 1970s and early 1980s. The key insight was to use a computer model of the process to **predict its future behavior** and then use an optimization algorithm to calculate the best control moves in advance. This was the birth of **Model Predictive Control**.
        
        **The Impact:** The results were revolutionary. MPC provided a way to achieve tight, stable control of previously "uncontrollable" processes. It became the standard for advanced process control in the refining, chemical, and petrochemical industries, where it is credited with saving billions of dollars annually. Today, the same technology is used in everything from the flight control systems of SpaceX rockets to the adaptive cruise control in modern cars. Its application to the complex, slow dynamics of biopharmaceutical manufacturing is a key part of the Pharma 4.0 revolution.
        """)
        
    with tabs[5]:
        st.markdown("""
        MPC is a state-of-the-art control methodology that is a direct implementation of the principles of **Process Analytical Technology (PAT)**.
        - **FDA Guidance for Industry - PAT:** The PAT guidance is built on the principle of "Design and control the process." It states that "process control is critical for a process that is operated in a state of control that ensures the final product quality." MPC is the most advanced form of this control.
        - **ICH Q8(R2) - Pharmaceutical Development:** A validated MPC system that keeps a process within its proven **Design Space** is a powerful demonstration of a robust **Control Strategy**.
        - **GAMP 5 & 21 CFR Part 11:** An MPC system used to control a GxP process is a highly complex, high-risk (GAMP Category 5) Computerized System. Both the software platform and the underlying process model require rigorous validation to prove they are fit for their intended use. The validation package would need to demonstrate the model's accuracy, the controller's robustness, and the system's reliability.
        """)

def render_rtrt():
    """Renders the comprehensive module for Real-Time Release Testing."""
    st.markdown("""
    #### Purpose & Application: The Ultimate Goal of PAT
    **Purpose:** To create a validated system that allows for **Real-Time Release Testing (RTRT)**‚Äîthe ability to release a batch based on in-process data, rather than waiting for slow, end-of-line laboratory tests.
    
    **Strategic Application:** This is the pinnacle of the Pharma 4.0 and Process Analytical Technology (PAT) initiatives. It represents a paradigm shift from "quality by testing" to "quality by design and control." Achieving RTRT is a massive competitive advantage, as it can slash release times from weeks to days, freeing up enormous amounts of capital and dramatically improving supply chain agility.
    """)
    st.info("""
    **Interactive Demo:** You are the Head of Quality.
    - **`PAT Model Accuracy`**: Controls how well your in-process model predicts the final lab result.
    - **`Lab Turnaround Time`**: The current bottleneck you are trying to eliminate.
    - **Observe:** The dashboard shows the instant PAT prediction versus the delayed lab result. The timeline below visualizes the massive reduction in "Awaiting QC" time that RTRT enables.
    """)

    with st.sidebar:
        st.subheader("RTRT Simulation Controls")
        model_accuracy = st.slider("PAT Model Accuracy (R¬≤)", 80, 100, 95, 1, help="The predictive accuracy of your real-time PAT model. Higher is better, and regulators will demand a very high R¬≤ to approve RTRT.")
        lab_delay = st.slider("Lab Turnaround Time (Days)", 1, 14, 7, 1, help="The time it takes to get a final result from the traditional QC lab. This is the bottleneck you are eliminating.")
        lab_noise = st.slider("Lab Method Noise (SD)", 0.5, 3.0, 1.5, 0.1, help="The random variability of the traditional QC lab method.")
        
    st.header("Real-Time Release Dashboard")
    fig = plot_rtrt_dashboard(model_accuracy, lab_delay, lab_noise)
    wip_reduction = lab_delay * 100000 # Example calculation
    
    col1, col2 = st.columns(2)
    col1.metric("Time Saved per Batch", f"{lab_delay} Days")
    col2.metric("WIP Inventory Reduction", f"${wip_reduction:,.0f}")
    st.plotly_chart(fig, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive into Real-Time Release")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **Interpreting the Dashboard:**
        - **Top Plot (Analytical Results):** This plot shows the core of the validation argument for RTRT. You must prove that your PAT model's predictions (green line) are a reliable surrogate for the traditional lab test (orange 'x' markers). The closer the green line is to the black "truth" line, the more accurate your model.
        - **Bottom Plot (Timeline):** This is the business case. It provides a stark visualization of the "before" and "after." The long orange "Awaiting QC" bar is the bottleneck that RTRT eliminates, allowing the batch to move from "In-Process" directly to "Released," slashing the total lead time.

        **The Strategic Insight:** RTRT is not just about having a good predictive model. It requires a holistic, validated system that combines **1) advanced PAT sensors**, **2) a highly accurate and validated predictive model**, and **3) a robust quality system and regulatory strategy** to support the approach.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The Multi-Million Dollar Waiting Game

        #### The Problem: The "Hurry Up and Wait" Supply Chain
        A company manufactures a high-value biologic drug. The manufacturing process itself takes 3 days. However, the final release testing in the QC lab‚Äîa series of complex, slow bioassays‚Äîtakes an additional **14 days**. This means a multi-million dollar batch of life-saving medicine sits in a refrigerated warehouse, in a state of "quarantine," for two weeks.

        #### The Impact: Strangled Cash Flow and a Brittle Supply Chain
        This QC testing bottleneck is a massive, often invisible, drain on the company's financial health and operational agility.
        - **Massive Trapped Capital:** With a 14-day quarantine period, the company can have hundreds of millions of dollars of finished product inventory that is physically complete but cannot be sold. This is a huge drag on working capital and cash flow.
        - **Supply Chain Fragility:** If a sudden surge in demand occurs (e.g., during a pandemic), the company cannot respond quickly. They have a warehouse full of product they can't release. This inability to be agile leads to drug shortages, lost revenue, and can have serious public health consequences.
        - **The Risk of the Unknown:** If a deviation is discovered during the final testing, it's too late. The entire batch must be rejected, leading to a massive write-off and a sudden hole in the supply plan.

        #### The Solution: The "Certificate of Analysis" from the Production Line
        Real-Time Release Testing (RTRT) is the solution. It is a paradigm shift that aims to generate the "Certificate of Analysis" directly from the manufacturing line, using data, not just from the final lab test. It is built on the core principle of **Process Analytical Technology (PAT)**: if you have a deep, validated understanding of your process and can monitor its Critical Quality Attributes (CQAs) in real-time, you can be assured of the final product quality without needing the slow, retrospective lab test.

        #### The Consequences: A Fast, Lean, and Agile Supply Chain
        - **Without This:** The QC lab is a permanent bottleneck, making the supply chain slow, expensive, and fragile.
        - **With This:** Achieving RTRT is a transformative competitive advantage. It **slashes inventory costs**, **frees up hundreds of millions in working capital**, and creates a hyper-agile supply chain that can respond to patient needs in days, not weeks. It is the ultimate goal of the Pharma 4.0 revolution, turning the quality unit from a gatekeeper into a real-time enabler of business velocity.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **RTRT (Real-Time Release Testing):** The ability to evaluate and ensure the quality of in-process and/or final product based on process data, which typically includes a valid combination of assessed material attributes and process controls.
        - **PAT (Process Analytical Technology):** A system for designing, analyzing, and controlling manufacturing through timely measurements of critical quality and performance attributes of raw and in-process materials and processes, with the goal of ensuring final product quality.
        - **CQA (Critical Quality Attribute):** A physical, chemical, biological or microbiological property or characteristic that should be within an appropriate limit, range, or distribution to ensure the desired product quality.
        - **Digital Twin:** A high-fidelity, real-time model of the process that is often used as the engine to make the RTRT predictions.
        - **Surrogate Model:** The PAT prediction is a "surrogate" for the traditional lab test. The validation of RTRT involves proving that this surrogate is a reliable and accurate replacement.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Model is Magic" Fallacy**
A team develops a highly accurate predictive model in R&D. They assume they can immediately use it to release batches from the manufacturing floor.
- **The Flaw:** They have confused a good model with a validated system. The model was never validated on real-world manufacturing data, the PAT sensors on the floor are not qualified, the software is not 21 CFR Part 11 compliant, and there is no approved regulatory filing that allows for this approach. The model is a "science project," not a GxP system.""")
        st.success("""üü¢ **THE GOLDEN RULE: RTRT is a Strategy, Not Just a Model**
Achieving RTRT is a multi-year, strategic initiative that requires a holistic approach.
1.  **Deep Process Understanding (QbD):** You must first have a deep, fundamental understanding of your process, with well-defined CQAs and CPPs.
2.  **Robust PAT Implementation:** You need qualified, real-time sensors to measure the critical parameters that feed the model.
3.  **Rigorous Model Validation:** The predictive model must be rigorously validated to prove its accuracy, precision, and robustness on real manufacturing data. This validation must be just as thorough as the validation of a traditional lab method.
4.  **Proactive Regulatory Engagement:** An RTRT strategy must be discussed and agreed upon with regulatory agencies *before* it is implemented for commercial release.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Steel Mills to Sterile Suites
        **The Problem:** For over a century, the dominant model for quality was "make and test." A batch of steel, a car, or a pharmaceutical tablet was produced, and then a sample was taken to a laboratory for testing. This created a fundamental time lag and inefficiency in every industry.

        **The 'Aha!' Moment (PAT Initiative):** In the early 2000s, the US Food and Drug Administration (FDA), led by visionaries like Dr. Janet Woodcock, looked at other high-tech industries like semiconductors and petrochemicals. They saw that these industries had moved far beyond the slow "make and test" paradigm. They were using advanced sensors and real-time models to control their processes and ensure quality was built-in from the start. The FDA realized that the pharmaceutical industry was lagging far behind.
        
        **The Impact:** In 2004, the FDA launched its landmark guidance, **"PAT ‚Äî A Framework for Innovative Pharmaceutical Development, Manufacturing, and Quality Assurance."** This was a call to arms for the industry to modernize. The concept of **Real-Time Release Testing (RTRT)** was the ultimate prize offered by this new PAT framework. It represented a complete paradigm shift‚Äîfrom Quality by Testing to Quality by Design and Control. While the adoption has been slow due to the high technical and regulatory hurdles, RTRT remains the "North Star" for the modern, efficient, and data-driven pharmaceutical manufacturing of the future.
        """)
        
    with tabs[5]:
        st.markdown("""
        RTRT is an advanced strategy that is explicitly defined and encouraged by global regulatory bodies as the ultimate goal of a modern quality system.
        - **FDA Guidance for Industry - PAT ‚Äî A Framework for Innovative Pharmaceutical Development, Manufacturing, and Quality Assurance:** This is the foundational guidance that introduced and defined RTRT. It outlines the expectation that RTRT is based on deep process understanding, real-time monitoring, and a robust control strategy.
        - **ICH Q8(R2) - Pharmaceutical Development:** The principles of Quality by Design (QbD) are the scientific foundation for RTRT. A well-defined Design Space and Control Strategy are prerequisites for justifying a real-time release approach.
        - **GAMP 5 & 21 CFR Part 11:** An RTRT system is the quintessential high-risk, GAMP Category 5 Computerized System. The entire system‚Äîfrom the PAT sensors to the data historian, the predictive model, and the final release decision logic‚Äîmust be rigorously validated to ensure data integrity, model accuracy, and system reliability.
        """)
def render_biosimilarity():
    """Renders the comprehensive module for Analytical Comparability & Biosimilarity."""
    st.markdown("""
    #### Purpose & Application: The Gauntlet of "Sameness"
    **Purpose:** To provide a statistically rigorous framework for demonstrating **analytical biosimilarity**. This dashboard simulates the "equivalence tier" approach, where multiple Critical Quality Attributes (CQAs) of a biosimilar product are statistically compared against a reference product to prove they are "highly similar."
    
    **Strategic Application:** This is a mission-critical activity for any company developing biosimilar or generic drug products. The entire business case rests on the ability to provide a convincing, multi-faceted, data-driven argument to regulators that your product is statistically indistinguishable from the innovator's product. This dashboard is the scorecard for that argument.
    """)
    st.info("""
    **Interactive Demo:** You are the Head of Biosimilar Development.
    - Use the sliders to introduce subtle differences in the `Potency` and `Impurity` profiles of your biosimilar candidate.
    - The dashboard will run a formal **Equivalence Test (TOST)** for each CQA.
    - The final verdict depends on **all CQAs** passing their individual, pre-defined equivalence margins (the grey dotted bars).
    """)

    with st.sidebar:
        st.subheader("Biosimilar Comparability Controls")
        mean_shift_potency = st.slider("Mean Shift in Potency (%)", -2.0, 2.0, 0.5, 0.1, help="Simulates a small bias in the biosimilar's potency assay. A large shift will cause this CQA to fail equivalence.")
        variance_factor_impurity = st.slider("Variance Factor for Aggregate", 1.0, 2.0, 1.2, 0.05, help="Simulates if the biosimilar's aggregate impurity profile is more variable than the original. High variability widens the CI, making it harder to prove equivalence.")
        
    st.header("Biosimilarity Equivalence Dashboard")
    fig, df_results = plot_biosimilarity_dashboard(mean_shift_potency, variance_factor_impurity)
    
    passing_cqas = df_results['equivalent'].sum()
    total_cqas = len(df_results)
    
    st.metric("Comparability Verdict", f"{passing_cqas} out of {total_cqas} CQAs are equivalent")
    st.plotly_chart(fig, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive into Analytical Comparability")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **Interpreting the Equivalence Tiers:**
        - **The Goalposts (Grey Dotted Bars):** For each CQA, a pre-defined **equivalence margin** is set based on its clinical criticality. Highly critical attributes like Potency have very tight margins.
        - **The Evidence (Colored Bars):** Each colored bar represents the **90% Confidence Interval** for the difference between your biosimilar and the original product for that CQA.
        - **The Verdict:** The test for a CQA **passes (green)** only if its entire confidence interval falls *within* the grey goalposts. If any part of the bar extends beyond the margin, the test **fails (red)**.
        
        **The Strategic Insight:** Demonstrating biosimilarity is not about being "close" on average; it is about proving, with high statistical confidence, that any difference is smaller than a pre-defined, clinically irrelevant amount **across all critical attributes**. A single failure on a key CQA can jeopardize the entire program.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The Multi-Billion Dollar Statistical Argument

        #### The Problem: The "Looks Similar" Fallacy
        A company invests hundreds of millions of dollars developing a biosimilar for a blockbuster biologic drug. Their analytical data shows that, on average, their product's key quality attributes "look pretty similar" to the innovator's product. They prepare a regulatory submission based on this qualitative assessment, supported by simple t-tests that show "no significant difference."

        #### The Impact: A Failed Submission and a Wasted Investment
        This approach is doomed to fail, potentially wasting a billion-dollar investment.
        - **Regulatory Rejection:** Global regulators (FDA, EMA) will immediately reject a submission based on the flawed logic that "failing to prove a difference" is the same as "proving similarity." The company has failed to meet the fundamental statistical requirement of a biosimilarity package.
        - **Unmanaged Clinical Risk:** The company's product might have a subtle but clinically significant difference in potency or impurity profile that their underpowered statistical analysis completely missed. If approved, this could lead to differences in patient outcomes and a potential recall.
        - **Total Loss of Investment:** Without regulatory approval, the hundreds of millions invested in R&D, clinical trials, and manufacturing scale-up are a complete write-off.

        #### The Solution: A Formal, Tiered Gauntlet of Equivalence
        A successful biosimilarity program is built on a **formal, pre-defined statistical analysis plan** that uses the rigorous logic of **equivalence testing**. The approach, demonstrated in this dashboard, is a statistical gauntlet:
        1.  **Tiering of Attributes:** CQAs are stratified into tiers based on their potential impact on patient safety and efficacy. Tier 1 attributes (e.g., Potency) face the most stringent statistical hurdles.
        2.  **Pre-defined Margins:** For each CQA, a scientifically-justified **equivalence margin** is defined *before* the study. This is the "goalpost" for success.
        3.  **Rigorous Equivalence Testing:** The data is analyzed using **TOST** to provide positive proof that any difference is well within these pre-defined margins.

        #### The Consequences: A Defensible Pathway to Market
        - **Without This:** A biosimilar program is a high-stakes gamble with a high probability of regulatory failure.
        - **With This:** The tiered equivalence testing framework provides the **objective, statistically rigorous, and defensible evidence** that regulators require. It is the **mathematical foundation of the entire biosimilar business model**. By successfully navigating this statistical gauntlet, the company can provide the high degree of assurance needed to gain regulatory approval, unlock a multi-billion dollar market, and provide patients with safe, effective, and more affordable medicines.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **Biosimilar:** A biologic medical product that is almost an identical copy of an original product that is manufactured by a different company.
        - **Comparability:** The demonstration that a product remains highly similar in its quality attributes before and after a manufacturing process change. The same statistical principles as biosimilarity apply.
        - **CQA (Critical Quality Attribute):** A property of the drug that should be within an appropriate limit to ensure the desired product quality.
        - **Equivalence Testing (TOST):** The statistical method used to prove that the difference between two groups (e.g., biosimilar vs. original) is within a pre-defined, practically insignificant margin.
        - **Tiering:** The practice of stratifying CQAs into different risk categories (e.g., Tiers 1, 2, and 3) based on their potential clinical impact. Tier 1 attributes are subjected to the most stringent equivalence criteria.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Cherry-Picking" Fallacy**
An analyst runs dozens of different statistical tests and only presents the ones that show a favorable result in the regulatory submission. They ignore the CQAs where equivalence could not be demonstrated.
- **The Flaw:** This is a serious breach of scientific and regulatory integrity. Regulators expect a full, transparent report on *all* pre-specified critical attributes. Hiding unfavorable results will lead to a complete loss of trust and a likely rejection.""")
        st.success("""üü¢ **THE GOLDEN RULE: Pre-Define the Entire Gauntlet**
A successful and defensible comparability study is based on a pre-defined and unchangeable plan.
1.  **Define the Tiers:** Before the study, formally classify all CQAs into risk-based tiers.
2.  **Define the Margins:** For each CQA, prospectively define and justify the equivalence margin based on scientific knowledge and clinical relevance.
3.  **Define the Statistics:** Pre-specify the exact statistical test that will be used for each CQA.
4.  **Execute and Report All:** Execute the plan and report the results for *all* pre-specified attributes, whether they pass or fail. Any failures must be addressed with a thorough scientific justification.
This disciplined approach is the only way to build a trustworthy and successful submission.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: The Biologic Revolution
        **The Problem:** The rise of biologic drugs (monoclonal antibodies, therapeutic proteins) in the 1990s and 2000s created a new regulatory and scientific challenge. Unlike small-molecule drugs that can be perfectly copied to create a "generic," large-molecule biologics are produced by living cells and are exquisitely sensitive to the manufacturing process. It is impossible to create a truly "identical" copy.
        
        **The 'Aha!' Moment:** Regulators, led by the EMA in Europe and the FDA in the US, recognized that a new pathway was needed. They created the concept of a **"biosimilar"**‚Äîa product that is not identical, but is **"highly similar"** to the reference product with no clinically meaningful differences. This created a massive statistical challenge: how do you prove "high similarity"?
        
        **The Impact:** The statistical framework of **equivalence testing**, which had already been perfected for small-molecule bioequivalence, was adopted as the core engine for this new challenge. However, it was elevated to a new level of complexity. Instead of just testing the equivalence of a single pharmacokinetic parameter, a biosimilar developer now had to prove the equivalence of a whole **"fingerprint"** of dozens of Critical Quality Attributes. The **tiered approach** to equivalence, demonstrated in this dashboard, became the industry-standard framework for managing this complexity and presenting a convincing, risk-based argument to regulators. This statistical framework is the key that unlocked the multi-billion dollar global biosimilar market.
        """)
        
    with tabs[5]:
        st.markdown("""
        Analytical comparability is governed by a specific set of guidelines for biologics and biosimilars.
        - **ICH Q5E - Comparability of Biotechnological/Biological Products:** This is the foundational guideline. It establishes the principle that the quality of a product must be shown to be highly similar before and after a manufacturing change. It introduces the concept of a "comparability exercise."
        - **FDA Guidance for Industry - "Scientific Considerations in Demonstrating Biosimilarity to a Reference Product":** This guidance explicitly outlines the FDA's expectation for a stepwise, risk-based approach to demonstrating biosimilarity, with a heavy emphasis on a comprehensive analytical comparison as the foundation.
        - **Biologics Price Competition and Innovation Act (BPCIA):** The US law that created the abbreviated licensure pathway for biosimilars, legally defining the standard as "highly similar" with "no clinically meaningful differences."
        - **Equivalence Testing (TOST):** While not a regulation itself, the use of equivalence testing is the universally accepted statistical method for fulfilling the requirements of the above guidelines.
        """)

def render_nonparametric_workbench():
    """Renders the comprehensive module for Non-Parametric Statistics."""
    st.markdown("""
    #### Purpose & Application: The Statistical "Off-Road Vehicle"
    **Purpose:** To provide a toolkit of **non-parametric** statistical methods. These are "distribution-free" tests that do not assume your data follows a specific shape (like the classic bell curve of the normal distribution).
    
    **Strategic Application:** This is your statistical "off-road vehicle," essential for situations where the assumptions of standard tests are violated.
    - **Handling Outliers:** Non-parametric tests are highly robust to outliers that would invalidate a standard t-test.
    - **Analyzing Skewed Data:** Many real-world processes, especially those involving counts or impurities, produce skewed, non-normal data.
    - **Small Sample Sizes:** When `n` is too small to confidently assume normality, non-parametric tests provide a safer, more conservative alternative.
    """)
    st.info("""
    **Interactive Demo:** You are comparing two batches. Use the slider to inject a single, large **outlier** into Group B.
    - **Observe:** The **t-test**, which is based on means, is highly sensitive to the outlier and incorrectly concludes the groups are different (a false positive).
    - **Observe:** The **Mann-Whitney U test**, which is based on ranks, is robust to the outlier and correctly concludes that the underlying distributions are the same.
    """)

    with st.sidebar:
        st.subheader("Non-Parametric Controls")
        outlier_magnitude = st.slider("Outlier Magnitude (in SDs)", 0, 10, 5, 1, help="Controls the size of a single outlier added to Group B. A value of 0 means no outlier. Watch how the t-test p-value collapses as the outlier grows.")
        
    st.header("Parametric vs. Non-Parametric Test Robustness")
    fig, ttest_p, mwu_p = plot_nonparametric_comparison(outlier_magnitude)
    
    col1, col2 = st.columns(2)
    with col1:
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        st.subheader("Statistical Verdict")
        st.metric("Student's t-test p-value", f"{ttest_p:.4f}")
        if ttest_p < 0.05: st.error("‚ùå t-test Verdict: Groups are different (misled by outlier).")
        else: st.success("‚úÖ t-test Verdict: No evidence of a difference found.")
        
        st.metric("Mann-Whitney U p-value", f"{mwu_p:.4f}")
        if mwu_p < 0.05: st.error("‚ùå M-W Verdict: Groups are different.")
        else: st.success("‚úÖ M-W Verdict: No evidence of a difference found (robust to outlier).")
    
    st.divider()
    st.subheader("Deeper Dive into Non-Parametric Methods")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Common Non-Parametric Tests", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **Interpreting the Simulation:**
        - **The t-test's Weakness:** The t-test works by comparing the **means** of the two groups. A single extreme outlier has a massive effect on the mean, pulling it far away from the true center of the data. This causes the t-test to detect a "significant difference" that is not real, but is just an artifact of the one bad data point.
        - **The Mann-Whitney's Strength:** The Mann-Whitney U test works by a completely different principle. It first **converts all the data into ranks** (1st, 2nd, 3rd, etc.) and then tests if the ranks from one group are systematically higher or lower than the other. The extreme outlier is simply given the highest rank, but its actual *magnitude* is ignored. This makes the test incredibly robust to outliers.

        **The Strategic Insight:** Your choice of statistical test is a critical risk-based decision. If your data is known to be clean and well-behaved, a parametric test like the t-test is more powerful. However, if you suspect the presence of outliers or your data is not normally distributed, a non-parametric test is a much safer and more reliable choice that protects you from drawing false conclusions.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The High Cost of a False Positive
    
        #### The Problem: The "Perfectly Normal" Assumption
        A company is running a critical comparability study to prove that a new raw material supplier is equivalent to the old one. The analyst runs a standard t-test on the data. Unbeknownst to them, a single, anomalous data point‚Äîcaused by a minor lab error‚Äîhas crept into the dataset.
    
        #### The Impact: A Multi-Million Dollar Decision Based on a Glitch
        The t-test is highly sensitive to this outlier. It produces a p-value of 0.04, leading to the formal conclusion that the two suppliers are **statistically different**.
        - **Unnecessary Rejection of a Good Supplier:** Based on this single false positive result, the company rejects the new, potentially cheaper and more reliable supplier. The multi-million dollar cost-saving initiative is cancelled.
        - **Wasted Investigation:** A massive investigation is launched to understand the "significant difference" between the suppliers. The team spends weeks of engineering time trying to find a root cause for a difference that doesn't actually exist.
        - **Loss of Trust in Data:** When the true cause (the single outlier) is eventually found, management loses confidence in the statistical methods and the data analysis team.
    
        #### The Solution: A Statistical "Safety Net"
        Non-parametric tests are the statistical "safety net" that protects against these kinds of costly errors. They are designed to be robust to the messy realities of real-world data. By converting data to ranks, they are inherently insensitive to the magnitude of extreme outliers. A Mann-Whitney U test on the same data would have produced a p-value > 0.05, correctly concluding that there is no evidence of a true difference between the suppliers and that the single strange point was likely just noise.
    
        #### The Consequences: Robust Decisions and Increased Efficiency
        - **Without This:** The organization is vulnerable to making major strategic errors based on statistically significant but practically meaningless results caused by data glitches.
        - **With This:** A culture of robust statistical practice, which includes the use of non-parametric tests when appropriate, leads to **more reliable and defensible conclusions**. It **prevents costly wild goose chases**, saves engineering resources, and ensures that critical business decisions are based on the true underlying signal in the data, not just the noise.
        """)
        
    with tabs[2]:
        st.markdown("""
        Non-parametric tests are often called "distribution-free" because they don't assume the data follows a specific distribution (like the normal distribution). Here are some of the most common non-parametric alternatives to standard parametric tests.

        | **Goal of the Test** | **Parametric Test (Assumes Normality)** | **Non-Parametric Alternative (No Assumption)** |
        | :--- | :--- | :--- |
        | Compare **two independent groups** | Independent Samples t-test | **Mann-Whitney U Test** (also called Wilcoxon Rank-Sum Test) |
        | Compare **two paired groups** (e.g., before & after) | Paired Samples t-test | **Wilcoxon Signed-Rank Test** |
        | Compare **three or more independent groups** | One-Way ANOVA | **Kruskal-Wallis Test** |
        | Compare **three or more related groups** | Repeated Measures ANOVA | **Friedman's Test** |
        | Measure the **correlation** between two variables | Pearson Correlation | **Spearman Rank Correlation** |
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Ignore the Assumptions" Fallacy**
An analyst runs a t-test on a small dataset with a clear outlier without first checking the assumptions of the test.
- **The Flaw:** All statistical tests are built on a set of assumptions. If you violate those assumptions, the results (especially the p-value) are not reliable. The most common violated assumption is that the data is normally distributed, a vulnerability that non-parametric tests are designed to fix.""")
        st.success("""üü¢ **THE GOLDEN RULE: Check Your Assumptions, Then Choose Your Test**
A robust statistical analysis always follows a disciplined workflow.
1.  **Visualize Your Data First (EDA):** Always start with a box plot or a histogram to understand the shape of your data. This is the best way to spot outliers and severe skewness.
2.  **Formally Test Assumptions:** Use a formal statistical test for normality (like the Shapiro-Wilk test) if you are unsure.
3.  **Choose the Right Tool:**
    - If the data looks reasonably symmetric, has no major outliers, and the sample size is large (e.g., >30), a **parametric test** is generally preferred as it has more statistical power.
    - If the data is skewed, contains outliers, or the sample size is very small, a **non-parametric test** is the safer, more robust, and more defensible choice.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: The Rise of "Distribution-Free" Methods
        **The Problem:** The "golden age" of statistics in the early 20th century, led by giants like R.A. Fisher, was built on a foundation of elegant but strict mathematical assumptions‚Äîchief among them, the assumption that the data followed a normal (Gaussian) distribution. This was a powerful simplification, but many real-world datasets, especially in fields like biology and social sciences, simply weren't that well-behaved.
        
        **The 'Aha!' Moment:** In the 1940s, a new school of thought emerged, led by statisticians like **Frank Wilcoxon**. Their brilliant insight was to get rid of the "tyranny of the normal distribution" by getting rid of the data's actual values altogether. They developed methods based on **ranks**. Instead of comparing the means of two groups, Wilcoxon's test (later refined by **Henry Mann** and **Donald Whitney**) simply asked: "If you mix all the data together and rank it from smallest to largest, are the ranks from Group A systematically higher or lower than the ranks from Group B?"
        
        **The Impact:** This was a revolutionary idea. By converting data to ranks, the test became immune to outliers and made no assumptions about the data's original shape. These "distribution-free" or **non-parametric** methods provided a robust and reliable toolkit for analyzing the messy, non-normal data that was common in the real world. While they are sometimes less powerful than parametric tests *if* the assumptions are met, their robustness has made them an essential part of any modern statistician's toolkit.
        """)
        
    with tabs[5]:
        st.markdown("""
        The use of valid statistical techniques is a core GxP requirement. Choosing the correct test based on the data's properties is a key part of demonstrating statistical rigor.
        - **21 CFR 820.250 (Statistical Techniques):** This regulation for medical devices requires manufacturers to establish and maintain procedures for identifying **"valid statistical techniques"**. Using a non-parametric test when the assumptions for a parametric test are clearly violated is a direct fulfillment of this requirement. Failing to do so could be cited as using an "invalid" technique.
        - **ICH Q9 (Quality Risk Management):** The risk of drawing an incorrect conclusion from a statistical test is a significant data integrity risk. Choosing a robust, non-parametric method can be a risk mitigation strategy when data quality is uncertain.
        - **Regulatory Submissions:** In a submission to the FDA or other agencies, the choice of statistical methods must be pre-specified and justified in the statistical analysis plan. If the data is expected to be non-normal, justifying the use of a non-parametric test upfront demonstrates statistical foresight and robustness.
        """)

def render_capa_effectiveness():
    """Renders the comprehensive module for CAPA Effectiveness."""
    st.markdown("""
    #### Purpose & Application: Closing the Loop
    **Purpose:** To provide a formal, data-driven method for proving that a **Corrective and Preventive Action (CAPA)** was effective. This dashboard moves beyond simply closing a CAPA to statistically demonstrating that the implemented fix actually improved the process.
    
    **Strategic Application:** This is the final, critical step in any robust CAPA system. It is the objective evidence required by auditors to prove that your quality system is not just identifying problems, but is **permanently solving them**. A CAPA without an effectiveness check is just a documented guess.
    """)
    st.info("""
    **Interactive Demo:** You are the Quality Engineer responsible for a CAPA.
    - A process was running off-center and with high variability (`Cpk < 1.0`).
    - Use the **`CAPA Improvement Magnitude`** slider to simulate how effective your implemented fix was.
    - The dashboard shows a clear "Before vs. After" comparison, with the change in Process Capability (Cpk) as the primary quantitative measure of success.
    """)

    with st.sidebar:
        st.subheader("CAPA Effectiveness Controls")
        improvement_magnitude = st.slider("CAPA Improvement Magnitude", 0.0, 2.0, 1.0, 0.1, help="How effective the implemented CAPA was at centering the process and reducing its variability. A higher value represents a more successful fix.")
        
    st.header("CAPA Effectiveness Dashboard")
    fig, cpk_before, cpk_after = plot_capa_effectiveness(improvement_magnitude)
    
    col1, col2 = st.columns(2)
    col1.metric("Process Capability (Cpk) Before CAPA", f"{cpk_before:.2f}")
    col2.metric("Process Capability (Cpk) After CAPA", f"{cpk_after:.2f}", delta=f"{cpk_after - cpk_before:.2f}")
    st.plotly_chart(fig, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive into CAPA Effectiveness")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **Interpreting the Dashboard:**
        - **Before CAPA (Red Histogram):** This shows the original problem. The process distribution is wide and significantly off-center, leading to a very poor Cpk.
        - **After CAPA (Green Histogram):** This shows the process after the fix. As you increase the **`Improvement Magnitude`**, the green distribution becomes narrower (less variation) and moves closer to the center of the specification limits.
        - **The Cpk Metric:** This is your primary KPI. It quantifies the improvement in a single number. A successful CAPA should result in a significant, measurable increase in Cpk.

        **The Strategic Insight:** A CAPA is not complete when the action is done; it is complete when the data proves the action was **effective**. This dashboard provides the objective evidence for that conclusion. The goal is not just to fix the problem, but to demonstrate a quantifiable improvement in process performance and robustness.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: From "Whack-a-Mole" to Permanent Solutions

        #### The Problem: The "Paper Tiger" CAPA System
        A company has a CAPA system that is excellent at paperwork but poor at problem-solving. A deviation occurs, a lengthy investigation is performed, and a corrective action is implemented (e.g., "Retrain operator"). The CAPA is then closed in the system. Management sees a report of "100% of CAPAs closed on time" and assumes the quality system is working perfectly.

        #### The Impact: The "Groundhog Day" of Recurring Failures
        This is a "paper tiger" quality system‚Äîit looks good on paper but has no real teeth.
        - **Recurring Deviations:** Six months later, the exact same deviation occurs. The company is trapped in a costly "Groundhog Day" cycle, investigating and re-investigating the same problems over and over because they never fixed the true root cause.
        - **Wasted Resources:** The engineering and quality teams are in a constant state of firefighting, their time consumed by managing recurring crises instead of working on value-added process improvement.
        - **Severe Audit Findings:** During an audit, an inspector will review the deviation log. When they see the same problem recurring every six months, they will issue a severe finding for having an **ineffective CAPA system**. This is a major red flag that can lead to a Warning Letter.

        #### The Solution: A Formal, Data-Driven "Proof of Effectiveness"
        A robust CAPA system has a mandatory final step: the **Effectiveness Check**. This is a formal, pre-defined plan to gather and analyze data *after* the corrective action has been implemented to prove, with objective evidence, that the problem has been solved and has not recurred. This dashboard simulates that proof. It uses key performance indicators like **Process Capability (Cpk)** to provide a quantitative "before and after" picture.

        #### The Consequences: A Learning Organization and a Bulletproof Audit Trail
        - **Without This:** The CAPA system is a bureaucratic illusion that provides a false sense of security while allowing underlying problems to fester.
        - **With This:** The mandatory effectiveness check transforms the CAPA system from a reactive "fix-it" process into a powerful **organizational learning loop**. It forces the team to confirm that their solutions are working and provides the hard data to justify further investment if they are not. For an auditor, a well-documented effectiveness check is the ultimate proof of a mature, functioning, and compliant quality system.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **CAPA (Corrective and Preventive Action):** A formal process within a Quality Management System (QMS) to investigate and correct non-conformities (Corrective Action) and to prevent their recurrence (Preventive Action).
        - **Root Cause Analysis (RCA):** The investigation performed to identify the underlying cause of the problem. A successful CAPA depends on an accurate RCA.
        - **Effectiveness Check:** A planned, documented, and evidence-based verification that the actions taken as part of a CAPA have successfully addressed the root cause and have not introduced any new, adverse effects.
        - **Process Capability (Cpk):** A key statistical metric used to measure process performance. A significant improvement in Cpk is strong evidence of CAPA effectiveness for a manufacturing process.
        - **Recurrence:** The reappearance of the same non-conformity after a CAPA has been implemented and closed. It is a direct indicator of an ineffective CAPA system.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Trust Me, It's Fixed" Method**
An engineer implements a fix for a process deviation. A manager asks if it worked. The engineer replies, "Yes, I watched the next run and it looked much better." The CAPA is closed.
- **The Flaw:** This is based on anecdote and opinion, not data. There is no objective, quantitative evidence that the process is truly more stable or capable than it was before. This would not withstand an audit.""")
        st.success("""üü¢ **THE GOLDEN RULE: If You Can't Measure It, You Can't Prove It Was Fixed**
A robust CAPA effectiveness check is a formal, data-driven mini-validation.
1.  **Define "Effective" Upfront:** The CAPA plan itself must define what "effective" means in measurable terms. For example: "The CAPA will be considered effective if the Cpk of the process increases to ‚â• 1.33 and the issue does not recur for 90 days."
2.  **Collect the Data:** Gather sufficient data from the process *after* the change has been implemented.
3.  **Analyze and Document:** Perform the statistical analysis (e.g., calculating the new Cpk) and create a formal report with the data, analysis, and a final conclusion. This report becomes the final, critical record in the CAPA file.""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From Military Defects to Modern Quality Systems
        The concept of a formal system for corrective action has its roots in military and quality management standards from the mid-20th century.
        
        **The Problem:** In complex manufacturing (like for military hardware), defects and deviations were common. The initial response was often a simple "fix-and-forget." This led to recurring problems that compromised quality and reliability.
        
        **The 'Aha!' Moment:** Quality pioneers like **W. Edwards Deming** and **Joseph Juran** championed a more systematic approach. The core idea, embodied in Deming's **Plan-Do-Check-Act (PDCA)** cycle, was that fixing a problem is not a single event, but a continuous loop.
        -   **Plan:** Identify the problem and a potential solution.
        -   **Do:** Implement the solution.
        -   **Check:** Gather data to verify if the solution worked. **This is the Effectiveness Check.**
        -   **Act:** If the solution worked, standardize it. If not, go back to the planning stage.
        
        **The Impact:** This simple but powerful idea was formalized into the **Corrective and Preventive Action (CAPA)** systems required by modern quality management standards like ISO 9001 and the GxP regulations for pharmaceuticals and medical devices. The "Effectiveness Check" is the mandatory "Check" step that ensures the quality system is actually learning and improving, not just spinning its wheels.
        """)
        
    with tabs[5]:
        st.markdown("""
        The requirement for a formal CAPA system, including effectiveness checks, is one of the most fundamental and frequently audited parts of any GxP Quality Management System.
        - **FDA 21 CFR 820.100 (Medical Devices):** This regulation explicitly requires manufacturers to establish procedures for CAPA. It mandates "(4) Verifying or validating the corrective and preventive action to ensure that such action is effective and does not adversely affect the finished device."
        - **FDA 21 CFR 211.192 (Pharmaceuticals):** Requires that investigations into discrepancies include "the conclusions and follow-up." A documented effectiveness check is the standard way to prove the follow-up was successful.
        - **ICH Q10 - Pharmaceutical Quality System:** Section 3.2.2 describes the CAPA system as a key element for continual improvement. It requires that the effectiveness of the actions taken should be monitored.
        - **ISO 13485:2016:** The international quality standard for medical devices has nearly identical requirements to 21 CFR 820 regarding the need to verify that corrective actions are effective.
        """)

def render_hfe():
    """Renders the comprehensive module for Usability & Human Factors Engineering."""
    st.markdown("""
    #### Purpose & Application: Designing for the Human Element
    **Purpose:** To systematically analyze and improve the usability of a medical device or system. **Human Factors Engineering (HFE)** is a scientific discipline that focuses on understanding the interactions between humans and a system to optimize for performance, safety, and user satisfaction.
    
    **Strategic Application:** This is a **mandatory design control activity** for most medical devices. It is the primary method for identifying and mitigating risks associated with **"use error"**‚Äîwhen a user, even with the best intentions, makes a mistake due to a confusing interface, poor ergonomics, or unclear instructions. A robust HFE validation file is a critical component of any regulatory submission (e.g., to the FDA).
    """)
    st.info("""
    **Interactive Demo:** You are the HFE Lead for a new diagnostic instrument.
    - Use the sliders to simulate the quality of the device's design and the complexity of the tasks a user must perform.
    - The **System Usability Scale (SUS) Score** provides a standardized, quantitative measure of perceived usability.
    - The **Task Failure Analysis** plot identifies which specific steps in the workflow are the most error-prone, guiding your design improvement efforts.
    """)

    with st.sidebar:
        st.subheader("HFE Simulation Controls")
        design_clarity = st.slider("UI/UX Design Clarity Score", 1, 10, 7, 1, help="A proxy for how intuitive and well-designed the device interface is. A higher score leads to better SUS scores and lower task failure rates.")
        task_complexity = st.slider("Critical Task Complexity", 1, 10, 5, 1, help="How inherently difficult the most critical user tasks are. Higher complexity increases the probability of use error.")

    st.header("Usability Study Results Dashboard")
    fig_tasks, sus_score = plot_hfe_dashboard(design_clarity, task_complexity)
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("System Usability Scale (SUS) Score", f"{sus_score:.1f}")
        if sus_score > 80.3: st.success("Grade: A (Excellent)")
        elif sus_score > 68: st.success("Grade: B (Good)")
        elif sus_score > 51: st.warning("Grade: C (Okay)")
        else: st.error("Grade: F (Unacceptable)")
    with col2:
        st.plotly_chart(fig_tasks, use_container_width=True)
    
    st.divider()
    st.subheader("Deeper Dive into Human Factors Engineering")
    tabs = st.tabs(["üí° Key Insights", "‚úÖ The Business Case", "üìã Glossary", "‚úÖ The Golden Rule", "üìñ Theory & History", "üèõÔ∏è Regulatory & Compliance"])
    
    with tabs[0]:
        st.markdown("""
        **Interpreting the Dashboard:**
        - **System Usability Scale (SUS) Score:** This is a standardized, 10-question survey that provides a quick, reliable, and globally recognized score for a system's perceived usability. A score of **68 is considered average**. A score below 51 indicates serious usability problems.
        - **Task Failure Analysis:** This plot is the primary diagnostic tool from a formative or summative usability study. It shows the observed failure rate for each critical user task. The highest red bars are the most critical "hot spots" in the user interface that must be redesigned to mitigate the risk of use error.

        **The Strategic Insight:** Try setting the **Design Clarity** to a low value and the **Task Complexity** to a high value. This simulates a classic HFE problem: a poorly designed interface for a difficult task. Notice how the SUS score plummets and the task failure rates skyrocket. This demonstrates that usability is not a matter of opinion; it is an emergent property of the design that can be measured, analyzed, and systematically improved.
        """)
        
    with tabs[1]:
        st.markdown("""
        ### The Business Case: The High Cost of a Confusing Device

        #### The Problem: The "Perfectly Engineered, Unusable Device"
        A team of brilliant engineers develops a new medical device that is a technological marvel. It is more precise, faster, and more feature-rich than any competitor. However, the user interface is an afterthought‚Äîa complex maze of confusing menus, ambiguous icons, and an unclear workflow.

        #### The Impact: Market Failure and Patient Harm
        This failure to design for the human element is a catastrophic, and surprisingly common, business and clinical failure.
        - **Patient Safety Crises and Recalls:** A nurse, under pressure in a busy hospital, misinterprets a confusing screen and administers the wrong dose of a drug from the infusion pump. A lab technician, confused by a complex workflow, skips a critical cleaning step on a diagnostic analyzer, leading to a false patient result. **Use error is a leading cause of medical device recalls**, resulting in immense reputational damage, legal liability, and direct patient harm.
        - **Market Rejection:** Even if no safety issues occur, a confusing device will be rejected by the market. Doctors and nurses will refuse to adopt a tool that is frustrating and inefficient to use in their daily workflow. The technologically superior product fails commercially.
        - **Regulatory Rejection:** The FDA and other global regulators now consider HFE a mandatory part of the design process. An inadequate HFE validation file is a common reason for a regulatory submission to be rejected, delaying the product launch by months or years.

        #### The Solution: A Formal, User-Centered Design Process
        Human Factors Engineering (HFE) is the formal, scientific discipline for **making technology work for people**. It is not about making a device look pretty; it is a rigorous, data-driven process of **designing safety and usability in from the start**. It involves:
        1.  **Understanding the User and Environment:** Deeply analyzing the intended users, their capabilities, and the real-world environment where the device will be used.
        2.  **Task Analysis:** Breaking down every critical user task to identify potential failure points.
        3.  **Iterative Testing (Formative Studies):** Building prototypes and testing them with real users early and often to identify and fix usability problems when changes are cheap.
        4.  **Final Validation (Summative Study):** A formal validation study that proves the final device design is safe and effective for its intended use by its intended users.

        #### The Consequences: A Safe, Effective, and Market-Leading Product
        - **Without This:** The product is a high-risk gamble. It is vulnerable to use-error-related recalls and is likely to be rejected by both regulators and the market.
        - **With This:** A robust HFE process is the key to creating a product that is not just technologically advanced, but also **safe, intuitive, and desirable**. It is a direct investment in **reducing patient risk, ensuring regulatory approval, and achieving commercial success**.
        """)
        
    with tabs[2]:
        st.markdown("""
        ##### Glossary of Key Terms
        - **Human Factors Engineering (HFE):** The discipline of applying what is known about human capabilities and limitations to the design of products, processes, systems, and work environments.
        - **Usability:** The extent to which a system can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use.
        - **Use Error:** An action or lack of action by a user that leads to a different result than intended by the manufacturer. It is a failure of the design, not the user.
        - **Formative Usability Study:** An early-stage, exploratory study performed during the design process to "form" and improve the user interface.
        - **Summative (or Validation) Usability Study:** A formal, final validation study performed on the finished device to demonstrate that it is safe and effective for its intended use. This is the study submitted to regulators.
        - **System Usability Scale (SUS):** A widely used, standardized 10-item questionnaire for assessing perceived usability. It is a quick and reliable way to get a quantitative usability score.
        """)
        
    with tabs[3]:
        st.error("""üî¥ **THE INCORRECT APPROACH: The "Let's Ask the Engineers" Fallacy**
The user interface is designed by the same engineers who designed the internal hardware and software. They assume that because the logic is clear to them, it will be clear to a nurse in a busy ICU.
- **The Flaw:** This is the classic "curse of knowledge." The designers are not the users. They have a deep, expert understanding that the end user does not possess. Designing without direct user input is guaranteed to produce a confusing and error-prone interface.""")
        st.success("""üü¢ **THE GOLDEN RULE: The User is Not Like You**
A robust HFE process is built on a foundation of deep empathy for the user and a rigorous, iterative testing process.
1.  **Test Early, Test Often:** Conduct multiple, small-scale **formative** usability studies with representative users throughout the entire design process. It is infinitely cheaper to fix a flaw on a cardboard prototype than on a finished product.
2.  **Simulate Real-World Conditions:** Usability validation studies must be conducted in a high-fidelity simulated environment that replicates the stresses, distractions, and conditions of actual use.
3.  **The Root Cause of Error is the Design:** When a user makes a mistake, the question is never "What's wrong with the user?" but always **"What's wrong with our design that allowed this error to occur?"**""")

    with tabs[4]:
        st.markdown("""
        #### Historical Context: From the Cockpit to the Clinic
        **The Problem:** During World War II, military aircraft were becoming incredibly complex. A shocking number of crashes were not due to enemy fire or mechanical failure, but to **pilot error**. Early investigations revealed that the cockpits were a confusing nightmare of poorly designed and inconsistently placed controls. In one infamous example, the controls for the landing gear and the wing flaps were identical and placed side-by-side, leading to pilots accidentally retracting their landing gear on final approach.

        **The 'Aha!' Moment:** Psychologists and engineers, notably **Alphonse Chapanis**, began to systematically study the "man-machine interface." This was the birth of modern **Human Factors Engineering**. They proved that by designing controls that were shaped differently, logically placed, and aligned with the pilot's mental model, they could dramatically reduce these "use errors" and save lives.
        
        **The Impact:** After the war, these principles spread to other high-risk industries, from nuclear power control rooms to industrial process control. The medical device industry, however, was slow to adopt these principles. A series of high-profile recalls and patient deaths in the 1990s and 2000s, caused by confusing user interfaces on devices like infusion pumps, led the FDA to recognize the critical importance of HFE. They made HFE a central and mandatory part of the **Design Controls** process, transforming it from a "nice-to-have" into a non-negotiable requirement for ensuring patient safety.
        """)
        
    with tabs[5]:
        st.markdown("""
        HFE and Usability Engineering are a mandatory regulatory requirement for most medical devices in the US and Europe.
        - **FDA 21 CFR 820.30 (Design Controls):** The FDA considers HFE to be a core part of the design validation process. The regulation requires that devices are validated to meet **"user needs and intended uses."** A summative HFE study is the primary evidence that this requirement has been met.
        - **FDA Guidance - "Applying Human Factors and Usability Engineering to Medical Devices":** This is the FDA's primary guidance document on the topic. It outlines the agency's expectations for a robust HFE process, including formative studies, risk analysis of use errors, and a summative validation study. The HFE validation report is a required part of a 510(k) or PMA submission.
        - **IEC 62366-1:2015:** This is the international standard for the application of usability engineering to medical devices. Compliance with this standard is often used to demonstrate conformity with regulatory requirements in the EU and other regions.
        """)
#========================================================================================== ENHACEMENTS ==========================================================================================
# SNIPPET 1: This is the complete and correct rendering function for the Validation Plan Wizard.

def render_validation_wizard():
    """Renders the interactive Validation Plan Wizard."""
    st.title("üßô‚Äç‚ôÇÔ∏è Validation Plan Wizard")
    st.markdown("Answer a few simple questions to get a recommended set of validation activities and tools from the Sentinel Toolkit. Click a tool name in the recommendation to navigate directly to it.")
    
    q1 = st.selectbox("1. What is the primary object of your validation?", 
                      ["A new manufacturing process", "A new piece of lab equipment or an instrument", "A new analytical assay or method", "A new software system"])
    
    q2 = st.selectbox("2. What is the primary goal of this activity?",
                      ["Initial characterization and understanding (R&D)", "Formal qualification and validation for GxP use", "Transferring a process/method to a new site/lab", "Routine monitoring of a commercial process"])
    
    st.divider()
    st.subheader("Recommended Validation Plan & Toolkit")
    
    # --- Helper function for creating navigation buttons ---
    def create_nav_button(tool_name):
        if st.button(f"‚ûî {tool_name}"):
            st.session_state.current_view = tool_name
            st.rerun()

    with st.container(border=True):
        if "process" in q1 and "characterization" in q2:
            st.markdown("**Focus:** Process Design & Characterization (FDA PV Stage 1)")
            st.markdown("- **Act 0 (Planning):** Start with a `TPP & CQA Cascade` and a `Quality Risk Management (FMEA)` study.")
            create_nav_button("TPP & CQA Cascade")
            create_nav_button("Quality Risk Management (QRM) Suite")
            st.markdown("- **Act I (Characterization):** Use `Assay Robustness (DOE)` and `Process Optimization: From DOE to AI` to build a deep understanding and define a Design Space.")
            create_nav_button("Process Optimization: From DOE to AI")
            st.markdown("- **Act II (Early Capability):** Perform initial runs and assess stability with `Process Stability (SPC)` and `Process Capability (Cpk)`.")
            create_nav_button("Process Capability (Cpk)")

        elif "process" in q1 and "qualification" in q2:
            st.markdown("**Focus:** Process Performance Qualification (PPQ) (FDA PV Stage 2)")
            st.markdown("- **Act 0 (Planning):** Finalize the `Validation Master Plan (VMP)`. Use `Sample Size for Qualification` to justify your sampling plan.")
            create_nav_button("Validation Master Plan (VMP) Builder")
            create_nav_button("Sample Size for Qualification")
            st.markdown("- **Act II (Execution & Analysis):** Execute the PPQ runs. Analyze the results for `Process Stability (SPC)` and `Process Capability (Cpk)`. Use `Tolerance Intervals` to set release specifications.")
            create_nav_button("Process Stability (SPC)")
            create_nav_button("Process Capability (Cpk)")
            create_nav_button("Tolerance Intervals")

        elif "instrument" in q1 and "qualification" in q2:
            st.markdown("**Focus:** Instrument IQ/OQ/PQ")
            st.markdown("- **Act 0 (Planning):** Define the `Analytical Target Profile (ATP)` (serves as URS). Perform a `Quality Risk Management (FMEA)` on the instrument. Execute `FAT & SAT`.")
            create_nav_button("Analytical Target Profile (ATP) Builder")
            create_nav_button("FAT & SAT")
            st.markdown("- **Act I (Characterization):** Assess the instrument's measurement system with `Gage R&R / VCA`.")
            create_nav_button("Gage R&R / VCA")
            st.markdown("- **Act II (Qualification):** Execute the formal IQ/OQ/PQ protocols. Analyze PQ data using `Process Stability (SPC)`.")
            create_nav_button("Process Stability (SPC)")
        else:
            st.markdown("Select a combination to see a recommended plan.")
#================================================================================ CASE STUDY LIBRARY ========================================================================================================
def render_case_study_library():
    """Renders the Case Study Library hub page."""
    st.title("üìö Case Study Library")
    st.markdown("Explore end-to-end V&V workflows through these realistic case studies. Selecting a case will provide a step-by-step guided tour through the relevant tools in the Sentinel.")
    st.divider()

    if 'case_study' not in st.session_state:
        st.session_state.case_study = {"active_case": None, "current_step": 0}

    # --- Case Selection Area ---
    case_key = st.selectbox(
        "Select a Case Study to Begin:",
        options=list(CASE_STUDIES.keys()),
        format_func=lambda k: CASE_STUDIES[k]['title'],
        index=None,
        placeholder="Choose a scenario..."
    )
    if st.button("üöÄ Start Selected Case Study", disabled=not case_key, type="primary"):
        st.session_state.case_study['active_case'] = case_key
        st.session_state.case_study['current_step'] = 0
        st.rerun()

    # --- Active Case Display Area ---
    active_case_key = st.session_state.case_study.get('active_case')
    if active_case_key:
        case = CASE_STUDIES[active_case_key]
        st.header(case['title'])
        st.markdown(f"**Scenario:** {case['description']}")
        
        if st.button("‚Ü©Ô∏è End Case Study & Return to Library"):
            st.session_state.case_study = {"active_case": None, "current_step": 0}
            st.rerun()
            
        st.subheader("Project Timeline & Toolkit")
        for i, step in enumerate(case['steps']):
            with st.container(border=True):
                col1, col2 = st.columns([0.8, 0.2])
                with col1:
                    st.markdown(f"##### {i+1}. {step['title']} `({step['act']})`")
                    st.markdown(step['explanation'])
                with col2:
                    st.markdown("<br>", unsafe_allow_html=True)
                    if st.button(f"Go to Step {i+1} ‚Üí", key=f"goto_{active_case_key}_{i}", use_container_width=True):
                        st.session_state.case_study['current_step'] = i
                        st.session_state.current_view = step['target_tool']
                        st.rerun()


#=========================================================================================== DOC CONTROL =======================================================================================================
def render_doc_control():
    """Renders the comprehensive, interactive module for Document Control & Training Management."""
    st.markdown("""
    #### Purpose & Application: The QMS Operations Simulator
    **Purpose:** To simulate the complex, interconnected workflows of a real GxP Quality Management System (QMS). This interactive dashboard models how **Change Control**, **Document Control**, **Training Management**, and **Re-Validation** are all inextricably linked.
    
    **Strategic Application:** This is a training and process-mapping tool for understanding the "gears" of a compliant QMS. It demonstrates how a single change request can trigger a cascade of activities, consuming resources and impacting operational readiness.
    """)
    st.info("""
    **Interactive Demo:** You are the **Quality System Owner**. Your goal is to implement a high-risk change to the manufacturing process.
    1.  Start at the **Change Control Board** and `Create` a new Change Request (CR) for the Process SOP.
    2.  Use the **User Actions** to move the prerequisite documents (`CAL-002`, `TM-101`) through their approval lifecycle first.
    3.  Once the prerequisites are `Effective`, approve the high-risk Process SOP (`SOP-001`). Notice that this triggers a **Re-Validation Requirement**.
    4.  "Execute" the validation and then make the SOP `Effective`. This will trigger new **Training Requirements**.
    5.  "Train" the operators and watch the **Production Readiness Dashboard** and **Time & Cost Tracker** update in real-time.
    """)

    # --- Session State Initialization for the Advanced Simulation ---
    if 'qms_sim' not in st.session_state:
        st.session_state.qms_sim = {
            "docs": {
                "SOP-001 Rev B (Process)": {"status": "Draft", "base": "SOP-001", "prereq": ["TM-101", "CAL-002"]},
                "TM-101 Rev C (Test Method)": {"status": "Draft", "base": "TM-101", "prereq": []},
                "CAL-002 Rev B (Instrument)": {"status": "Draft", "base": "CAL-002", "prereq": []},
            },
            "training": {
                "Alice": {"SOP-001 Rev A": "Complete", "TM-101 Rev B": "Complete", "CAL-002 Rev A": "Complete"},
                "Bob": {"SOP-001 Rev A": "Complete", "TM-101 Rev B": "Complete", "CAL-002 Rev A": "Complete"},
                "Charlie": {"SOP-001 Rev A": "Complete", "TM-101 Rev B": "Complete", "CAL-002 Rev A": "Complete"},
            },
            "change_requests": {},
            "audit_trail": ["- [INIT] QMS Simulation Initialized."],
            "effective_revs": {"SOP-001": "A", "TM-101": "B", "CAL-002": "A"},
            "cost": 0,
            "time": 0,
            "validation_needed": False
        }
    sim = st.session_state.qms_sim

    # --- Main Layout ---
    col1, col2 = st.columns([0.6, 0.4])

    with col1:
        st.subheader("Change Control Board")
        cr_doc = st.selectbox("Select Document for New Change Request (CR)", list(sim['docs'].keys()))
        cr_risk = st.radio("Change Risk Level", ["Low (Typo, Clarification)", "Medium (Parameter Change)", "High (New Equipment/Process)"], horizontal=True, index=2)
        
        if st.button("üìù Create Change Request", disabled=(cr_doc in sim['change_requests'])):
            sim['change_requests'][cr_doc] = {'risk': cr_risk, 'status': 'Open'}
            sim['audit_trail'].append(f"- [CHANGE] CR opened for {cr_doc} (Risk: {cr_risk.split(' ')[0]}).")
            sim['time'] += 4
            sim['cost'] += 500
            st.rerun()

        st.subheader("Document Control Workflow")
        for doc, props in sim['docs'].items():
            status_colors = {"Draft": "grey", "In Review": "orange", "Approved": "blue", "Effective": "green", "Blocked": "red"}
            status = props['status']
            
            # More robust prerequisite check
            prereqs_met = True
            for prereq_base in props.get('prereq', []):
                effective_rev_num = sim['effective_revs'].get(prereq_base)
                if not effective_rev_num:
                    prereqs_met = False
                    break
                prereq_doc_full_name = next((d for d in sim['docs'] if d.startswith(f"{prereq_base} Rev {effective_rev_num}")), None)
                if not prereq_doc_full_name or sim['docs'].get(prereq_doc_full_name, {}).get('status') != "Effective":
                    prereqs_met = False
                    break
            
            if status == "Approved" and not prereqs_met:
                status = "Blocked"

            st.markdown(f"**{doc}**: <span style='color:{status_colors.get(status, 'grey')}; font-weight:bold;'>{status}</span>", unsafe_allow_html=True)
            
            action_cols = st.columns(5)
            cr_open = doc in sim['change_requests']
            
            if props['status'] == "Draft":
                if action_cols[0].button("Submit for Review", key=f"submit_{doc}", use_container_width=True, disabled=not cr_open):
                    props['status'] = "In Review"; sim['audit_trail'].append(f"- [AUTHOR] {doc} submitted."); sim['time'] += 8; sim['cost'] += 1000; st.rerun()
            elif props['status'] == "In Review":
                if action_cols[0].button("Approve (QA)", key=f"approve_{doc}", use_container_width=True, disabled=not cr_open):
                    props['status'] = "Approved"; sim['audit_trail'].append(f"- [QA] {doc} approved."); sim['time'] += 16; sim['cost'] += 2000
                    if sim['change_requests'][doc]['risk'] == "High (New Equipment/Process)":
                        sim['validation_needed'] = True
                        sim['audit_trail'].append(f"- [SYSTEM] High-risk change on {doc} triggers re-validation requirement.")
                    st.rerun()
                if action_cols[1].button("Reject", key=f"reject_{doc}", use_container_width=True, disabled=not cr_open):
                    props['status'] = "Draft"; sim['audit_trail'].append(f"- [REVIEWER] {doc} rejected."); sim['time'] += 8; sim['cost'] += 1000; st.rerun()

            if status == "Approved" and prereqs_met and not (sim.get('validation_needed', False) and "Process" in doc):
                if action_cols[2].button("Make Effective", key=f"effective_{doc}", type="primary", use_container_width=True, disabled=not cr_open):
                    props['status'] = "Effective"
                    sim['audit_trail'].append(f"- [QA] {doc} is now Effective.")
                    sim['time'] += 4
                    sim['cost'] += 500
                    
                    # --- THIS IS THE ROBUST FIX FOR THE KEYERROR ---
                    doc_base = props.get('base')
                    if doc_base and ' Rev ' in doc:
                        try:
                            rev = doc.split(' Rev ')[1].split(' ')[0]
                            sim['effective_revs'][doc_base] = rev
                            # Trigger retraining for all users
                            for user in sim['training']:
                                sim['training'][user][doc] = "Required"
                            sim['change_requests'][doc]['status'] = "Closed"
                        except IndexError:
                            st.error(f"Could not parse revision from document name: {doc}")
                    else:
                        st.error(f"Error: Document '{doc}' is missing its 'base' key or ' Rev ' naming. Cannot make effective.")
                    # --- END OF FIX ---
                    st.rerun()

            st.caption(f"Prerequisites: {props.get('prereq', []) if props.get('prereq') else 'None'}")
            st.markdown("---")

        if sim.get('validation_needed'):
            st.warning("**Re-Validation Required for High-Risk Change!**")
            if st.button("Execute Re-Validation Protocol (PQ)"):
                sim['validation_needed'] = False
                sim['audit_trail'].append("- [VALIDATION] PQ protocol executed successfully.")
                sim['time'] += 160; sim['cost'] += 25000
                st.rerun()

    with col2:
        st.subheader("Time & Cost Tracker")
        kpi_cols = st.columns(2)
        kpi_cols[0].metric("Man-Hours Consumed", f"{sim['time']} hrs")
        kpi_cols[1].metric("Total Change Cost", f"${sim['cost']:,}")

        st.subheader("Training Matrix (Current Effective Revs)")
        effective_docs = [f"{doc_base} Rev {rev}" for doc_base, rev in sim['effective_revs'].items()]
        display_data = {}
        for user, trainings in sim['training'].items():
            user_status = {}
            for doc in effective_docs:
                # If a new rev exists that isn't trained, it's required.
                new_rev_name = next((d for d in sim['docs'] if d.startswith(doc.split(' Rev ')[0]) and d != doc), None)
                if new_rev_name and sim['training'][user].get(new_rev_name) == 'Required':
                    user_status[doc] = 'Superseded'
                    user_status[new_rev_name] = 'Required'
                elif trainings.get(doc) == 'Complete':
                     user_status[doc] = 'Complete'
            display_data[user] = user_status
        
        flat_data = []
        all_doc_revs = sorted(list(set(doc for user_data in display_data.values() for doc in user_data.keys())))
        for user, user_data in display_data.items():
            row = {'Operator': user}
            for doc_rev in all_doc_revs:
                row[doc_rev] = user_data.get(doc_rev, 'N/A')
            flat_data.append(row)
        
        if flat_data:
            df_display = pd.DataFrame(flat_data).set_index('Operator')
            st.dataframe(df_display.style.applymap(
                lambda val: 'background-color: #FFCDD2' if val == 'Required' else ('background-color: #C8E6C9' if val == 'Complete' else 'background-color: #E0E0E0' if val=='Superseded' else ''))
            )
        
        st.subheader("Production Readiness Dashboard")
        all_ready = True
        for user in sim['training']:
            is_ready = all(
                sim['training'][user].get(f"{base} Rev {rev}") == "Complete"
                for base, rev in sim['effective_revs'].items()
            )
            if is_ready:
                st.success(f"**Operator {user}:** ‚úÖ Ready for Production")
            else:
                st.error(f"**Operator {user}:** ‚ùå BLOCKED (Training Overdue)")
                all_ready = False
        if all_ready:
            st.info("All operators are fully trained on all effective documents.")

        st.subheader("Live Audit Trail")
        st.code("\n".join(sim['audit_trail'][-5:]), language="markdown")
#====================================================================================  A U D I T ==================================================================================================
def render_audit_readiness():
    """Renders the Audit Readiness & Inspection Management module."""
    st.title("üïµÔ∏è Audit Readiness & Inspection Management")
    st.markdown("This module helps prepare for a regulatory inspection by simulating common audit questions and mapping them to the evidence provided by the V&V Sentinel Toolkit.")
    
    col1, col2 = st.columns([0.4, 0.6])
    with col1:
        st.subheader("Self-Assessment Score")
        scores = {
            'QMS & Documentation': st.slider("QMS & Documentation", 0, 10, 8, key="audit_qms", 
                                             help="How robust and well-maintained is your overall Quality Management System? Are documents controlled, training records up-to-date, and CAPAs effective?"),
            'Design Controls & DHF': st.slider("Design Controls & DHF", 0, 10, 7, key="audit_dhf", 
                                                help="Is your Design History File complete and traceable? Can you link every user requirement to its verification and validation test?"),
            'Process Validation': st.slider("Process Validation", 0, 10, 9, key="audit_pv", 
                                            help="Is your process validation package complete and scientifically sound? Does it cover all three stages (Design, PPQ, CPV) with robust data?"),
            'Method Validation': st.slider("Method Validation", 0, 10, 6, key="audit_mv", 
                                           help="Are all analytical methods used for GxP decisions fully validated per ICH Q2? Is data available for accuracy, precision, specificity, etc.?"),
            'Data Integrity': st.slider("Data Integrity", 0, 10, 8, key="audit_di", 
                                        help="How strong are your data integrity controls? Does your system meet ALCOA+ principles and 21 CFR Part 11 requirements for audit trails and e-signatures?")
        }
        st.plotly_chart(plot_audit_readiness_spider(scores), use_container_width=True)
        
    with col2:
        st.subheader("Mock Audit Simulators")

        # --- NEW TABBED GADGET STRUCTURE ---
        tab1, tab2 = st.tabs(["Auditor Q&A Drill", "Evidence Package Builder"])

        with tab1:
            st.info("Practice responding to specific, challenging auditor questions one by one.")
            # --- THIS IS THE ORIGINAL Q&A GADGET, RESTORED AS REQUESTED ---
            audit_questions = {
                "Design Controls & QMS (ISO 13485, 21 CFR 820)": {
                    "Show me your Design History File for this device. I want to trace a user need from your URS all the way to its validation test case.": "Design Controls & DHF",
                    "A deviation occurred 6 months ago. Show me the CAPA, the root cause investigation, and the data proving the corrective action was effective.": "CAPA Effectiveness Checker",
                    "How can you prove that every user requirement was tested? Show me the traceability matrix.": "Requirements Traceability Matrix (RTM)",
                    "Walk me through your change control process for a critical software patch, including your impact assessment and regression testing strategy.": "Gap Analysis & Change Control",
                    "This device is for point-of-care use. How did you validate the design against potential use errors by the intended user population? Show me the HFE report.": "Usability & Human Factors Engineering (HFE)",
                },
                "Risk Management (ISO 14971)": {
                    "Your validation plan focuses heavily on certain parameters. Show me the formal risk assessment (FMEA) that justifies this focus and shows how you prioritized risks.": "Quality Risk Management (QRM) Suite",
                    "How did you assess the reliability of this critical electronic component and its impact on the overall system risk profile?": "Component Reliability Testing",
                    "A contamination event was traced to a cleanroom pressure failure. Show me the top-down fault tree analysis you performed to find the system-level vulnerabilities.": "Root Cause Analysis (RCA)",
                    "You are using a novel AI model. How have you assessed the risks associated with model drift or incorrect predictions from this non-deterministic system?": "Explainable AI (XAI)",
                },
                "Process Validation & Control (Pharma, Production Line)": {
                    "Show me the evidence that your process consistently meets quality targets.": "Process Capability (Cpk)",
                    "How do you demonstrate comparability after a site transfer?": "Equivalence Testing (TOST)",
                    "Show me the evidence that your process was in a state of statistical control *before* you performed the capability analysis for your PPQ.": "Process Stability (SPC)",
                    "Your PPQ report claims a Cpk of 1.8. Show me the data and calculations supporting this claim of high capability.": "Process Capability (Cpk)",
                    "You claim the process at Site B is equivalent to Site A. Show me the formal statistical equivalence test and the pre-defined margin for that claim, not just a t-test.": "Equivalence Testing (TOST)",
                    "This process has been running for two years. How do you monitor for small, gradual drifts that a standard Shewhart chart would miss?": "Small Shift Detection",
                    "You have multiple correlated parameters on this bioreactor. How do you monitor the holistic health of the process, not just individual variables?": "Multivariate SPC",
                    "You scaled up from a 200L to a 2000L bioreactor. Show me the comparability protocol and the statistical evidence proving the process performance is equivalent.": "Statistical Equivalence for Process Transfer",
                    "How did you establish the Design Space for this process? Show me the DOE and the statistical model supporting it.": "Process Optimization: From DOE to AI",
                    "You've identified temperature as a CPP. Show me the Control Plan for this parameter and the Out-of-Control Action Plan for an operator.": "Process Control Plan Builder",
                    "How did you leverage your Factory Acceptance Test (FAT) to reduce the scope of your on-site Operational Qualification (OQ)?": "FAT & SAT",
                },
                "Method & Assay Validation (IVD, GAMP)": {
                    "How do you know your measurement system is reliable?": "Gage R&R / VCA",
                    "How did you determine the clinical cutoff for this diagnostic? Show me the ROC curve analysis and the justification for balancing sensitivity and specificity.": "ROC Curve Analysis",
                    "What is the validated sensitivity of your impurity assay? Show me the data supporting your claimed Limit of Quantitation.": "LOD & LOQ",
                    "How did you prove this method is accurate across its full range, not just at a single control point? I want to see the residual analysis.": "Linearity & Range",
                    "Your new analytical method was transferred from R&D. Show me the formal study that proves it agrees with the original reference method.": "Method Comparison",
                    "You have a non-linear bioassay. How did you validate the 4PL curve-fitting model and its weighting scheme?": "Non-Linear Regression (4PL/5PL)",
                    "Your stability protocol pools data from three batches. Show me the ANCOVA results that justify this pooling decision as per ICH Q1E.": "Stability Analysis (Shelf-Life)",
                },
                "Planning & Justification": {
                    "How did you justify the sampling plan for your PPQ?": "Sample Size for Qualification",
                    "Walk me through the 'golden thread' from your Target Product Profile down to the specific process parameters you control on the line.": "TPP & CQA Cascade",
                    "Your PPQ protocol specifies 59 samples. Show me the statistical justification for this number, including your assumptions for confidence and reliability.": "Sample Size for Qualification",
                },
                "Lifecycle & Advanced Topics": {
                    "A deviation occurred 6 months ago on this process. Show me the data proving your CAPA was effective and resulted in a sustained improvement.": "CAPA Effectiveness Checker",
                    "How did you establish the expiration date for this product? Show me the statistical analysis, including the test for data poolability across batches.": "Stability Analysis (Shelf-Life)",
                    "Your new component supplier was qualified based on reliability testing. Show me the survival analysis and the predicted B10 life.": "Reliability / Survival Analysis",
                    "You're using an AI model for batch release. How do you know it's not a 'black box'? Show me how you validated its reasoning.": "Explainable AI (XAI)",
                }
            }
        
            q_category = st.selectbox("Select Audit Category:", list(audit_questions.keys()), key="q_cat")
            auditor_question = st.selectbox("Select a Question:", list(audit_questions[q_category].keys()), key="q_select")
        
            with st.container(border=True):
                st.write(f"**Auditor:** '{auditor_question}'")
                st.markdown("---")
                st.write("**Your Response:** 'The objective evidence for that is generated using the following tool from our V&V toolkit...'")

                all_tools_flat = [tool for act in all_tools.values() for tool in act]
                tool_options = sorted(list(set(all_tools_flat)))
                
                correct_answer = audit_questions[q_category][auditor_question]
                
                try:
                    default_index = tool_options.index(correct_answer)
                except ValueError:
                    default_index = 0

                user_choice = st.selectbox("Select the correct tool to provide as evidence:", tool_options, index=default_index, key="q_choice")
                
                if st.button("Submit Response", key="q_submit"):
                    if user_choice == correct_answer:
                        st.success(f"**Correct!** The `{user_choice}` tool provides the direct, objective evidence to answer this question.")
                    else:
                        st.error(f"**Incorrect.** While related, the best evidence comes from the `{correct_answer}` tool. The `{user_choice}` tool is used for a different purpose.")

        with tab2:
            st.info("""
            **Scenario:** An auditor has requested the complete validation package for the recent technology transfer of 'Product X' to a new manufacturing site.
            
            **Your Task:** Select all the necessary documents and analyses from the list below to create a complete and defensible evidence package.
            """)
            
            EVIDENCE_LIBRARY = {
                "Validation Master Plan (VMP) Builder": "The high-level strategy and plan for the entire transfer validation.",
                "Quality Risk Management (QRM) Suite": "The FMEA identifying risks specific to the new site and process.",
                "Process Stability (SPC)": "Proof that the process was in a state of statistical control at the new site.",
                "Process Capability (Cpk)": "Proof that the new process is capable of meeting specifications.",
                "Statistical Equivalence for Process Transfer": "The formal statistical proof that the new process is equivalent to the original.",
                "Gage R&R / VCA": "Qualification of the key measurement systems at the new site.",
                "CAPA Effectiveness Checker": "Evidence that any deviations during the transfer were effectively resolved.",
                "Component Reliability Testing": "Reliability data for any new components introduced at the new site.",
                "Linearity & Range": "Linearity study for an assay; not typically a primary tech transfer document.",
                "Exploratory Data Analysis (EDA)": "Exploratory data; not a formal validation document for an audit."
            }
            
            REQUIRED_EVIDENCE_SCENARIO = [
                "Validation Master Plan (VMP) Builder",
                "Quality Risk Management (QRM) Suite",
                "Process Stability (SPC)",
                "Process Capability (Cpk)",
                "Statistical Equivalence for Process Transfer",
                "Gage R&R / VCA"
            ]

            user_selection = st.multiselect(
                "Select the documents and analyses to include in your submission package:",
                options=list(EVIDENCE_LIBRARY.keys()),
                format_func=lambda x: f"{x}: {EVIDENCE_LIBRARY[x]}"
            )
            
            if st.button("Submit Package for Review", type="primary", use_container_width=True, key="pkg_submit"):
                st.subheader("Auditor's Feedback on Your Package")
                
                required_set = set(REQUIRED_EVIDENCE_SCENARIO)
                selected_set = set(user_selection)
                
                correctly_included = required_set.intersection(selected_set)
                missing_docs = required_set - selected_set
                unnecessary_docs = selected_set - required_set
                
                score = len(correctly_included) / (len(required_set) + len(unnecessary_docs)) if (len(required_set) + len(unnecessary_docs)) > 0 else 0

                st.metric("Package Completeness Score", f"{score:.0%}", help="Calculated as: Correctly Included / (Total Required + Unnecessarily Included). Penalizes both missing and extraneous documents.")

                if missing_docs:
                    st.error("**Critical Gaps Found!** The following required evidence is missing:")
                    for doc in sorted(list(missing_docs)):
                        st.markdown(f"- **{doc}:** {EVIDENCE_LIBRARY[doc]}")
                
                if unnecessary_docs:
                    st.warning("**Unnecessary Documents Included:** While not a critical failure, providing irrelevant documents can confuse the narrative and invite unnecessary questions. The following are not required for this specific request:")
                    for doc in sorted(list(unnecessary_docs)):
                        st.markdown(f"- **{doc}**")

                if not missing_docs and not unnecessary_docs:
                    st.success("**Excellent!** Your evidence package is complete, concise, and fully addresses the auditor's request. This demonstrates a mature and well-organized quality system.")
                elif not missing_docs and unnecessary_docs:
                    st.info("**Package is Compliant but Not Lean:** You've included all required evidence, but also extra documents. Aim for a more focused submission next time.")
        # --- END OF GADGET REPLACEMENT ---

@st.cache_data
def plot_audit_readiness_spider(scores):
    """Generates a spider chart for audit readiness scores."""
    categories = list(scores.keys())
    fig = go.Figure()
    fig.add_trace(go.Scatterpolar(
        r=list(scores.values()), 
        theta=categories, 
        fill='toself', 
        name='Current Readiness', 
        line=dict(color=PRIMARY_COLOR)
    ))
    fig.add_trace(go.Scatterpolar(
        r=[10]*len(categories), 
        theta=categories, 
        name='Target State', 
        line=dict(color='grey', dash='dot')
    ))
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True, 
                range=[0, 10]
            )
        ), 
        showlegend=False, 
        title="<b>Audit Readiness Score</b>",
        margin=dict(l=40, r=40, t=60, b=40)
    )
    return fig

#============================================================================================= CASE STUDIES =====================================================================================================
# SNIPPET 1: Add this entire data dictionary to your app.py file, e.g., after the CSS section.

CASE_STUDIES = {
    "mab_transfer": {
        "title": "Case Study: Tech Transfer of a Monoclonal Antibody",
        "description": "Follow the journey of transferring a validated biopharmaceutical process for 'BioMab' from an R&D facility to a new commercial manufacturing site. This case focuses on proving statistical equivalence and ensuring process control.",
        "steps": [
            {
                "act": "Act 0", "title": "Initial Risk Assessment", "target_tool": "Quality Risk Management (QRM) Suite",
                "explanation": "Before the transfer, we conduct an FMEA to identify the highest-risk failure modes. We've identified that the chromatography step is critical and highly sensitive to operational differences between sites.",
                "params": {"project_type": "Pharma Process (MAb)", "tool_choice": "FMEA"}
            },
            {
                "act": "Act II", "title": "Baseline Performance at Site A", "target_tool": "Process Capability (Cpk)",
                "explanation": "We need a baseline of our 'golden' process. We analyze data from the last 30 batches at the R&D site (Site A) to confirm it is stable and highly capable (Cpk > 1.67). This is our benchmark.",
                "params": {"scenario": "Ideal (High Cpk)"}
            },
            {
                "act": "Act II", "title": "PPQ at the New Site B", "target_tool": "Process Capability (Cpk)",
                "explanation": "The first three PPQ batches at the new site (Site B) are complete. The process is stable, but the data shows a slight upward shift and increased variability compared to Site A. While still capable (Cpk > 1.33), is it equivalent?",
                "params": {"scenario": "Shifted (Low Cpk)"}
            },
            {
                "act": "Act II", "title": "Statistical Equivalence Verdict", "target_tool": "Statistical Equivalence for Process Transfer",
                "explanation": "This is the final exam. We use a formal equivalence test on the Cpk values from both sites. The 90% confidence interval for the difference must fall entirely within our pre-defined margin of ¬±0.20 Cpk units.",
                "params": {"cpk_site_a": 1.67, "mean_shift": 1.2, "var_change_factor": 1.3, "n_samples": 100, "margin": 0.20}
            }
        ]
    },
    "ivd_dev": {
        "title": "Case Study: Validation of a New Point-of-Care IVD",
        "description": "Experience the development and validation journey for 'CancerDetect,' a new Class II, software-driven point-of-care diagnostic device for an early-stage cancer biomarker.",
        "steps": [
            {
                "act": "Act 0", "title": "Determine Regulatory Pathway", "target_tool": "IVD & Medical Device Regulatory Framework",
                "explanation": "Our first step is strategic. As a moderate-risk device with a similar predicate on the market, we determine the 510(k) pathway is the most appropriate for our 'CancerDetect' POC device.",
                "params": {"product_concept": "Point-of-Care (POC) Device"}
            },
            {
                "act": "Act 0", "title": "Define the 'Contract'", "target_tool": "Analytical Target Profile (ATP) Builder",
                "explanation": "We create the ATP, our formal 'contract' for the device's performance. The team agrees we need at least 98% clinical sensitivity and 99% specificity to be commercially viable.",
                "params": {"project_type": "IVD Kit (ELISA)", "atp_values": [98.0, 99.0, 15.0, 7, 18], "show_results": False}
            },
            {
                "act": "Act I", "title": "Assay Characterization (ROC)", "target_tool": "ROC Curve Analysis",
                "explanation": "Early R&D data is used to generate an ROC curve. The excellent AUC of 0.978 gives us confidence that our chosen reagents can meet the ATP targets. We select an initial cutoff that prioritizes high sensitivity.",
                "params": {"diseased_mean": 75.0, "population_sd": 10.0, "cutoff": 58}
            },
            {
                "act": "Act I", "title": "Usability Validation (HFE)", "target_tool": "Usability & Human Factors Engineering (HFE)",
                "explanation": "A summative usability study is conducted with 30 nurses. The results show a high SUS score, but the Task Failure Analysis reveals a critical issue: users are frequently making errors during the 'Run Sample' step. This use error must be mitigated via a design change before the 510(k) submission.",
                "params": {"design_clarity": 6, "task_complexity": 8}
            }
        ]
    },
    "ai_lifecycle": {
        "title": "Case Study: AI Model Lifecycle Management",
        "description": "Deploy and manage an AI model used for Predictive QC in a commercial manufacturing process. This case study covers model validation, real-time monitoring, and proactive control.",
        "steps": [
            {
                "act": "Act I", "title": "Model Development & Selection", "target_tool": "Predictive Modeling Suite",
                "explanation": "We need to predict batch failures based on two in-process parameters. The relationship is non-linear, so a simple Logistic Regression fails (low AUC). A well-tuned MLP Neural Network provides the best predictive performance.",
                "params": {"boundary_radius": 8, "mlp_params": {'layers': (64, 32), 'activation': 'relu', 'learning_rate': 0.001}}
            },
            {
                "act": "Act III", "title": "Model Validation (XAI)", "target_tool": "Explainable AI (XAI)",
                "explanation": "Before deploying, we use XAI to validate the model's logic. The SHAP plots confirm it's using scientifically valid features (like 'Reagent Age') and not spurious correlations, satisfying a key GMLP requirement.",
                "params": {"case_to_explain": "highest_risk", "dependence_feature": "Reagent Age (Days)"}
            },
            {
                "act": "Act III", "title": "Real-Time Monitoring (Digital Twin)", "target_tool": "Digital Twin & Real-Time Simulation",
                "explanation": "The validated AI model is deployed as a Digital Twin. It monitors the live process, and when a fault is injected at Time=50, the twin's forecast diverges, and the Health Score immediately alarms.",
                "params": {"fault_type": "Shift", "fault_magnitude": 6.0, "fault_time": 50}
            },
            {
                "act": "Act III", "title": "Proactive Control (MPC)", "target_tool": "Model Predictive Control (MPC)",
                "explanation": "The final evolution. The Digital Twin is now used as the engine for an MPC system. The MPC uses the twin's predictions to act *proactively*, resulting in much tighter control and stability compared to a reactive system.",
                "params": {"disturbance_size": 5.0, "control_aggressiveness": 0.6}
            }
        ]
    }
}

# ==============================================================================
# MAIN APP LOGIC AND LAYOUT
# ==============================================================================

# --- Initialize Session State (Must be the first Streamlit command) ---
if 'current_view' not in st.session_state:
    st.session_state.current_view = 'Introduction'
if 'case_study' not in st.session_state:
    st.session_state.case_study = {"active_case": None, "current_step": 0}

# --- DEFINE THE APP'S STRUCTURE (Global Constants) ---
all_tools = {
    "ACT 0: PLANNING & STRATEGY": [
        "TPP & CQA Cascade", "Analytical Target Profile (ATP) Builder", "IVD & Medical Device Regulatory Framework",
        "V&V Strategy & Justification", "Validation Master Plan (VMP) Builder", "Design Controls & DHF",
        "Design for Excellence (DfX)", "FAT & SAT", "Requirements Traceability Matrix (RTM)",
        "Quality Risk Management (QRM) Suite", "Root Cause Analysis (RCA)", "Gap Analysis & Change Control",
        "CAPA Effectiveness Checker"
    ],
    "ACT I: FOUNDATION & CHARACTERIZATION": [
        "Exploratory Data Analysis (EDA)", "Confidence Interval Concept", "Confidence Intervals for Proportions",
        "Bayesian Inference", "Core Validation Parameters", "LOD & LOQ", "Linearity & Range",
        "Non-Linear Regression (4PL/5PL)", "Gage R&R / VCA", "Attribute Agreement Analysis",
        "Comprehensive Diagnostic Validation", "ROC Curve Analysis", "Component Reliability Testing",
        "Usability & Human Factors Engineering (HFE)", "Assay Robustness (DOE)", "Mixture Design (Formulations)",
        "Split-Plot Designs", "Process Optimization: From DOE to AI", "Bayesian Optimization",
        "Causal Inference", "Causal ML / Double ML"
    ],
    "ACT II: TRANSFER & STABILITY": [
        "Sample Size for Qualification", "Process Stability (SPC)", "Process Capability (Cpk)",
        "Tolerance Intervals", "Method Comparison", "Equivalence Testing (TOST)",
        "Non-Parametric Statistics Workbench", "Wasserstein Distance", "Two-Process Comparability Suite",
        "Multi-Process Comparability Suite", "Analytical Comparability & Biosimilarity Dashboard",
        "Statistical Equivalence for Process Transfer", "Advanced Stability Design",
        "First Time Yield & Cost of Quality", "Lean Manufacturing & VSM", "Production Line Sync (ODE)",
        "Monte Carlo Simulation for Risk Analysis"
    ],
    "ACT III: LIFECYCLE & PREDICTIVE MGMT": [
        "Overall Equipment Effectiveness (OEE)", "Process Control Plan Builder", "Run Validation (Westgard)",
        "Small Shift Detection", "Multivariate SPC", "Stability Analysis (Shelf-Life)",
        "Reliability / Survival Analysis", "Time Series Forecasting Suite", "Prophet Forecasting",
        "Multivariate Analysis (MVA)", "Predictive Modeling Suite", "Explainable AI (XAI)",
        "Clustering (Unsupervised)", "Anomaly Detection", "MEWMA + XGBoost Diagnostics",
        "BOCPD + ML Features", "Kalman Filter + Residual Chart", "TCN + CUSUM",
        "LSTM Autoencoder + Hybrid Monitoring", "PSO + Autoencoder", "RL for Chart Tuning",
        "Digital Twin & Real-Time Simulation", "Model Predictive Control (MPC)",
        "Real-Time Release Testing (RTRT) Dashboard", "Advanced AI Concepts"
    ]
}

PAGE_DISPATCHER = {
    # New Top-Level Pages
    "Validation Plan Wizard": render_validation_wizard, "Case Study Library": render_case_study_library,
    "Document Control & Training Sim": render_doc_control, "Audit Readiness Sim": render_audit_readiness,
    "Search": render_search_page,
    # Act 0
    "TPP & CQA Cascade": render_tpp_cqa_cascade, "Analytical Target Profile (ATP) Builder": render_atp_builder,
    "IVD & Medical Device Regulatory Framework": render_ivd_regulatory_framework, "Quality Risk Management (QRM) Suite": render_qrm_suite,
    "V&V Strategy & Justification": render_vv_strategy_justification, "Design Controls & DHF": render_design_controls_dhf, 
    "FAT & SAT": render_fat_sat, "Design for Excellence (DfX)": render_dfx_dashboard,
    "Validation Master Plan (VMP) Builder": render_vmp_builder, "Requirements Traceability Matrix (RTM)": render_rtm_builder,
    "Gap Analysis & Change Control": render_gap_analysis_change_control, "Root Cause Analysis (RCA)": render_rca_suite,
    "CAPA Effectiveness Checker": render_capa_effectiveness,
    # Act I
    "Exploratory Data Analysis (EDA)": render_eda_dashboard, "Confidence Interval Concept": render_ci_concept,
    "Confidence Intervals for Proportions": render_proportion_cis, "Core Validation Parameters": render_core_validation_params,
    "LOD & LOQ": render_lod_loq, "Linearity & Range": render_linearity, "Non-Linear Regression (4PL/5PL)": render_4pl_regression,
    "Gage R&R / VCA": render_gage_rr, "Attribute Agreement Analysis": render_attribute_agreement,
    "Comprehensive Diagnostic Validation": render_diagnostic_validation_suite, "Component Reliability Testing": render_component_reliability,
    "ROC Curve Analysis": render_roc_curve, "Usability & Human Factors Engineering (HFE)": render_hfe,
    "Assay Robustness (DOE)": render_assay_robustness_doe, "Mixture Design (Formulations)": render_mixture_design,
    "Process Optimization: From DOE to AI": render_process_optimization_suite, "Bayesian Optimization": render_bayesian_optimization,
    "Split-Plot Designs": render_split_plot, "Causal Inference": render_causal_inference, "Causal ML / Double ML": render_causal_ml,
    # Act II
    "Sample Size for Qualification": render_sample_size_calculator, "Process Stability (SPC)": render_spc_charts,
    "Process Capability (Cpk)": render_capability, "Tolerance Intervals": render_tolerance_intervals,
    "Method Comparison": render_method_comparison, "Equivalence Testing (TOST)": render_tost,
    "Non-Parametric Statistics Workbench": render_nonparametric_workbench, "Wasserstein Distance": render_wasserstein_distance,
    "Two-Process Comparability Suite": render_two_process_suite, "Multi-Process Comparability Suite": render_multi_process_suite,
    "Analytical Comparability & Biosimilarity Dashboard": render_biosimilarity, "Statistical Equivalence for Process Transfer": render_statistical_equivalence_for_process_transfer,
    "Advanced Stability Design": render_stability_design, "First Time Yield & Cost of Quality": render_fty_coq,
    "Lean Manufacturing & VSM": render_lean_manufacturing, "Production Line Sync (ODE)": render_ode_line_sync,
    "Monte Carlo Simulation for Risk Analysis": render_monte_carlo_simulation, "Bayesian Inference": render_bayesian,
    # Act III
    "Overall Equipment Effectiveness (OEE)": render_oee, "Process Control Plan Builder": render_control_plan_builder,
    "Run Validation (Westgard)": render_multi_rule, "Small Shift Detection": render_ewma_cusum,
    "Multivariate SPC": render_multivariate_spc, "Stability Analysis (Shelf-Life)": render_stability_analysis,
    "Reliability / Survival Analysis": render_survival_analysis, "Time Series Forecasting Suite": render_time_series_suite,
    "Prophet Forecasting": render_prophet_forecasting, "Multivariate Analysis (MVA)": render_mva_pls,
    "Predictive Modeling Suite": render_predictive_modeling_suite, "Explainable AI (XAI)": render_xai_shap,
    "Clustering (Unsupervised)": render_clustering, "Anomaly Detection": render_anomaly_detection,
    "MEWMA + XGBoost Diagnostics": render_mewma_xgboost, "BOCPD + ML Features": render_bocpd_ml_features,
    "Kalman Filter + Residual Chart": render_kalman_nn_residual, "TCN + CUSUM": render_tcn_cusum,
    "LSTM Autoencoder + Hybrid Monitoring": render_lstm_autoencoder_monitoring, "PSO + Autoencoder": render_pso_autoencoder,
    "RL for Chart Tuning": render_rl_tuning, "Digital Twin & Real-Time Simulation": render_digital_twin,
    "Model Predictive Control (MPC)": render_mpc, "Real-Time Release Testing (RTRT) Dashboard": render_rtrt,
    "Advanced AI Concepts": render_advanced_ai_concepts,
}

# --- SIDEBAR NAVIGATION RENDERING (Consolidated and Corrected) ---
with st.sidebar:
    st.title("üß∞ Toolkit Navigation")
    
    if st.button("üöÄ Project Framework", use_container_width=True, key="nav_intro"):
        st.session_state.current_view = 'Introduction'
        if 'case_study' in st.session_state: st.session_state.case_study['active_case'] = None
        st.rerun()

    if st.button("üîé Search Toolkit", use_container_width=True, key="nav_search"):
        st.session_state.current_view = 'Search'
        st.rerun()
    
    st.divider()
    st.subheader("GUIDES & SIMULATORS")

    if st.session_state.get('case_study', {}).get('active_case'):
        if st.button("üìö Return to Case Study Hub", use_container_width=True, type="primary", key="nav_case_hub_return"):
            st.session_state.current_view = 'Case Study Library'
            st.rerun()
    else:
        if st.button("üìö Case Study Library", use_container_width=True, key="nav_case_hub_main"):
            st.session_state.current_view = 'Case Study Library'
            st.rerun()
    
    if st.button("üßô‚Äç‚ôÇÔ∏è Validation Plan Wizard", use_container_width=True, key="nav_wizard"):
        st.session_state.current_view = 'Validation Plan Wizard'
        st.rerun()
    if st.button("üìë Document Control & Training Sim", use_container_width=True, key="nav_doc_control"):
        st.session_state.current_view = 'Document Control & Training Sim'
        st.rerun()
    if st.button("üïµÔ∏è Audit Readiness Sim", use_container_width=True, key="nav_audit"):
        st.session_state.current_view = 'Audit Readiness Sim'
        st.rerun()
            
    st.divider()

    for act_title, act_tools in all_tools.items():
        st.subheader(act_title)
        for tool in act_tools:
            if st.button(tool, key=tool, use_container_width=True):
                st.session_state.current_view = tool
                if 'case_study' in st.session_state: st.session_state.case_study['active_case'] = None
                st.rerun()
    
# --- MAIN CONTENT AREA DISPATCHER ---
view = st.session_state.get('current_view', 'Introduction')
render_function = PAGE_DISPATCHER.get(view, render_introduction_content)
render_function()
